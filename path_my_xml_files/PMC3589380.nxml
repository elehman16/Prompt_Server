<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23472216</article-id><article-id pub-id-type="pmc">3589380</article-id><article-id pub-id-type="publisher-id">PONE-D-12-34029</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0058632</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>fMRI</subject></subj-group></subj-group><subj-group><subject>Sensory Systems</subject><subj-group><subject>Auditory System</subject><subject>Visual System</subject></subj-group></subj-group><subj-group><subject>Cognitive Neuroscience</subject><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Engineering</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Image Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Medicine</subject><subj-group><subject>Mental Health</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and Behavioral Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Beyond Motor Scheme: A Supramodal Distributed Representation in the Action-Observation Network</article-title><alt-title alt-title-type="running-head">Modality-Independent Representation of Actions</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ricciardi</surname><given-names>Emiliano</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Handjaras</surname><given-names>Giacomo</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Bonino</surname><given-names>Daniela</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Vecchi</surname><given-names>Tomaso</given-names></name><xref ref-type="aff" rid="aff3">
<sup>3</sup>
</xref><xref ref-type="aff" rid="aff4">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Fadiga</surname><given-names>Luciano</given-names></name><xref ref-type="aff" rid="aff5">
<sup>5</sup>
</xref><xref ref-type="aff" rid="aff6">
<sup>6</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Pietrini</surname><given-names>Pietro</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="aff7">
<sup>7</sup>
</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<addr-line>Laboratory of Clinical Biochemistry and Molecular Biology, Dept. of Surgery, Medical, Molecular, and Critical Area Pathology, University of Pisa, Pisa, Italy</addr-line>
</aff><aff id="aff2">
<label>2</label>
<addr-line>MRI Lab, Fondazione &#x0201c;G. Monasterio&#x0201d; Regione Toscana/C.N.R., Pisa, Italy</addr-line>
</aff><aff id="aff3">
<label>3</label>
<addr-line>Dept. of Brain and Behavioral Sciences, University of Pavia, Pavia, Italy</addr-line>
</aff><aff id="aff4">
<label>4</label>
<addr-line>Brain Connectivity Center, IRCCS Mondino, Pavia, Italy</addr-line>
</aff><aff id="aff5">
<label>5</label>
<addr-line>Section of Human Physiology, University of Ferrara, Ferrara, Italy</addr-line>
</aff><aff id="aff6">
<label>6</label>
<addr-line>RBCS Department, Italian Institute of Technology, Genova, Italy</addr-line>
</aff><aff id="aff7">
<label>7</label>
<addr-line>Clinical Psychology Branch, Pisa University Hospital, Pisa, Italy</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Di Russo</surname><given-names>Francesco</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Rome, Italy</addr-line>
</aff><author-notes><corresp id="cor1">* E-mail: <email>emiliano.ricciardi@bioclinica.unipi.it</email></corresp><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p>Reviewed the results: DB TV LF PP. Integrated and approved the final version of the manuscript: DB TV LF. Conceived and designed the experiments: ER GH DB TV LF PP. Performed the experiments: ER DB PP. Analyzed the data: ER GH. Contributed reagents/materials/analysis tools: ER GH. Wrote the paper: ER GH PP.</p></fn></author-notes><pub-date pub-type="collection"><year>2013</year></pub-date><pub-date pub-type="epub"><day>5</day><month>3</month><year>2013</year></pub-date><volume>8</volume><issue>3</issue><elocation-id>e58632</elocation-id><history><date date-type="received"><day>3</day><month>11</month><year>2012</year></date><date date-type="accepted"><day>5</day><month>2</month><year>2013</year></date></history><permissions><copyright-statement>&#x000a9; 2013 Ricciardi et al</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Ricciardi et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>The representation of actions within the action-observation network is thought to rely on a distributed functional organization. Furthermore, recent findings indicate that the action-observation network encodes not merely the observed motor act, but rather a representation that is independent from a specific sensory modality or sensory experience. In the present study, we wished to determine to what extent this distributed and &#x02018;more abstract&#x02019; representation of action is truly supramodal, i.e. shares a common coding across sensory modalities. To this aim, a pattern recognition approach was employed to analyze neural responses in sighted and congenitally blind subjects during visual and/or auditory presentation of hand-made actions. Multivoxel pattern analyses-based classifiers discriminated action from non-action stimuli across sensory conditions (visual and auditory) and experimental groups (blind and sighted). Moreover, these classifiers labeled as &#x02018;action&#x02019; the pattern of neural responses evoked during actual motor execution. Interestingly, discriminative information for the action/non action classification was located in a bilateral, but left-prevalent, network that strongly overlaps with brain regions known to form the action-observation network and the human mirror system. The ability to identify action features with a multivoxel pattern analyses-based classifier in both sighted and blind individuals and independently from the sensory modality conveying the stimuli clearly supports the hypothesis of a supramodal, distributed functional representation of actions, mainly within the action-observation network.</p></abstract><funding-group><funding-statement>This work was supported by European Union Grants 'THE: The Hand Embodied' to E.R. and P.P. and &#x02018;Poeticon++&#x02019; to L.F., by the Italian Ministero dell'Istruzione, dell'Universit&#x000e0; e della Ricerca grant to P.P., by the Young Investigator Award of the University of Pisa-Faculty of Medicine to E.R. and by the Strategic Project Region-University to L.F. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>The ability to understand others&#x02019; actions and intentions from distinct sensory clues is central for daily social interactions. The human mirror system (hMS), as part of a broader action-observation network (AON <xref rid="pone.0058632-Kilner1" ref-type="bibr">[1]</xref>, <xref rid="pone.0058632-Molenberghs1" ref-type="bibr">[2]</xref>), plays a major role in this function <xref rid="pone.0058632-Fadiga1" ref-type="bibr">[3]</xref>, <xref rid="pone.0058632-Rizzolatti1" ref-type="bibr">[4]</xref>, <xref rid="pone.0058632-Rizzolatti2" ref-type="bibr">[5]</xref>. The hMS is activated both when individuals perform a goal-directed action and when they observe another individual performing the same action. For this reason, the hMS is considered able to transform sensory information into (motor) knowledge and, through this mechanism, to mediate understanding of motor acts from others <xref rid="pone.0058632-FabbriDestro1" ref-type="bibr">[6]</xref>, <xref rid="pone.0058632-Gallese1" ref-type="bibr">[7]</xref>.</p><p>Recent studies have proposed that the representation of actions within the premotor, inferior frontal, parietal and temporal regions of the AON may be based on a distributed and overlapping functional organization <xref rid="pone.0058632-Oosterhof1" ref-type="bibr">[8]</xref>, <xref rid="pone.0058632-Dinstein1" ref-type="bibr">[9]</xref>, similarly to what has already been described for the representation of objects and sounds in other cortical areas (e.g. <xref rid="pone.0058632-Haxby1" ref-type="bibr">[10]</xref>, <xref rid="pone.0058632-Staeren1" ref-type="bibr">[11]</xref>, <xref rid="pone.0058632-Pietrini1" ref-type="bibr">[12]</xref>). Distributed brain responses in specific subregions of the action-responsive fronto-parietal network can be used to discriminate the content, the effectors, or even the behavioral significance of different motor acts, when actions are either observed or performed, or even covertly imagined <xref rid="pone.0058632-Oosterhof1" ref-type="bibr">[8]</xref>, <xref rid="pone.0058632-Dinstein1" ref-type="bibr">[9]</xref>, <xref rid="pone.0058632-Jastorff1" ref-type="bibr">[13]</xref>, <xref rid="pone.0058632-Oosterhof2" ref-type="bibr">[14]</xref>, <xref rid="pone.0058632-Oosterhof3" ref-type="bibr">[15]</xref>, <xref rid="pone.0058632-Molenberghs2" ref-type="bibr">[16]</xref>. Therefore, various subregions within the AON are differentially recruited to control or define specific aspects of motor acts, and the overall response of the AON network seems to contribute to the cross-modal visuo-motor coding of distinct actions <xref rid="pone.0058632-Jastorff1" ref-type="bibr">[13]</xref>, <xref rid="pone.0058632-Cattaneo1" ref-type="bibr">[17]</xref>.</p><p>However, it is still unknown whether this distributed representation of actions is shared across sensory modalities. How do we mentally represent &#x02018;hammering&#x02019; when just listening to the strikes on a nail, or &#x02018;knocking&#x02019; when recognizing the hitting on a door? Though originally both in monkeys and humans the mirror system was thought to rely on visuomotor features, a neural response within the mirror areas has been demonstrated also when simply hearing the sound of an action <xref rid="pone.0058632-Etzel1" ref-type="bibr">[18]</xref>, <xref rid="pone.0058632-Galati1" ref-type="bibr">[19]</xref>, <xref rid="pone.0058632-Gazzola1" ref-type="bibr">[20]</xref>, <xref rid="pone.0058632-Keysers1" ref-type="bibr">[21]</xref>, <xref rid="pone.0058632-Kohler1" ref-type="bibr">[22]</xref>, <xref rid="pone.0058632-Lewis1" ref-type="bibr">[23]</xref>, <xref rid="pone.0058632-Lewis2" ref-type="bibr">[24]</xref>. Furthermore, individuals that had no previous visual experience still retain the ability to learn actions and behaviors from others. To this purpose, we previously showed that congenitally blind individuals activate a premotor-temporo-parietal cortical network in response to aurally presented actions that overlaps both with hMS areas found in sighted subjects in response to visually and aurally presented stimuli, and with the brain response elicited by motor pantomime of the same actions <xref rid="pone.0058632-Ricciardi1" ref-type="bibr">[25]</xref>. Altogether, these findings indicate that the hMS, as part of the AON, codes not merely the observed motor act, but rather a more abstract representation that is independent from a specific sensory modality or experience <xref rid="pone.0058632-Cattaneo1" ref-type="bibr">[17]</xref>, <xref rid="pone.0058632-Cattaneo2" ref-type="bibr">[26]</xref>, <xref rid="pone.0058632-Ricciardi2" ref-type="bibr">[27]</xref>, <xref rid="pone.0058632-Kupers1" ref-type="bibr">[28]</xref>.</p><p>Nonetheless, whether the more abstract representation of action is truly <italic>supramodal</italic>, i.e. shares a common coding across sensory modalities, is still unknown. To this aim, we used a pattern recognition approach to analyze neural responses in sighted and congenitally blind individuals during visual and/or auditory perception of a set of hand-made actions, and during the actual motor pantomime of manipulation tasks <xref rid="pone.0058632-Ricciardi1" ref-type="bibr">[25]</xref>.</p><p>Specifically, we used a pattern-classification approach (multi-voxel pattern analysis - MVPA) to decode the information that is represented in a spatially distributed pattern of activity, and to identify as well those brain regions that significantly contribute to the discrimination <xref rid="pone.0058632-Pereira1" ref-type="bibr">[29]</xref>, <xref rid="pone.0058632-Poldrack1" ref-type="bibr">[30]</xref>. We first expected that an MVPA would be able to distinguish between the neural patterns associated with auditory and visual stimuli of actions and non-actions using distributed patterns of response in both sighted and blind individuals. Then, we posited that, because of the hypothesized <italic>supramodal</italic> nature of action representation, an MVPA would be able to classify action and non-action stimuli across the visual and auditory modalities and across the sighted and blind groups, and to recognize as an &#x02018;action&#x02019; the neural patterns associated with actual motor performances.</p></sec><sec sec-type="materials|methods" id="s2"><title>Materials and Methods</title><p>As described in greater details in the original report of the present dataset <xref rid="pone.0058632-Ricciardi1" ref-type="bibr">[25]</xref>, we used a functional magnetic resonance imaging (fMRI) sparse sampling six-run block design to examine neural activity in congenitally blind and sighted healthy volunteers while they alternated between auditory presentation of hand-executed actions or non-action environmental sounds, and execution of a &#x02018;virtual&#x02019; tool or object manipulation task (motor pantomime). In the sighted group, three additional runs were acquired during a visual version of an identical task of motor pantomime and presentation of action or environmental movies.</p><sec id="s2a"><title>Subjects</title><p>Eight blind (six female, mean age &#x000b1; S.D.: 44 &#x000b1; 16 years) - seven with congenital blindness (causes of blindness: congenital glaucoma, retinopathy of prematurity, and congenital optic nerve atrophy) and one who became completely blind at age 2 years due to congenital glaucoma and had no recollection of any visual experience - and 14 sighted (five female, 32 &#x000b1; 13 years) right-handed healthy individuals were recruited for the study. All subjects received a medical examination, including routine blood tests and a brain structural MRI scan to exclude any disorder that could affect brain function and metabolism, other than blindness in the blind group.</p></sec><sec id="s2b"><title>Ethics Statement</title><p>All participants gave their written informed consent after the study procedures and potential risks had been explained. The study was conducted under a protocol approved by the University of Pisa Ethical Committee (protocol n. 1616/2003), and was developed in accordance with the Protocol of Helsinki (2008).</p></sec><sec id="s2c"><title>Auditory Stimuli</title><p>Twenty action and ten environmental sound samples [44.1 Hz, 16 bit quantization, stereo, Free Sound Project, average mean square power and duration normalized] were presented by a MR-compatible pneumatic headphone system (PureSound Audio System Wardray Premise). Speech commands for the motor pantomime task were digitally recorded names of objects/tools to be virtually handled, and a beep sound after 10 s signaled the subject to stop executing the action. Both sounds and speech commands lasted for 10 s.</p></sec><sec id="s2d"><title>Visual Stimuli</title><p>Ten second long movies of action and environmental scenes were presented on a rear projection screen viewed through a mirror (visual field: 25&#x000b0; wide and 20&#x000b0; high). Motor commands were triggered by words. Each action showed images of the action being performed by the right hand of an actor viewed from a third person perspective.</p></sec><sec id="s2e"><title>Image Acquisition and Experimental Task</title><p>Gradient echo echoplanar (GRE-EPI) images were acquired with a 1.5 Tesla scanner (Signa General Electric). A scan cycle was composed of 5-mm-thick 21 axial slices (FOV&#x0200a;=&#x0200a;24 cm, TE&#x0200a;=&#x0200a;40 ms, FA&#x0200a;=&#x0200a;90, 128&#x000d7;128 pixels) collected in 2,500 ms followed by a silent gap of 2,500 ms (sparse sampling). We obtained six time series of 65 brain volumes while each subject listened to sounds, and three time series while the sighted volunteers only looked at movies. Stimuli were randomly presented with an interstimulus interval of 5 s. Each time series began and ended with 15 s of no stimuli.</p><p>During the auditory scanning sessions, volunteers were asked to listen to and recognize sounds while keeping their eyes closed, and also to execute the motor pantomimes when randomly prompted by a human voice command naming a specific tool. During the visual sessions, volunteers were asked to look at movies, and to execute the motor pantomimes when prompted by words. Sensory modality (auditory or visual) was constant for each time series, but auditory and visual runs were alternated in randomized order across sighted subjects. Fifteen stimuli were presented in each time series (equally distributed across stimulus classes) and randomly intermixed with five target pantomime commands. Stimulus presentation was handled by using the software package Presentation&#x000ae; (<ext-link ext-link-type="uri" xlink:href="http://www.neurobs.com">http://www.neurobs.com</ext-link>). High-resolution T1-weighted spoiled gradient recall images were obtained for each subject to provide detailed brain anatomy.</p></sec><sec id="s2f"><title>MultiVoxel Pattern Analysis</title><p>We used the AFNI package (<ext-link ext-link-type="uri" xlink:href="http://afni.nimh.nih.gov/afni">http://afni.nimh.nih.gov/afni</ext-link> - <xref rid="pone.0058632-Cox1" ref-type="bibr">[31]</xref> and related software plug-ins for data preprocessing and the BrainVISA/Anatomist package (<ext-link ext-link-type="uri" xlink:href="http://brainvisa.info">http://brainvisa.info</ext-link>) for visual rendering of functional imaging data. After standard preprocessing (the different runs were concatenated, co-registered, temporally aligned, and spatially smoothed with an isotropic Gaussian filter, &#x003c3;&#x0200a;=&#x0200a;2.5 mm) <xref rid="pone.0058632-Ricciardi1" ref-type="bibr">[25]</xref>, the BOLD response magnitude to each stimulus was modeled with a separate regressor in a deconvolution analysis, and calculated by averaging the &#x003b2;-weights of the second, third and fourth volumes of each gamma impulsive response. The decision to use &#x003b2; estimates instead of the less noisy response-amplitude estimate t-values <xref rid="pone.0058632-Misaki1" ref-type="bibr">[32]</xref> was based on our intention to test classification performances across different stimulus categories whose &#x003b2; and standard error estimates may diverge, and thus result to be more sensitive to differences. The response patterns of each stimulus were transformed into the Talairach and Tournoux Atlas <xref rid="pone.0058632-Tailairach1" ref-type="bibr">[33]</xref>, and resampled into isometric 2 mm voxels for group analysis. A template included in the AFNI distribution (the &#x02018;Colin&#x02019; brain <xref rid="pone.0058632-Holmes1" ref-type="bibr">[34]</xref> was used to select cortical voxels.</p><p>Furthermore, all the response magnitudes to each stimulus were scaled from &#x02212;1 to +1, using a hyperbolic tangent, to generate input vectors for the Support Vector Machine (SVM) classifiers <xref rid="pone.0058632-Schlkopf1" ref-type="bibr">[35]</xref>, <xref rid="pone.0058632-Cortes1" ref-type="bibr">[36]</xref>. The software &#x02018;SVMlight&#x02019; <xref rid="pone.0058632-Joachims1" ref-type="bibr">[37]</xref> was used to implement the SVM classifiers. Linear SVM classifiers were trained with a small, fixed, data-driven regularization parameter (see <xref rid="pone.0058632-Joachims1" ref-type="bibr">[37]</xref> for further details), to avoid overfitting the data during the training phase (i.e. soft margin SVM).</p><p>Three distinct linear binary classifiers were built in order to separate the patterns of neural response to action and non-action stimuli across the different experimental conditions, that is in blind (sounds only) and sighted (sounds and videos) individuals. Due to the imbalanced numbers of stimuli across the experimental conditions in the sighted group, non-action sounds (n&#x0200a;=&#x0200a;10) and videos (n&#x0200a;=&#x0200a;11) were randomly upsampled (doubled) to match the size of the action sound set (n&#x0200a;=&#x0200a;20), while action videos (n&#x0200a;=&#x0200a;23) were randomly downsampled to match the number of action sounds. This choice of combining up-/down-sampling technique with linear SVM has been already considered robust and effective on the predictive performance of learned classifier <xref rid="pone.0058632-Japkowicz1" ref-type="bibr">[38]</xref>. Accordingly, the resulting matrix to be classified was made of 560 examples across all sighted individuals (280 non-action stimuli and 280 action stimuli, as resulting from 20 action and non-action stimuli for 14 subjects) for each classifier, and 320 examples (160 non action stimuli and 160 action stimuli, as resulting from 20 stimuli for 8 subjects) across congenitally blind individuals. In addition, we included 280 examples of motor pantomimes from sighted subjects (140 during the auditory sessions, 140 during the visual sessions for 14 subjects) and 80 examples of motor pantomimes from blind subjects (80 during the auditory sessions, for 8 subjects) for testing the capability of our SVM classifiers to identify the motor gestures as &#x02018;actions&#x02019;, in accordance with the &#x02018;mirror&#x02019; rationale of a brain network responding both to action recognition and action real performance. On a general basis, decoding techniques require a high number of examples/stimuli (action and environmental sounds) as compared to the usually high dimensionality of the feature/voxel space <xref rid="pone.0058632-Mitchell1" ref-type="bibr">[39]</xref>. The relative limited number of stimuli in our experiment does not allow here to combine a whole brain approach (taking into account all the distributed information across the cortex) and a single subject decoding. Thus, the whole dataset of examples was used for an across-subjects classification. This procedure strongly relies on commonalities across the individual patterns of responses in accordance with the aims of the analysis to evaluate action/non action representations across the visual and auditory modalities and across the sighted and blind individuals.</p><p>In order to select only those voxels strongly related to the discrimination, we built a procedure combining 4-fold nested cross-validation - NCV <xref rid="pone.0058632-Pereira1" ref-type="bibr">[29]</xref> and a Recursive Feature Elimination algorithm &#x02013; RFE <xref rid="pone.0058632-DeMartino1" ref-type="bibr">[40]</xref> to recursively prune irrelevant voxels based on their discrimination ability, and to avoid overfitting in model selection <xref rid="pone.0058632-Cawley1" ref-type="bibr">[41]</xref>. According to the NCV procedure, the stimuli were first divided in four subsets and each fold was tested using one subset after being trained on the other ones. Each RFE iteration of each fold consisted of several steps and generated a specific feature set. Initially, the classifiers were trained with the examples assigned to the fold. The mean of all feature weights of the support vectors was estimated during training. Further, the absolute values of the weight vectors were calculated, and the 2% of the features with the lowest weights were then discarded. A cluster correction with an arbitrary minimum cluster size of 150 voxels (1,200 &#x000b5;L), nearest-neighbor, was performed to remove small, isolated clusters and to reduce the total numbers of iterations. Finally, a discriminative map was obtained by mapping the weights vector onto the Talairach and Tournoux Atlas. This procedure was iterated until all the features/voxels were discarded. For each iteration, an accuracy performance was computed on the testing set of the fold. Then, comparing the accuracies from all folds and RFE iterations, the best feature sets for the three classifiers of the same fold were selected based on their highest mean accuracy <xref rid="pone.0058632-Pereira1" ref-type="bibr">[29]</xref>, <xref rid="pone.0058632-DeMartino1" ref-type="bibr">[40]</xref>, <xref rid="pone.0058632-Cawley1" ref-type="bibr">[41]</xref>.</p><p>Potential drawbacks of the application of SVM and RFE subsist both in the choice of the number of voxels to be discarded at each iteration, and in the presence of outliers in the data sample that could lead to suboptimal selection of voxels <xref rid="pone.0058632-Guyon1" ref-type="bibr">[42]</xref>, <xref rid="pone.0058632-Guyon2" ref-type="bibr">[43]</xref>. To mitigate such possibilities, we used a computational expensive RFE algorithm with a relative low number of discarded voxels, and a normalization of data to diminish the role of outliers, respectively. Moreover, this procedure that combine RFE and 4-fold NCV, generated 4 above chance classifiers that rely on different sets of features/voxels across the three experimental conditions (blind - sounds only - and sighted - sounds and videos-). The accuracy of the best fold and the mean accuracy (&#x000b1;S.D.) of all folds were reported. All subsequent analyses, as described below, were indeed limited to the three classifiers (and their features/voxels) of the best fold.</p><p>When the three best classifiers were extracted with the RFE algorithm (action vs. non-action stimuli in blind - sounds only - and sighted - sounds and videos - subjects), our classifiers were tested with the motor pantomime examples to confirm the capability to identify the motor execution of virtual gestures as &#x02018;actions&#x02019;. In addition, to prove the hypothesis of a more abstract representation of action features, an evaluation across stimulus categories and experimental groups was performed.</p><p>To examine the degree of overlap in information across the different sensory modalities/groups and to identify those brain areas contributing to the supramodal representation of actions, using the RFE procedure, we built a common &#x02018;supramodal&#x02019; SVM classifier using the training data from all stimulus classes of the best fold. Moreover, the discriminative map of this supramodal classifier was employed in a &#x02018;knock-out&#x02019; procedure <xref rid="pone.0058632-Carlson1" ref-type="bibr">[44]</xref>. First, we created a mask defining the discriminative voxels of the supramodal classifier - &#x02018;knock-out&#x02019; mask. This &#x02018;knock-out&#x02019; voxels were then removed from the three best discriminative maps and the potential changes (reductions) in the accuracy of our three classifiers were determined. Subsequently, restricting our volume of interest to this &#x02018;knock-out&#x02019; map only, we built again three distinct linear SVM classifiers to separate action vs. non-action stimuli and to estimate the potential changes (increases) in classification performances related to this set of voxels within and across experimental conditions.</p><p>The classifier accuracy values were tested as significantly different from chance with a permutation test (n&#x0200a;=&#x0200a;500), randomly changing the labels of examples during the training phase, to avoid biased performance evaluation related to the oversampled non-action stimuli and the different nature of the experimental stimuli (i.e. sounds, videos, and motor pantomime) <xref rid="pone.0058632-Pereira1" ref-type="bibr">[29]</xref>. Furthermore, differences in within group accuracy estimates of the original SVM classifier when considering the whole discrimination map (<xref ref-type="table" rid="pone-0058632-t001">Table 1</xref>) vs. when excluding the knock-out voxels (<xref ref-type="table" rid="pone-0058632-t002">Table 2</xref>-B) were assessed separately with a non-parametric Wilcoxon signed-rank test <xref rid="pone.0058632-Pereira1" ref-type="bibr">[29]</xref>. These differences were then aggregated across comparisons with the Fisher&#x02019;s method <xref rid="pone.0058632-Lowry1" ref-type="bibr">[45]</xref>.</p><table-wrap id="pone-0058632-t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0058632.t001</object-id><label>Table 1</label><caption><title>Accuracy of each SVM classifiers in a within- and across-experimental condition evaluation.</title></caption><alternatives><graphic id="pone-0058632-t001-1" xlink:href="pone.0058632.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td colspan="2" align="left" rowspan="1"/><td colspan="2" align="left" rowspan="1">Sighted</td><td align="left" rowspan="1" colspan="1">Blind</td></tr><tr><td colspan="2" align="left" rowspan="1"/><td align="left" rowspan="1" colspan="1">SVM classifier trainedon visual stimuli</td><td align="left" rowspan="1" colspan="1">SVM classifier trained on auditory stimuli</td><td align="left" rowspan="1" colspan="1">SVM classifier trained on auditory stimuli</td></tr></thead><tbody><tr><td colspan="5" align="left" rowspan="1">
<italic>A. Whole brain</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Sighted</td><td align="left" rowspan="1" colspan="1">Visual</td><td align="left" rowspan="1" colspan="1">80.7%<xref ref-type="table-fn" rid="nt101">***</xref>
</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Auditory</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">75.7%<xref ref-type="table-fn" rid="nt101">***</xref>
</td><td align="left" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" rowspan="1" colspan="1">Blind</td><td align="left" rowspan="1" colspan="1">Auditory</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">76.2%<xref ref-type="table-fn" rid="nt101">***</xref>
</td></tr><tr><td colspan="5" align="left" rowspan="1">
<italic>B. After excluding the knock-out map</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Sighted</td><td align="left" rowspan="1" colspan="1">Visual</td><td align="left" rowspan="1" colspan="1">77.1%<xref ref-type="table-fn" rid="nt101">***</xref>
</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Auditory</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">74.3%<xref ref-type="table-fn" rid="nt101">***</xref>
</td><td align="left" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" rowspan="1" colspan="1">Blind</td><td align="left" rowspan="1" colspan="1">Auditory</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">n.s.</td><td align="left" rowspan="1" colspan="1">73.7%<xref ref-type="table-fn" rid="nt101">***</xref>
</td></tr><tr><td colspan="5" align="left" rowspan="1">
<italic>C. By restricting to the knock-out supramodal map</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Sighted</td><td align="left" rowspan="1" colspan="1">Visual</td><td align="left" rowspan="1" colspan="1">73.6%<xref ref-type="table-fn" rid="nt101">***</xref>
</td><td align="left" rowspan="1" colspan="1">61.1%<xref ref-type="table-fn" rid="nt102">**</xref>
</td><td align="left" rowspan="1" colspan="1">n.s.</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Auditory</td><td align="left" rowspan="1" colspan="1">59.6%<xref ref-type="table-fn" rid="nt103">*</xref>
</td><td align="left" rowspan="1" colspan="1">67.1%<xref ref-type="table-fn" rid="nt101">***</xref>
</td><td align="left" rowspan="1" colspan="1">57.7%<xref ref-type="table-fn" rid="nt103">*</xref>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Blind</td><td align="left" rowspan="1" colspan="1">Auditory</td><td align="left" rowspan="1" colspan="1">60.3%<xref ref-type="table-fn" rid="nt102">**</xref>
</td><td align="left" rowspan="1" colspan="1">61.5%<xref ref-type="table-fn" rid="nt102">**</xref>
</td><td align="left" rowspan="1" colspan="1">70.0%<xref ref-type="table-fn" rid="nt101">***</xref>
</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt101"><label>***</label><p>p-value &#x0003c; 0.005,</p></fn><fn id="nt102"><label>**</label><p>p-value &#x0003c; 0.01,</p></fn><fn id="nt103"><label>*</label><p>p-value &#x0003c; 0.05 at permutation test.</p></fn></table-wrap-foot></table-wrap><table-wrap id="pone-0058632-t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0058632.t002</object-id><label>Table 2</label><caption><title>Brain regions obtained with a &#x0201c;knock out&#x0201d; procedure to examine the degree of overlap in information between the representations of different experimental conditions/groups.</title></caption><alternatives><graphic id="pone-0058632-t002-2" xlink:href="pone.0058632.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Brain areas</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td colspan="3" align="left" rowspan="1">Coordinates</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Hem</td><td align="left" rowspan="1" colspan="1">BA</td><td align="left" rowspan="1" colspan="1">x</td><td align="left" rowspan="1" colspan="1">y</td><td align="left" rowspan="1" colspan="1">z</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Superior Frontal</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">63</td><td align="left" rowspan="1" colspan="1">&#x02212;2</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">&#x02212;7</td><td align="left" rowspan="1" colspan="1">&#x02212;1</td><td align="left" rowspan="1" colspan="1">64</td></tr><tr><td align="left" rowspan="1" colspan="1">Middle Frontal</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">25</td><td align="left" rowspan="1" colspan="1">&#x02212;3</td><td align="left" rowspan="1" colspan="1">58</td></tr><tr><td align="left" rowspan="1" colspan="1">Inferior Frontal</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">44</td><td align="left" rowspan="1" colspan="1">45</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">28</td></tr><tr><td align="left" rowspan="1" colspan="1">Anterior Cingulate</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">24</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">20</td></tr><tr><td align="left" rowspan="1" colspan="1">Postcentral</td><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">&#x02212;27</td><td align="left" rowspan="1" colspan="1">&#x02212;33</td><td align="left" rowspan="1" colspan="1">42</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">&#x02212;35</td><td align="left" rowspan="1" colspan="1">&#x02212;31</td><td align="left" rowspan="1" colspan="1">52</td></tr><tr><td align="left" rowspan="1" colspan="1">Superior Parietal</td><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">&#x02212;25</td><td align="left" rowspan="1" colspan="1">&#x02212;65</td><td align="left" rowspan="1" colspan="1">62</td></tr><tr><td align="left" rowspan="1" colspan="1">Inferior Parietal</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">40</td><td align="left" rowspan="1" colspan="1">55</td><td align="left" rowspan="1" colspan="1">&#x02212;43</td><td align="left" rowspan="1" colspan="1">28</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">40</td><td align="left" rowspan="1" colspan="1">&#x02212;43</td><td align="left" rowspan="1" colspan="1">&#x02212;33</td><td align="left" rowspan="1" colspan="1">46</td></tr><tr><td align="left" rowspan="1" colspan="1">Superior Temporal</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">38</td><td align="left" rowspan="1" colspan="1">41</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">&#x02212;18</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">22</td><td align="left" rowspan="1" colspan="1">&#x02212;45</td><td align="left" rowspan="1" colspan="1">&#x02212;9</td><td align="left" rowspan="1" colspan="1">&#x02212;6</td></tr><tr><td align="left" rowspan="1" colspan="1">Middle Temporal</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">21</td><td align="left" rowspan="1" colspan="1">55</td><td align="left" rowspan="1" colspan="1">&#x02212;35</td><td align="left" rowspan="1" colspan="1">4</td></tr><tr><td align="left" rowspan="1" colspan="1">Fusiform</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">19</td><td align="left" rowspan="1" colspan="1">38</td><td align="left" rowspan="1" colspan="1">&#x02212;69</td><td align="left" rowspan="1" colspan="1">&#x02212;16</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">37</td><td align="left" rowspan="1" colspan="1">&#x02212;45</td><td align="left" rowspan="1" colspan="1">&#x02212;61</td><td align="left" rowspan="1" colspan="1">&#x02212;18</td></tr><tr><td align="left" rowspan="1" colspan="1">Parahippocampal</td><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">28</td><td align="left" rowspan="1" colspan="1">&#x02212;28</td><td align="left" rowspan="1" colspan="1">&#x02212;6</td><td align="left" rowspan="1" colspan="1">&#x02212;20</td></tr><tr><td align="left" rowspan="1" colspan="1">Cuneus</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">&#x02212;87</td><td align="left" rowspan="1" colspan="1">6</td></tr><tr><td align="left" rowspan="1" colspan="1">Middle Occipital</td><td align="left" rowspan="1" colspan="1">L</td><td align="left" rowspan="1" colspan="1">19</td><td align="left" rowspan="1" colspan="1">&#x02212;39</td><td align="left" rowspan="1" colspan="1">&#x02212;73</td><td align="left" rowspan="1" colspan="1">8</td></tr></tbody></table></alternatives></table-wrap></sec></sec><sec id="s3"><title>Results</title><sec id="s3a"><title>MVPA Discrimination of Action and Non-action Stimuli by using Sensory Modality- and Group-specific Classifiers within Condition</title><p>In the MVPA, the SVM classifiers that had been trained separately for each group (sighted and blind individuals) and for each sensory modality (auditory and visual) were able to discriminate action from non action stimuli with a accuracy ranging from 75.7% to 80.7% (mean accuracy&#x000b1;S.D. across folders: sighted group, SVM classifier trained on visual stimuli&#x0200a;=&#x0200a;76.6&#x000b1;0.04%; SVM classifier trained on auditory stimuli&#x0200a;=&#x0200a;72.1&#x000b1;0.03%; blind group, SVM classifier trained on auditory stimuli&#x0200a;=&#x0200a;74.4&#x000b1;0.05%) (<xref ref-type="table" rid="pone-0058632-t001">Table 1</xref>-A).</p><p>The discrimination maps for each SVM classifier are shown in <xref ref-type="fig" rid="pone-0058632-g001">Figure 1</xref>. Middle and inferior frontal, premotor, inferior and superior parietal and middle/superior temporal regions, predominantly in the left hemisphere, provided the most relevant information for stimuli classification. Furthermore, specific differences in the discrimination maps were visible among different experimental conditions, and additional discriminative voxels were found in bilateral striate and extrastriate regions, dorsolateral and medial prefrontal cortex, anterior cingulate, and precuneus.</p><fig id="pone-0058632-g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0058632.g001</object-id><label>Figure 1</label><caption><title>Discriminative maps of the three distinct linear binary SVM classifiers to separate action (red scale) from non-action (blue scale) stimuli in sighted (sounds and videos) and blind (sounds only) subjects, as obtained by using a RFE algorithm.</title><p>Color intensity reflects the weights of the support vectors, after transformation into Z scores. Spatially normalized volumes are projected a single-subject inflated pial surface template in the Talairach-Tournoux standard space. Ventral and dorsal areas of the premotor cortex (vPM e dPM), inferior frontal (IF) cortex, superior and middle temporal gyri (ST/MT), superior (SPL) and inferior parietal lobule (IPL).</p></caption><graphic xlink:href="pone.0058632.g001"/></fig></sec><sec id="s3b"><title>MVPA Discrimination of Action and Non-action Stimuli by using Sensory Modality- and Group-specific Classifiers Across Conditions and Groups</title><p>The across-condition and across-group evaluation showed that the SVM-classifier trained within condition did not reach a significant discrimination accuracy between action vs. environmental stimuli (<xref ref-type="table" rid="pone-0058632-t001">Table 1</xref>-A).</p></sec><sec id="s3c"><title>MVPA Discrimination of Action and Non-action Stimuli by using a Combined &#x02018;Supramodal&#x02019; Classifier and the &#x0201c;Knock out&#x0201d; Approach</title><p>To test the more abstract representation of action feature, we defined a combined supramodal classifier and used a &#x0201c;knock out&#x0201d; procedure to examine the degree of overlap in information between the representations of actions across the different experimental conditions and groups.</p><p>The combined supramodal classifier was able to recognize the action feature with an overall accuracy of 66.7% (p-value &#x0003c; 0.005 at permutation test). Within its discriminative map, we identified voxels that were mainly located in AON areas <xref rid="pone.0058632-Molenberghs1" ref-type="bibr">[2]</xref>, such as the left superior parietal, right inferior parietal, bilateral ventral and right dorsal premotor area, bilateral middle/superior temporal cortex (<xref ref-type="fig" rid="pone-0058632-g002">Figure 2</xref>, <xref ref-type="table" rid="pone-0058632-t002">Table 2</xref>). Additional common voxels were found in bilateral striate and extrastriate, dorsolateral and medial prefrontal cortex, anterior cingulate, bilateral precuneus and posterior cingulate cortex.</p><fig id="pone-0058632-g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0058632.g002</object-id><label>Figure 2</label><caption><title>Map of the combined &#x02018;supramodal&#x02019; SVM classifier that was defined by using the training data from all action (red scale) and non-action (blue scale) stimuli classes, and was employed in a &#x02018;knock-out&#x02019; procedure.</title><p>Spatially normalized volumes are projected onto a single-subject inflated pial surface template in the Talairach-Tournoux standard space. Ventral and dorsal areas of the premotor cortex (vPM e dPM), inferior frontal (IF) cortex, superior and middle temporal gyri (ST/MT), superior (SPL) and inferior parietal lobule (IPL).</p></caption><graphic xlink:href="pone.0058632.g002"/></fig><p>The &#x0201c;knock out&#x0201d; procedure, that is, the exclusion of the discriminative areas defined by the supramodal classifier from the three best discriminative maps, resulted in a significantly decreased accuracy as compared to the original within-category discriminations for the SVM classifiers in both sighted and blind individuals (p&#x0003c;0.05, Wilcoxon signed-rank test and Fisher&#x02019;s method), though they were still able to perform a significant within-category discrimination (<xref ref-type="table" rid="pone-0058632-t001">Table 1</xref>-B). Still, no across condition/group discrimination resulted significant (<xref ref-type="table" rid="pone-0058632-t001">Table 1</xref>-B).</p><p>In contrast, when relying just on these supramodal voxels, all three SVM classifiers were able to reach a significant within-category discrimination, and the across experimental condition/group discriminations showed significant accuracies (<xref ref-type="table" rid="pone-0058632-t001">Table 1</xref>-C). In details, the visual SVM classifier showed significant accuracy in classifying auditory stimuli sessions in sighted and blind subjects. Also the auditory SVM classifiers trained in sighted and blind individuals correctly performed an action vs. non action discrimination across stimuli categories (i.e. sensory modality) and experimental groups, with the only exception of visual stimuli for the auditory SVM classifier trained in blind individuals (<xref ref-type="table" rid="pone-0058632-t001">Table 1</xref>-C).</p></sec><sec id="s3d"><title>Discrimination of Motor Pantomimes</title><p>As detailed in <xref ref-type="table" rid="pone-0058632-t003">Table 3</xref>-A, in a whole brain approach, the SVM classifier trained on visual stimuli for the sighted group was significantly able to recognize as &#x02018;actions&#x02019; the motor pantomimes in sighted individuals with a high accuracy of discrimination (85.3%). At a more conservative level, none of the auditory SVM classifiers as trained in either sighted or blind individuals were able to recognize as &#x02018;actions&#x02019; the motor pantomimes in sighted individuals; in addition, none of the classifiers were able to significantly recognize as &#x02018;actions&#x02019; the neural patterns of motor pantomime in the blind individuals. In addition, the neural responses during motor pantomime performance were still recognized as &#x02018;actions&#x02019; by both the visual and the auditory SVM classifiers of sighted individuals when limiting to the discrimination map of the supramodal classifier (<xref ref-type="table" rid="pone-0058632-t003">Table 3</xref>-B), further indicating that these functional overlapping voxels mainly contribute to the representation of motor acts. When volume of interest was restricted to the discrimination map of the supramodal classifier (<xref ref-type="table" rid="pone-0058632-t003">Table 3</xref>-C), only the SVM classifier trained on the visual stimuli in sighted individual was significantly able to classify the neural responses during motor pantomime performances in sighted individuals.</p><table-wrap id="pone-0058632-t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0058632.t003</object-id><label>Table 3</label><caption><title>Accuracy of each SVM classifiers in recognizing motor pantomime as &#x02018;action&#x02019;.</title></caption><alternatives><graphic id="pone-0058632-t003-3" xlink:href="pone.0058632.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td colspan="2" align="left" rowspan="1">Sighted</td><td align="left" rowspan="1" colspan="1">Blind</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SVM classifier trained on visual stimuli</td><td align="left" rowspan="1" colspan="1">SVM classifier trained on auditory stimuli</td><td align="left" rowspan="1" colspan="1">SVM classifier trained on auditory stimuli</td></tr></thead><tbody><tr><td colspan="4" align="left" rowspan="1">
<italic>A. Whole brain discrimination</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Sighted</td><td align="left" rowspan="1" colspan="1">85.3%<xref ref-type="table-fn" rid="nt105">***</xref>
</td><td align="left" rowspan="1" colspan="1">73.9%<xref ref-type="table-fn" rid="nt107">*</xref>
</td><td align="left" rowspan="1" colspan="1">53.2%<xref ref-type="table-fn" rid="nt107">*</xref>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Blind</td><td align="left" rowspan="1" colspan="1">60%<xref ref-type="table-fn" rid="nt107">*</xref>
</td><td align="left" rowspan="1" colspan="1">65%<xref ref-type="table-fn" rid="nt107">*</xref>
</td><td align="left" rowspan="1" colspan="1">61.2%<xref ref-type="table-fn" rid="nt107">*</xref>
</td></tr><tr><td colspan="4" align="left" rowspan="1">
<italic>B. After excluding the knock-out map</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">84.3%<xref ref-type="table-fn" rid="nt105">***</xref>
</td><td align="left" rowspan="1" colspan="1">71.7%<xref ref-type="table-fn" rid="nt107">*</xref>
</td><td align="left" rowspan="1" colspan="1">46.4%</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">57.5%<xref ref-type="table-fn" rid="nt107">*</xref>
</td><td align="left" rowspan="1" colspan="1">63.7%<xref ref-type="table-fn" rid="nt107">*</xref>
</td><td align="left" rowspan="1" colspan="1">65%<xref ref-type="table-fn" rid="nt107">*</xref>
</td></tr><tr><td colspan="4" align="left" rowspan="1">
<italic>C. By restricting to the knock-out map</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Sighted</td><td align="left" rowspan="1" colspan="1">85.3%<xref ref-type="table-fn" rid="nt105">***</xref>
</td><td align="left" rowspan="1" colspan="1">76.8%<xref ref-type="table-fn" rid="nt106">**</xref>
</td><td align="left" rowspan="1" colspan="1">67.5%<xref ref-type="table-fn" rid="nt107">*</xref>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Blind</td><td align="left" rowspan="1" colspan="1">70%<xref ref-type="table-fn" rid="nt107">*</xref>
</td><td align="left" rowspan="1" colspan="1">52.5%</td><td align="left" rowspan="1" colspan="1">61.5%<xref ref-type="table-fn" rid="nt107">*</xref>
</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt104"><label>a</label><p>visual and auditory runs have been considered together.</p></fn><fn id="nt105"><label>***</label><p>p-value &#x0003c; 0.005,</p></fn><fn id="nt106"><label>**</label><p>p-value &#x0003c; 0.05 at permutation test;</p></fn><fn id="nt107"><label>*</label><p>p-value &#x0003c; 0.05 at the binomial test.</p></fn></table-wrap-foot></table-wrap><p>On the other hand, when using a less conservative approach with a binomial test, both SVM classifiers trained on auditory stimuli of sighted and blind individuals were able to recognize as &#x02018;actions&#x02019; the motor pantomimes in a whole brain discrimination (<xref ref-type="table" rid="pone-0058632-t003">Table 3</xref>-A), worsened their accuracy performance after excluding the knock-out map (<xref ref-type="table" rid="pone-0058632-t003">Table 3</xref>-B), and improved when limiting to the discrimination map of the supramodal classifier (<xref ref-type="table" rid="pone-0058632-t003">Table 3</xref>-C).</p><p>Finally, the combined supramodal classifier was able to recognize as &#x02018;actions&#x02019; the motor pantomimes both on visual stimuli (accuracy 85.3%, p-value &#x0003c; 0.005 at permutation test) and on auditory stimuli (accuracy 76.8%, p-value &#x0003c; 0.05 at permutation test) in sighted individuals.</p></sec></sec><sec id="s4"><title>Discussion</title><p>Here we used a multivariate pattern recognition method to distinguish neural responses in congenitally blind and sighted participants during the visual and auditory perception of a set of hand-made action and environmental stimuli, to test the hypothesis that motor acts are represented in a distributed and truly <italic>supramodal</italic> fashion. In a &#x02018;classical&#x02019; univariate analysis of the same functional dataset, we had previously shown that blind individuals activate a premotor-temporo-parietal network that subserves a &#x02018;mirror&#x02019; response to aurally presented actions, and that such a network overlaps with the hMS, as part of the AON, found in sighted individuals in response to both visually and aurally presented stimuli <xref rid="pone.0058632-Ricciardi1" ref-type="bibr">[25]</xref>. However, we did not assessed whether this &#x02018;more abstract&#x02019; representation of action is truly <italic>supramodal</italic>, i.e. shares a common coding across visual and auditory sensory modalities. Recently, MVPA has been employed to study the representation of different categories of stimuli within the same perceptual modality <xref rid="pone.0058632-Pereira1" ref-type="bibr">[29]</xref>, <xref rid="pone.0058632-Poldrack1" ref-type="bibr">[30]</xref>. In this study, for the first time, an MVPA specifically evaluated the representation of the same stimulus category (action) within and across different sensory modalities (visual, auditory or motor) and experimental groups (sighted and congenitally blind).</p><sec id="s4a"><title>Action Discrimination by using Sensory Modality- and Group-specific Classifiers within Condition</title><p>Single MVPA-based classifiers, trained separately for each experimental condition and group, were able to significantly discriminate action from non action stimuli within condition. These observations in sighted and congenitally blind individuals using visual and auditory stimuli showed for the first time that sounds can be successfully used to distinguish neural responses to action as compared to environmental stimuli, and expand previous functional studies that explored the distributed and overlapping representation of visually-presented motor acts using MVPA-based approaches <xref rid="pone.0058632-Oosterhof1" ref-type="bibr">[8]</xref>, <xref rid="pone.0058632-Dinstein1" ref-type="bibr">[9]</xref>, <xref rid="pone.0058632-Jastorff1" ref-type="bibr">[13]</xref>, <xref rid="pone.0058632-Oosterhof2" ref-type="bibr">[14]</xref>, <xref rid="pone.0058632-Oosterhof3" ref-type="bibr">[15]</xref>, <xref rid="pone.0058632-Molenberghs2" ref-type="bibr">[16]</xref>.</p><p>As a matter of facts, distributed activity in left and right anterior intraparietal cortex has been previously used to discriminate the content of three different motor acts (&#x02018;<italic>rock-paper-scissors</italic>&#x02019; game), when these actions were either observed or performed <xref rid="pone.0058632-Dinstein1" ref-type="bibr">[9]</xref>. Action-dedicated regions were also described in lateral occipito-temporal and left postcentral/anterior parietal cortex: the postcentral area carries distributed information about the effectors used to perform the action, while parietal regions about the action goal <xref rid="pone.0058632-Oosterhof1" ref-type="bibr">[8]</xref>. Similarly, while viewing videos of different motor acts, distributed action representations can be clustered according to the specific relationship between agent and object (i.e. their behavioral significance) in the inferior parietal cortex, or according to the effector (foot, hand, mouth) used to perform the action in the premotor cortex <xref rid="pone.0058632-Jastorff1" ref-type="bibr">[13]</xref>, although both effector-dependent and effector independent representations have been shown coexist in this inferior frontal and precentral region <xref rid="pone.0058632-Cattaneo3" ref-type="bibr">[46]</xref>.</p><p>In our study, the discrimination maps of the three classifiers (<xref ref-type="fig" rid="pone-0058632-g001">Figure 1</xref>) identified the most relevant information about &#x02018;action feature&#x02019; as primarily located in a distributed and bilateral, though left prevalent, prefrontal, premotor, parietal and temporal circuit. This network of discrimination of action from non action stimuli mainly included brain regions within the hMS-related action recognition network <xref rid="pone.0058632-Molenberghs1" ref-type="bibr">[2]</xref>, as the inferior and superior parietal, intraparietal, dorsal and ventral premotor, and inferior frontal areas.</p><p>Nonetheless, specific differences across sensory conditions and groups also were present. For instance, in sighted subjects the classifier trained on auditory stimuli relied more on right inferior frontal regions while homologous left areas contributed more to visual action recognition. This may suggest that the features that can be extracted by MVPA may not be directly linked to action <italic>per se</italic>, but rather be related to other physical/semantic attributes of the stimuli (e.g. dynamicity, imageability, conveying modality, onset time, etc. &#x02013; for a detailed list of stimuli, refer to <xref rid="pone.0058632-Ricciardi1" ref-type="bibr">[25]</xref> or to their specific contents (presence/absence of manipulable objects, presence/absence of living/non living entities, etc.), and thus be processed across different brain areas <xref rid="pone.0058632-Pereira1" ref-type="bibr">[29]</xref>, <xref rid="pone.0058632-Poldrack1" ref-type="bibr">[30]</xref>, <xref rid="pone.0058632-OToole1" ref-type="bibr">[47]</xref>. Furthermore, we cannot exclude that the discriminative maps for each experimental condition might have been influenced by stimulus-specific differences in the switch from resting activity to task-associated response, due to a different Default-Mode Network (DMN) recruitment by the two stimulus classes. Nonetheless, when observing brain regions involved in the discrimination of stimuli, no specific DMN regions appear to consistently contribute to action vs. environmental stimulus classification. In addition, the current experimental setup could hardly induce DMN-related self-oriented or introspective activities.</p><p>Each action was carefully chosen to be as different from the others as possible, and distinct sets of actions were selected for the visual, auditory and motor pantomime conditions. In addition, our subsequent analyses aimed at determining action coding across different sensory modalities (visual, auditory or motor) and experimental groups, thus enhancing the common features of action representation and limiting any possible confounds related to stimuli selection. Even so, a protocol for a MVPA approach would have benefit from better-selected control or baseline conditions, and balanced/matched stimuli across classes to modulate for a wider gamut of features.</p></sec><sec id="s4b"><title>Action Discrimination by using Both Sensory Modality- and Group-specific Classifiers Across Conditions/groups and a Combined &#x02018;Supramodal&#x02019; Classifier</title><p>To what extent are the distributed representations of actions related to the specific sensory modality used for action perception?</p><p>First, to validate the degree of overlap in information across the experimental conditions (i.e., visual or auditory modality) and groups, the classifiers that had been trained on each single experimental condition were employed in an across sensory modalities and groups evaluation. The three classifiers (i.e., visual stimuli in sighted individuals, auditory stimuli in sighted and in blind subjects) were unable to discriminate the action features across conditions or groups. This confirms our previous consideration that specific differences in the features that are &#x02018;employed&#x02019; by the single classifiers across sensory conditions and groups were present.</p><p>Second, the same pattern recognition approach was applied across experimental conditions to define a combined classifier. Indeed, accordingly to our hypothesis of a more abstract representation of actions, this classifier was able to discriminate significantly action stimuli, independently from both the sensory modality conveying the information and the experimental group. The brain areas belonging to the AON - specifically superior and inferior parietal, ventral and right dorsal premotor, middle/superior temporal areas - primarily contributed to this discrimination ability.</p><p>Third, this hMS-based supramodal network was then utilized in a &#x02018;knock-out&#x02019; procedure <xref rid="pone.0058632-Carlson1" ref-type="bibr">[44]</xref>. We reasoned that if these common voxels of the combined classifier retain a more abstract functional representation of actions, then their subtraction from the distinct linear binary classifiers should results in a significantly diminished accuracy in separating action vs. non-action stimuli. Conversely, if this network retains a reliable representation of actions, when limiting only to the supramodal common areas, the distinct classifiers should both maintain significant discrimination accuracies within experimental conditions (comparable with the accuracy levels of the whole brain classification), and have a greater accuracy to recognize action stimuli across experimental conditions.</p><p>As expected, the removal of functional overlapping voxels significantly decreased the within-condition discrimination accuracy of the classifiers that had been trained on each single condition, as compared to the original classification performance. On the other hand, when extracting the common representation of actions, the classifiers both maintained a significant within-condition discrimination accuracy and improved accuracies across-conditions. Both the classifiers trained on visual and auditory stimuli in sighted individuals and the classifier trained in blind individuals showed a significant across conditions/groups discrimination of action vs. non action stimuli. Once more, the value of the combined classifier and the &#x0201c;knock-out&#x0201d; procedure in enhancing the characterization of the supramodal representation of actions relies on the possibility to identify those common brain regions that contribute to discriminate action from non-action perception across the different experimental conditions, independently from the specific features that could drive stimuli separation within a single sensory modality.</p></sec><sec id="s4c"><title>Motor Pantomimes Distinguished as Actions</title><p>In line with the &#x02018;mirror rationale&#x02019; that the distributed and supramodal representation of actions should retain substantial information during both action recognition and performance, we also tested whether patterns of neural response for actual motor performance were recognized as &#x02018;actions&#x02019; from a specific MVPA-based classifiers. Since our test set was unbalanced, as the SVM classifiers were used to recognize &#x02018;actions&#x02019; in motor pantomimes but with no alternative control conditions, we applied a more conservative correction, in order to limit false positive results.</p><p>In a whole brain approach, only the visual classifiers of sighted participants was significantly able to recognize as an &#x02018;action&#x02019; the patterns of neural response to motor pantomimes in sighted individuals but not in the blind ones. Interestingly, this greater discrimination ability of the classifier trained on visual stimuli in sighted individuals across the different analyses may be related to the somehow prevalent visuomotor nature of the hMS itself, as shown by the stronger and more extended unimodal visual response of this action-specific network (Gazzola et al., 2006; Ricciardi et al., 2009). This discrimination ability became slightly smaller when excluding the &#x02018;knock-out&#x02019; supramodal map, and, conversely, remained equally accurate when the classification was restricted to the &#x02018;knock-out&#x02019; supramodal map. In addition, when restricting to the supramodal map also the auditory classifiers from sighted participants were significantly able to recognize motor acts.</p><p>In addition, when using a less conservative approach, both SVM classifiers trained on auditory stimuli of sighted and blind individuals were also able to recognize as &#x02018;actions&#x02019; the motor pantomimes. In line with the observations with the visual classifiers of sighted participants, this discrimination ability became smaller or greater when excluding or restricting to, respectively, the &#x02018;knock-out&#x02019; supramodal map.</p><p>Interestingly, the classification accuracies of the three SVM classifiers trained on either visual or auditory stimuli in sighted and blind individuals resulted the highest when restricted to the &#x02018;knock-out&#x02019; supramodal map, thus further supporting a more abstract functional representation of motor acts in these regions belonging to the AON. Consistently, motor pantomimes were correctly classified by the combined supramodal classifier for visual and auditory stimuli in sighted individuals.</p></sec><sec id="s4d"><title>Conclusions</title><p>The present study demonstrates for the first time that a MVPA can be used successfully to discriminate functional representations (of actions) in both sighted and blind individuals. The ability to identify (action) features across sensory modalities and experimental groups supports the hypothesis of a distributed and truly supramodal functional representation of actions within the brain areas of the hMS, and leads to two main considerations.</p><p>First, these results are consistent with previous functional studies in both sighted and congenitally blind individuals that have shown the existence of supramodal networks able to process external information regardless of the sensory modality through which the information is acquired <xref rid="pone.0058632-Pietrini1" ref-type="bibr">[12]</xref>, <xref rid="pone.0058632-Ricciardi2" ref-type="bibr">[27]</xref>, <xref rid="pone.0058632-Kupers1" ref-type="bibr">[28]</xref>. Homologies do not only limit to the spatial localization of the patterns of neural activations, but mainly involve the content (i.e. action or non action) of the neural responses: for instance, overlapping category-related patterns of response across sensory modalities have been found in both sighted and congenitally blind individuals (Mahon et al., 2009; Pietrini et al., 2004b). Applied to the assessment of supramodal functional organization, pattern recognition approaches have been employed here to classify neural responses across experimental samples (i.e., congenitally blind and sighted individuals) and sensory modalities, and consequently to localize those cortical regions that functionally contribute to a supramodal representation.</p><p>Second, this more abstract functional organization enables congenitally blind individuals to acquire knowledge about different perceptual, cognitive and affective aspects of an external world that they have never seen <xref rid="pone.0058632-Pietrini1" ref-type="bibr">[12]</xref>, <xref rid="pone.0058632-Ricciardi2" ref-type="bibr">[27]</xref>, <xref rid="pone.0058632-Kupers1" ref-type="bibr">[28]</xref>. The demonstration of a more abstract, sensory independent representation of actions within the hMS supports the rationale of a cognitive system that might play a major role not only in action recognition and intention understanding, but also in learning by imitation, empathy, and language development <xref rid="pone.0058632-Gallese1" ref-type="bibr">[7]</xref>, <xref rid="pone.0058632-Cattaneo1" ref-type="bibr">[17]</xref>.</p></sec></sec></body><back><ack><p>This paper is dedicated to the memory of our former mentor, Prof. Mario Guazzelli.</p></ack><ref-list><title>References</title><ref id="pone.0058632-Kilner1"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Kilner</surname><given-names>JM</given-names></name> (<year>2011</year>) <article-title>More than one pathway to action understanding</article-title>. <source>Trends Cogn Sci</source>
<volume>15</volume>: <fpage>352</fpage>&#x02013;<lpage>357</lpage>.<pub-id pub-id-type="pmid">21775191</pub-id></mixed-citation></ref><ref id="pone.0058632-Molenberghs1"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Molenberghs</surname><given-names>P</given-names></name>, <name><surname>Cunnington</surname><given-names>R</given-names></name>, <name><surname>Mattingley</surname><given-names>JB</given-names></name> (<year>2012</year>) <article-title>Brain regions with mirror properties: a meta-analysis of 125 human fMRI studies</article-title>. <source>Neurosci Biobehav Rev</source>
<volume>36</volume>: <fpage>341</fpage>&#x02013;<lpage>349</lpage>.<pub-id pub-id-type="pmid">21782846</pub-id></mixed-citation></ref><ref id="pone.0058632-Fadiga1"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Fadiga</surname><given-names>L</given-names></name>, <name><surname>Fogassi</surname><given-names>L</given-names></name>, <name><surname>Pavesi</surname><given-names>G</given-names></name>, <name><surname>Rizzolatti</surname><given-names>G</given-names></name> (<year>1995</year>) <article-title>Motor facilitation during action observation: a magnetic stimulation study</article-title>. <source>J Neurophysiol</source>
<volume>73</volume>: <fpage>2608</fpage>&#x02013;<lpage>2611</lpage>.<pub-id pub-id-type="pmid">7666169</pub-id></mixed-citation></ref><ref id="pone.0058632-Rizzolatti1"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Rizzolatti</surname><given-names>G</given-names></name>, <name><surname>Fadiga</surname><given-names>L</given-names></name>, <name><surname>Gallese</surname><given-names>V</given-names></name>, <name><surname>Fogassi</surname><given-names>L</given-names></name> (<year>1996</year>) <article-title>Premotor cortex and the recognition of motor actions</article-title>. <source>Brain Res Cogn Brain Res</source>
<volume>3</volume>: <fpage>131</fpage>&#x02013;<lpage>141</lpage>.<pub-id pub-id-type="pmid">8713554</pub-id></mixed-citation></ref><ref id="pone.0058632-Rizzolatti2"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Rizzolatti</surname><given-names>G</given-names></name>, <name><surname>Craighero</surname><given-names>L</given-names></name> (<year>2004</year>) <article-title>The mirror-neuron system</article-title>. <source>Annu Rev Neurosci</source>
<volume>27</volume>: <fpage>169</fpage>&#x02013;<lpage>192</lpage>.<pub-id pub-id-type="pmid">15217330</pub-id></mixed-citation></ref><ref id="pone.0058632-FabbriDestro1"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Fabbri-Destro</surname><given-names>M</given-names></name>, <name><surname>Rizzolatti</surname><given-names>G</given-names></name> (<year>2008</year>) <article-title>Mirror neurons and mirror systems in monkeys and humans</article-title>. <source>Physiology (Bethesda)</source>
<volume>23</volume>: <fpage>171</fpage>&#x02013;<lpage>179</lpage>.<pub-id pub-id-type="pmid">18556470</pub-id></mixed-citation></ref><ref id="pone.0058632-Gallese1"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Gallese</surname><given-names>V</given-names></name>, <name><surname>Keysers</surname><given-names>C</given-names></name>, <name><surname>Rizzolatti</surname><given-names>G</given-names></name> (<year>2004</year>) <article-title>A unifying view of the basis of social cognition</article-title>. <source>Trends Cogn Sci</source>
<volume>8</volume>: <fpage>396</fpage>&#x02013;<lpage>403</lpage>.<pub-id pub-id-type="pmid">15350240</pub-id></mixed-citation></ref><ref id="pone.0058632-Oosterhof1"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Oosterhof</surname><given-names>NN</given-names></name>, <name><surname>Wiggett</surname><given-names>AJ</given-names></name>, <name><surname>Diedrichsen</surname><given-names>J</given-names></name>, <name><surname>Tipper</surname><given-names>SP</given-names></name>, <name><surname>Downing</surname><given-names>PE</given-names></name> (<year>2010</year>) <article-title>Surface-based information mapping reveals crossmodal vision-action representations in human parietal and occipitotemporal cortex</article-title>. <source>J Neurophysiol</source>
<volume>104</volume>: <fpage>1077</fpage>&#x02013;<lpage>1089</lpage>.<pub-id pub-id-type="pmid">20538772</pub-id></mixed-citation></ref><ref id="pone.0058632-Dinstein1"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Dinstein</surname><given-names>I</given-names></name>, <name><surname>Gardner</surname><given-names>JL</given-names></name>, <name><surname>Jazayeri</surname><given-names>M</given-names></name>, <name><surname>Heeger</surname><given-names>DJ</given-names></name> (<year>2008</year>) <article-title>Executed and observed movements have different distributed representations in human aIPS</article-title>. <source>J Neurosci</source>
<volume>28</volume>: <fpage>11231</fpage>&#x02013;<lpage>11239</lpage>.<pub-id pub-id-type="pmid">18971465</pub-id></mixed-citation></ref><ref id="pone.0058632-Haxby1"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Haxby</surname><given-names>JV</given-names></name>, <name><surname>Gobbini</surname><given-names>MI</given-names></name>, <name><surname>Furey</surname><given-names>ML</given-names></name>, <name><surname>Ishai</surname><given-names>A</given-names></name>, <name><surname>Schouten</surname><given-names>JL</given-names></name>, <etal>et al</etal> (<year>2001</year>) <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>. <source>Science</source>
<volume>293</volume>: <fpage>2425</fpage>&#x02013;<lpage>2430</lpage>.<pub-id pub-id-type="pmid">11577229</pub-id></mixed-citation></ref><ref id="pone.0058632-Staeren1"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Staeren</surname><given-names>N</given-names></name>, <name><surname>Renvall</surname><given-names>H</given-names></name>, <name><surname>De Martino</surname><given-names>F</given-names></name>, <name><surname>Goebel</surname><given-names>R</given-names></name>, <name><surname>Formisano</surname><given-names>E</given-names></name> (<year>2009</year>) <article-title>Sound categories are represented as distributed patterns in the human auditory cortex</article-title>. <source>Curr Biol</source>
<volume>19</volume>: <fpage>498</fpage>&#x02013;<lpage>502</lpage>.<pub-id pub-id-type="pmid">19268594</pub-id></mixed-citation></ref><ref id="pone.0058632-Pietrini1"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Pietrini</surname><given-names>P</given-names></name>, <name><surname>Furey</surname><given-names>ML</given-names></name>, <name><surname>Ricciardi</surname><given-names>E</given-names></name>, <name><surname>Gobbini</surname><given-names>MI</given-names></name>, <name><surname>Wu</surname><given-names>WH</given-names></name>, <etal>et al</etal> (<year>2004</year>) <article-title>Beyond sensory images: Object-based representation in the human ventral pathway</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>101</volume>: <fpage>5658</fpage>&#x02013;<lpage>5663</lpage>.<pub-id pub-id-type="pmid">15064396</pub-id></mixed-citation></ref><ref id="pone.0058632-Jastorff1"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Jastorff</surname><given-names>J</given-names></name>, <name><surname>Begliomini</surname><given-names>C</given-names></name>, <name><surname>Fabbri-Destro</surname><given-names>M</given-names></name>, <name><surname>Rizzolatti</surname><given-names>G</given-names></name>, <name><surname>Orban</surname><given-names>GA</given-names></name> (<year>2010</year>) <article-title>Coding observed motor acts: different organizational principles in the parietal and premotor cortex of humans</article-title>. <source>J Neurophysiol</source>
<volume>104</volume>: <fpage>128</fpage>&#x02013;<lpage>140</lpage>.<pub-id pub-id-type="pmid">20445039</pub-id></mixed-citation></ref><ref id="pone.0058632-Oosterhof2"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Oosterhof</surname><given-names>NN</given-names></name>, <name><surname>Tipper</surname><given-names>SP</given-names></name>, <name><surname>Downing</surname><given-names>PE</given-names></name> (<year>2012</year>) <article-title>Visuo-motor imagery of specific manual actions: A multi-variate pattern analysis fMRI study</article-title>. <source>Neuroimage</source>
<volume>63</volume>: <fpage>262</fpage>&#x02013;<lpage>271</lpage>.<pub-id pub-id-type="pmid">22766163</pub-id></mixed-citation></ref><ref id="pone.0058632-Oosterhof3"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Oosterhof</surname><given-names>NN</given-names></name>, <name><surname>Tipper</surname><given-names>SP</given-names></name>, <name><surname>Downing</surname><given-names>PE</given-names></name> (<year>2012</year>) <article-title>Viewpoint (in)dependence of action representations: an MVPA study</article-title>. <source>J Cogn Neurosci</source>
<volume>24</volume>: <fpage>975</fpage>&#x02013;<lpage>989</lpage>.<pub-id pub-id-type="pmid">22264198</pub-id></mixed-citation></ref><ref id="pone.0058632-Molenberghs2"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Molenberghs</surname><given-names>P</given-names></name>, <name><surname>Hayward</surname><given-names>L</given-names></name>, <name><surname>Mattingley</surname><given-names>JB</given-names></name>, <name><surname>Cunnington</surname><given-names>R</given-names></name> (<year>2012</year>) <article-title>Activation patterns during action observation are modulated by context in mirror system areas</article-title>. <source>Neuroimage</source>
<volume>59</volume>: <fpage>608</fpage>&#x02013;<lpage>615</lpage>.<pub-id pub-id-type="pmid">21840404</pub-id></mixed-citation></ref><ref id="pone.0058632-Cattaneo1"><label>17</label><mixed-citation publication-type="journal">
<name><surname>Cattaneo</surname><given-names>L</given-names></name>, <name><surname>Rizzolatti</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>The mirror neuron system</article-title>. <source>Arch Neurol</source>
<volume>66</volume>: <fpage>557</fpage>&#x02013;<lpage>560</lpage>.<pub-id pub-id-type="pmid">19433654</pub-id></mixed-citation></ref><ref id="pone.0058632-Etzel1"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Etzel</surname><given-names>JA</given-names></name>, <name><surname>Gazzola</surname><given-names>V</given-names></name>, <name><surname>Keysers</surname><given-names>C</given-names></name> (<year>2008</year>) <article-title>Testing simulation theory with cross-modal multivariate classification of fMRI data</article-title>. <source>PLoS ONE</source>
<volume>3</volume>: <fpage>e3690</fpage>.<pub-id pub-id-type="pmid">18997869</pub-id></mixed-citation></ref><ref id="pone.0058632-Galati1"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Galati</surname><given-names>G</given-names></name>, <name><surname>Committeri</surname><given-names>G</given-names></name>, <name><surname>Spitoni</surname><given-names>G</given-names></name>, <name><surname>Aprile</surname><given-names>T</given-names></name>, <name><surname>Di Russo</surname><given-names>F</given-names></name>, <etal>et al</etal> (<year>2008</year>) <article-title>A selective representation of the meaning of actions in the auditory mirror system</article-title>. <source>Neuroimage</source>
<volume>40</volume>: <fpage>1274</fpage>&#x02013;<lpage>1286</lpage>.<pub-id pub-id-type="pmid">18276163</pub-id></mixed-citation></ref><ref id="pone.0058632-Gazzola1"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Gazzola</surname><given-names>V</given-names></name>, <name><surname>Aziz-Zadeh</surname><given-names>L</given-names></name>, <name><surname>Keysers</surname><given-names>C</given-names></name> (<year>2006</year>) <article-title>Empathy and the somatotopic auditory mirror system in humans</article-title>. <source>Curr Biol</source>
<volume>16</volume>: <fpage>1824</fpage>&#x02013;<lpage>1829</lpage>.<pub-id pub-id-type="pmid">16979560</pub-id></mixed-citation></ref><ref id="pone.0058632-Keysers1"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Keysers</surname><given-names>C</given-names></name>, <name><surname>Kohler</surname><given-names>E</given-names></name>, <name><surname>Umilta</surname><given-names>MA</given-names></name>, <name><surname>Nanetti</surname><given-names>L</given-names></name>, <name><surname>Fogassi</surname><given-names>L</given-names></name>, <etal>et al</etal> (<year>2003</year>) <article-title>Audiovisual mirror neurons and action recognition</article-title>. <source>Exp Brain Res</source>
<volume>153</volume>: <fpage>628</fpage>&#x02013;<lpage>636</lpage>.<pub-id pub-id-type="pmid">12937876</pub-id></mixed-citation></ref><ref id="pone.0058632-Kohler1"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Kohler</surname><given-names>E</given-names></name>, <name><surname>Keysers</surname><given-names>C</given-names></name>, <name><surname>Umilta</surname><given-names>MA</given-names></name>, <name><surname>Fogassi</surname><given-names>L</given-names></name>, <name><surname>Gallese</surname><given-names>V</given-names></name>, <etal>et al</etal> (<year>2002</year>) <article-title>Hearing sounds, understanding actions: action representation in mirror neurons</article-title>. <source>Science</source>
<volume>297</volume>: <fpage>846</fpage>&#x02013;<lpage>848</lpage>.<pub-id pub-id-type="pmid">12161656</pub-id></mixed-citation></ref><ref id="pone.0058632-Lewis1"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Lewis</surname><given-names>JW</given-names></name>, <name><surname>Brefczynski</surname><given-names>JA</given-names></name>, <name><surname>Phinney</surname><given-names>RE</given-names></name>, <name><surname>Janik</surname><given-names>JJ</given-names></name>, <name><surname>DeYoe</surname><given-names>EA</given-names></name> (<year>2005</year>) <article-title>Distinct cortical pathways for processing tool versus animal sounds</article-title>. <source>J Neurosci</source>
<volume>25</volume>: <fpage>5148</fpage>&#x02013;<lpage>5158</lpage>.<pub-id pub-id-type="pmid">15917455</pub-id></mixed-citation></ref><ref id="pone.0058632-Lewis2"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Lewis</surname><given-names>JW</given-names></name>, <name><surname>Frum</surname><given-names>C</given-names></name>, <name><surname>Brefczynski-Lewis</surname><given-names>JA</given-names></name>, <name><surname>Talkington</surname><given-names>WJ</given-names></name>, <name><surname>Walker</surname><given-names>NA</given-names></name>, <etal>et al</etal> (<year>2011</year>) <article-title>Cortical network differences in the sighted versus early blind for recognition of human-produced action sounds</article-title>. <source>Human Brain Mapping</source>
<volume>32</volume>: <fpage>2241</fpage>&#x02013;<lpage>2255</lpage>.<pub-id pub-id-type="pmid">21305666</pub-id></mixed-citation></ref><ref id="pone.0058632-Ricciardi1"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Ricciardi</surname><given-names>E</given-names></name>, <name><surname>Bonino</surname><given-names>D</given-names></name>, <name><surname>Sani</surname><given-names>L</given-names></name>, <name><surname>Vecchi</surname><given-names>T</given-names></name>, <name><surname>Guazzelli</surname><given-names>M</given-names></name>, <etal>et al</etal> (<year>2009</year>) <article-title>Do we really need vision? How blind people "see" the actions of others</article-title>. <source>J Neurosci</source>
<volume>29</volume>: <fpage>9719</fpage>&#x02013;<lpage>9724</lpage>.<pub-id pub-id-type="pmid">19657025</pub-id></mixed-citation></ref><ref id="pone.0058632-Cattaneo2"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Cattaneo</surname><given-names>Z</given-names></name>, <name><surname>Vecchi</surname><given-names>T</given-names></name>, <name><surname>Cornoldi</surname><given-names>C</given-names></name>, <name><surname>Mammarella</surname><given-names>I</given-names></name>, <name><surname>Bonino</surname><given-names>D</given-names></name>, <etal>et al</etal> (<year>2008</year>) <article-title>Imagery and spatial processes in blindness and visual impairment</article-title>. <source>Neurosci Biobehav Rev</source>
<volume>32</volume>: <fpage>1346</fpage>&#x02013;<lpage>1360</lpage>.<pub-id pub-id-type="pmid">18571726</pub-id></mixed-citation></ref><ref id="pone.0058632-Ricciardi2"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Ricciardi</surname><given-names>E</given-names></name>, <name><surname>Pietrini</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>New light from the dark: what blindness can teach us about brain function</article-title>. <source>Curr Opin Neurol</source>
<volume>24</volume>: <fpage>357</fpage>&#x02013;<lpage>363</lpage>.<pub-id pub-id-type="pmid">21677583</pub-id></mixed-citation></ref><ref id="pone.0058632-Kupers1"><label>28</label><mixed-citation publication-type="journal">
<name><surname>Kupers</surname><given-names>R</given-names></name>, <name><surname>Pietrini</surname><given-names>P</given-names></name>, <name><surname>Ricciardi</surname><given-names>E</given-names></name>, <name><surname>Ptito</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>The nature of consciousness in the visually deprived brain</article-title>. <source>Frontiers in Psychology</source>
<volume>2</volume>: <fpage>1</fpage>&#x02013;<lpage>14</lpage>.<pub-id pub-id-type="pmid">21713130</pub-id></mixed-citation></ref><ref id="pone.0058632-Pereira1"><label>29</label><mixed-citation publication-type="journal">
<name><surname>Pereira</surname><given-names>F</given-names></name>, <name><surname>Mitchell</surname><given-names>T</given-names></name>, <name><surname>Botvinick</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title>. <source>Neuroimage</source>
<volume>45</volume>: <fpage>S199</fpage>&#x02013;<lpage>209</lpage>.<pub-id pub-id-type="pmid">19070668</pub-id></mixed-citation></ref><ref id="pone.0058632-Poldrack1"><label>30</label><mixed-citation publication-type="journal">
<name><surname>Poldrack</surname><given-names>RA</given-names></name>, <name><surname>Halchenko</surname><given-names>YO</given-names></name>, <name><surname>Hanson</surname><given-names>SJ</given-names></name> (<year>2009</year>) <article-title>Decoding the large-scale structure of brain function by classifying mental States across individuals</article-title>. <source>Psychol Sci</source>
<volume>20</volume>: <fpage>1364</fpage>&#x02013;<lpage>1372</lpage>.<pub-id pub-id-type="pmid">19883493</pub-id></mixed-citation></ref><ref id="pone.0058632-Cox1"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Cox</surname><given-names>RW</given-names></name> (<year>1996</year>) <article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title>. <source>Comput Biomed Res</source>
<volume>29</volume>: <fpage>162</fpage>&#x02013;<lpage>173</lpage>.<pub-id pub-id-type="pmid">8812068</pub-id></mixed-citation></ref><ref id="pone.0058632-Misaki1"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Misaki</surname><given-names>M</given-names></name>, <name><surname>Kim</surname><given-names>Y</given-names></name>, <name><surname>Bandettini</surname><given-names>PA</given-names></name>, <name><surname>Kriegeskorte</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>. <source>Neuroimage</source>
<volume>53</volume>: <fpage>103</fpage>&#x02013;<lpage>118</lpage>.<pub-id pub-id-type="pmid">20580933</pub-id></mixed-citation></ref><ref id="pone.0058632-Tailairach1"><label>33</label><mixed-citation publication-type="other">Tailairach J, Tournoux P (1988) Co-Planar Stereotaxic Atlas of the Human Brain. New York: Thieme Medical Publisher, Inc.</mixed-citation></ref><ref id="pone.0058632-Holmes1"><label>34</label><mixed-citation publication-type="journal">
<name><surname>Holmes</surname><given-names>CJ</given-names></name>, <name><surname>Hoge</surname><given-names>R</given-names></name>, <name><surname>Collins</surname><given-names>L</given-names></name>, <name><surname>Woods</surname><given-names>R</given-names></name>, <name><surname>Toga</surname><given-names>AW</given-names></name>, <etal>et al</etal> (<year>1998</year>) <article-title>Enhancement of MR images using registration for signal averaging</article-title>. <source>J Comput Assist Tomogr</source>
<volume>22</volume>: <fpage>324</fpage>&#x02013;<lpage>333</lpage>.<pub-id pub-id-type="pmid">9530404</pub-id></mixed-citation></ref><ref id="pone.0058632-Schlkopf1"><label>35</label><mixed-citation publication-type="other">Sch&#x000f6;lkopf B, Burges CJC, Smola AJ (1999) Advances in kernel methods: support vector learning. Cambridge, Mass.: MIT Press.</mixed-citation></ref><ref id="pone.0058632-Cortes1"><label>36</label><mixed-citation publication-type="journal">
<name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Vapnik</surname><given-names>V</given-names></name> (<year>1995</year>) <article-title>Support-vector networks</article-title>. <source>Machine Learning</source>
<volume>20</volume>: <fpage>273</fpage>&#x02013;<lpage>297</lpage>.</mixed-citation></ref><ref id="pone.0058632-Joachims1"><label>37</label><mixed-citation publication-type="other">Joachims T (1999) Making large-Scale SVM Learning Practical. In: Sch&#x000f6;lkopf B, Burges C, Smola A, editors. Advances in Kernel Methods - Support Vector Learning. Cambridge, MA, USA: MIT Press. 41&#x02013;56.</mixed-citation></ref><ref id="pone.0058632-Japkowicz1"><label>38</label><mixed-citation publication-type="journal">
<name><surname>Japkowicz</surname><given-names>N</given-names></name>, <name><surname>Stephen</surname><given-names>S</given-names></name> (<year>2002</year>) <article-title>The class Imbalance Problem: A Systematic Study</article-title>. <source>Intelligent Data Analysis</source>
<volume>6</volume>: <fpage>429</fpage>&#x02013;<lpage>450</lpage>.</mixed-citation></ref><ref id="pone.0058632-Mitchell1"><label>39</label><mixed-citation publication-type="journal">
<name><surname>Mitchell</surname><given-names>T</given-names></name>, <name><surname>Hutchinson</surname><given-names>R</given-names></name>, <name><surname>Niculescu</surname><given-names>R</given-names></name>, <name><surname>Pereira</surname><given-names>F</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <etal>et al</etal> (<year>2004</year>) <article-title>Learning to Decode Cognitive States from Brain Images</article-title>. <source>Mach Learn</source>
<volume>57</volume>: <fpage>145</fpage>&#x02013;<lpage>175</lpage>.</mixed-citation></ref><ref id="pone.0058632-DeMartino1"><label>40</label><mixed-citation publication-type="journal">
<name><surname>De Martino</surname><given-names>F</given-names></name>, <name><surname>Valente</surname><given-names>G</given-names></name>, <name><surname>Staeren</surname><given-names>N</given-names></name>, <name><surname>Ashburner</surname><given-names>J</given-names></name>, <name><surname>Goebel</surname><given-names>R</given-names></name>, <etal>et al</etal> (<year>2008</year>) <article-title>Combining multivariate voxel selection and support vector machines for mapping and classification of fMRI spatial patterns</article-title>. <source>Neuroimage</source>
<volume>43</volume>: <fpage>44</fpage>&#x02013;<lpage>58</lpage>.<pub-id pub-id-type="pmid">18672070</pub-id></mixed-citation></ref><ref id="pone.0058632-Cawley1"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Cawley</surname><given-names>GC</given-names></name>, <name><surname>Talbot</surname><given-names>NLC</given-names></name> (<year>2010</year>) <article-title>On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation</article-title>. <source>Journal of Machine Learning Research</source>
<volume>11</volume>: <fpage>2079</fpage>&#x02013;<lpage>2107</lpage>.</mixed-citation></ref><ref id="pone.0058632-Guyon1"><label>42</label><mixed-citation publication-type="journal">
<name><surname>Guyon</surname><given-names>I</given-names></name>, <name><surname>Elisseef</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>An introduction to variable and feature selection</article-title>. <source>Journal of Machine Learning Research</source>
<volume>3</volume>: <fpage>1157</fpage>&#x02013;<lpage>1182</lpage>.</mixed-citation></ref><ref id="pone.0058632-Guyon2"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Guyon</surname><given-names>I</given-names></name>, <name><surname>Weston</surname><given-names>J</given-names></name>, <name><surname>Barnhill</surname><given-names>S</given-names></name>, <name><surname>Vapnik</surname><given-names>V</given-names></name> (<year>2002</year>) <article-title>Gene selection for cancer classification using support vector machines</article-title>. <source>Machine Learning</source>
<volume>46</volume>: <fpage>389</fpage>&#x02013;<lpage>422</lpage>.</mixed-citation></ref><ref id="pone.0058632-Carlson1"><label>44</label><mixed-citation publication-type="journal">
<name><surname>Carlson</surname><given-names>TA</given-names></name>, <name><surname>Schrater</surname><given-names>P</given-names></name>, <name><surname>He</surname><given-names>S</given-names></name> (<year>2003</year>) <article-title>Patterns of activity in the categorical representations of objects</article-title>. <source>J Cogn Neurosci</source>
<volume>15</volume>: <fpage>704</fpage>&#x02013;<lpage>717</lpage>.<pub-id pub-id-type="pmid">12965044</pub-id></mixed-citation></ref><ref id="pone.0058632-Lowry1"><label>45</label><mixed-citation publication-type="other">Lowry R (1999) 1999&#x02013;2007. Concepts and Applications of Inferential Statistics. Available: <ext-link ext-link-type="uri" xlink:href="http://vassarstats.net/textbook/">http://vassarstats.net/textbook/</ext-link>. Accessed 2013 Feb 8.</mixed-citation></ref><ref id="pone.0058632-Cattaneo3"><label>46</label><mixed-citation publication-type="journal">
<name><surname>Cattaneo</surname><given-names>L</given-names></name>, <name><surname>Sandrini</surname><given-names>M</given-names></name>, <name><surname>Schwarzbach</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>State-dependent TMS reveals a hierarchical representation of observed acts in the temporal, parietal, and premotor cortices</article-title>. <source>Cereb Cortex</source>
<volume>20</volume>: <fpage>2252</fpage>&#x02013;<lpage>2258</lpage>.<pub-id pub-id-type="pmid">20051360</pub-id></mixed-citation></ref><ref id="pone.0058632-OToole1"><label>47</label><mixed-citation publication-type="journal">
<name><surname>O'Toole</surname><given-names>AJ</given-names></name>, <name><surname>Jiang</surname><given-names>F</given-names></name>, <name><surname>Abdi</surname><given-names>H</given-names></name>, <name><surname>Penard</surname><given-names>N</given-names></name>, <name><surname>Dunlop</surname><given-names>JP</given-names></name>, <etal>et al</etal> (<year>2007</year>) <article-title>Theoretical, statistical, and practical perspectives on pattern-based classification approaches to the analysis of functional neuroimaging data</article-title>. <source>J Cogn Neurosci</source>
<volume>19</volume>: <fpage>1735</fpage>&#x02013;<lpage>1752</lpage>.<pub-id pub-id-type="pmid">17958478</pub-id></mixed-citation></ref></ref-list></back></article>
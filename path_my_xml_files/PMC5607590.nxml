<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Emerg Themes Epidemiol</journal-id><journal-id journal-id-type="iso-abbrev">Emerg Themes Epidemiol</journal-id><journal-title-group><journal-title>Emerging Themes in Epidemiology</journal-title></journal-title-group><issn pub-type="epub">1742-7622</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28943885</article-id><article-id pub-id-type="pmc">5607590</article-id><article-id pub-id-type="publisher-id">64</article-id><article-id pub-id-type="doi">10.1186/s12982-017-0064-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Analytic Perspective</subject></subj-group></article-categories><title-group><article-title>Decision trees in epidemiological research</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Venkatasubramaniam</surname><given-names>Ashwini</given-names></name><address><email>a.venkatasubramaniam.1@research.gla.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wolfson</surname><given-names>Julian</given-names></name><address><email>julianw@umn.edu</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Mitchell</surname><given-names>Nathan</given-names></name><address><email>mitc0186@umn.edu</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Barnes</surname><given-names>Timothy</given-names></name><address><email>tlbarnes@umn.edu</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>JaKa</surname><given-names>Meghan</given-names></name><address><email>Meghan.JaKa@allina.com</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>French</surname><given-names>Simone</given-names></name><address><email>frenc001@umn.edu</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2193 314X</institution-id><institution-id institution-id-type="GRID">grid.8756.c</institution-id><institution>Urban Big Data Centre, </institution><institution>University of Glasgow, </institution></institution-wrap>7 Lilybank Gardens, Glasgow, G12 8RZ UK </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000000419368657</institution-id><institution-id institution-id-type="GRID">grid.17635.36</institution-id><institution>Division of Biostatistics, </institution><institution>University of Minnesota, Twin Cities, </institution></institution-wrap>A453 Mayo Building, MMC 303, 420 Delaware St SE, Minneapolis, MN 55455 USA </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000000419368657</institution-id><institution-id institution-id-type="GRID">grid.17635.36</institution-id><institution>Division of Epidemiology and Community Health, </institution><institution>University of Minnesota, Twin Cities, </institution></institution-wrap>West Bank Office Building, 1300 South Second St, Suite 300, Minneapolis, MN 55454 USA </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 8739 9261</institution-id><institution-id institution-id-type="GRID">grid.413636.5</institution-id><institution>Division of Applied Research, </institution><institution>Allina Health, </institution></institution-wrap>2925 Chicago Ave, Minneapolis, MN 55407 USA </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>9</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>9</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>14</volume><elocation-id>11</elocation-id><history><date date-type="received"><day>26</day><month>5</month><year>2017</year></date><date date-type="accepted"><day>30</day><month>8</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">In many studies, it is of interest to identify population subgroups that are relatively homogeneous with respect to an outcome. The nature of these subgroups can provide insight into effect mechanisms and suggest targets for tailored interventions. However, identifying relevant subgroups can be challenging with standard statistical methods.</p></sec><sec><title>Main text</title><p id="Par2">We review the literature on decision trees, a family of techniques for partitioning the population, on the basis of covariates, into distinct subgroups who share similar values of an outcome variable. We compare two decision tree methods, the popular Classification and Regression tree (CART) technique and the newer Conditional Inference tree (CTree) technique, assessing their performance in a simulation study and using data from the Box Lunch Study, a randomized controlled trial of a portion size intervention. Both CART and CTree identify homogeneous population subgroups and offer improved prediction accuracy relative to regression-based approaches when subgroups are truly present in the data. An important distinction between CART and CTree is that the latter uses a formal statistical hypothesis testing framework in building decision trees, which simplifies the process of identifying and interpreting the final tree model. We also introduce a novel way to visualize the subgroups defined by decision trees. Our novel graphical visualization provides a more scientifically meaningful characterization of the subgroups identified by decision trees.</p></sec><sec><title>Conclusions</title><p id="Par3">Decision trees are a useful tool for identifying homogeneous subgroups defined by combinations of individual characteristics. While all decision tree techniques generate subgroups, we advocate the use of the newer CTree technique due to its simplicity and ease of interpretation.</p></sec><sec><title>Electronic supplementary material</title><p>The online version of this article (doi:10.1186/s12982-017-0064-4) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Decision trees</kwd><kwd>Subgroup heterogeneity</kwd><kwd>Predictors</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000062</institution-id><institution>National Institute of Diabetes and Digestive and Kidney Diseases</institution></institution-wrap></funding-source><award-id>R01DK 081714</award-id><principal-award-recipient><name><surname>French</surname><given-names>Simone</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2017</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par11">The framing of medical research hypotheses and development of public health interventions often involve the identification of high-risk groups and the effects of individual factors on the relevant outcome [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. For example, the prevalence of obesity in the United States has more than doubled in the past 30&#x000a0;years [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>] and this trend can be associated with a complex combination of factors in the data. However, excessive calorie consumption and inadequate physical activity are not solely responsible for this problem; numerous other factors such as socio-economic differences, demographic characteristics, physical environment, genetics, eating behaviors, etc. also influence the energy intake balance and weight status.</p><p id="Par12">While individual effects can be measured efficiently, characterizing these factors in relation to an outcome of interest can be challenging. Effects of continuous variables (e.g., age) may be non-linear, and vary with other continuous (e.g., years of education) and categorical (e.g., sex) variables. Regression models have long been utilized for prediction and to examine the relationships between covariates and responses of interest. However, their ability to identify interactions between covariates and relevant population subgroups is restricted by the data analyst&#x02019;s decision about how covariates are defined and included in the model. For example, even in the very simple case of partitioning the population into two maximally distinct groups on the basis of a single continuous predictor <italic>X</italic>, one would need to fit separate models with categorical predictors indicating that <italic>X</italic> exceeded a particular threshold value, for many different threshold values. Since many candidate models may have to be investigated in this somewhat <italic>ad hoc</italic> manner, Type I error may be inflated.</p><p id="Par13">The main goal of this paper is to introduce and describe the family of statistical methods known as decision trees, a family which is particularly well-suited to exploring potentially non-linear relationships between variables and identifying population subgroups who are homogeneous with respect to outcomes. Decision trees have been utilized to identify joint effects of air pollutants [<xref ref-type="bibr" rid="CR5">5</xref>], generate a realistic research hypothesis for tuberculosis diagnosis [<xref ref-type="bibr" rid="CR6">6</xref>], and recognize high-risk subgroups to aid tobacco control [<xref ref-type="bibr" rid="CR7">7</xref>]. After providing a brief overview of decision trees, we introduce a novel data visualization technique for summarizing the subgroups identified by the trees. Next, we explore the differences between a commonly used technique for building decision trees, CART, and the conditional inference tree (CTree) approach which has not been widely used in epidemiological applications. Based on simulation results and analyses of real data, we discuss the relative strengths and weaknesses of these two approaches and the resulting implications for data analysis.</p><sec id="Sec2"><title>Application: the Box Lunch Study</title><p id="Par14">Throughout this paper, we present examples and analyses based on variables collected in the Box Lunch Study (BLS), a randomized controlled trial designed to evaluate the effect of portion size availability on caloric intake and weight gain in a free living sample of working adults. The main randomized comparisons of the BLS (along with details of ethics approval and consent information) have been reported elsewhere [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. However, the data also provides the opportunity to explore associations between outcomes and individual characteristics. Available covariates include demographic (e.g. age, gender, race, height, education), lifestyle (e.g. smoking status, physical activity levels), and psycho-social measures (e.g. frequency of self-weighing, degree of satisfaction with current weight). Responses to the Three Factor Eating Questionnaire (TFEQ) [<xref ref-type="bibr" rid="CR10">10</xref>] quantifying the constructs of hunger, disinhibtion, and restraint were also recorded. The BLS also collected data on some novel, laboratory-based psycho-social measures that had not previously been measured in a randomized trial setting such as the relative reinforcement of food (rrvf), liking and wanting.</p></sec><sec id="Sec3"><title>Software availability</title><p id="Par15">The analyses, simulations, and visualizations presented in this paper were all produced using the freely-available statistical software R [<xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref>]. External packages and functions used are referenced in the text. Code for our novel visualization is available at&#x000a0;<ext-link ext-link-type="uri" xlink:href="https://github.com/AshwiniKV/visTree">https://github.com/AshwiniKV/visTree</ext-link> and for reproducing our example trees and our simulation study at <ext-link ext-link-type="uri" xlink:href="https://github.com/AshwiniKV/obesity%5fdecision%5ftrees">https://github.com/AshwiniKV/obesity_decision_trees</ext-link>.</p></sec></sec><sec id="Sec4"><title>Methods</title><sec id="Sec5"><title>A brief introduction to decision trees</title><p id="Par16">A decision tree is a statistical model for predicting an outcome on the basis of covariates. The model implies a <italic>prediction rule</italic> defining disjoint subsets of the data, i.e., population subgroups that are defined hierarchically via a sequence of binary partitions of the data. The set of hierarchical binary partitions can be represented as a tree, hence the name. The predicted outcome in each subset is determined by averaging the outcomes of the individuals in the subset. The goal is to create a prediction rule (i.e., a tree) which minimizes a <italic>loss function</italic> that measures the discrepancy between the predicted and true values.</p><p id="Par17">Decision trees have several components, as illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> which summarizes the association between the outcome of daily caloric intake and hunger, dis-inhibition, restrained eating, relative reinforcement, liking, and wanting. <italic>Nodes</italic> contain subsets of the observations; the <italic>root node</italic> of a tree (labeled with a &#x02018;1&#x02019; in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>) contains all observations (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=226$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>226</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq1.gif"/></alternatives></inline-formula> in the Box Lunch Study). The key step in algorithms for constructing decision trees is the <italic>splitting step</italic>, where the decision is made on how to partition the sample (or sub-sample, for nodes below the root) into two disjoint subsets according to covariate values. The splits below a node are represented as <italic>branches</italic> in the tree. Splitting continues recursively down each branch until a <italic>stopping rule</italic> is triggered. A node where the stopping rule is satisfied is referred to as a <italic>leaf</italic> or a <italic>terminal node</italic>. Taken together, the terminal nodes define a disjoint partition of the original sample; each observation belongs to exactly one terminal node, depending on its covariates. A prediction for a new observation&#x02019;s outcome is made by determining (based on that observation&#x02019;s covariates) which leaf it belongs to, then combining the outcomes of the existing observations within that leaf to get a predicted value.<fig id="Fig1"><label>Fig. 1</label><caption><p>Decision tree showing the association between daily caloric intake (in kcal/day) and hunger, dis-inhibition, restrained eating, relative reinforcement, liking, and wanting. All measures are obtained at baseline in the Box Lunch Study</p></caption><graphic xlink:href="12982_2017_64_Fig1_HTML" id="MO1"/></fig>
</p><p id="Par18">In Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, both the outcome and predictors are standardized column-wise to have mean zero and variance equal to one. Standardization puts all the predictors on the same scale, which may be helpful when, as here, some of the predictors (e.g., rrvf, liking, and wanting) are measures that do not have universally agreed-upon units or methods of measurement<xref ref-type="fn" rid="Fn1">1</xref>. For example, in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, the root node with a label &#x02018;1&#x02019; as node ID partitions the population into two groups: (1) subjects whose hunger measurement is less than or equal to 1.69 standard deviations above the mean hunger, and (2) subjects whose hunger is greater than 1.69 standard deviations above the mean. Standardizing the outcome allows for a similar interpretation of the leaf nodes: the leaf with node ID&#x000a0;=&#x000a0;6 has a value of 0.26, indicating that the mean 24-h energy intake for the subjects contained in this node (i.e., those with hunger <inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\le }1.69$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mn>1.69</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq3.gif"/></alternatives></inline-formula>, liking <inline-formula id="IEq4"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${&#x0003e;}-0.28$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mo>-</mml:mo><mml:mn>0.28</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq4.gif"/></alternatives></inline-formula>, and rrvf <inline-formula id="IEq5"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${&#x0003e;}-1.26$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mo>-</mml:mo><mml:mn>1.26</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq5.gif"/></alternatives></inline-formula>) is 0.26 standard deviations above the overall mean of 24-h energy intake. A mean of 0.26 standard deviations of 24-h energy intake corresponds to a value of 2190 kilo-calories<xref ref-type="fn" rid="Fn2">2</xref>.</p></sec><sec id="Sec6"><title>Adjusting for covariates</title><p id="Par21">Often, factors such as age, sex, and education level may influence the outcome of interest and be associated with other predictors (i.e., they are confounders), but their effects are not of primary interest. In linear regression, it is common practice to adjust for such variables by including them in the regression model.</p><p id="Par22">In decision trees, an analogue to covariate adjustment involves building the tree using <italic>adjusted residuals</italic>, i.e., residuals from a regression model containing the confounders. To be precise, suppose that one wished to assess the effects of the predictors described in the previous sections, adjusting for age, sex, and BMI. Letting <italic>Y</italic> denote 24-h energy intake, one would first fit the model<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y = \beta _0 + \beta _1 \ {\hbox {Age}} + \beta _2 \ {\hbox {Sex}} + \beta _3 \ {\hbox {BMI}} + \epsilon$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="4pt"/><mml:mtext>Age</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="4pt"/><mml:mtext>Sex</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mspace width="4pt"/><mml:mtext>BMI</mml:mtext><mml:mo>+</mml:mo><mml:mi mathvariant="italic">&#x003f5;</mml:mi></mml:mrow></mml:math><graphic xlink:href="12982_2017_64_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>Given coefficient estimates <inline-formula id="IEq7"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\beta }}_0, {\hat{\beta }}_1, {\hat{\beta }}_2$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq7.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq8"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\beta }}_3$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq8.gif"/></alternatives></inline-formula>, the age-, sex, and BMI-adjusted residuals for 24-h energy intake, <inline-formula id="IEq9"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y^*$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq9.gif"/></alternatives></inline-formula>, are<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y^* = Y - {\hat{\beta }}_0 - {\hat{\beta }}_1 \ {\hbox {Age}} - \hat{\beta }_2 \ {\hbox {Sex}} - {\hat{\beta }}_3 \ {\hbox {BMI}}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mspace width="4pt"/><mml:mtext>Age</mml:mtext><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mspace width="4pt"/><mml:mtext>Sex</mml:mtext><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">&#x003b2;</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub><mml:mspace width="4pt"/><mml:mtext>BMI</mml:mtext></mml:mrow></mml:math><graphic xlink:href="12982_2017_64_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>The residuals <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y^*$$\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mi>Y</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq10.gif"/></alternatives></inline-formula> can then be used as the outcome in a regression tree including the predictors of interest. This adjusted residuals technique can be easily applied using standard software.</p></sec><sec id="Sec7"><title>Visualizing subgroups in decision trees</title><p id="Par23">One of the most attractive features of decision trees is that they partition a population sample into subgroups with distinct means. However, the typical display of a decision tree (e.g., Figs.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>) does not always allow researchers to easily characterize these subgroups. The problem is particularly acute if some of the predictor variables do not have an interpretable scale built on established norms: the relative reinforcing value of food and degree of liking/wanting measured in the Box Lunch Study are novel and have not yet been widely used, so a standard unit of measurement has not yet been established.<fig id="Fig2"><label>Fig. 2</label><caption><p>Regression tree showing the association between Energy kcal/day and hunger, dis-inhibition, restrained eating, relative reinforcement of food, liking, and wanting</p></caption><graphic xlink:href="12982_2017_64_Fig2_HTML" id="MO4"/></fig>
</p><p id="Par24">To address this limitation, we developed a software tool for visualizing the composition of subgroups defined by decision trees. The visualization consists of a grid of plots, one corresponding to each terminal node (i.e., population subgroup). In Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, each plot in this grid of plots corresponds to one of the four terminal nodes (population subgroups) in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, i.e. nodes 3, 5, 6, and 7. In the background of each plot is a histogram summarizing the distribution of the outcome variable (here, 24-h energy intake) for the individuals in the terminal node/subgroup. For example, the top left plot in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> shows a distribution of (standardized) 24-h energy intake that is right-skewed. The numbers along the x-axis are the average 24-h energy intake within each individual bin of the histogram. The mean of the values contained in the bins of the histogram are presented for each individual bin. The vertical line shows the overall mean of the subgroup; the mean and subgroup size are shown in the plot title. Overlaid on the background are colored bars; the length and position of the bars represent the set of predictor values, on the percentile scale, which define the subgroup. The subgroup corresponding to the top left plot of Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> is defined by liking values below &#x02212;0.28, which represents the 39th population percentile and hunger values that are below 1.69, which represents the 91st percentile.<fig id="Fig3"><label>Fig. 3</label><caption><p>Graphical visualization of the conditional inference tree in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, where the visualization consists of a grid of plots and each plot corresponds to a terminal node</p></caption><graphic xlink:href="12982_2017_64_Fig3_HTML" id="MO5"/></fig>
</p><p id="Par25">This visualization summarizes, at a glance, the characteristics of the groups determined by the regression tree. For instance, in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, the four groups could be characterized as:<list list-type="bullet"><list-item><p id="Par26">
<italic>Group 1&#x000a0;</italic>(<inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N = 86$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>86</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq11.gif"/></alternatives></inline-formula>): Moderate to low liking, all but very high hunger. This group has below-average energy intake (standardized mean = &#x02212;0.46).</p></list-item><list-item><p id="Par27">
<italic>Group 2&#x000a0;</italic>(<inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N = 22$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>22</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq12.gif"/></alternatives></inline-formula>): Moderate to high liking, very low relative reinforcing value of food, all but very high hunger. This group has moderate to low energy intake.</p></list-item><list-item><p id="Par28">
<italic>Group 3&#x000a0;</italic>(<inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N = 104$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>104</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq13.gif"/></alternatives></inline-formula>): Moderate to high liking, all but very low relative reinforcing value of food, all but very high hunger. This group has moderate to high energy intake.</p></list-item><list-item><p id="Par29">
<italic>Group 4&#x000a0;</italic>(<inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N = 14$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq14.gif"/></alternatives></inline-formula>): Very high hunger. This group has very high energy intake.</p></list-item></list>The prediction rules defining these subgroups provide insight into the individual characteristics that can affect the outcome, and can be used to define categorical variables that could yield more meaningful and interpretable comparisons in future analyses.</p></sec><sec id="Sec8"><title>Methods for building decision trees</title><sec id="Sec9"><title>Classification and regression trees (CART)</title><p id="Par30">The most popular method for constructing decision trees, known as CART (Classification and Regression Trees) was introduced by Breiman [<xref ref-type="bibr" rid="CR18">18</xref>]. In a CART (e.g., Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>), a split is sought to minimize the <italic>relative sum of squared errors</italic> in the two partitions resulting from the split. The search for splits in CART takes place across two dimensions simultaneously: the covariate to split on and splitting point within that covariate. In other words, the splitting step in CART is greedy: the best split is sought across all covariates and candidate split points for those covariates. For binary and categorical covariates, all possible values are considered as possible split points; for continuous covariates, an equally-spaced grid covering the range of possible values is usually considered.</p><p id="Par31">Because it searches over all possible splits on all covariates, CART is vulnerable to the so-called <italic>biased variable selection problem</italic>; there are more potential &#x0201c;good&#x0201d; splits on a continuous-valued covariate (or one with a large number of distinct values) than on a binary covariate. This tendency of CART to favor variables with many possible splits has been described in [<xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref>] and [<xref ref-type="bibr" rid="CR21">21</xref>].</p><p id="Par32">Furthermore, the nature of the splitting process makes it difficult to describe the statistical properties of any particular split. For instance, CART is not concerned with the notion of Type I error since it does not control the rate at which a regression tree identifies population subgroups when there is truly no heterogeneity in the mean of the outcome.</p></sec><sec id="Sec10"><title>Conditional inference trees (CTree)</title><p id="Par33">As an alternative to CART, Hothorn et al. [<xref ref-type="bibr" rid="CR22">22</xref>] proposed the conditional inference tree (CTree). Unlike CART, CTree (e.g., Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>) separates the splitting process into two distinct steps. The first step is to determine the variable to split on based on a measure of association between each covariate and the outcome of interest. Then, after the splitting variable has been determined, the best split point for that variable is calculated.</p><p id="Par34">In contrast to CART, CTree follows formal statistical inference procedures in each splitting step. The association between each covariate and the outcome is quantified using the coefficient in a regression model (linear regression for continuous outcomes and other suitable regression models for other outcome types), and a node is only chosen to be split if there is sufficient evidence to reject the <italic>global null hypothesis</italic>, i.e., the hypothesis that none of the covariates has a univariate association with the outcome. If the global null hypothesis is rejected, then the covariate that displays the strongest association with the outcome of interest is selected as a candidate for splitting. If the minimum p-value is larger than the multiplicity adjusted significance threshold, then no variable is selected for splitting and the node is declared a terminal node. Note that, despite its name, CTree bases splitting decisions on marginal (i.e., univariate) regression models; the &#x0201c;conditional&#x0201d; refers to the fact that, following the initial split, subsequent inference takes place within subgroups, i.e., conditional on subgroup membership.</p></sec><sec id="Sec11"><title>Stopping rules</title><p id="Par35">In both CART and CTree, splitting continues until a <italic>stopping rule</italic> triggers. In CART, splitting stops when the relative reduction in error resulting from the best split falls below a pre-specified threshold known as the <italic>complexity parameter</italic>. Typical values of this parameter are in the range of 0.001&#x02013;0.05. To prevent overfitting, it is common practice to construct trees for a sequence of values of this parameter, and select the final value by minimizing prediction error estimated by cross-validation or on an independent test set. This process is referred to as <italic>pruning</italic> [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. A slightly more conservative stopping rule sets the final complexity parameter to the value which yields a prediction error one standard deviation larger than the minimum estimated by cross-validation or on an independent test set. This is known as the <italic>1-SE rule</italic>. As noted above, CTree&#x02019;s stopping rule is simple: splitting stops if the global null hypothesis is not rejected at the pre-determined, multiplicity adjusted level of significance.</p></sec></sec><sec id="Sec12"><title>Comparing CART and CTree: a simulation study</title><p id="Par36">In this section, we describe simulated and real data and develop scenarios within a simulation study to highlight distinctions between CART and CTree. We also compare their predictive performance to standard regression models in a variety of settings and perform simulations utilizing the R statistical software package, version 3.3.0&#x000a0;[<xref ref-type="bibr" rid="CR11">11</xref>]. The results of this study are presented in &#x0201c;<xref rid="Sec16" ref-type="sec">Results</xref>&#x0201d; section. The CART algorithm was implemented using the rpart package&#x000a0;[<xref ref-type="bibr" rid="CR13">13</xref>], while the CTree was implemented via the partykit package&#x000a0;[<xref ref-type="bibr" rid="CR12">12</xref>]. We considered a variety of scenarios where we varied the data-generating function, covariate type (categorical vs. continuous), the sparsity (proportion of variables predicting the outcome), the total sample size, and the complexity parameter for CART.</p><p id="Par37">For all scenarios other than the one where sample size was varied, the sample size was fixed at 250 and in all scenarios trees were constructed using six covariates. Continuous outcomes were generated as independent <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N(\eta ,1)$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">&#x003b7;</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq15.gif"/></alternatives></inline-formula> with linear predictor <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta$$\end{document}</tex-math><mml:math id="M32"><mml:mi mathvariant="italic">&#x003b7;</mml:mi></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq16.gif"/></alternatives></inline-formula> varying across scenarios as described below. Continuous covariates were generated from independent Normal distributions with mean zero and unit variance; binary covariates were generated as independent Bernoulli(<inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 0.5$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq17.gif"/></alternatives></inline-formula>). Pruning for CART was carried out using both the minimum and the 1-SE rule, with the 1-SE rule being implemented using the DMwR package&#x000a0;[<xref ref-type="bibr" rid="CR14">14</xref>]. The tree-generating functions rpart (for CART) and ctree (for CTrees) were applied with arguments specifying a minimum of 20 observations for a node to be considered for splitting and a minimum of 7 observations in a terminal node. The complexity parameter for CART was held at the default value of 0.01. The level of significance in the CTree was held at the default value of <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha = 0.05$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi mathvariant="italic">&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq18.gif"/></alternatives></inline-formula>.</p><p id="Par38">For each scenario, 10,000 simulations were performed, where in each simulation a training dataset was simulated and used to construct the trees, and tree performance was evaluated on an independently generated test dataset. Prediction error and tree complexity were summarized respectively via the mean squared error (MSE) and the number of terminal nodes (equal to the total number of splits in the tree, plus one).</p><sec id="Sec13"><title>Effect of the data generating process</title><p id="Par39">Decision trees perform well in situations where the underlying population is partitioned into a relatively small number of subgroups with distinct means. However, they are less suited to scenarios in which the outcome varies continuously with covariate values.</p><p id="Par40">We started by generating independent normally distributed outcomes according to a pre-specified tree structure, i.e., set of splits to seven terminal nodes with mean values (&#x02212;1.88, &#x02212;0.30, &#x02212;0.31, 0.25, &#x02212;0.09, 2.23, 1.35), and unit variance. The candidate covariates for this tree included six continuous covariates (<inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_1, \dots , X_6$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq19.gif"/></alternatives></inline-formula>), mimicking the six covariates considered in the introductory examples above. This CTree is grown to consist of seven terminal nodes with splits at hunger, liking, rrvf, and disinhibition.</p><p id="Par41">In a different scenario, continuous responses are generated from <inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N(\eta ,1)$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">&#x003b7;</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq20.gif"/></alternatives></inline-formula> where <inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta$$\end{document}</tex-math><mml:math id="M42"><mml:mi mathvariant="italic">&#x003b7;</mml:mi></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq21.gif"/></alternatives></inline-formula> follows a regression model defined as<disp-formula id="Equ3"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta = 1.5 X_1 + 1.25 X_2 + 1 X_3 + 0.85 X_4 + 0.75 X_5 + 0 X_6$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:mi mathvariant="italic">&#x003b7;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1.25</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0.85</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0.75</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math><graphic xlink:href="12982_2017_64_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>and <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_1 \dots X_6$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq22.gif"/></alternatives></inline-formula> are simulated as independent normally distributed continuous covariates. We also generated a hybrid model from normally distributed data with unit variance according to <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N(\eta ,1)$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">&#x003b7;</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq23.gif"/></alternatives></inline-formula> with<disp-formula id="Equ4"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \eta &#x00026;\,= \,0.5 X_1 + 0.45 X_2 + 0.3 X_3 + 1.5\,{\mathbb{1}}(X_1 \le 0, X_2&#x0003e; 0, X_3 \le 0) +\\&#x00026;\qquad 0.25\,{\mathbb{1}}(X_1 \le 0, X_3 &#x0003e;0) + 0.14\,{\mathbb{1}} ( X_1&#x0003e; 0, X_2&#x0003e;  0), \end{aligned}$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi mathvariant="italic">&#x003b7;</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="0.166667em"/><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:mn>0.5</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0.45</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0.3</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1.5</mml:mn><mml:mspace width="0.166667em"/><mml:mn mathvariant="double-struck">1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mn>0.25</mml:mn><mml:mspace width="0.166667em"/><mml:mn mathvariant="double-struck">1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.14</mml:mn><mml:mspace width="0.166667em"/><mml:mn mathvariant="double-struck">1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12982_2017_64_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_1, X_2$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq24.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq25"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_3$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq25.gif"/></alternatives></inline-formula> are simulated as independent normally distributed continuous covariates and are utilized to form distinct subgroups represented by three different indicator functions, indicated by <inline-formula id="IEq26"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf 1$$\end{document}</tex-math><mml:math id="M56"><mml:mn mathvariant="bold">1</mml:mn></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq26.gif"/></alternatives></inline-formula>. This hybrid model includes main effects of three continuous covariates along with interaction terms and subgroup indicators constructed from these covariates.</p></sec><sec id="Sec14"><title>Type I error</title><p id="Par42">We also evaluated the Type I error rate of the different tree-building algorithms. For a tree, we say that a Type I error occurs if a tree splits on a variable that has no association with the outcome. To evaluate Type I error, we generated six independent and normally distributed continuous covariates and a response with mean zero and unit variance, unrelated to the covariates.</p></sec><sec id="Sec15"><title>Effect of sample size</title><p id="Par43">Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> summarize the predictive performance of tree types as sample size changes. For each sample size <inline-formula id="IEq27"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n = 30, 250, 500, 1000, 3000$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mn>3000</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq27.gif"/></alternatives></inline-formula>, and 5000 we generated six covariates and continuous responses were generated from a N(<inline-formula id="IEq28"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta , 1$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi mathvariant="italic">&#x003b7;</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq28.gif"/></alternatives></inline-formula>) with <inline-formula id="IEq29"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta$$\end{document}</tex-math><mml:math id="M62"><mml:mi mathvariant="italic">&#x003b7;</mml:mi></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq29.gif"/></alternatives></inline-formula> following a linear regression model:<disp-formula id="Equ5"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta = 1.5 X_1 + 1.25 X_2 + 1 X_3 + 0.85 X_4 + 0.75 X_5 + 0 X_6.$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:mi mathvariant="italic">&#x003b7;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1.25</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0.85</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0.75</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>0</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="12982_2017_64_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>
<fig id="Fig4"><label>Fig. 4</label><caption><p>Prediction error and tree size for different sample sizes in log-scale (<inline-formula id="IEq30"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n = 30, 250, 500, 1000, 3000, 5000$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mn>3000</mml:mn><mml:mo>,</mml:mo><mml:mn>5000</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq30.gif"/></alternatives></inline-formula>) when data is generated from a linear regression model with continuous covariates</p></caption><graphic xlink:href="12982_2017_64_Fig4_HTML" id="MO9"/></fig>
</p></sec></sec></sec><sec id="Sec16"><title>Results</title><sec id="Sec17"><title>Comparing CART and CTree: a simulation study</title><sec id="Sec18"><title>Effect of the data generating process</title><p id="Par44">The set of <italic>Tree</italic> results for the model that generates data from a tree structure in the first five rows of Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> summarizes the estimated prediction error (MSE) and tree complexity (mean, 20th, and 80th percentile number of terminal nodes) of CTree on the generated data with a comparison to three other tree algorithms: the unpruned CART, CART with two types of pruning, and with the results from a linear regression model. As expected, all the tree-based techniques have lower MSE than linear regression. In this case, CTree produces trees with a similar number of terminal nodes to the CART pruned with the 1-SE rule but lower number of nodes when compared to the regular pruned CART. The CTree and both types of pruned CARTs have results for decision trees with 3&#x02013;4 terminal nodes, in contrast to the generated tree structure with seven terminal nodes. This is likely due to the fact that our simulated tree data contained several nodes with very similar means.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Aggregated simulation results that describe the effect of multiple types of data generating processes</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">True model</th><th align="left" rowspan="2">Type</th><th align="left" colspan="2">MSE</th><th align="left" colspan="3">Terminal nodes</th></tr><tr><th align="left">Mean</th><th align="left">SD</th><th align="left">Mean</th><th align="left">20th</th><th align="left">80th</th></tr></thead><tbody><tr><td align="left" rowspan="5">Tree</td><td align="left">CART</td><td char="." align="char">1.26</td><td char="." align="char">0.151</td><td char="." align="char">7.01</td><td char="." align="char">6</td><td char="." align="char">8</td></tr><tr><td align="left">Pruned CART</td><td char="." align="char">1.22</td><td char="." align="char">0.137</td><td char="." align="char">4.27</td><td char="." align="char">3</td><td char="." align="char">5</td></tr><tr><td align="left">Pruned CART (1-SE)</td><td char="." align="char">1.25</td><td char="." align="char">0.139</td><td char="." align="char">3.31</td><td char="." align="char">3</td><td char="." align="char">4</td></tr><tr><td align="left">CTree</td><td char="." align="char">1.27</td><td char="." align="char">0.154</td><td char="." align="char">3.72</td><td char="." align="char">3</td><td char="." align="char">4</td></tr><tr><td align="left">Linear regression</td><td char="." align="char">2.04</td><td char="." align="char">0.179</td><td char="." align="char"/><td char="." align="char"/><td char="." align="char"/></tr><tr><td align="left" rowspan="5">Regression</td><td align="left">CART</td><td char="." align="char">4.12</td><td char="." align="char">0.413</td><td char="." align="char">15.24</td><td char="." align="char">14</td><td char="." align="char">16</td></tr><tr><td align="left">Pruned CART</td><td char="." align="char">4.19</td><td char="." align="char">0.442</td><td char="." align="char">13.97</td><td char="." align="char">12</td><td char="." align="char">16</td></tr><tr><td align="left">Pruned CART (1-SE)</td><td char="." align="char">4.55</td><td char="." align="char">0.509</td><td char="." align="char">8.66</td><td char="." align="char">6</td><td char="." align="char">11</td></tr><tr><td align="left">CTree</td><td char="." align="char">4.14</td><td char="." align="char">0.409</td><td char="." align="char">13.96</td><td char="." align="char">13</td><td char="." align="char">15</td></tr><tr><td align="left">Linear regression</td><td char="." align="char">1.03</td><td char="." align="char">0.093</td><td char="." align="char"/><td char="." align="char"/><td char="." align="char"/></tr><tr><td align="left" rowspan="5">Hybrid</td><td align="left">CART</td><td char="." align="char">1.39</td><td char="." align="char">0.138</td><td char="." align="char">13.1</td><td char="." align="char">11</td><td char="." align="char">15</td></tr><tr><td align="left">Pruned CART</td><td char="." align="char">1.37</td><td char="." align="char">0.131</td><td char="." align="char">5.96</td><td char="." align="char">3</td><td char="." align="char">9</td></tr><tr><td align="left">Pruned CART (1-SE)</td><td char="." align="char">1.39</td><td char="." align="char">0.133</td><td char="." align="char">2.69</td><td char="." align="char">2</td><td char="." align="char">3</td></tr><tr><td align="left">CTree</td><td char="." align="char">1.34</td><td char="." align="char">0.126</td><td char="." align="char">5.42</td><td char="." align="char">4</td><td char="." align="char">6</td></tr><tr><td align="left">Linear regression</td><td char="." align="char">1.17</td><td char="." align="char">0.106</td><td char="." align="char"/><td char="." align="char"/><td char="." align="char"/></tr></tbody></table><table-wrap-foot><p>These sources of data include a tree structure, a regression model and a hybrid model that combines the two structures</p></table-wrap-foot></table-wrap>
</p><p id="Par45">The second set of results in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> (<italic>Regression</italic>) summarize performance for all four model types. The (correctly specified) linear regression model has far better predictive performance than the tree models. Interestingly, CTree has better predictive accuracy than the pruned versions of CART, a result which agrees with the findings of Schaffer [<xref ref-type="bibr" rid="CR25">25</xref>] that pruning does not necessarily improve predictive accuracy, particularly when there are many (here, infinitely many) subgroups.</p><p id="Par46">For the <italic>hybrid</italic> scenario when data is generated from the defined hybrid model, we compare the performance of the trees to a partially misspecified linear regression model containing only the main effect terms for the continuous covariates and the results in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> show that predictive accuracies are relatively similar.</p></sec><sec id="Sec19"><title>Type I error</title><p id="Par47">The results are presented in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. We found that the unpruned CART algorithm continues to split and grow unlike the pruned CARTs and CTree. CARTs pruned using a 1-SE rule are rather conservative with a very low Type I error while the pruned CART and CTree have Type I errors that are closer to 0.05. As noted below, explicit control of the Type I error rate is an advantage of the CTree approach.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Aggregated results of simulations that evaluate Type I error of different tree building algorithms</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Type</th><th align="left" colspan="2">MSE</th><th align="left">Type I error</th></tr><tr><th align="left">Mean</th><th align="left">SD</th><th align="left">Mean</th></tr></thead><tbody><tr><td align="left">CART</td><td char="." align="char">0.65</td><td char="." align="char">0.07</td><td char="." align="char">1</td></tr><tr><td align="left">Pruned CART</td><td char="." align="char">0.99</td><td char="." align="char">0.091</td><td char="." align="char">0.0559</td></tr><tr><td align="left">Pruned CART (1-SE)</td><td char="." align="char">1</td><td char="." align="char">0.089</td><td char="." align="char">0.0003</td></tr><tr><td align="left">CTree</td><td char="." align="char">0.99</td><td char="." align="char">0.089</td><td char="." align="char">0.0513</td></tr><tr><td align="left">Linear regression</td><td char="." align="char">0.97</td><td char="." align="char">0.088</td><td char="." align="char"/></tr></tbody></table></table-wrap>
</p></sec><sec id="Sec20"><title>Effect of sample size</title><p id="Par48">We observe in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> that as sample size increases, the MSE of CTree continues to improve while that of the CART variants levels off beyond <inline-formula id="IEq31"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=500$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq31.gif"/></alternatives></inline-formula>. The reason for this behavior is that CART&#x02019;s stopping rules are based on a complexity parameter, which sets a lower bound for improvement in model fit which is insensitive to sample size. In the rpart package, the default complexity parameter value is 0.01, so splitting stops if no split improves model fit by at least 1%. In this setting, the covariates have continuous linear effects, which implies an infinite number of population subgroups. Hence, most splits will yield small improvements in model fit, and CART variants will &#x0201c;stop too soon&#x0201d; and have poor predictive performance. In contrast, the stopping criterion for the CTree is based on <italic>p</italic> values, and maintaining a fixed <italic>p</italic> value threshold with increasing sample size allows splits associated with smaller and smaller effect sizes to be represented in the tree.</p></sec></sec><sec id="Sec21"><title>Application</title><p id="Par49">We illustrate the application of decision trees to the Box Lunch Study by comparing a linear regression model and decision tree that seek to predict 24-h energy intake (in kcal/day) using a set of 25 covariates measured at baseline. These prediction models were built on the covariates introduced in &#x0201c;<xref rid="Sec2" ref-type="sec">Application: the Box Lunch Study</xref>&#x0201d; section such as restrained eating, rrvf, liking as well as other covariates that record demographic characteristics including age, sex, and BMI. Other covariates included were psycho-social measures such as &#x0201c;Influence of weight on ability to judge personal self&#x0201d;, &#x0201c;Ability to limit food intake to control weight (days/month)&#x0201d;, and &#x0201c;Frequency of weighing oneself&#x0201d;.</p><p id="Par50">To provide a baseline for comparison, we present results from a linear regression model in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. The covariates listed are those selected using backward elimination with the AIC. While there are many significant covariates in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, this linear regression does not provide any information about potential interactions nor does it identify particular population subgroups that share similar values of the outcome.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Linear regression output for modeling 24-h energy intake using a &#x0201c;suitable&#x0201d; set of predictors</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Estimate</th><th align="left">SE</th><th align="left">
<italic>t</italic> value</th><th align="left">Pr(<inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${&#x0003e;}|\hbox {t}|$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mtext>t</mml:mtext><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq32.gif"/></alternatives></inline-formula>)</th></tr></thead><tbody><tr><td align="left">(Intercept)</td><td align="left">1279.36</td><td char="." align="char">211.78</td><td char="." align="char">6.04</td><td align="left">&#x0003c;0.001***</td></tr><tr><td align="left">Sex: male</td><td align="left">378.03</td><td char="." align="char">66.30</td><td char="." align="char">5.70</td><td align="left">&#x0003c;0.001***</td></tr><tr><td align="left">Body mass index</td><td align="left">16.68</td><td char="." align="char">6.96</td><td char="." align="char">2.40</td><td align="left">0.017*</td></tr><tr><td align="left">Snack-energy kcal/day</td><td align="left">1.29</td><td char="." align="char">0.12</td><td char="." align="char">10.76</td><td align="left">&#x0003c;0.001***</td></tr><tr><td align="left">Fruit/vegetable svg/day</td><td align="left">38.84</td><td char="." align="char">14.94</td><td char="." align="char">2.60</td><td align="left">0.010**</td></tr><tr><td align="left">Sugar-sweetened beverage svg/day</td><td align="left">114.20</td><td char="." align="char">30.3234</td><td char="." align="char">3.77</td><td align="left">&#x0003c;0.001***</td></tr><tr><td align="left">Contour drawing rating scale-body dissatisfaction [1&#x02013;9]</td><td align="left">&#x02212;48.44</td><td char="." align="char">26.2195</td><td char="." align="char">&#x02212;1.85</td><td align="left">0.066</td></tr><tr><td align="left" colspan="5">Frequency of self-weigh</td></tr><tr><td align="left">&#x000a0;Never</td><td align="left" colspan="4">(Ref)</td></tr><tr><td align="left">&#x000a0;About once a year or less</td><td align="left">&#x02212;405.34</td><td char="." align="char">145.47</td><td char="." align="char">&#x02212;2.79</td><td align="left">0.006**</td></tr><tr><td align="left">&#x000a0;Every couple of months</td><td align="left">&#x02212;247.32</td><td char="." align="char">137.55</td><td char="." align="char">&#x02212;1.80</td><td align="left">0.074</td></tr><tr><td align="left">&#x000a0;Every month</td><td align="left">&#x02212;374.43</td><td char="." align="char">147.96</td><td char="." align="char">&#x02212;2.53</td><td align="left">0.012*</td></tr><tr><td align="left">&#x000a0;Every week</td><td align="left">&#x02212;414.77</td><td char="." align="char">138.67</td><td char="." align="char">&#x02212;2.99</td><td align="left">0.003**</td></tr><tr><td align="left">&#x000a0;Every day</td><td align="left">&#x02212;450.17</td><td char="." align="char">166.89</td><td char="." align="char">&#x02212;2.70</td><td align="left">0.008**</td></tr><tr><td align="left" colspan="5">Fast food frequency</td></tr><tr><td align="left">&#x000a0;Never</td><td align="left" colspan="4">(Ref)</td></tr><tr><td align="left">&#x000a0;1&#x02013;3 times last month</td><td align="left">14.13</td><td char="." align="char">77.01</td><td char="." align="char">0.18</td><td align="left">0.855</td></tr><tr><td align="left">&#x000a0;1&#x02013;2 times per week</td><td align="left">35.63</td><td char="." align="char">95.42</td><td char="." align="char">0.37</td><td align="left">0.709</td></tr><tr><td align="left">&#x000a0;3&#x02013;4 times per week</td><td align="left">&#x02212;187.55</td><td char="." align="char">204.63</td><td char="." align="char">&#x02212;0.92</td><td align="left">0.360</td></tr><tr><td align="left">&#x000a0;5&#x02013;6 times per week</td><td align="left">&#x02212;235.81</td><td char="." align="char">237.61</td><td char="." align="char">&#x02212;0.99</td><td align="left">0.322</td></tr><tr><td align="left">&#x000a0;7 or more times per week</td><td align="left">738.04</td><td char="." align="char">238.35</td><td char="." align="char">3.10</td><td align="left">0.002**</td></tr><tr><td align="left">Hunger</td><td align="left">32.52</td><td char="." align="char">10.15</td><td char="." align="char">3.20</td><td align="left">0.002**</td></tr><tr><td align="left">Wanting</td><td align="left">2.88</td><td char="." align="char">0.85</td><td char="." align="char">3.40</td><td align="left">&#x0003c;0.001***</td></tr></tbody></table><table-wrap-foot><p>This &#x0201c;suitable&#x0201d; set of predictors is chosen using a backward elimination process, such that the AIC for the relevant model is minimized</p></table-wrap-foot></table-wrap>
</p><p id="Par51">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows a conditional inference tree to predict total energy intake, adjusted for age, sex, and BMI, from 22 baseline covariates. The corresponding CART regression tree is provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>. The overall structure and splitting of the CART and CTree are similar, though CART has more splits than CTree. The prediction mean-squared error (using scaled energy intake values) for the conditional inference tree in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> is 0.67 compared to 0.48 for the linear regression in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. While the mean squared error is lower for linear regression, it may provide only limited scientific insight into the complex mechanisms underlying energy intake. Only the decision tree enables the identification of meaningful population subgroups and allows for formal inference about the defined groupings. For example, at the top level of the tree, the variable most strongly associated with (adjusted) total energy intake is snack calories (skcal, <inline-formula id="IEq49"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p &#x0003c; 0.001$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq49.gif"/></alternatives></inline-formula>). Splitting the population according to snack calories <inline-formula id="IEq50"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M74"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq50.gif"/></alternatives></inline-formula>798.22 versus &#x0003e;798.22 produces two subgroups. Within the first group (following the left branch in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>), snack calories remain the most significant predictor of total energy intake (<inline-formula id="IEq51"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p &#x0003c; 0.001$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq51.gif"/></alternatives></inline-formula>), while in the second group (the right branch of Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>) none of the covariates are significantly associated with the outcome. The first group (skcal <inline-formula id="IEq52"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M78"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq52.gif"/></alternatives></inline-formula>798.22) again splits into two groups: snacking calories <inline-formula id="IEq53"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M80"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq53.gif"/></alternatives></inline-formula>339.79 and &#x0003e;339.79 (but <inline-formula id="IEq54"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M82"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq54.gif"/></alternatives></inline-formula>798.22). In the former, &#x0201c;low snacking&#x0201d; group, the covariate most strongly associated with total energy intake is servings of sugar-sweetened beverages (srvgssb, <inline-formula id="IEq55"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 0.01$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq55.gif"/></alternatives></inline-formula>), which defines subgroups according to whether individuals consumed <inline-formula id="IEq56"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M86"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq56.gif"/></alternatives></inline-formula> or &#x0003e;0.53 SSBs per day. In the latter, the strongest association is with hunger (<inline-formula id="IEq57"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 0.01$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq57.gif"/></alternatives></inline-formula>), which splits into subgroups according to <italic>hunger</italic>
<inline-formula id="IEq58"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M90"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq58.gif"/></alternatives></inline-formula>7 or &#x0003e;7. The lower hunger group splits one more time on snack calories. Within the former &#x0201c;low snacking&#x0201d; group that splits to define a subgroup that consumes <inline-formula id="IEq59"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M92"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq59.gif"/></alternatives></inline-formula>0.53 SSBs per day, the covariate most strongly associated with energy intake is servings of fruits and vegetables (srvgfv0, <inline-formula id="IEq60"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 0.044$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.044</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq60.gif"/></alternatives></inline-formula>), which defines subgroups according to whether individuals consumed <inline-formula id="IEq61"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M96"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq61.gif"/></alternatives></inline-formula> or &#x0003e;2.04 servings per day.<fig id="Fig5"><label>Fig. 5</label><caption><p>Conditional inference tree representing the relationship between adjusted residuals for daily energy intake (adjusted for age, sex, and BMI) and 22 baseline covariates. Added Node ID labels in the terminal node. This is consistent with the titles for each subplot in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> and the CTree in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>
</p></caption><graphic xlink:href="12982_2017_64_Fig5_HTML" id="MO10"/></fig>
</p><p id="Par53">In general, decision trees are typically used to describe the associations between a set of covariates and an outcome, and thereby identify population subgroups with different outcome values. In our setup, there is no one particular exposure or treatment variable of interest, so there is not one focal variable whose effect may be modified by others. However, recursive partitioning does identify relevant interactions between covariates, i.e., combinations of covariate values which result in different (mean) values of the outcome. Hence, if the term &#x0201c;effect modification&#x0201d; is identified with &#x0201c;interaction&#x0201d;, then decision trees can be viewed as a tool for exploring effect modification.</p><p id="Par54">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> is composed of 7 sub-plots that represent each of the terminal nodes (i.e., subgroups) in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. The top left sub-plot in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> corresponds to node #5 (<inline-formula id="IEq62"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n = 23$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>23</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq62.gif"/></alternatives></inline-formula>) in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. The mean of adjusted residuals is &#x02212;702.94, indicating that on average, individuals in this node have a daily energy intake 702.94 kcal lower than the age-, sex-, and BMI-adjusted population mean. In the top left sub-plot in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, colored horizontal bars describe the population subgroup of node #5: individuals with low to moderate servings per day of sugar-sweetened beverages (<inline-formula id="IEq63"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M100"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq63.gif"/></alternatives></inline-formula>0.53 servings per day, i.e., below the 60th population percentile), low servings per day of fruits and vegetables (<inline-formula id="IEq64"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M102"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq64.gif"/></alternatives></inline-formula>2.04 servings per day, i.e., below the 25th population percentile) and low to moderate snack calories (<inline-formula id="IEq65"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M104"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq65.gif"/></alternatives></inline-formula>339.79 kcal per day, below the 50th population percentile).<fig id="Fig6"><label>Fig. 6</label><caption><p>Graphical visualization to display the composition of the 7 subgroups defined by the tree in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>
</p></caption><graphic xlink:href="12982_2017_64_Fig6_HTML" id="MO12"/></fig>
</p><p id="Par55">The bottom row of plots corresponds to the three nodes which had the highest adjusted average caloric intake (+455.47, +486.66, and +1210.44&#x000a0;kcal/day relative to the adjusted population mean, respectively). These nodes defined three distinct subgroups: (1) low to moderate hunger (<inline-formula id="IEq66"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><mml:math id="M106"><mml:mo>&#x02264;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq66.gif"/></alternatives></inline-formula>7, below the 80th percentile) and relatively high snacking (627&#x02013;798&#x000a0;kcal/day, between the 89th and 92nd percentiles); (2) high hunger (&#x0003e;7, above the 80th percentile) and moderate snacking (340&#x02013;798&#x000a0;kcal/day, between the 58th and 92nd percentiles); and (3) very high snacking calories (<inline-formula id="IEq67"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge$$\end{document}</tex-math><mml:math id="M108"><mml:mo>&#x02265;</mml:mo></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq67.gif"/></alternatives></inline-formula>&#x000a0;798&#x000a0;kcal/day, above the 92nd percentile). The fact that the first two of these groups have relatively similar adjusted mean daily caloric intake while being defined by distinct combinations of hunger and snacking levels (low hunger, moderate to high snacking in the first group vs. high hunger, moderate snacking in the second) suggests that there are multiple pathways which lead to similar levels of consumption of excess calories. These distinct pathways may require different intervention strategies: for example, the low hunger but moderate to high snacking group might be effectively targeted by an approach which sought to reduce snacking opportunities, under the logic that due to their relatively low hunger level they are more likely to be snacking out of convenience than to satisfy a craving. The high hunger but more moderate snacking group, on the other hand, might be more responsive to an approach aimed at managing cravings. Yet another approach might be required to optimize outcomes for the third group whose extremely high adjusted daily caloric intake (+1210.44&#x000a0;kcal/day relative to the population) was associated with extremely high snacking but not hunger.</p></sec></sec><sec id="Sec22"><title>Conclusions</title><p id="Par56">Decision trees can be a powerful tool in a researcher&#x02019;s data analysis toolbox, providing a way to identify relevant population subgroups which may provide insight into associations and effect mechanisms, and suggest strategies for tailoring interventions. In this paper, we compared two techniques for constructing decision trees, CART and CTree, and introduced a novel graphical visualization technique for decision trees which allows a researcher to see and compare the characteristics of these subgroups. Our focus was on describing relationships between a relatively small number of continuous or binary covariates and continuous outcomes in studies with moderate sample sizes, but decision trees can easily be extended to problems with larger sample sizes [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>], greater number of covariates, and for modeling other covariate and outcome types [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR29">29</xref>]. The CTree approach in particular accommodates a wide variety of data types, including categorical and time-to-event outcomes, within the same statistical framework.</p><p id="Par57">While the data we used to illustrate the application of decision trees arose from a randomized controlled trial, we performed cross-sectional analyses on baseline data and hence did not use information on treatment assignment. As with any technique based on identifying statistical associations, decision tree methods do not estimate causal effects of individual characteristics or exposures in such cross-section analyses. The adjustment procedure we describe above allows the researcher to account for measured variables that are thought to be confounders, but the additional flexibility provided by decision tree models cannot correct for bias due to unmeasured confounding. Hence, conclusions based on decision tree analysis should be viewed as exploratory. In ongoing work, we are extending the decision tree framework to characterize (causal) treatment effect heterogeneity (i.e., causal effect modification) in the context of randomized intervention studies.</p><p id="Par58">The two decision tree fitting techniques we compared in this paper, CART and CTree have different strengths and weaknesses. CART has the advantage of availability: it is widely implemented in standard statistical software packages, while to our knowledge, conditional inference trees are currently only implemented in R. In our experiments, CART often had slightly higher predictive accuracy than CTree due to its additional flexibility. However, CTree offers several advantages over CART. First, CTree yields a simpler tree building process as compared to CART, since in CTree a single overall Type I error rate parameter (<inline-formula id="IEq68"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M110"><mml:mi mathvariant="italic">&#x003b1;</mml:mi></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq68.gif"/></alternatives></inline-formula>) controls the size of the tree and removes the need for pruning. The <inline-formula id="IEq69"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M112"><mml:mi mathvariant="italic">&#x003b1;</mml:mi></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq69.gif"/></alternatives></inline-formula> value can be set independent of the outcome type (e.g., continuous, binary, time to event, etc.), unlike for CART where the complexity parameter depends on the splitting criterion which may differ depending on the outcome type. By using formal inferential techniques incorporating multiplicity adjustments to select splits, CTree provides statistical guarantees and valid <italic>p</italic> values at each split. Hence, the researcher deciding which technique to use must consider the relative value of giving up a small amount of model flexibility and predictive accuracy to simplify modeling and gain the ability to make formal statistical statements based on the results from the fitted tree.</p></sec></body><back><app-group><app id="App1"><sec id="Sec130"><title>Additional file</title><p>
<media position="anchor" xlink:href="12982_2017_64_MOESM1_ESM.png" id="MOESM1"><caption><p>
<bold>Additional file 1.</bold> Regression tree representing the relationship between adjusted residuals for energy intake (adjusted for age, sex, and BMI) and 22 baseline covariates</p></caption></media>
</p></sec></app></app-group><glossary><title>Abbreviations</title><def-list><def-item><term>CART</term><def><p id="Par4">classification and regression trees</p></def></def-item><def-item><term>CTree</term><def><p id="Par5">conditional inference tree</p></def></def-item><def-item><term>BLS</term><def><p id="Par6">Box Lunch Study</p></def></def-item><def-item><term>BMI</term><def><p id="Par7">body mass index</p></def></def-item><def-item><term>MSE</term><def><p id="Par8">mean squared error</p></def></def-item><def-item><term>SE</term><def><p id="Par9">standard error</p></def></def-item><def-item><term>TFEQ</term><def><p id="Par10">three factor eating questionnaire</p></def></def-item></def-list></glossary><fn-group><fn id="Fn1"><label>1</label><p id="Par19">Some studies record participant&#x02019;s self-reported level of wanting and liking using quantitative scales (e.g., [<xref ref-type="bibr" rid="CR15">15</xref>]), while other studies measure this via brain activity during a motivational state (e.g., [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]).</p></fn><fn id="Fn2"><label>2</label><p id="Par20">This standardized value of 0.26 is calculated from <inline-formula id="IEq6"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(2190 - 2012)/685.55$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2190</mml:mn><mml:mo>-</mml:mo><mml:mn>2012</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mn>685.55</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12982_2017_64_Article_IEq6.gif"/></alternatives></inline-formula>, where 2012 is the mean energy intake and 685.55 is its standard deviation.</p></fn><fn><p><bold>Electronic supplementary material</bold></p><p>The online version of this article (doi:10.1186/s12982-017-0064-4) contains supplementary material, which is available to authorized users.</p></fn></fn-group><ack><title>Authors' contributions</title><p>AV prepared the manuscript and conducted the simulation study. AV and JW developed the visualization tool. JW, NM, TB, MJ, and SF provided feedback and edited the manuscript. All authors read and approved the final manuscript.</p><sec id="FPar6"><title>Acknowledgements</title><p id="Par65">Not applicable.</p></sec><sec id="FPar4"><title>Competing interests</title><p id="Par63">The authors declare that they have no competing interests.</p></sec><sec id="FPar3"><title>Availability of data and material</title><p id="Par62">The datasets generated and/or analysed during the current study are not publicly available, but may be available on reasonable request. Please contact Dr. Simone French (frenc001@umn.edu) to request access.</p></sec><sec id="FPar2"><title>Consent for publication</title><p id="Par61">Not applicable.</p></sec><sec id="FPar1"><title>Ethics approval and consent to participate</title><p id="Par60">This paper performs secondary analysis of de-identified data collected in the Box Lunch Study, a randomized trial whose main results have been previously published. More details on ethics approval and participant consent in the Box Lunch Study can be found in [<xref ref-type="bibr" rid="CR8">8</xref>].</p></sec><sec id="FPar5"><title>Funding</title><p id="Par64">Data analyzed in this paper are from the Box Lunch Study, which was supported by a grant from NIH/NIDDK R01DK 081714.</p></sec><sec id="FPar7"><title>Publisher&#x02019;s Note</title><p id="Par66">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Hulst</surname><given-names>A</given-names></name><name><surname>Roy-Gagnon</surname><given-names>M-H</given-names></name><name><surname>Gauvin</surname><given-names>L</given-names></name><name><surname>Kestens</surname><given-names>Y</given-names></name><name><surname>Henderson</surname><given-names>M</given-names></name><name><surname>Barnett</surname><given-names>TA</given-names></name></person-group><article-title>Identifying risk profiles for childhood obesity using recursive partitioning based on individual, familial, and neighborhood environment factors</article-title><source>Int J Behav Nutr Phys Act.</source><year>2015</year><volume>12</volume><issue>1</issue><fpage>17</fpage><pub-id pub-id-type="doi">10.1186/s12966-015-0175-7</pub-id><pub-id pub-id-type="pmid">25881227</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garzotto</surname><given-names>M</given-names></name><name><surname>Beer</surname><given-names>TM</given-names></name><name><surname>Hudson</surname><given-names>RG</given-names></name><name><surname>Peters</surname><given-names>L</given-names></name><name><surname>Hsieh</surname><given-names>Y-C</given-names></name><name><surname>Barrera</surname><given-names>E</given-names></name><name><surname>Klein</surname><given-names>T</given-names></name><name><surname>Mori</surname><given-names>M</given-names></name></person-group><article-title>Improved detection of prostate cancer using classification and regression tree analysis</article-title><source>J Clin Oncol.</source><year>2005</year><volume>23</volume><issue>19</issue><fpage>4322</fpage><lpage>4329</lpage><pub-id pub-id-type="doi">10.1200/JCO.2005.11.136</pub-id><pub-id pub-id-type="pmid">15781880</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogden</surname><given-names>CL</given-names></name><name><surname>Carroll</surname><given-names>MD</given-names></name><name><surname>Curtin</surname><given-names>LR</given-names></name><name><surname>McDowell</surname><given-names>MA</given-names></name><name><surname>Tabak</surname><given-names>CJ</given-names></name><name><surname>Flegal</surname><given-names>KM</given-names></name></person-group><article-title>Prevalence of overweight and obesity in the united states, 1999&#x02013;2004</article-title><source>Jama</source><year>2006</year><volume>295</volume><issue>13</issue><fpage>1549</fpage><lpage>1555</lpage><pub-id pub-id-type="doi">10.1001/jama.295.13.1549</pub-id><pub-id pub-id-type="pmid">16595758</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flegal</surname><given-names>KM</given-names></name><name><surname>Kruszon-Moran</surname><given-names>D</given-names></name><name><surname>Carroll</surname><given-names>MD</given-names></name><name><surname>Fryar</surname><given-names>CD</given-names></name><name><surname>Ogden</surname><given-names>CL</given-names></name></person-group><article-title>Trends in obesity among adults in the united states, 2005 to 2014</article-title><source>JAMA</source><year>2016</year><volume>315</volume><issue>21</issue><fpage>2284</fpage><lpage>2291</lpage><pub-id pub-id-type="doi">10.1001/jama.2016.6458</pub-id><pub-id pub-id-type="pmid">27272580</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gass</surname><given-names>K</given-names></name><name><surname>Klein</surname><given-names>M</given-names></name><name><surname>Chang</surname><given-names>HH</given-names></name><name><surname>Flanders</surname><given-names>WD</given-names></name><name><surname>Strickland</surname><given-names>MJ</given-names></name></person-group><article-title>Classification and regression trees for epidemiologic research: an air pollution example</article-title><source>Environ. Health</source><year>2014</year><volume>13</volume><issue>1</issue><fpage>17</fpage><pub-id pub-id-type="doi">10.1186/1476-069X-13-17</pub-id><pub-id pub-id-type="pmid">24625053</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguiar</surname><given-names>FS</given-names></name><name><surname>Almeida</surname><given-names>LL</given-names></name><name><surname>Ruffino-Netto</surname><given-names>A</given-names></name><name><surname>Kritski</surname><given-names>AL</given-names></name><name><surname>Mello</surname><given-names>FC</given-names></name><name><surname>Werneck</surname><given-names>GL</given-names></name></person-group><article-title>Classification and regression tree (cart) model to predict pulmonary tuberculosis in hospitalized patients</article-title><source>BMC Pulm Med.</source><year>2012</year><volume>12</volume><issue>1</issue><fpage>40</fpage><pub-id pub-id-type="doi">10.1186/1471-2466-12-40</pub-id><pub-id pub-id-type="pmid">22871182</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>Y</given-names></name><name><surname>Nollen</surname><given-names>N</given-names></name><name><surname>Ahluwahlia</surname><given-names>JS</given-names></name><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Mayo</surname><given-names>MS</given-names></name></person-group><article-title>An application in identifying high-risk populations in alternative tobacco product use utilizing logistic regression and cart: a heuristic comparison</article-title><source>BMC Public Health</source><year>2015</year><volume>15</volume><issue>1</issue><fpage>341</fpage><pub-id pub-id-type="doi">10.1186/s12889-015-1582-z</pub-id><pub-id pub-id-type="pmid">25879872</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>SA</given-names></name><name><surname>Mitchell</surname><given-names>NR</given-names></name><name><surname>Wolfson</surname><given-names>J</given-names></name><name><surname>Harnack</surname><given-names>LJ</given-names></name><name><surname>Jeffery</surname><given-names>RW</given-names></name><name><surname>Gerlach</surname><given-names>AF</given-names></name><name><surname>Blundell</surname><given-names>JE</given-names></name><name><surname>Pentel</surname><given-names>PR</given-names></name></person-group><article-title>Portion size effects on weight gain in a free living setting</article-title><source>Obesity</source><year>2014</year><volume>22</volume><issue>6</issue><fpage>1400</fpage><lpage>1405</lpage><pub-id pub-id-type="doi">10.1002/oby.20720</pub-id><pub-id pub-id-type="pmid">24510841</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>SA</given-names></name><name><surname>Mitchell</surname><given-names>NR</given-names></name><name><surname>Wolfson</surname><given-names>J</given-names></name><name><surname>Finlayson</surname><given-names>G</given-names></name><name><surname>Blundell</surname><given-names>JE</given-names></name><name><surname>Jeffery</surname><given-names>RW</given-names></name></person-group><article-title>Questionnaire and laboratory measures of eating behavior. Associations with energy intake and BMI in a community sample of working adults</article-title><source>Appetite</source><year>2014</year><volume>72</volume><fpage>50</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.appet.2013.09.020</pub-id><pub-id pub-id-type="pmid">24096082</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stunkard</surname><given-names>AJ</given-names></name><name><surname>Messick</surname><given-names>S</given-names></name></person-group><article-title>The three-factor eating questionnaire to measure dietary restraint, disinhibition and hunger</article-title><source>J Psychosom Res.</source><year>1985</year><volume>29</volume><issue>1</issue><fpage>71</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/0022-3999(85)90010-8</pub-id><pub-id pub-id-type="pmid">3981480</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">R Core Team: R: a language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. 2016. <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hothorn</surname><given-names>T</given-names></name><name><surname>Zeileis</surname><given-names>A</given-names></name></person-group><article-title>Partykit: a modular toolkit for recursive partytioning in R</article-title><source>J Mach Learn Res</source><year>2015</year><volume>16</volume><fpage>3905</fpage><lpage>3909</lpage></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Therneau T, Atkinson B, Ripley B. Rpart: recursive partitioning and regression trees. R package version 4.1-8. 2014. <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=rpart">http://CRAN.R-project.org/package=rpart</ext-link></mixed-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Torgo</surname><given-names>L</given-names></name></person-group><source>Data mining with R, learning with case studies</source><year>2010</year><publisher-loc>Boca Raton</publisher-loc><publisher-name>Chapman and Hall/CRC</publisher-name></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNeil</surname><given-names>J</given-names></name><name><surname>Cadieux</surname><given-names>S</given-names></name><name><surname>Finlayson</surname><given-names>G</given-names></name><name><surname>Blundell</surname><given-names>J</given-names></name><name><surname>Doucet</surname><given-names>E</given-names></name></person-group><article-title>Associations between sleep parameters and food reward</article-title><source>J Sleep Res.</source><year>2015</year><volume>24</volume><issue>3</issue><fpage>346</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1111/jsr.12275</pub-id><pub-id pub-id-type="pmid">25644582</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>L</given-names></name><name><surname>Carr</surname><given-names>K</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name><name><surname>Fletcher</surname><given-names>K</given-names></name></person-group><article-title>Food reinforcement, energy intake, and macronutrient choice</article-title><source>Am J Clin Nutr.</source><year>2011</year><volume>94</volume><issue>1</issue><fpage>12</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.3945/ajcn.110.010314</pub-id><pub-id pub-id-type="pmid">21543545</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pool</surname><given-names>E</given-names></name><name><surname>Sennwald</surname><given-names>V</given-names></name><name><surname>Delplanque</surname><given-names>S</given-names></name><name><surname>Brosch</surname><given-names>T</given-names></name><name><surname>Sander</surname><given-names>D</given-names></name></person-group><article-title>Measuring wanting and liking from animals to humans: a systematic review</article-title><source>Neurosci Biobehav Rev.</source><year>2016</year><volume>63</volume><fpage>124</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.01.006</pub-id><pub-id pub-id-type="pmid">26851575</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Olshen</surname><given-names>R</given-names></name><name><surname>Stone</surname><given-names>C</given-names></name></person-group><source>Classification and regression trees</source><year>1984</year><publisher-loc>Boca Raton</publisher-loc><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loh</surname><given-names>W</given-names></name><name><surname>Shih</surname><given-names>Y</given-names></name></person-group><article-title>Split selection methods for classification trees</article-title><source>Stat Sin.</source><year>1997</year><volume>7</volume><issue>4</issue><fpage>815</fpage><lpage>840</lpage></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name></person-group><article-title>Technical note: bias in information-based measures in decision tree induction</article-title><source>Mach Learn.</source><year>1994</year><volume>15</volume><issue>3</issue><fpage>321</fpage><lpage>329</lpage></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shih</surname><given-names>Y</given-names></name></person-group><article-title>A note on split selection bias in classification trees</article-title><source>Comput Stat Data Anal.</source><year>2004</year><volume>45</volume><issue>3</issue><fpage>457</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1016/S0167-9473(03)00064-1</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hothorn</surname><given-names>T</given-names></name><name><surname>Hornik</surname><given-names>K</given-names></name><name><surname>Zeileis</surname><given-names>A</given-names></name></person-group><article-title>Unbiased recursive partitioning: a conditional inference framework</article-title><source>J Comput Graph Stat.</source><year>2006</year><volume>15</volume><issue>3</issue><fpage>651</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1198/106186006X133933</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esposito</surname><given-names>F</given-names></name><name><surname>Malerba</surname><given-names>D</given-names></name><name><surname>Semeraro</surname><given-names>G</given-names></name><name><surname>Kay</surname><given-names>J</given-names></name></person-group><article-title>A comparative analysis of methods for pruning decision trees</article-title><source>IEEE Trans Pattern Anal Mach Intell.</source><year>1997</year><volume>19</volume><issue>5</issue><fpage>476</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1109/34.589207</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mingers</surname><given-names>J</given-names></name></person-group><article-title>An empirical comparison of pruning methods for decision tree induction</article-title><source>Mach Learn.</source><year>1989</year><volume>4</volume><issue>2</issue><fpage>227</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1023/A:1022604100933</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaffer</surname><given-names>C</given-names></name></person-group><article-title>Overfitting avoidance as bias</article-title><source>Mach Learn.</source><year>1993</year><volume>10</volume><issue>2</issue><fpage>153</fpage><lpage>178</lpage></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atienza</surname><given-names>AA</given-names></name><name><surname>Yaroch</surname><given-names>AL</given-names></name><name><surname>M&#x000e3;sse</surname><given-names>LC</given-names></name><name><surname>Moser</surname><given-names>RP</given-names></name><name><surname>Hesse</surname><given-names>BW</given-names></name><name><surname>King</surname><given-names>AC</given-names></name></person-group><article-title>Identifying sedentary subgroups: the national cancer institute&#x02019;s health information national trends survey</article-title><source>Am J Prev Med.</source><year>2006</year><volume>31</volume><issue>5</issue><fpage>383</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1016/j.amepre.2006.07.024</pub-id><pub-id pub-id-type="pmid">17046409</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>AC</given-names></name><name><surname>Salvo</surname><given-names>D</given-names></name><name><surname>Banda</surname><given-names>JA</given-names></name><name><surname>Ahn</surname><given-names>DK</given-names></name><name><surname>Gill</surname><given-names>TM</given-names></name><name><surname>Miller</surname><given-names>M</given-names></name><name><surname>Newman</surname><given-names>AB</given-names></name><name><surname>Fielding</surname><given-names>RA</given-names></name><name><surname>Siordia</surname><given-names>C</given-names></name><name><surname>Moore</surname><given-names>S</given-names></name><etal/></person-group><article-title>An observational study identifying obese subgroups among older adults at increased risk of mobility disability: do perceptions of the neighborhood environment matter?</article-title><source>Int J Behav Nutr Phys Act.</source><year>2015</year><volume>12</volume><issue>1</issue><fpage>1</fpage><pub-id pub-id-type="doi">10.1186/s12966-014-0159-z</pub-id><pub-id pub-id-type="pmid">25592201</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y-C</given-names></name><name><surname>Lee</surname><given-names>W-J</given-names></name><name><surname>Lin</surname><given-names>Y-C</given-names></name><name><surname>Liew</surname><given-names>P-L</given-names></name><name><surname>Lee</surname><given-names>CK</given-names></name><name><surname>Lin</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>T-S</given-names></name></person-group><article-title>Obesity and the decision tree: predictors of sustained weight loss after bariatric surgery</article-title><source>Hepato Gastroenterol.</source><year>2008</year><volume>56</volume><issue>96</issue><fpage>1745</fpage><lpage>1749</lpage></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>SY</given-names></name><name><surname>Vitolins</surname><given-names>MZ</given-names></name><name><surname>Fenton</surname><given-names>J</given-names></name><name><surname>Frazier-Wood</surname><given-names>AC</given-names></name><name><surname>Hursting</surname><given-names>SD</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name></person-group><article-title>Risk profiles for weight gain among postmenopausal women: a classification and regression tree analysis approach</article-title><source>PLoS ONE</source><year>2015</year><volume>10</volume><issue>3</issue><fpage>0121430</fpage></element-citation></ref></ref-list></back></article>
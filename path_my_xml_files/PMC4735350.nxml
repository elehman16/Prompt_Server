<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1662-4548</issn><issn pub-type="epub">1662-453X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26869874</article-id><article-id pub-id-type="pmc">4735350</article-id><article-id pub-id-type="doi">10.3389/fnins.2016.00017</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Methods</subject></subj-group></subj-group></article-categories><title-group><article-title>Memory Efficient PCA Methods for Large Group ICA</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rachakonda</surname><given-names>Srinivas</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/4523/overview"/></contrib><contrib contrib-type="author"><name><surname>Silva</surname><given-names>Rogers F.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/17874/overview"/></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Jingyu</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/17893/overview"/></contrib><contrib contrib-type="author"><name><surname>Calhoun</surname><given-names>Vince D.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/884/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>The Mind Research Network and Lovelace Biomedical and Environmental Research Institute</institution><country>Albuquerque, NM, USA</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Electrical and Computer Engineering, The University of New Mexico</institution><country>Albuquerque, NM, USA</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Computer Science, The University of New Mexico</institution><country>Albuquerque, NM, USA</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Pedro Antonio Valdes-Sosa, Centro de Neurociencias de Cuba, Cuba</p></fn><fn fn-type="edited-by"><p>Reviewed by: Anand Joshi, University of Southern California, USA; Eugene Duff, University of Oxford, UK</p></fn><corresp id="fn001">*Correspondence: Srinivas Rachakonda <email xlink:type="simple">srachakonda@mrn.org</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Brain Imaging Methods, a section of the journal Frontiers in Neuroscience</p></fn></author-notes><pub-date pub-type="epub"><day>02</day><month>2</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>10</volume><elocation-id>17</elocation-id><history><date date-type="received"><day>08</day><month>10</month><year>2015</year></date><date date-type="accepted"><day>12</day><month>1</month><year>2016</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016 Rachakonda, Silva, Liu and Calhoun.</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Rachakonda, Silva, Liu and Calhoun</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Principal component analysis (PCA) is widely used for data reduction in group independent component analysis (ICA) of fMRI data. Commonly, group-level PCA of temporally concatenated datasets is computed prior to ICA of the group principal components. This work focuses on reducing very high dimensional temporally concatenated datasets into its group PCA space. Existing randomized PCA methods can determine the PCA subspace with minimal memory requirements and, thus, are ideal for solving large PCA problems. Since the number of dataloads is not typically optimized, we extend one of these methods to compute PCA of very large datasets with a minimal number of dataloads. This method is coined multi power iteration (MPOWIT). The key idea behind MPOWIT is to estimate a subspace larger than the desired one, while checking for convergence of only the smaller subset of interest. The number of iterations is reduced considerably (as well as the number of dataloads), accelerating convergence without loss of accuracy. More importantly, in the proposed implementation of MPOWIT, the memory required for successful recovery of the group principal components becomes independent of the number of subjects analyzed. Highly efficient subsampled eigenvalue decomposition techniques are also introduced, furnishing excellent PCA subspace approximations that can be used for intelligent initialization of randomized methods such as MPOWIT. Together, these developments enable efficient estimation of accurate principal components, as we illustrate by solving a 1600-subject group-level PCA of fMRI with standard acquisition parameters, on a regular desktop computer with only 4 GB RAM, in just a few hours. MPOWIT is also highly scalable and could realistically solve group-level PCA of fMRI on thousands of subjects, or more, using standard hardware, limited only by time, not memory. Also, the MPOWIT algorithm is highly parallelizable, which would enable fast, distributed implementations ideal for big data analysis. Implications to other methods such as expectation maximization PCA (EM PCA) are also presented. Based on our results, general recommendations for efficient application of PCA methods are given according to problem size and available computational resources. MPOWIT and all other methods discussed here are implemented and readily available in the open source GIFT software.</p></abstract><kwd-group><kwd>group ICA</kwd><kwd>big data</kwd><kwd>PCA</kwd><kwd>subspace iteration</kwd><kwd>EVD</kwd><kwd>SVD</kwd><kwd>memory</kwd><kwd>power iteration</kwd></kwd-group><counts><fig-count count="8"/><table-count count="0"/><equation-count count="37"/><ref-count count="35"/><page-count count="15"/><word-count count="11048"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Principal component analysis (PCA) is used as both a data reduction and de-noising method in group independent component analysis (ICA) (Calhoun et al., <xref rid="B8" ref-type="bibr">2001</xref>; Beckmann and Smith, <xref rid="B2" ref-type="bibr">2004</xref>; Calhoun and Adali, <xref rid="B7" ref-type="bibr">2012</xref>). PCA is typically carried out by computing the eigenvalue decomposition (EVD) of the sample covariance matrix (<italic>C</italic>) or by using singular value decomposition (SVD) directly on the data. For large datasets, both EVD (plus computation of <italic>C</italic>) and SVD become computationally intensive in both memory and speed. For instance, group ICA is commonly used for functional magnetic resonance imaging (fMRI) studies. A typical fMRI study may collect image volumes from a single subject for about 20 min using TR = 1000 ms and 3 &#x000d7; 3 &#x000d7; 3 mm voxel resolution, resulting in approximately 53 &#x000d7; 63 &#x000d7; 46 &#x000d7; 1200 data points. To compute group PCA using the standard EVD of <italic>C</italic> approach on 100 subjects stacked in the temporal dimension (and only the nearly 70000 in-brain voxels) requires approximately 100 GB RAM and more than 16 h on a Linux server. Using the SVD approach would incur similar memory requirements as EVD (plus computation of <italic>C</italic>). In either case, the computational requirements can quickly become prohibitive, especially with the constant advance of imaging techniques [such as multi-band EPI sequences (Feinberg et al., <xref rid="B13" ref-type="bibr">2010</xref>; Feinberg and Setsompop, <xref rid="B12" ref-type="bibr">2013</xref>)] and a tendency to share data within the imaging community. This means very large size imaging data will become even more common for fMRI studies, encouraging the development of novel computational methods to face the upcoming challenges.</p><p>There are several methods to estimate dominant PCA components with minimal memory requirements, like sequential SVD, cascade recursive least squares (CRLS) PCA, and randomized PCA approaches, to name a few. Sequential or &#x0201c;online&#x0201d; SVD is usually applied in a streaming memory setting where the data streams over time and only a single pass over the datasets is possible. There exist algorithms (Brand, <xref rid="B6" ref-type="bibr">2003</xref>; Li, <xref rid="B23" ref-type="bibr">2004</xref>; Funk, <xref rid="B16" ref-type="bibr">2006</xref>) which provide incremental SVD update and downdate capacity. However, principal components obtained with sequential SVD approaches are typically not as accurate as those from EVD of <italic>C</italic> and, therefore, sequential SVD approaches are considered not suitable for data reduction in group ICA analyses. CRLS PCA (Wang et al., <xref rid="B35" ref-type="bibr">2006</xref>) uses a subspace deflation technique to extract dominant components of interest with limited training. The number of training epochs required is dependent on the data and, therefore, the CRLS PCA algorithm has slower performance in very large datasets and when higher model order (i.e., high number of components) needs to be estimated.</p><p>Randomized PCA methods are a class of algorithms that iteratively estimate the principal components from the data and are particularly useful when only a few components need to be estimated from very large datasets. They provide a much more efficient solution than the EVD approach, which always estimates the complete set of eigenvectors, many of which are eventually discarded for data reduction and de-noising purposes. Clearly, iterative approaches can make a much more intelligent use of the available computational resources. Some popular and upcoming randomized PCA approaches are: implicitly restarted Arnoldi iteration (IRAM; (Lehoucq and Sorensen, <xref rid="B22" ref-type="bibr">1996</xref>)), power iteration (Recktenwald, <xref rid="B28" ref-type="bibr">2000</xref>), subspace iteration (Rutishauser, <xref rid="B30" ref-type="bibr">1970</xref>) expectation maximization PCA (EM PCA) (Roweis, <xref rid="B29" ref-type="bibr">1997</xref>), and &#x0201c;Large PCA&#x0201d; (Halko et al., <xref rid="B17" ref-type="bibr">2011a</xref>). IRAM as implemented in ARPACK (Lehoucq et al., <xref rid="B21" ref-type="bibr">1998</xref>) requires that the sample covariance matrix be computed from the data and, thus, has higher computational demands on memory. Power iteration determines PCA components in a so-called &#x0201c;deflationary&#x0201d; mode (i.e., one at a time) and has very poor convergence properties when more than one component needs to be extracted from the data. Also, the error accumulates in subsequent estimations. Subspace iteration is a symmetric version of the power iteration method which extracts multiple components simultaneously from the data using explicit orthogonalization of the subspace in each iteration. EM PCA uses expectation and maximization steps to estimate multiple components simultaneously from the data. Both EM PCA and subspace iteration methods converge faster when only a few components are estimated from very large datasets and have slower convergence properties when a higher number of components needs to be estimated. More recently, Large PCA (Halko et al., <xref rid="B17" ref-type="bibr">2011a</xref>) was proposed to evaluate the principal components from very large datasets. Large PCA is a randomized version of the block Lanczos method (Kuczynski and Wozniakowski, <xref rid="B20" ref-type="bibr">1992</xref>) and is highly dependent on appropriate block size determination (typically large) in order to give accurate results with default settings.</p><p>In this paper, we show how to overcome the problem of slow convergence in subspace iteration when a high number of components is estimated by introducing a new approach, named multi power iteration (MPOWIT). Our approach takes into account the number of dataloads, which has often been overlooked in the development of randomized PCA methods. We also show that both subspace iteration and EM PCA methods converge to the same subspace in each iteration. Thus, the acceleration scheme we propose in MPOWIT can also be applied to EM PCA. In addition, we compare the performance of MPOWIT with existing PCA methods like EVD and Large PCA using real fMRI data from 1600 subjects with standard acquisition parameters. Moreover, acknowledging the recent popularization and promising developments in the area of multi-band EPI sequences (Feinberg and Setsompop, <xref rid="B12" ref-type="bibr">2013</xref>), we provide performance assessments of the PCA methods discussed here in the case of hypothetical 5000-subject fMRI studies using TR = 200 ms, 2 &#x000d7; 2 &#x000d7; 2 mm voxel resolution, and 30 min-long sessions. Based on our current estimates, group-level PCA using our new randomized PCA approach and retaining 200 principal components in the subject-level PCA could be performed on such data in 40 h (nearly 36 h just loading the subject-level PCA results) using a Windows desktop with 4 GB RAM. Alternatively, the same analysis could be performed in just 2 h using a Linux server with 512 GB RAM (assuming the subject-level PCA results are kept in memory after their estimation). In either case, this is without the additional benefits of GPU acceleration or parallelization.</p><p>We provide descriptions of EVD, Subsampled PCA, Large PCA, MPOWIT and EM PCA in the Materials and Methods Section. The same section also includes a description of the datasets and experiments conducted for each PCA method. Experiments are performed on the real fMRI data. In the Results Section, we present our experimental results and compare the performance of MPOWIT with existing PCA methods. Finally, we discuss these results and draw conclusions based on the analyses we performed. Additional details are provided in the appendices, including a proof that EM PCA is equivalent to subspace iteration.</p></sec><sec sec-type="materials and methods" id="s2"><title>Materials and methods</title><sec><title>Group ICA</title><p>In this paper, we are interested in group ICA of fMRI data as originally described in Calhoun et al. (<xref rid="B8" ref-type="bibr">2001</xref>) and further expanded and reviewed in Erhardt et al. (<xref rid="B11" ref-type="bibr">2011</xref>) and Calhoun and Adali (<xref rid="B7" ref-type="bibr">2012</xref>). In this technique, <italic>Z</italic><sub><italic>i</italic></sub> are the fMRI data of subject <italic>i</italic> with dimension <italic>v</italic> &#x000d7; <italic>t</italic>, where <italic>v</italic> and <italic>t</italic> are the number of voxels and time points, respectively. <italic>Z</italic><sub><italic>i</italic></sub> is mean-centered on zero at each time point. Each subject's data is reduced along the time dimension using PCA to retain the top <italic>p</italic> components, which are then whitened<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>. Following, all <italic>M</italic> subjects are stacked along the (reduced) temporal dimension. Let <italic>Y</italic> = [<italic>Y</italic><sub>1</sub>, <italic>Y</italic><sub>2</sub>, &#x02026;, <italic>Y</italic><sub><italic>M</italic></sub>] be the temporally concatenated data where <italic>Y</italic><sub><italic>i</italic></sub> is the zero-mean <italic>v</italic> &#x000d7; <italic>p</italic> PCA-reduced data of subject <italic>i</italic>. Group-level PCA is then performed on the temporally reduced concatenated data, resulting in <italic>k</italic> group principal components in the group-level PCA space <italic>X</italic>. Figure <xref ref-type="fig" rid="F1">1</xref> shows a graphical representation of group PCA.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Group PCA framework</bold>. Graphical representation of the two-step PCA approach to group-level PCA we consider in this work. The first step performs subject-level PCA on each subject, followed by (optional) whitening. The second step performs group-level PCA on the stacked (concatenated) reduced-data from all subjects, and is the focus of our attention. Particularly, if time and memory resources were unlimited, standard PCA routines for subject-level PCA (such as EVD) would suffice for group-level PCA on the stacked data. For that reason, we consider that to be the &#x0201c;ideal&#x0201d; scenario and strive to replicate its results in the more realistic case of limited resources.</p></caption><graphic xlink:href="fnins-10-00017-g0001"/></fig><p>Group ICA is regularly being used to analyze large numbers of subjects (Biswal et al., <xref rid="B4" ref-type="bibr">2010</xref>; Allen et al., <xref rid="B1" ref-type="bibr">2011</xref>), and multi-band EPI sequences (Feinberg et al., <xref rid="B13" ref-type="bibr">2010</xref>; Setsompop et al., <xref rid="B32" ref-type="bibr">2012</xref>) put even more memory demands on group PCA if the number of components retained in the first PCA step is increased significantly. We previously implemented some memory efficient ways to solve the PCA problem on large datasets in the GIFT<xref ref-type="fn" rid="fn0002"><sup>2</sup></xref> toolbox using EVD, SVD and EM PCA. Options are provided in the GIFT toolbox to select the appropriate PCA method based on the problem size and computer RAM requirements. In this paper, we present ways to further accelerate the group-level PCA step using new algorithms, and discuss the scalability of PCA algorithms based on the problem size.</p></sec><sec><title>Group-level PCA</title><p>Group ICA of temporally concatenated fMRI can be used to identify either spatial independent components (Calhoun et al., <xref rid="B8" ref-type="bibr">2001</xref>; Allen et al., <xref rid="B1" ref-type="bibr">2011</xref>) or temporal independent components (Smith et al., <xref rid="B34" ref-type="bibr">2012</xref>). In spatial group ICA, subject-level PCA is typically carried out to reduce the time dimension prior to group-level PCA, as described in Figure <xref ref-type="fig" rid="F1">1</xref>. However, in temporal group ICA, subject-level PCA to reduce the spatial dimension is neither required nor recommended<xref ref-type="fn" rid="fn0003"><sup>3</sup></xref>; instead, group-level PCA is carried out directly on the input fMRI data <italic>Z</italic><sub><italic>i</italic></sub>. In the following, we present algorithms for PCA estimation assuming the case depicted in Figure <xref ref-type="fig" rid="F1">1</xref> and deferring any comments about temporal group-level PCA to the Discussion Section.</p><p>In the following, we present a selection of approaches for group PCA, starting with the traditional EVD method, which we consider the standard for accuracy in later comparisons. Then, based on considerations made on the EVD method and properties of the fMRI data, two approaches are proposed for efficient approximate estimation of the group PCA solution, namely subsampled voxel PCA (SVP) and subsampled time PCA (STP). Both approaches are useful for efficient initialization and accelerated convergence of the highly accurate randomized methods presented later. Following, Large PCA, a recent block Lanczos method with high accuracy and potential for application in large group PCA, is introduced for the purpose of comparison. Power methods, including the introduction of our novel MPOWIT technique, are discussed next. The connections and implications of MPOWIT on the popular expectation maximization PCA (EM PCA) approach are presented lastly. At every stage, we strive to present every algorithmic improvement and theoretical development in the context of fMRI data under various conditions. Nevertheless, all considerations should be straightforwardly extensible to other modalities and datatypes. Throughout the following, <italic>F</italic> is the eigenvector matrix required for GICA1 back-reconstruction (Erhardt et al., <xref rid="B11" ref-type="bibr">2011</xref>). Finally, we assume that <italic>k</italic> &#x0003c; &#x0003c; <italic>Mp</italic> and <italic>k</italic> &#x0003c; &#x0003c; <italic>v</italic> in all time complexity assessments presented hereafter, unless otherwise noted.</p><sec><title>Eigenvalue decomposition (EVD)</title><p>Using the EVD approach, the group-level PCA space <italic>X</italic> can be determined from the temporally stacked (concatenated) zero-mean data <italic>Y</italic> as follows:</p><list list-type="alpha-lower"><list-item><p>Compute the sample covariance matrix <italic>C</italic> in the smallest dimension of the data (Wang et al., <xref rid="B35" ref-type="bibr">2006</xref>). If <italic>Mp</italic> &#x0003c; <italic>v</italic>, the <italic>Mp</italic> &#x000d7; <italic>Mp</italic> covariance matrix is:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item><list-item><p>EVD factorizes the (symmetric) covariance matrix <italic>C</italic> into eigenvectors <italic>F</italic> and eigenvalues &#x0039b;:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>&#x0039b;</mml:mi><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item><list-item><p>The <italic>v</italic> &#x000d7; <italic>k</italic> group-level PCA space <italic>X</italic> is obtained by projecting the top <italic>k</italic> eigenvectors (columns) of <italic>F</italic> with largest eigenvalues onto the data, as shown below:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>M</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item></list><p>From Equation (1) and the description in Figure <xref ref-type="fig" rid="F1">1</xref>, we note that <italic>C</italic> has structure and could be visualized as a cross-covariance matrix between the <italic>M</italic> subject-level PCA components, with <inline-formula><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as shown below:
<disp-formula id="E4"><label>(4)</label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none none none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#x02026;</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#x02026;</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>&#x02026;</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Exploiting this structure, <italic>C</italic> can be computed with only two datasets in memory at a time, instead of stacking the entire data to form <italic>Y</italic>, leading to fewer computations and memory usage. However, as <italic>M</italic> increases, computation of the cross-covariance matrix becomes very slow since it requires <inline-formula><mml:math id="M6"><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> steps and an equal number of dataloads (hard disk swaps and data transfers). Therefore, EVD of <italic>C</italic> is a fast solution for small datasets but can quickly become inefficient on very large datasets. Moreover, Equation (3) incurs <italic>M</italic> additional dataloads, though it allows a memory efficient implementation with only one dataset in memory at a time. Also, note that computing the covariance matrix itself has time complexity <italic>O</italic>(<italic>v</italic>(<italic>Mp</italic>)<sup>2</sup>) when <italic>Mp</italic> &#x0003c; <italic>v</italic> and <italic>O</italic>(<italic>Mpv</italic><sup>2</sup>) when <italic>v</italic> &#x0003c; <italic>Mp</italic>. The latter, however, has the convenience of requiring only one dataload per subject. Even better, in that case the covariance matrix in the voxel dimension, defined as <inline-formula><mml:math id="M7"><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> in order to retain the same eigenvalues as <italic>C</italic>, can be written as a sum of subject-specific covariances <inline-formula><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, and an efficient approach to compute the EVD of <italic>C</italic><sup><italic>v</italic></sup> is:
<disp-formula id="E5"><label>(5)</label><mml:math id="M9"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c7;</mml:mi><mml:mi>&#x0039b;</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The final estimate of the group-level PCA space is obtained as <inline-formula><mml:math id="M10"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c7;</mml:mi><mml:msqrt><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:math></inline-formula>. Equation (5) gives an upper bound on the <italic>memory</italic> required for group ICA of fMRI using the EVD method [i.e., <italic>O</italic>(<italic>v</italic><sup>2</sup>) bytes], and note it only requires <italic>M</italic> dataloads. Particularly, for large-<italic>M</italic> region of interest- (ROI-) based group ICA <italic>v</italic> &#x0226a; <italic>Mp</italic>. Thus, the left-hand equality in Equation (5) is highly efficient in memory for big ROI studies and should be the method of choice for computation of <italic>C</italic><sup><italic>v</italic></sup> rather than stacking the entire data in memory to form <italic>Y</italic>.</p><p>Clearly, trade-offs exist between time, memory, and dataloads depending on the exact values of <italic>v</italic>, <italic>M</italic>, and <italic>p</italic>. In some cases, which we consider later, it might be worth giving up on some computing speed in exchange for a largely disproportional improvement (reduction) in dataloads or memory footprint, and vice-versa, especially if numerical accuracy with respect to EVD of <italic>C</italic> is not compromised. This will be a recurring theme in the following sections.</p></sec><sec><title>Subsampled PCA</title><p>While EVD is very well-developed and accurate, it still becomes computationally and memory intensive when applied to large data<xref ref-type="fn" rid="fn0004"><sup>4</sup></xref> because it requires computation of large covariance matrices. For that reason, before discussing other PCA methods, we introduce the concept of &#x0201c;subsampling&#x0201d; (i.e., partitioning the data into subsets) and propose its use to efficiently determine an approximate initial PCA space that can be incrementally refined in a multi-stage approach. We consider two methods for subsampling, both of which could be used to efficiently initialize the PCA subspace of the randomized PCA methods discussed later. In the first method, data is subsampled in voxel space whereas in the second method the time dimension is subsampled. While voxel and time are specific to fMRI data, the concept of subsampling can be easily extended to any other data modality.</p><sec><title>Subsampled voxel PCA (SVP)</title><p>Subsampled voxel PCA (SVP) works by setting up a scenario in which <italic>v</italic> &#x0003c; <italic>Mp</italic> so that Equation (5) can be used to efficiently approximate <italic>X</italic> with only a few dataloads per subject. First, consider that, typically, fMRI data is spatially smoothed during the initial preprocessing to improve signal-to-noise ratio (SNR), which introduces data dependency in the spatial domain. Therefore, the actual number of independent and identically distributed (i.i.d.) samples present in the data is less than the voxel dimension <italic>v</italic> (Li et al., <xref rid="B24" ref-type="bibr">2007</xref>). A set of approximately independent samples could be obtained by subsampling the data in the voxel dimension. In our experiments, we selected a value of 2 for subsampling depth in x, y, and z directions as this only reduces the number of in-brain voxels by a factor of 2<sup>3</sup> = 8, i.e., <italic>v</italic>&#x02032; = <italic>v</italic>&#x02215;8. Thus, the resulting covariance matrix is still fairly approximate to the original one. All-odd and all-even subsampling depths are processed separately to estimate eigenvectors <italic>F</italic><sub><italic>a</italic></sub> and <italic>F</italic><sub><italic>b</italic></sub>, respectively, using EVD with Equation (5) since typically <italic>v</italic>&#x02032; &#x0003c; &#x0003c; <italic>Mp</italic>. <italic>F</italic><sub><italic>a</italic></sub> and <italic>F</italic><sub><italic>b</italic></sub> are projected onto the data <italic>Y</italic> (Equation 8) in order to bring <italic>X</italic><sub><italic>a</italic></sub> and <italic>X</italic><sub><italic>b</italic></sub>, respectively, back to dimension <italic>v</italic> from <italic>v</italic>&#x02032;. These are finally stacked in the time dimension to determine a final (common) PCA subspace (Equations 9 and 10):
<disp-formula id="E6"><label>(6)</label><mml:math id="M11"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E7"><label>(7)</label><mml:math id="M12"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E8"><label>(8)</label><mml:math id="M13"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E9"><label>(9)</label><mml:math id="M14"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>&#x0039b;</mml:mi><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E10"><label>(10)</label><mml:math id="M15"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:msup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p><italic>Y</italic><sub><italic>a</italic></sub> and <italic>Y</italic><sub><italic>b</italic></sub> refer to subsampled data in odd- and even-voxel spaces, respectively. <italic>F</italic><sub><italic>a</italic></sub> and <italic>F</italic><sub><italic>b</italic></sub> are estimated in subsampled space <italic>v</italic>&#x02032; from covariance matrices <inline-formula><mml:math id="M16"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M17"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula>, respectively, using Equations (6) and (7). A very high number of components (i.e., much larger than <italic>k</italic>; here, around 500, assuming <italic>k</italic> &#x02248; 100) are estimated in this intermediate PCA stage (Equation 6) to minimize error due to approximation. At the end of the estimation, only the <italic>k</italic> dominant components are extracted from <italic>X</italic> (Equation 10). Note that the use of Equation (5) for EVD of <inline-formula><mml:math id="M18"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M19"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula> allows SVP to operate in unstacked way (i.e., loading each subject's dataset at a time instead of stacking all datasets to form <italic>Y</italic>) and would require at most two dataloads per subject [one for Equation (6) and one for Equations (7) and (8)].</p><p>SVP is much faster compared to EVD as the voxel dimension is smaller by at least a factor of 8 but only gives an approximate PCA solution. SVP PCA estimates are a great initial solution for any of the randomized PCA methods discussed later, inducing to considerably faster convergence.</p></sec><sec><title>Sub-sampled time PCA (STP)</title><p>The time (stacked) dimension increases as more and more subjects are analyzed in a group PCA framework (Figure <xref ref-type="fig" rid="F1">1</xref>). By default, initial versions of the GIFT toolbox (Calhoun et al., <xref rid="B9" ref-type="bibr">2004</xref>) used a three-step data reduction method for large dataset analysis in order to reduce the memory requirements from the group PCA framework of Figure <xref ref-type="fig" rid="F1">1</xref>. This three-step reduction operated as follows: (1) reduced datasets from the first PCA step (<italic>Y</italic><sub><italic>i</italic></sub>) were randomly organized in groups of size <italic>g</italic> = 4; (2) PCA was applied on each group separately (including whitening<xref ref-type="fn" rid="fn0005"><sup>5</sup></xref>); (3) reduced group datasets were concatenated and a final PCA step was applied. This approach had the following shortcomings: (1) whitening in the intermediate group PCA (step two above) normalized the variance of components from each group and, therefore, the principal component weights were not correctly reflected in the final PCA step; (2) error of approximation increased if a low number of components was estimated in the intermediate group PCA step; (3) memory overhead increased if higher number of components were estimated in the intermediate group PCA step.</p><p>Here, we present a modified version of this three-step data reduction, which we call sub-sampled time PCA (STP). It estimates the PCA subspace <italic>X</italic> by incremental updates based on a different group (&#x0201c;sub-sample&#x0201d;) of subjects stacked in time. First, we do not use whitening in the intermediate group PCA (step two above). Second, the final group PCA space is incrementally updated, incorporating the estimates from the previous group PCA before the next group is considered. This reduces the memory overhead incurred by temporal concatenation. Third, a high number of components (around <italic>k</italic>&#x02032; = 500) is estimated in every group PCA update. The following equations summarize the proposed STP procedure for <italic>Mp</italic> &#x0003c; <italic>v</italic>:
<disp-formula id="E11"><label>(11)</label><mml:math id="M20"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E12"><label>(12)</label><mml:math id="M21"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E13"><label>(13)</label><mml:math id="M22"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E14"><label>(14)</label><mml:math id="M23"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable style="text-align:axis;" equalrows="false" columnlines="none" equalcolumns="false" class="array"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mi>&#x0039b;</mml:mi><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E15"><label>(15)</label><mml:math id="M24"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>W</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p><italic>C</italic><sub><italic>g</italic></sub> and <italic>F</italic><sub><italic>g</italic></sub> are the <italic>gp</italic> &#x000d7; <italic>gp</italic> covariance matrix in (stacked) time dimension and the eigenvectors of a group's data <italic>Y</italic><sub><italic>g</italic></sub>, respectively. Assuming <italic>gp</italic> &#x02265; <italic>k</italic>&#x02032;, eigenvectors <italic>F</italic><sub><italic>g</italic></sub> are projected onto the data <italic>Y</italic><sub><italic>g</italic></sub> to obtain a <italic>v</italic> &#x000d7; <italic>k</italic>&#x02032; subgroup PCA subspace <italic>X</italic><sub><italic>g</italic></sub> (Equation 13). Equations (11)&#x02013;(13) are repeated for the next group to compute PCA estimates for data <italic>Y</italic><sub><italic>g</italic>+1</sub>. PCA estimates of group <italic>Y</italic><sub><italic>g</italic></sub> and group <italic>Y</italic><sub><italic>g</italic>+1</sub> are stacked in time dimension and the common <italic>v</italic> &#x000d7; <italic>k</italic>&#x02032; PCA subspace <italic>X</italic><sub><italic>g</italic></sub> is obtained using Equations (14) and (15), since typically 2<italic>k</italic>&#x02032; &#x0003c; <italic>v</italic> in this case. Equations (11)&#x02013;(15) are repeated until the last group is loaded. Only the <italic>k</italic> dominant PCA components are retained from the final matrix <italic>X</italic><sub><italic>g</italic></sub> at the end of the estimation: <inline-formula><mml:math id="M25"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p>STP requires only a single pass through the data to determine an approximate PCA space and is a very useful method when data loading is a bottleneck. Both the estimation accuracy and memory requirements are proportional to the number of subjects included in each group and number of components estimated in the intermediate group PCA. In this paper, we select number of subjects in each group as <italic>g</italic> = 20, which has a small memory burden and yet gives a great approximation to the group PCA solution. Of note, STP not only generalizes the original three-step PCA approach but also includes MELODIC's incremental group PCA (MIGP; Smith et al., <xref rid="B33" ref-type="bibr">2014</xref>) as a special case when <italic>g</italic> = 1.</p></sec></sec><sec><title>Large PCA</title><p>Large PCA (Halko et al., <xref rid="B17" ref-type="bibr">2011a</xref>) is a randomized version of the block Lanczos method (Kuczynski and Wozniakowski, <xref rid="B20" ref-type="bibr">1992</xref>) that estimates a low rank PCA approximation of matrix <italic>F</italic> [see Equation (2)]. In block Lanczos methods, intermediate subspace estimates from every previous iteration are retained, each forming an additional &#x0201c;block&#x0201d; for the next iteration. This is different from subspace iteration (discussed next), which <italic>updates</italic> the PCA estimates instead, refining them until convergence is achieved. Similar to subspace iteration, Large PCA also exploits the powers of <italic>YY</italic><sup><italic>T</italic></sup> to obtain the reduced PCA space <italic>X</italic>. The Large PCA algorithm operates as follows:</p><p>A Krylov subspace <italic>Kr</italic> based on powers of the <italic>YY</italic><sup><italic>T</italic></sup> matrix is generated iteratively from an initial standard Gaussian random matrix <italic>F</italic><sub>0</sub> of size <italic>Mp</italic> &#x000d7; <italic>b</italic>, where <italic>b</italic> is the block length (typically, slightly larger than <italic>k</italic>):
<disp-formula id="E16"><label>(16)</label><mml:math id="M26"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>F</italic><sub>0</sub> = <italic>G</italic><sub><italic>Mp</italic> &#x000d7; <italic>b</italic></sub>, <italic>X</italic><sub>0</sub> = <italic>YF</italic><sub>0</sub>, and <inline-formula><mml:math id="M27"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p><italic>Kr</italic> is of size <italic>v</italic> &#x000d7; (<italic>j</italic> + 1)<italic>b</italic>, where <italic>v</italic> is the number of voxels, and <italic>j</italic> &#x02265; 1 is the number of additional blocks required to obtain an accurate solution. The formation of <italic>Kr</italic> requires (<italic>j</italic> + 1)<italic>M</italic> dataloads and only one subject's dataset in memory at a time (unstacked <italic>Y</italic>). Of course, if enough RAM is available in the system to retain all subject's datasets in memory simultaneously (stacked <italic>Y</italic>), then <italic>M</italic> dataloads would suffice to compute <italic>Kr</italic> and also Equations (17)&#x02013;(21) below.</p><p>After <italic>Kr</italic> is formed, an economy-size QR decomposition is performed on it (the columns of &#x003c7; are orthonormal and <italic>R</italic> is an upper triangular real valued matrix):
<disp-formula id="E17"><label>(17)</label><mml:math id="M28"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c7;</mml:mi><mml:mi>R</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Following, &#x003c7; is projected onto the data matrix as follows:
<disp-formula id="E18"><label>(18)</label><mml:math id="M29"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="-tex-caligraphic">F</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>&#x003c7;</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
and compute an economy-size SVD on matrix <inline-formula><mml:math id="M30"><mml:mrow><mml:mi mathvariant="-tex-caligraphic">F</mml:mi></mml:mrow></mml:math></inline-formula>:
<disp-formula id="E19"><label>(19)</label><mml:math id="M31"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="-tex-caligraphic">F</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>S</mml:mi><mml:mi>W</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In order to obtain the PCA space <italic>X</italic>, the matrix product below is more efficient than Equation (3) because it does not require the additional dataload:
<disp-formula id="E20"><label>(20)</label><mml:math id="M32"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c7;</mml:mi><mml:mi>W</mml:mi><mml:msqrt><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Finally, retrieve only the first <italic>k</italic> dominant columns of <italic>X</italic> and <italic>F</italic>, and use the first <italic>k</italic> rows and <italic>k</italic> columns of <italic>S</italic>. Note that Equation (18) requires <italic>M</italic> additional dataloads, for a total of (<italic>j</italic> + 2)<italic>M</italic> dataloads in Large PCA with fixed <italic>j</italic> (unstacked <italic>Y</italic>).</p><p>The choice of <italic>j</italic> is problem dependent: for fixed dataset and datatype, increasing <italic>j</italic> dictates the attainable accuracy with respect to EVD; on the other hand, a fixed <italic>j</italic> gives different accuracy for different datasets and datatypes. Thus, the recommendation in the original publication (Halko et al., <xref rid="B17" ref-type="bibr">2011a</xref>) was to set <italic>b</italic> to a large-enough value that would guarantee accuracy of the solution for a given data type, using fixed <italic>j</italic> = 2. While this approach guarantees a small number of dataloads, it does so by increasing the memory burden, due to larger <italic>b</italic>. In our experiments using the recommended settings, we have noticed that the memory usage<xref ref-type="fn" rid="fn0006"><sup>6</sup></xref> incurred was much undesirable for large fMRI datasets. Moreover, the attained accuracy with respect to EVD seemed inconsistent across different fMRI datasets, suggesting every new dataset would require specific adjustments for better accuracy. Although increasing the size of <italic>Kr</italic> with larger <italic>b</italic> and/or larger <italic>j</italic> improves accuracy, without a direct check for convergence only blind adjustments are possible with the recommended approach. We then noticed that convergence could be assessed by computing the norm of the difference between the top <italic>k</italic> singular values of sequentially increasing <italic>Kr</italic> (Equation 16) and verifying that it meets some tolerance &#x003b4;, as indicated in Equation (21). However, this implies that Equations (17)&#x02013;(19) need to be computed for each <italic>X</italic><sub><italic>j</italic></sub> increment to <italic>Kr</italic>. Besides the additional computational burden, this increases the number of dataloads to (2<italic>j</italic> + 1)<italic>M</italic>. Based on an analysis presented in the Appendix of Supplementary Material (see Section Parameter Selection for Large PCA) a good compromise was to fix <italic>b</italic> = 170, generate the initial <italic>Kr</italic> with <italic>j</italic><sub>0</sub> = 6 before the first estimation of the singular values (Equation 21), and continue to augment <italic>Kr</italic> and estimate its singular values for <italic>j</italic> &#x0003e; <italic>j</italic><sub>0</sub> until convergence was attained. Our approach incurs a total of <italic>j</italic>2<italic>M</italic> &#x02212; <italic>j</italic><sub>0</sub><italic>M</italic> + 2<italic>M</italic> dataloads for <italic>j</italic> &#x0003e; <italic>j</italic><sub>0</sub> and some additional computational effort, but controlled memory usage, all while guaranteeing the accuracy of the solution with respect to EVD. Finally, Equation (20) is computed only after convergence is attained.</p><disp-formula id="E21"><label>(21)</label><mml:math id="M33"><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mtext>&#x0200a;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mtext>&#x0200a;</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mi>&#x003b4;</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p>The time complexity of large PCA over all <italic>j</italic> &#x0003e; <italic>j</italic><sub>0</sub> iterations is <italic>O</italic>(<italic>Mpvbj</italic>). Since both Equations (16) and (18) can be implemented with a &#x0201c;for loop,&#x0201d; only one subject's dataset is required in memory at a time. Thus, Large PCA (un-stacked) would be suitable for large group-level PCA of fMRI data. Finally, when Large PCA is initialized with STP or SVP, <italic>X</italic><sub>0</sub> is set to equal <italic>X</italic><sup><italic>STP</italic></sup> or <italic>X</italic><sup><italic>SVP</italic></sup>, respectively, and we recommend <italic>j</italic><sub>0</sub> = 1 since convergence is attained much faster with this initialization.</p></sec><sec><title>Multi power iteration (MPOWIT)</title><p>Power iteration is an iterative technique which uses powers of the covariance matrix <inline-formula><mml:math id="M34"><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> to estimate one component (a single column of <italic>X</italic>) at a time, with subsequent components determined after removing the variance associated with previous components from the data (known as deflationary mode). While the powers of <italic>C</italic><sup><italic>v</italic></sup> contain the same eigenvectors (<italic>X</italic>) as <italic>C</italic><sup><italic>v</italic></sup> itself, the largest eigenvalues become more dominant, emphasizing the direction of largest variability. However, power iteration techniques require a normalization step to avoid ill-conditioned situations. Different normalization approaches and the choice of initial PCA subspace mark the key differences among power iteration techniques. In traditional power iteration, the <italic>L</italic><sub>2</sub>-norm of the PCA estimates is used for normalization in each iteration, as shown below:
<disp-formula id="E22"><label>(22)</label><mml:math id="M35"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E23"><label>(23)</label><mml:math id="M36"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E24"><label>(24)</label><mml:math id="M37"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Subspace iteration (Rutishauser, <xref rid="B30" ref-type="bibr">1970</xref>), also known as orthogonal iteration, extends power iteration to estimate multiple components simultaneously from the data (known as symmetric mode). It also uses powers of the covariance matrix <inline-formula><mml:math id="M38"><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, iteratively estimating a subspace projection that contains the top <italic>k</italic> components of the PCA space <italic>X</italic>. It typically uses QR factorization, instead of <italic>L</italic><sub>2</sub>-norm, to orthonormalize intermediate estimates <italic>X</italic><sub><italic>j</italic></sub> at each iteration and prevent them from becoming ill-conditioned. The following equations summarize subspace iteration (Saad, <xref rid="B31" ref-type="bibr">2011</xref>):
<disp-formula id="E25"><label>(25)</label><mml:math id="M39"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E26"><label>(26)</label><mml:math id="M40"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E27"><label>(27)</label><mml:math id="M41"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>orth</italic>(&#x000b7;) is an operation that returns an orthonormal basis, such as the QR factorization. The algorithm is initialized with a <italic>v</italic> &#x000d7; <italic>k</italic> Gaussian random matrix. <italic>X</italic><sub><italic>j</italic></sub> is the subspace estimate at the <italic>j</italic><sup><italic>th</italic></sup> iteration. Equations (26) and (27) are iterated until convergence. Subspace iteration is straightforward to implement but has slow convergence, especially for the last few eigenvalues, which converge much more slowly. Preconditioning techniques like shift-and-invert and Chebyshev polynomial (Saad, <xref rid="B31" ref-type="bibr">2011</xref>) have been used on the covariance matrix to accelerate the subspace iteration method. Still, computing the covariance matrix is costly when the data are large. Hence, subspace iteration is not a popular method as compared to block Lanczos methods.</p><p>Here, we introduce a novel method called MPOWIT, which accelerates the subspace iteration method. It relies on making the projecting subspace larger than the desired eigen space in order to overcome the slow convergence associated with the subspace iteration approach. The MPOWIT algorithm starts with a standard Gaussian random matrix of size <italic>v</italic> &#x000d7; <italic>lk</italic>, following with an initial power iteration and the set of operations below:
<disp-formula id="E28"><label>(28)</label><mml:math id="M42"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c7;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:msup><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x0039b;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>0</mml:mn></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="E29"><label>(29)</label><mml:math id="M43"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="-tex-caligraphic">F</mml:mi><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E30"><label>(30)</label><mml:math id="M44"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E31"><label>(31)</label><mml:math id="M45"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>v</italic> is the number of voxels, <italic>l</italic> is an integer multiplier, and <italic>k</italic> is the number of desired eigenvectors. The main innovation in MPOWIT stems from the realization, through experience, that a small fraction of the top principal components converges much faster than the rest. Thus, a larger subspace leads to fast retrieval of the top <italic>k</italic> components when <italic>k</italic> is only a fraction of that subspace's dimensionality. We also propose a faster implementation of <italic>orth</italic>(&#x000b7;) for MPOWIT to return an orthonormal basis for the column space of its operand efficiently. Since <italic>lk</italic> is typically small compared to the rank of the data, Equations (2) and (3) can be used as follows: first, perform a full EVD of <inline-formula><mml:math id="M46"><mml:msubsup><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="-tex-caligraphic">F</mml:mi><mml:mi mathvariant="-tex-caligraphic">D</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="-tex-caligraphic">F</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> to obtain <inline-formula><mml:math id="M47"><mml:mrow><mml:mi mathvariant="-tex-caligraphic">F</mml:mi></mml:mrow></mml:math></inline-formula>, followed by <inline-formula><mml:math id="M48"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="-tex-caligraphic">F</mml:mi><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> where <italic>L</italic> is a diagonal matrix containing the <italic>L</italic><sub>2</sub>-norm of each column of &#x003c7;<sub><italic>j</italic>&#x02212;1</sub><inline-formula><mml:math id="M49"><mml:mrow><mml:mi mathvariant="-tex-caligraphic">F</mml:mi></mml:mrow></mml:math></inline-formula>. This strategy is typically two or three times faster than economy-size QR factorization (based on the default MATLAB implementations) and not memory intensive for <italic>lk</italic> &#x02264; 500 (for <italic>k</italic> = 100, we set <italic>l</italic> = 5 based on the analysis in Appendix Section How to Select the Projecting Subspace Size (<italic>l</italic>) for MPOWIT?; Supplementary Material). Furthermore, since computing <italic>YY</italic><sup><italic>T</italic></sup> on large data is inefficient in memory, the associative matrix multiplications shown in the center and right hand side of Equation (30) are used instead. Finally, Equation (31) is the EVD of <inline-formula><mml:math id="M50"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, which is implemented using the function <italic>eigs</italic>(&#x000b7;)<xref ref-type="fn" rid="fn0007"><sup>7</sup></xref> in MATLAB to efficiently retrieve only the top <italic>k</italic> eigenvalues &#x0039b;<sub><italic>j</italic></sub>. Equations (29)&#x02013;(31) are iterated for <italic>j</italic> &#x02265; 1 until the top <italic>k</italic> eigenvalues in the subspace projection converge to within the specified tolerance &#x003b4;, as shown in Equation (32), and the choice of &#x0039b;<sub>0</sub> = 0, where 0 is a <italic>k</italic> &#x000d7; <italic>k</italic> matrix of zeros, guarantees the algorithm will not stop before <italic>j</italic> = 2:
<disp-formula id="E32"><label>(32)</label><mml:math id="M51"><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>&#x0039b;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x0039b;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mtext>&#x0200a;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mtext>&#x0200a;</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>After convergence, the reduced PCA space <italic>X</italic> is obtained as:
<disp-formula id="E33"><label>(33)</label><mml:math id="M52"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
and the eigenvectors <italic>F</italic> follow from <italic>F</italic> = <italic>Y</italic><sup><italic>T</italic></sup><italic>X</italic>, then normalize the columns of <italic>F</italic> to unit <italic>L</italic><sub>2</sub>-norm. A rank <italic>k</italic> PCA approximation is obtained by retrieving the first <italic>k</italic> dominant columns of the matrices <italic>X</italic> and <italic>F</italic>.</p><p>The time complexity of MPOWIT over all <italic>j</italic> iterations is <italic>O</italic>(<italic>Mpvlkj</italic>). Based on Equation (30), the algorithm requires only <italic>M</italic> dataloads per iteration and a total of (<italic>j</italic> + 1)<italic>M</italic> dataloads, which makes MPOWIT scalable for large data analysis. Furthermore, based on the original power iteration algorithm, simple <italic>L</italic><sub>2</sub>-norm normalization of the columns of <italic>X</italic><sub><italic>j</italic></sub> without full orthogonalization would suffice in Equation (29). However, in our experiments, we determined that assessments about the convergence of the algorithm are considerably more reliable if they are based on the eigenvalues &#x0039b;<sub><italic>j</italic></sub> obtained from orthogonal <italic>X</italic><sub><italic>j</italic></sub> instead. Thus, explicit orthonormalization of <italic>X</italic><sub><italic>j</italic></sub> in each iteration is preferred.</p><p>Also, when MPOWIT is initialized with STP or SVP, <italic>X</italic><sub>0</sub> is set as the top <italic>lk</italic> components of <italic>X</italic><sup><italic>STP</italic></sup> or <italic>X</italic><sup><italic>SVP</italic></sup>, respectively, and <inline-formula><mml:math id="M53"><mml:msub><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> or <inline-formula><mml:math id="M54"><mml:msub><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> since all eigenvalues are available at the end of the STP and SVP procedures. Initializing with a sub-sampling technique accelerates convergence and, more importantly, prevents dataloads from becoming a bottleneck in the analysis pipeline. Lastly, note that MPOWIT differs from a block version of subspace iteration (Mitliagkas et al., <xref rid="B27" ref-type="bibr">2013</xref>) where regular subspace iteration is applied in &#x0201c;online&#x0201d; or &#x0201c;streaming memory&#x0201d; mode, i.e., making a single pass over the data. Although, this approach minimizes the number of dataloads (to <italic>M</italic>), the PCA solution is only approximate with respect to the EVD solution and, thus, not recommended for group ICA of fMRI data.</p></sec><sec><title>MPOWIT and expectation maximization PCA (EM PCA)</title><p>To complete our discussion on methods for PCA of large datasets, we present expectation maximization PCA (EM PCA). Our focus is on the connections and the implications that certain MPOWIT concepts have on this popular technique. EM PCA (Roweis, <xref rid="B29" ref-type="bibr">1997</xref>) uses expectation and maximization steps to determine the PCA subspace. The algorithm operates as follows:
<disp-formula id="E34"><label>(34)</label><mml:math id="M55"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E35"><label>(35)</label><mml:math id="M56"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="E36"><label>(36)</label><mml:math id="M57"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In Equation (34), a Gaussian random matrix of dimensions <italic>v</italic> &#x000d7; <italic>k</italic> is selected as the initial PCA subspace. In the expectation step (Equation 35), the PCA subspace <italic>X</italic><sub><italic>j</italic>&#x02212;1</sub> is fixed and the transformation matrix <italic>F</italic><sub><italic>j</italic></sub> is determined, while in the maximization step (Equation 36), <italic>F</italic><sub><italic>j</italic></sub> is fixed and the subspace <italic>X</italic><sub><italic>j</italic></sub> is determined. Equations (35) and (36) are iterated until the algorithm converges to within the specified error for tolerance as shown below:
<disp-formula id="E37"><label>(37)</label><mml:math id="M58"><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mtext>&#x0200a;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mtext>&#x0200a;</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>After convergence, the reduced PCA space <italic>X</italic> is determined using Equations (31) and (33): first, perform a full EVD of <inline-formula><mml:math id="M59"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>, followed by <inline-formula><mml:math id="M60"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>.</p><p>The time complexity of EM PCA over all iterations is only <italic>O</italic>(<italic>Mpvkj</italic>) but it takes a considerably larger number of iterations to converge when compared to Large PCA and MPOWIT methods. This is because EM PCA has the same convergence properties of subspace iteration. In fact, as we prove in Appendix Section Proof: MPOWIT and EM-PCA Converge to the Same PCA Subspace (<italic>X</italic>) (Supplementary Material), EM PCA returns the same subspace estimate as MPOWIT if both run for the same number of iterations and use the same initial guess. Naturally, the acceleration schemes used for subspace iteration [see Section Multi Power Iteration (MPOWIT) and Appendix Section How to Select the Projecting Subspace Size (<italic>l</italic>) for MPOWIT?; Supplementary Material] are equally applicable and useful for EM PCA. However, as seen from Equations (35) and (36), EM PCA requires loading the data into memory <italic>twice</italic> if un-stacked PCA is performed. Since dataloading is a bottleneck for very large group analyses, EM PCA is still slower than MPOWIT when PCA is carried out on un-stacked data (i.e., on each <italic>Y</italic><sub><italic>i</italic></sub> at a time rather than the entire stacked <italic>Y</italic> at once). Therefore, we forgo further comparisons with EM PCA in the Results Section.</p><p>As a final remark on methods, our MPOWIT method relates to normalized power iteration (Martinsson et al., <xref rid="B26" ref-type="bibr">2010</xref>; Halko et al., <xref rid="B18" ref-type="bibr">2011b</xref>). However, normalized power iteration is more a variation of the EM PCA method where both the expectation (Equation 35) and maximization (Equation 36) steps are orthonormalized. Hence, the subspaces in each iteration are the same for both EM PCA and normalized power iteration. Also, we note that orthonormalization of the expectation step (Equation 35) in normalized power iteration is redundant and, therefore, normalized power iteration has the same shortcomings as EM PCA in large un-stacked group analyses, i.e., both require two dataloads per iteration. Thus, the time required to solve group PCA is significantly longer than with MPOWIT when a large number of subjects are included [see definition of &#x0201c;large&#x0201d; in Section Sub-Sampled Time PCA (STP)].</p></sec></sec><sec><title>Data and preprocessing</title><p>We use 1600 pre-processed subjects from resting state fMRI data (a superset of the data presented in Allen et al., <xref rid="B1" ref-type="bibr">2011</xref>) and perform group PCA. Pre-processing steps include image realignment using INRIalign (Freire et al., <xref rid="B14" ref-type="bibr">2002</xref>), slice-timing correction using the middle slice as reference, spatial normalization (Friston et al., <xref rid="B15" ref-type="bibr">1995</xref>) and 3D Gaussian smoothing with a kernel size of 10 &#x000d7; 10 &#x000d7; 10 mm. No normalization is done on the BOLD fMRI timeseries (e.g., dividing by the variance or mean as is sometimes done for ICA approaches). Scans from 3 to 150 are included in the analysis to match the same time-points across subjects. A common mask is applied on all subjects to include only in-brain voxels. The common mask for all the subjects is generated by using element-wise multiplication on the individual subject masks. A widely used approach to generate individual subject masks is to eliminate non-brain voxels by keeping voxels with values above or equal to the mean over an entire volume for each timepoint.</p><p>In the initial subject-level PCA step of group ICA (Calhoun et al., <xref rid="B8" ref-type="bibr">2001</xref>), each individual subject's fMRI data of dimensions <italic>v</italic> &#x000d7; <italic>t</italic> is reduced to a few whitened principal components of dimensions <italic>v</italic> &#x000d7; <italic>p</italic> (see Figure <xref ref-type="fig" rid="F1">1</xref>). We use EVD to reduce subject specific fMRI data and retain <italic>p</italic> = 100 components, capturing near-maximal individual subject variability during the first PCA step (Erhardt et al., <xref rid="B11" ref-type="bibr">2011</xref>). In the second PCA step (group-level PCA), data from each subject in the first PCA step is stacked along the (reduced) time dimension. As the number of subjects increases, the memory requirement increases since the data is temporally concatenated. On all reported experiments, without loss of generality, we used a typical fMRI acquisition setting where the whole scanning field is sampled at 3 &#x000d7; 3 &#x000d7; 3 mm voxel resolution (resulting in a matrix of dimensions 53 &#x000d7; 63 &#x000d7; 46) and the number of time points is 148 with TR = 2000 ms.</p></sec><sec><title>Experiments</title><p>A number of experiments have been conducted to assess memory usage and computation time for all group PCA methods discussed previously. Firstly, we assessed memory usage for varying number of subjects (<italic>M</italic>). Next, for each group-level PCA algorithm tested in this paper, we conducted 20 different group ICA analyses, varying the number of subjects (<italic>M</italic>) used in each analysis by 100, 200, 400, 800, and 1600, and the number of components (<italic>k</italic>) by 25, 50, 75, and 100. The group-level PCA algorithms considered were EVD, Large PCA, multi power iteration (MPOWIT), subsampled voxel PCA (SVP), and subsampled time PCA (STP). For STP, the number of subjects in each group (<italic>g</italic>) was selected to be as large as possible and such that the analysis did not exceed 4 GB RAM (<italic>g</italic> = 20 in our analyses). Since, Large PCA and MPOWIT could also be carried out by loading one dataset at a time per PCA iteration, we also included the un-stacked group PCA cases in the analyses. In total, 7 &#x000d7; (5 &#x000d7; 4) = 7 &#x000d7; 20 = 140 different group-level PCA cases were considered for comparison in terms of their accuracy and required computing time. In addition, the number of iterations until convergence was assessed for Large PCA and MPOWIT on each scenario. Finally, we illustrated the total group ICA pipeline computing times attainable using the MPOWIT method (stacked and un-stacked) in the group-level PCA stage (including dataloading times).</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><sec><title>Memory requirements</title><p>After applying a common binary mask to all time points, there were 66,745 in-brain voxels per time point. Figure <xref ref-type="fig" rid="F2">2</xref> summarizes the memory requirements for each group PCA algorithm applied on this data, using the parameters specified in Section Accuracy, Computing Time, and Convergence. Note that for <italic>M</italic> &#x02265; 800, the voxel dimension is smaller than the stacked time dimension (i.e., <italic>v</italic> &#x0003c; <italic>Mp</italic>). Still, we cannot use Equation (5) to compute the covariance matrix in the voxel dimension because the voxel dimension is also very large and it may take several hours to compute if loading each dataset at a time. Thus, EVD is not scalable for large data analysis as both the memory burden to compute the covariance matrix and the computational burden to solve the eigenvalue problem increase exponentially. On the other hand, the un-stacked versions of randomized PCA approaches like Large PCA and MPOWIT are scalable for large datasets, meaning that Large PCA and MPOWIT could load each subject's dataset <italic>Y</italic><sub><italic>i</italic></sub> at a time for each PCA iteration. Thus, un-stacked versions of these algorithms are also considered. Subsampled EVD methods like SVP and STP are also considered as these have fixed memory requirements and are independent of the number of subjects analyzed.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Approximate memory required in gigabytes (GB) to solve the group PCA problem</bold>. The number of in-brain voxels is <italic>v</italic> = 66745, the number of PCA components in the first PCA step is <italic>p</italic> = 100, the number of components in the second PCA step is <italic>k</italic> = 100 and the number of subjects (<italic>M</italic>) selected are 100, 200, 400, 800, and 1600. We used the number of block iterations <italic>j</italic> = 6 and block size <italic>b</italic> = 170 for Large PCA (see Appendix Section Parameter Selection for Large PCA; Supplementary Material), and the block multiplier <italic>l</italic> was set to 5 for MPOWIT (see Appendix Section How to Select the Projecting Subspace Size (<italic>l</italic>) for MPOWIT?; Supplementary Material). The number of subjects in each group (<italic>g</italic>) is set to 20 when estimating PCA using STP. We give the equations used to estimate the memory required by each PCA algorithm discussed in this paper in Appendix Section How Much Memory is Required during the Group-Level PCA Step? (Supplementary Material).</p></caption><graphic xlink:href="fnins-10-00017-g0002"/></fig><p>Some notes about multi-band EPI sequences and subject-level PCA are in order here. First, even if the fMRI data were acquired at a 2 &#x000d7; 2 &#x000d7; 2 mm voxel resolution (roughly, <italic>v</italic> = 180000 in-brain voxels) and collected for 30 min using multi-band EPI sequences with TR = 200 ms, there would be no significant impact on the memory required to solve the first PCA step, including computations of the subject-specific covariance matrix. This is because the time dimension would still be considered &#x0201c;small&#x0201d; since <italic>t</italic> = 30 &#x000d7; 60 &#x000d7; 5 = 9000 &#x0003c; 10000. With less than 10,000 time points, the first PCA step could be easily solved by loading the data in blocks along the voxel dimension, summing covariance matrices of dimension <italic>t</italic> &#x000d7; <italic>t</italic> across blocks, i.e., <inline-formula><mml:math id="M61"><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mi>&#x0039b;</mml:mi><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, and using the EVD (IRAM) method [<italic>eigs</italic>(&#x000b7;) function in MATLAB]. The memory required by EVD to solve the ensuing group-level PCA, however, increases with the number of components <italic>p</italic> retained in the first PCA step, the number of subjects <italic>M</italic>, and the number of in-brain voxels <italic>v</italic>, assuming the entire temporally stacked data <italic>Y</italic> is loaded in memory, which makes it unscalable. Unlike EVD, MPOWIT (un-stacked) would solve this group PCA problem using less than 4 GB RAM even if <italic>p</italic> was increased from 100 to 200 components and <italic>v</italic> was increased from 60000 to 180000 in-brain voxels, assuming <italic>M</italic> = 5000 subjects, highlighting its scalability strengths.</p></sec><sec><title>Accuracy, computing time, and convergence</title><p>Here, we present results for the group PCA experiments described in Section Data and Preprocessing. If not specified otherwise, all processes were tested on a server running Linux Centos OS release 6.4 with 512 GB RAM, and MATLAB R2012a. We also note that the files with the results from the initial subject-level PCA step were always saved as uncompressed &#x0201c;.mat&#x0201d; files to later speed up the data loading process during the group PCA step. The parameter settings used to solve the group PCA problem for each algorithm are described below:</p><list list-type="alpha-lower"><list-item><p><bold>EVD</bold>: The covariance matrix is always computed in the smallest dimension of the temporally stacked data. The IRAM algorithm is used to find the desired eigenvectors. In this study, we used MATLAB's <italic>eigs</italic> (&#x000b7;) function, which is built on the IRAM method. The maximum error tolerance selected was &#x003b4; = 10<sup>&#x02212;6</sup> and the maximum number of iterations was set to 1000.</p></list-item><list-item><p><bold>Large PCA</bold>: Here, we used settings based on the Pareto-optimal study in Appendix Section Parameter Selection for Large PCA (Supplementary Material). The block length was set to <italic>b</italic> = 170, the number of initial block iterations <italic>j</italic><sub>0</sub> was set to 6 and the error tolerance was set to 10<sup>&#x02212;6</sup>. Note that as the number of block iterations (<italic>j</italic>) executed until convergence increases, the performance of the algorithm decreases (i.e., higher memory and lower speed).</p></list-item><list-item><p><bold>MPOWIT</bold>: The maximum error tolerance and maximum number of iterations were set to 10<sup>&#x02212;6</sup> and 1000, respectively. The multiplier <italic>l</italic> was set to 5 (see Appendix Section How to Select the Projecting Subspace Size (<italic>l</italic>) for MPOWIT?; Supplementary Material) to improve the rate of convergence. As the value of <italic>l</italic> increases, the rate of convergence and accuracy increase but the computational performance decreases (i.e., higher memory and lower speed in each iteration).</p></list-item><list-item><p><bold>SVP</bold>: A value of 2 is selected as subsampling depth and 500 intermediate PCA components are estimated for odd and even voxel spaces.</p></list-item><list-item><p><bold>STP</bold>: The number of subjects included in each group is 20, and 500 intermediate PCA components are estimated per group.</p></list-item></list><p>The <italic>L</italic><sub>2</sub>-norm of the absolute difference between the top <italic>k</italic> eigenvalues obtained from the randomized PCA methods and those from the EVD (IRAM) method were used to determine the accuracy of the estimated PCA components. The eigenvalues from the EVD method are always considered the ground-truth.</p><p>From Figure <xref ref-type="fig" rid="F3">3</xref>, it is evident that both the Large PCA and MPOWIT methods give more accurate results than subsampled PCA methods SVP and STP across all model orders. Overall, STP estimates components with higher accuracy than SVP across all model orders. We also note that, generally, Large PCA estimates PCA components with slightly better accuracy than MPOWIT. Figure <xref ref-type="fig" rid="F4">4</xref> compares the computing time taken in minutes by each algorithm to solve the group-level PCA problem. Subsampled PCA methods like SVP and STP outperform EVD and unstacked versions of MPOWIT and Large PCA at the cost of accuracy. MPOWIT and Large PCA outperform EVD when the entire data is loaded in memory, i.e., when <italic>Y</italic> fits in RAM. When the data is not loaded in memory, MPOWIT (un-stacked) marginally outperforms EVD and Large PCA (un-stacked) at lower model orders (<italic>k</italic> = 25 and <italic>k</italic> = 50) whereas at higher model orders (<italic>k</italic> = 75 and <italic>k</italic> = 100), Large PCA (un-stacked) marginally outperforms both EVD and MPOWIT (un-stacked). Finally, Figure <xref ref-type="fig" rid="F5">5</xref> shows the number of iterations MPOWIT and Large PCA take to converge. Overall, Large PCA takes fewer iterations to converge than MPOWIT due to larger block sizes. However, note from Figure <xref ref-type="fig" rid="F2">2</xref> that MPOWIT (un-stacked) requires considerably less memory than Large PCA (un-stacked) with fairly sublinear increase in memory use as <italic>M</italic> increases.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Estimation error as compared to EVD (IRAM)</bold>. <italic>L</italic><sub>2</sub>-norm of error is computed between the eigenvalues of each method and the eigenvalues of the EVD method.</p></caption><graphic xlink:href="fnins-10-00017-g0003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Computing time (in minutes) taken to solve group-level PCA using EVD (IRAM), Large PCA, MPOWIT, SVP, and STP algorithms</bold>. Using different numbers of subjects and components. The computing time of both Large PCA (un-stacked) and MPOWIT (un-stacked) are also reported.</p></caption><graphic xlink:href="fnins-10-00017-g0004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Number of iterations required for convergence for both Large PCA and MPOWIT algorithms</bold>.</p></caption><graphic xlink:href="fnins-10-00017-g0005"/></fig></sec><sec><title>Group ICA and subject back-reconstruction</title><p>Spatial ICA was performed on the final group-level PCA components to determine maximally statistically independent components. The Infomax ICA algorithm (Bell and Sejnowski, <xref rid="B3" ref-type="bibr">1995</xref>) was repeated 10 times in an ICASSO framework (Himberg et al., <xref rid="B19" ref-type="bibr">2004</xref>) with a different random initialization at each run. The most stable run estimates were used instead of using centrotype estimates (Ma et al., <xref rid="B25" ref-type="bibr">2011</xref>). We used the GICA1 back-reconstruction method (Erhardt et al., <xref rid="B11" ref-type="bibr">2011</xref>) to reconstruct individual subject component maps and timecourses for each analysis. Individual subject component maps and timecourses were then scaled to Z-scores. In Figure <xref ref-type="fig" rid="F6">6</xref>, we illustrate the total group ICA analysis computing times attainable using the MPOWIT method (stacked and un-stacked, respectively) in the group-level PCA stage (including dataloading time).</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Total time to solve the entire group ICA analysis using MPOWIT for the group PCA step (in minutes)</bold>. Computing times when all the data is loaded in memory (stacked) and when datasets are loaded each at a time in every iteration (un-stacked).</p></caption><graphic xlink:href="fnins-10-00017-g0006"/></fig></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>We demonstrated the entire group ICA process including the group PCA step on a Linux server with 512 GB RAM. We infer from Figure <xref ref-type="fig" rid="F2">2</xref> that a large group ICA analysis using un-stacked versions of MPOWIT and Large PCA can be performed on machines with only 4 GB RAM. Moreover, the un-stacked versions of MPOWIT and Large PCA are designed to solve the group PCA problem by loading one dataset at a time. This is a nice feature which makes these algorithms ideal for PCA of &#x0201c;big data&#x0201d; where chunks of data can be extracted one at a time using memory mapping techniques.</p><p>Comparing among EVD, Large PCA, and MPOWIT, we notice that MPOWIT and Large PCA outperform EVD (IRAM) in terms of speed when all datasets are already loaded in memory. As depicted in Figure <xref ref-type="fig" rid="F4">4</xref>, pink (Large PCA) and light green (MPOWIT) bars are always shorter than the blue (EVD) bar. When only one subject's dataset can be loaded in memory at a time, in which case un-stacked MPOWIT and un-stacked Large PCA have to be used, the computation time increases as showed by the dark green (MPOWIT un-stacked) and red bars (Large PCA un-stacked) in Figure <xref ref-type="fig" rid="F4">4</xref>. Overall, blue and dark green bars are comparable at various model orders. Dark green bars marginally outperform dark red and blue bars at lower model orders whereas dark red bars marginally outperform dark green and blue bars at higher model orders. We note that un-stacked Large PCA uses larger block size (Krylov subspace) than un-stacked MPOWIT and, therefore, converges faster. In our experiments, both Large PCA and MPOWIT take at least seven iterations to converge to the PCA solution. With increasing datasets, data loading could be a big bottleneck in computational performance. To speed up the process further, we recommend using PCA estimates from subsampled PCA [Section Sub-sampled Time PCA (STP)] instead of random initialization. In our examples, MPOWIT and Large PCA methods provide very similar principal components, differing by less than 10<sup>&#x02212;6</sup>.</p><p>The PCA methods we discussed in this paper are generic and can be applied to any dataset without any major modifications. However, our goal here is to demonstrate the applicability of these algorithms to real-valued fMRI data in the context of group ICA.</p><p>The execution time for the largest group ICA analysis in this paper, i.e., 1600 subjects and 100 independent components, using the un-stacked version of our MPOWIT PCA algorithm for the group-level PCA, was 329 min (5.48 h, Figure <xref ref-type="fig" rid="F6">6</xref>). Reloading the datasets from the first PCA step was fast in our Linux server due to the Operating System's &#x0201c;cache effect.&#x0201d; We repeated the 1600 subject group-PCA problem with model orders 50 and 100 on a Windows desktop with 4 GB RAM using both MPOWIT and Large PCA. To speed up the PCA estimation of MPOWIT and Large PCA, 500 PCA components from subsampled PCA [Section Sub-Sampled Time PCA (STP)] were used as the initial PCA subspace. Since components estimated by STP method are more accurate than SVP method (Figure <xref ref-type="fig" rid="F3">3</xref>), STP components are used as initial subspace. Figure <xref ref-type="fig" rid="F7">7</xref> shows that both MPOWIT and Large PCA successfully recovered 100 group PCA components with high accuracy in little more than 3 h (including the STP PCA estimation). MPOWIT required three iterations and large PCA required two block iterations to solve the largest group PCA problem in this paper which would have been an impossible problem to solve using EVD or assuming the entire (reduced) data from the first PCA step had to be loaded in memory during the group PCA step (Figure <xref ref-type="fig" rid="F2">2</xref>). A significant speedup can still be achieved if the entire group ICA process is run in parallel using, for example, GPU acceleration or distributed clusters.</p><fig id="F7" position="float"><label>Figure 7</label><caption><p><bold>Comparison of MPOWIT and Large PCA on Windows desktop with 4 GB RAM</bold>. Initial PCA subspace is selected from STP method to accelerate algorithms MPOWIT and Large PCA for very large data (both un-stacked). Error in top row second column is computed using <italic>L</italic><sub>2</sub> - norm of difference between the eigenvalues obtained from the selected PCA method and EVD method.</p></caption><graphic xlink:href="fnins-10-00017-g0007"/></fig><p>Our results also emphasize that EVD is not preferred for large scale analysis as there is an extra cost in storing the covariance matrix in memory. One way to overcome this issue is based on Equation (5). If one dimension of the data is fixed, the covariance matrix can be computed in that dimension and the net covariance matrix could be computed by adding the covariance matrices across subjects. Moreover, since the covariance matrix is symmetric, only the lower or upper triangular portions need to be stored in memory (for eigenvalue problems). LAPACK<xref ref-type="fn" rid="fn0008"><sup>8</sup></xref> eigen solvers such as DSPEVX or SSPEVX leverage this property and use only lower or upper triangular portions of the matrix. However, the performance (computing speed) of these algorithms will still be poor if the covariance matrix is very large.</p><p>The PCA methods applied for performing spatial group ICA are equally valid for temporal group ICA (Smith et al., <xref rid="B34" ref-type="bibr">2012</xref>). In temporal group ICA, subject-level PCA is not performed and, thus, group PCA is computed directly on the preprocessed fMRI datasets stacked along the time dimension (Boubela et al., <xref rid="B5" ref-type="bibr">2013</xref>). The memory requirements for the temporal group PCA step using the PCA methods we presented here could be computed using <italic>p</italic> = <italic>t</italic> (See Appendix Section How Much Memory is Required during the Group-Level PCA Step?; Supplementary Material). Notice that loading the full preprocessed fMRI datasets instead of loading just the subject-level PCA results (as in spatial group PCA) is considerably more time-consuming. Conveniently, our MPOWIT PCA approach supports processing un-stacked datasets and, thus, is highly suitable for very large temporal group PCA of multi-band EPI fMRI.</p><p>Based on the methods discussed here, we present our findings in a flowchart (Figure <xref ref-type="fig" rid="F8">8</xref>), indicating our recommendations for selection of the PCA method given the problem size and its order of complexity (<italic>k</italic>). The EVD (IRAM) method is preferred when the problem size is small (i.e., <italic>v</italic> &#x02264; 10000 or <italic>Mp</italic> &#x02264; 10000). Note, however, that MPOWIT could also be applied directly on the covariance matrix [Equation (30), left-hand side]. Thus, if the smallest dimension of the data is less than or equal to 10000, applying MPOWIT on the covariance matrix requires only a single dataload per subject and is more computationally efficient than Equation (30) (right-hand side), which requires one dataload per subject in each iteration. Finally, randomized PCA methods such as MPOWIT, EM PCA (accelerated), and Large PCA are efficient in memory and/or speed on large data problems that could be fit in the computer RAM. However, when the data is too large to fit in the computer RAM, un-stacked versions of MPOWIT or Large PCA are preferred with PCA subspace initialization from STP method. At most 2&#x02013;3 iterations are required to estimate PCA components with high accuracy when the PCA subspace is initialized using STP. However, the performance of Large PCA is highly dependent on the correct tuning of <italic>j</italic><sub>0</sub> and <italic>b</italic> (see Appendix Section Parameter Selection for Large PCA; Supplementary Material) for each given dataset. MPOWIT, on the other hand, seems to produce comparable results with little need for tuning of <italic>l</italic> (see Appendix Section How to Select the Projecting Subspace Size (<italic>l</italic>) for MPOWIT?; Supplementary Material).</p><fig id="F8" position="float"><label>Figure 8</label><caption><p><bold>PCA Algorithm selection</bold>. This flowchart summarizes our recommendations for selection of the most appropriate PCA method given the dimensions of the data (v &#x000d7; Mp), where v is the number of in-brain voxels, <italic>p</italic> is the number of components retained in the first PCA step, and <italic>M</italic> is the number of subjects.</p></caption><graphic xlink:href="fnins-10-00017-g0008"/></fig></sec><sec sec-type="conclusions" id="s5"><title>Conclusions</title><p>We presented a new approach for PCA-based data reduction for group ICA called MPOWIT and demonstrated that it can efficiently solve the large scale PCA problem without compromising accuracy. The un-stacked version of MPOWIT takes almost the same time to complete the analysis as compared to EVD but requires much less RAM. We showed that MPOWIT enables group ICA on very large cohorts using standard fMRI acquisition parameters within 4 GB RAM. Computationally efficient data reduction approaches like MPOWIT are becoming more important due to the larger datasets resulting from new studies using high-frequency multi-band EPI sequences and from an increased tendency to share data in the neuroimaging community. Even in such challenging scenarios, un-stacked MPOWIT could realistically solve group-level PCA on virtually any number of subjects, limited only by time, not memory, constraints. Given its high scalability and right fit for parallelism, MPOWIT sets the stage for future groundbreaking developments toward extremely efficient PCA of big data using GPU acceleration and distributed implementations.</p></sec><sec id="s6"><title>Author contributions</title><p>SR&#x02014;Developed PCA algorithm (MPOWIT) for data reduction. RS&#x02014;Worked extensively on revising paper and helped with mathematical proofs. JL&#x02014;Gave good feedback on improving the manuscript. VC&#x02014;Gave good feedback on improving the manuscript and also work was done under his supervision.</p></sec><sec><title>Funding</title><p>This work was funded by NIH 2R01EB000840, R01EB020407, and COBRE 5P20RR021938/P20GM103472 (Calhoun).</p><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>The authors would like to thank Tim Mounce, IT administrator at the MIND Research Network, for valuable help installing and mounting software and drives in our Linux servers. We would also like to thank Dr. T&#x000fc;lay Adal&#x00131;, Professor at the University of Maryland Baltimore County, for giving valuable comments.</p></ack><fn-group><fn id="fn0001"><p><sup>1</sup>The choice of whitening or not the subject-level data changes the final group PCA estimates (Calhoun et al., <xref rid="B10" ref-type="bibr">2015</xref>). However, all it does is preprocess the input data in a different way. While we advocate whitening of subject-level fMRI data for group spatial ICA due to its denoising properties, it remains optional and open for debate (Smith et al., <xref rid="B33" ref-type="bibr">2014</xref>). However, whitening of subject-level data does not alter the correctness of the group PCA methods we present in this paper, which are correct regardless of the choice of preprocessing.</p></fn><fn id="fn0002"><p><sup>2</sup><ext-link ext-link-type="uri" xlink:href="http://mialab.mrn.org/software/gift/">http://mialab.mrn.org/software/gift/</ext-link></p></fn><fn id="fn0003"><p><sup>3</sup>Direct reduction of the largest dimension of subject-level data is <italic>never</italic> recommended as it causes greater loss of information and largely inferior estimates than otherwise. Instead, a better approach is to carry out group PCA in the time dimension as in Figure <xref ref-type="fig" rid="F1">1</xref>, <italic>without</italic> subject-level whitening, followed by <inline-formula><mml:math id="M62"><mml:msup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0039b;</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02215;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to perform reduction of the voxel dimension. Group temporal ICA can then be carried out on the temporally concatenated <inline-formula><mml:math id="M63"><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></inline-formula></p></fn><fn id="fn0004"><p><sup>4</sup>Large data means <italic>both</italic> dimensions of the data are larger than 10000 (assuming the data is stored using double precision).</p></fn><fn id="fn0005"><p><sup>5</sup>This was in addition to subject-level whitening.</p></fn><fn id="fn0006"><p><sup>6</sup>Notice that the Krylov subspace size increases with larger block sizes, as well as the time to solve economy-size singular value decomposition for each iteration.</p></fn><fn id="fn0007"><p><sup>7</sup>MATLAB's <italic>eigs</italic>(&#x000b7;) function is based on the IRAM method to estimate desired eigenvectors and eigenvalues.</p></fn><fn id="fn0008"><p><sup>8</sup><ext-link ext-link-type="uri" xlink:href="http://www.netlib.org/lapack/">http://www.netlib.org/lapack/</ext-link></p></fn></fn-group><sec sec-type="supplementary-material" id="s7"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnins.2016.00017">http://journal.frontiersin.org/article/10.3389/fnins.2016.00017</ext-link></p><p><supplementary-material content-type="local-data" id="SM1"><media xlink:href="DataSheet1.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></p></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>E. A.</given-names></name><name><surname>Erhardt</surname><given-names>E. B.</given-names></name><name><surname>Damaraju</surname><given-names>E.</given-names></name><name><surname>Gruner</surname><given-names>W.</given-names></name><name><surname>Segall</surname><given-names>J. M.</given-names></name><name><surname>Silva</surname><given-names>R. F.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>A baseline for the multivariate comparison of resting-state networks</article-title>. <source>Front. Syst. Neurosci.</source>
<volume>5</volume>:<issue>2</issue>. <pub-id pub-id-type="doi">10.3389/fnsys.2011.00002</pub-id><pub-id pub-id-type="pmid">21442040</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name></person-group> (<year>2004</year>). <article-title>Probabilistic independent component analysis for functional magnetic resonance imaging</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>23</volume>, <fpage>137</fpage>&#x02013;<lpage>152</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2003.822821</pub-id><pub-id pub-id-type="pmid">14964560</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>A. J.</given-names></name><name><surname>Sejnowski</surname><given-names>T. J.</given-names></name></person-group> (<year>1995</year>). <article-title>An information maximisation approach to blind separation and blind deconvolution</article-title>. <source>Neural Comput</source>. <volume>7</volume>, <fpage>1129</fpage>&#x02013;<lpage>1159</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id><pub-id pub-id-type="pmid">7584893</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswal</surname><given-names>B. B.</given-names></name><name><surname>Mennes</surname><given-names>M.</given-names></name><name><surname>Zuo</surname><given-names>X.-N.</given-names></name><name><surname>Gohel</surname><given-names>S.</given-names></name><name><surname>Kelly</surname><given-names>C.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Toward discovery science of human brain function</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>107</volume>, <fpage>4734</fpage>&#x02013;<lpage>4739</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0911855107</pub-id><pub-id pub-id-type="pmid">20176931</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boubela</surname><given-names>R. N.</given-names></name><name><surname>Kalcher</surname><given-names>K.</given-names></name><name><surname>Huf</surname><given-names>W.</given-names></name><name><surname>Kronnerwetter</surname><given-names>C.</given-names></name><name><surname>Filzmoser</surname><given-names>P.</given-names></name><name><surname>Moser</surname><given-names>E.</given-names></name></person-group> (<year>2013</year>). <article-title>Beyond noise: using temporal ICA to extract meaningful information from high-frequency fMRI signal fluctuations during rest</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>7</volume>:<issue>168</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00168</pub-id><pub-id pub-id-type="pmid">23641208</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brand</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>Fast online SVD revisions for lightweight recommender systems</article-title>, in <source>2003 SIAM International Conference on Data Mining</source>, eds <person-group person-group-type="editor"><name><surname>Barbara</surname><given-names>D.</given-names></name><name><surname>Kamath</surname><given-names>C.</given-names></name></person-group> (<publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>SIAM</publisher-name>).</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name></person-group> (<year>2012</year>). <article-title>Multisubject independent component analysis of fMRI: a decade of intrinsic networks, default mode, and neurodiagnostic discovery</article-title>. <source>IEEE Rev. Biomed. Eng.</source>
<volume>5</volume>, <fpage>60</fpage>&#x02013;<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1109/RBME.2012.2211076</pub-id><pub-id pub-id-type="pmid">23231989</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Pearlson</surname><given-names>G. D.</given-names></name><name><surname>Pekar</surname><given-names>J. J.</given-names></name></person-group> (<year>2001</year>). <article-title>A method for making group inferences from functional MRI data using independent component analysis</article-title>. <source>Hum. Brain Mapp</source>. <volume>14</volume>, <fpage>140</fpage>&#x02013;<lpage>151</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.1048</pub-id><pub-id pub-id-type="pmid">11559959</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Egolf</surname><given-names>E.</given-names></name><name><surname>Rachakonda</surname><given-names>S.</given-names></name></person-group> (<year>2004</year>). <source>Group ICA of fMRI Toobox</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://mialab.mrn.org/software/gift">http://mialab.mrn.org/software/gift</ext-link></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Silva</surname><given-names>R. F.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Rachakonda</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>Comparison of PCA approaches for very large group ICA</article-title>. <source>Neuroimage</source>
<volume>118</volume>, <fpage>662</fpage>&#x02013;<lpage>666</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.047</pub-id><pub-id pub-id-type="pmid">26021216</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erhardt</surname><given-names>E. B.</given-names></name><name><surname>Rachakonda</surname><given-names>S.</given-names></name><name><surname>Bedrick</surname><given-names>E. J.</given-names></name><name><surname>Allen</surname><given-names>E. A.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Comparison of multi-subject ICA methods for analysis of fMRI data</article-title>. <source>Hum. Brain Mapp.</source>
<volume>32</volume>, <fpage>2075</fpage>&#x02013;<lpage>2095</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.21170</pub-id><pub-id pub-id-type="pmid">21162045</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>D. A.</given-names></name><name><surname>Setsompop</surname><given-names>K.</given-names></name></person-group> (<year>2013</year>). <article-title>Ultra-fast MRI of the human brain with simultaneous multi-slice imaging</article-title>. <source>J. Magn. Reson.</source>
<volume>229</volume>, <fpage>90</fpage>&#x02013;<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmr.2013.02.002</pub-id><pub-id pub-id-type="pmid">23473893</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>D. A.</given-names></name><name><surname>Moeller</surname><given-names>S.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Auerbach</surname><given-names>E.</given-names></name><name><surname>Ramanna</surname><given-names>S.</given-names></name><name><surname>Glasser</surname><given-names>M. F.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Multiplexed echo planar imaging for Sub-Second Whole Brain FMRI and fast diffusion imaging</article-title>. <source>PLoS ONE</source>
<volume>5</volume>:<fpage>e15710</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0015710</pub-id><pub-id pub-id-type="pmid">21187930</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freire</surname><given-names>L.</given-names></name><name><surname>Roche</surname><given-names>A.</given-names></name><name><surname>Mangin</surname><given-names>J. F.</given-names></name></person-group> (<year>2002</year>). <article-title>What is the best similarity measure for motion correction in fMRI time series?</article-title>
<source>IEEE Trans. Med. Imaging</source>
<volume>21</volume>, <fpage>470</fpage>&#x02013;<lpage>484</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2002.1009383</pub-id><pub-id pub-id-type="pmid">12071618</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name><name><surname>Holmes</surname><given-names>A.</given-names></name><name><surname>Worsley</surname><given-names>K. J.</given-names></name><name><surname>Poline</surname><given-names>J. P.</given-names></name><name><surname>Frith</surname><given-names>C. D.</given-names></name><name><surname>Frackowiak</surname><given-names>R. S.</given-names></name></person-group> (<year>1995</year>). <article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title>. <source>Hum. Brain Mapp</source>. <volume>2</volume>, <fpage>189</fpage>&#x02013;<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Funk</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). <source>Netflix Update: Try This at Home.</source> Available online at: <ext-link ext-link-type="uri" xlink:href="http://sifter.org/~simon/journal/20061211.html">http://sifter.org/~simon/journal/20061211.html</ext-link> (Accessed April 10, 2015).</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halko</surname><given-names>N.</given-names></name><name><surname>Martinsson</surname><given-names>P. G.</given-names></name><name><surname>Shkolnisky</surname><given-names>Y.</given-names></name><name><surname>Tygert</surname><given-names>M.</given-names></name></person-group> (<year>2011a</year>). <article-title>An algorithm for the principal component analysis of large data sets</article-title>. <source>SIAM J. Sci. Comput</source>. <volume>33</volume>, <fpage>2580</fpage>&#x02013;<lpage>2594</lpage>. <pub-id pub-id-type="doi">10.1137/100804139</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halko</surname><given-names>N.</given-names></name><name><surname>Martinsson</surname><given-names>P. G.</given-names></name><name><surname>Tropp</surname><given-names>J. A.</given-names></name></person-group> (<year>2011b</year>). <article-title>Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions</article-title>. <source>SIAM Rev</source>. <volume>53</volume>, <fpage>217</fpage>&#x02013;<lpage>288</lpage>. <pub-id pub-id-type="doi">10.1137/090771806</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himberg</surname><given-names>J.</given-names></name><name><surname>Hyvarinen</surname><given-names>A.</given-names></name><name><surname>Esposito</surname><given-names>F.</given-names></name></person-group> (<year>2004</year>). <article-title>Validating the independent components of neuroimaging time series via clustering and visualization</article-title>. <source>Neuroimage</source>
<volume>22</volume>, <fpage>1214</fpage>&#x02013;<lpage>1222</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.027</pub-id><pub-id pub-id-type="pmid">15219593</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuczynski</surname><given-names>J.</given-names></name><name><surname>Wozniakowski</surname><given-names>H.</given-names></name></person-group> (<year>1992</year>). <article-title>Estimating the largest eigenvalue by the power and lanczos algorithms with a random start</article-title>. <source>SIAM J. Matrix Anal. Appl</source>. <volume>13</volume>, <fpage>1094</fpage>&#x02013;<lpage>1122</lpage>. <pub-id pub-id-type="doi">10.1137/0613066</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lehoucq</surname><given-names>R.</given-names></name><name><surname>Sorensen</surname><given-names>D.</given-names></name><name><surname>Yang</surname><given-names>C.</given-names></name></person-group> (<year>1998</year>). <source>ARPACK Users' Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods.</source>
<publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>SIAM</publisher-name>.</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehoucq</surname><given-names>R. B.</given-names></name><name><surname>Sorensen</surname><given-names>D. C.</given-names></name></person-group> (<year>1996</year>). <article-title>Deflation techniques for an implicitly restarted Arnoldi iteration</article-title>. <source>SIAM J. Matrix Anal. Appl.</source>
<volume>17</volume>, <fpage>789</fpage>&#x02013;<lpage>821</lpage>. <pub-id pub-id-type="doi">10.1137/S0895479895281484</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name></person-group> (<year>2004</year>). <article-title>On incremental and robust subspace learning</article-title>. <source>Pattern Recognit.</source>
<volume>37</volume>, <fpage>1509</fpage>&#x02013;<lpage>1518</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2003.11.010</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.-O.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name></person-group> (<year>2007</year>). <article-title>Estimating the number of independent components for functional magnetic resonance imaging data</article-title>. <source>Hum. Brain Mapp.</source>
<volume>28</volume>, <fpage>1251</fpage>&#x02013;<lpage>1266</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20359</pub-id><pub-id pub-id-type="pmid">17274023</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>S.</given-names></name><name><surname>Correa</surname><given-names>N. M.</given-names></name><name><surname>Li</surname><given-names>X. L.</given-names></name><name><surname>Eichele</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Adali</surname><given-names>T.</given-names></name></person-group> (<year>2011</year>). <article-title>Automatic identification of functional clusters in fMRI data using spatial dependence</article-title>. <source>IEEE Trans. Biomed. Eng.</source>
<volume>58</volume>, <fpage>3406</fpage>&#x02013;<lpage>3417</lpage>. <pub-id pub-id-type="doi">10.1109/TBME.2011.2167149</pub-id><pub-id pub-id-type="pmid">21900068</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Martinsson</surname><given-names>P.-G.</given-names></name><name><surname>Szlam</surname><given-names>A.</given-names></name><name><surname>Tygert</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Normalized power iterations for the computation of SVD</article-title>, in <source>Neural Information Processing Systems Workshop 2010</source>. (<publisher-loc>Whistler, BC</publisher-loc>).</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mitliagkas</surname><given-names>I.</given-names></name><name><surname>Caramanis</surname><given-names>C.</given-names></name><name><surname>Jain</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>Memory limited, streaming PCA</article-title>, in <source>Advances in Neural Information Processing Systems 26</source>, eds <person-group person-group-type="editor"><name><surname>Burges</surname><given-names>C. J. C.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name><name><surname>Ghahramani</surname><given-names>Z.</given-names></name><name><surname>Weinberger</surname><given-names>K. Q.</given-names></name></person-group> (<publisher-name>Curran Associates, Inc</publisher-name>), <fpage>2886</fpage>&#x02013;<lpage>2894</lpage>.</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Recktenwald</surname><given-names>W.</given-names></name></person-group> (<year>2000</year>). <source>Introduction to Numerical Methods and MATLAB: Implementations and Applications</source>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roweis</surname><given-names>S.</given-names></name></person-group> (<year>1997</year>). <article-title>EM Algorithms for PCA and SPCA</article-title>, in <source>Neural Information Processing Systems (NIPS 1997)</source>, eds <person-group person-group-type="editor"><name><surname>Jordan</surname><given-names>M.</given-names></name><name><surname>Kearns</surname><given-names>M.</given-names></name><name><surname>Solla</surname><given-names>S.</given-names></name></person-group> (<publisher-loc>Denver, CO</publisher-loc>: <publisher-name>The MIT Press</publisher-name>).</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutishauser</surname><given-names>H.</given-names></name></person-group> (<year>1970</year>). <article-title>Simultaneous iteration method for symmetric matrices</article-title>. <source>Num. Math.</source>
<volume>16</volume>, <fpage>205</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1007/BF02219773</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Saad</surname><given-names>Y.</given-names></name></person-group> (<year>2011</year>). <source>Numerical Methods for Large Eigenvalue Problems: Revised Edition.</source>
<publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>SIAM</publisher-name>.</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Setsompop</surname><given-names>K.</given-names></name><name><surname>Gagoski</surname><given-names>B. A.</given-names></name><name><surname>Polimeni</surname><given-names>J. R.</given-names></name><name><surname>Witzel</surname><given-names>T.</given-names></name><name><surname>Wedeen</surname><given-names>V. J.</given-names></name><name><surname>Wald</surname><given-names>L. L.</given-names></name></person-group> (<year>2012</year>). <article-title>Blipped-controlled aliasing in parallel imaging for simultaneous multislice echo planar imaging with reduced g-factor penalty</article-title>. <source>Magn. Reson. Med.</source>
<volume>67</volume>, <fpage>1210</fpage>&#x02013;<lpage>1224</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.23097</pub-id><pub-id pub-id-type="pmid">21858868</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Hyv&#x000e4;rinen</surname><given-names>A.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Miller</surname><given-names>K. L.</given-names></name><name><surname>Beckmann</surname><given-names>C. F.</given-names></name></person-group> (<year>2014</year>). <article-title>Group-PCA for very large fMRI datasets</article-title>. <source>Neuroimage</source>
<volume>101</volume>, <fpage>738</fpage>&#x02013;<lpage>749</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.051</pub-id><pub-id pub-id-type="pmid">25094018</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Miller</surname><given-names>K. L.</given-names></name><name><surname>Moeller</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>J. Q.</given-names></name><name><surname>Auerbach</surname><given-names>E. J.</given-names></name><name><surname>Woolrich</surname><given-names>M. W.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Temporally-independent functional modes of spontaneous brain activity</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>109</volume>, <fpage>3131</fpage>&#x02013;<lpage>3136</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1121329109</pub-id><pub-id pub-id-type="pmid">22323591</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Rao</surname><given-names>H.</given-names></name><name><surname>Detre</surname><given-names>J. A.</given-names></name><name><surname>Childress</surname><given-names>A. R.</given-names></name></person-group> (<year>2006</year>). <article-title>Strategies for reducing large fMRI data sets for ICA</article-title>. <source>Magn. Reson. Imaging</source>
<volume>24</volume>, <fpage>591</fpage>&#x02013;<lpage>596</lpage>. <pub-id pub-id-type="doi">10.1016/j.mri.2005.12.013</pub-id><pub-id pub-id-type="pmid">16735180</pub-id></mixed-citation></ref></ref-list></back></article>
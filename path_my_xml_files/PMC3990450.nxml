<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Comput Sci</journal-id><journal-id journal-id-type="iso-abbrev">J Comput Sci</journal-id><journal-title-group><journal-title>Journal of Computational Science</journal-title></journal-title-group><issn pub-type="ppub">1877-7503</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">24748902</article-id><article-id pub-id-type="pmc">3990450</article-id><article-id pub-id-type="publisher-id">S1877-7503(13)00018-5</article-id><article-id pub-id-type="doi">10.1016/j.jocs.2013.01.006</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Scalable and fault tolerant orthogonalization based on randomized distributed data aggregation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gansterer</surname><given-names>Wilfried N.</given-names></name><email>wilfried.gansterer@univie.ac.at</email><xref rid="cor0005" ref-type="corresp">&#x0204e;</xref></contrib><contrib contrib-type="author"><name><surname>Niederbrucker</surname><given-names>Gerhard</given-names></name></contrib><contrib contrib-type="author"><name><surname>Strakov&#x000e1;</surname><given-names>Hana</given-names></name></contrib><contrib contrib-type="author"><name><surname>Schulze Grotthoff</surname><given-names>Stefan</given-names></name></contrib></contrib-group><aff>University of Vienna, Research Group Theory and Applications of Algorithms, W&#x000e4;hringer Stra&#x000df;e 29, A-1090 Vienna, Austria</aff><author-notes><corresp id="cor0005"><label>&#x0204e;</label>Corresponding author. Tel.: +43 1 4277 78311; fax: +43 1 4277 878311. <email>wilfried.gansterer@univie.ac.at</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>1</day><month>11</month><year>2013</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="ppub">.--><pub-date pub-type="ppub"><month>11</month><year>2013</year></pub-date><volume>4</volume><issue>6</issue><fpage>480</fpage><lpage>488</lpage><history><date date-type="received"><day>31</day><month>3</month><year>2012</year></date><date date-type="rev-recd"><day>12</day><month>9</month><year>2012</year></date><date date-type="accepted"><day>16</day><month>1</month><year>2013</year></date></history><permissions><copyright-statement>&#x000a9; 2013 2013 Elsevier B.V. All rights reserved.</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Elsevier B.V.</copyright-holder><license xlink:href="https://creativecommons.org/licenses/by-nc-nd/3.0/"><license-p>Open Access under <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/3.0/">CC BY-NC-ND 3.0</ext-link> license</license-p></license></permissions><abstract abstract-type="graphical"><title>Highlights</title><p>&#x025ba; We present the push-flow algorithm (PF), a new distributed data aggregation algorithm (DDAA). &#x025ba; PF has better resilience properties than previously existing DDAAs. &#x025ba; PF has very good asymptotic scaling behavior on hypercube topologies. &#x025ba; Based on DDAAs, we design the new rdmGS algorithm for orthogonalizing a set of vectors in a decentralized distributed fashion. &#x025ba; rdmGS is capable of producing fully accurate results even if several nodes fail permanently.</p></abstract><abstract><p>The construction of distributed algorithms for matrix computations built on top of distributed data aggregation algorithms with randomized communication schedules is investigated. For this purpose, a new aggregation algorithm for summing or averaging distributed values, the push-flow algorithm, is developed, which achieves superior resilience properties with respect to failures compared to existing aggregation methods. It is illustrated that on a hypercube topology it asymptotically requires the same number of iterations as the optimal all-to-all reduction operation and that it scales well with the number of nodes. Orthogonalization is studied as a prototypical matrix computation task. A new fault tolerant distributed orthogonalization method rdmGS, which can produce accurate results even in the presence of node failures, is built on top of distributed data aggregation algorithms.</p></abstract><kwd-group><title>Keywords</title><kwd>Distributed reduction operation</kwd><kwd>Push-flow algorithm</kwd><kwd>Distributed orthogonalization</kwd><kwd>Distributed matrix computations</kwd><kwd>Fault tolerant matrix computations</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec0005"><label>1</label><title>Introduction</title><p>Algorithms for future large-scale computer systems have to be designed to provide resilience to various types of failures and to require less synchronization between nodes than state-of-the-art parallel algorithms. The basic idea underlying this paper is to investigate the construction of suitable distributed algorithms for matrix computations built on top of <italic>distributed data aggregation algorithms</italic> (<italic>DDAAs</italic>). DDAAs can be seen as distributed versions of all-to-all reduction operations, in particular for summing or for averaging the elements of a long vector distributed over many nodes. Consequently, distributed versions of basically all types of <sc>Blas</sc> operations can potentially be constructed based on DDAAs.</p><p>We first evaluate the strengths and weaknesses of existing distributed data aggregation algorithms. Then, we present the <italic>push-flow algorithm</italic>, a new DDAA which overcomes some drawbacks of existing methods in terms of resilience. Finally, we show how DDAAs can be used as building blocks for developing a scalable and fault tolerant distributed orthogonalization/QR factorization method as a prototypical matrix computation task.</p><sec id="sec0010"><label>1.1</label><title>Problem setting</title><p>We consider a large-scale computer system with <italic>N</italic> nodes arranged in a fixed (but otherwise arbitrary) topology as target system for the computations. Every node knows which nodes are its neighbors, but does not need to have any global information about the network. Our focus is on <italic>distributed</italic> algorithms for such large-scale systems. We clearly distinguish distributed from <italic>parallel</italic> algorithms. The latter are usually designed for small to medium-sized static and reliable systems with regular and globally known topology, where synchronized computation across nodes in the network can be guaranteed. However, parallel algorithms have major drawbacks in possibly decentralized large-scale systems with arbitrary topologies and potentially unreliable components (e.g., failing nodes or communication links), where synchronization of the nodes may be difficult to achieve. Examples, where such a setup is very common, are various problems arising in the analysis of large networks, such as community detection problems <xref rid="bib0005" ref-type="bibr">[1]</xref> or temporally changing social networks <xref rid="bib0010 bib0015" ref-type="bibr">[2,3]</xref>, in particular, in cases where no <italic>global</italic> view of the network is available.</p><p>In such situations, distributed algorithms provide much greater flexibility with respect to the hardware infrastructure than classical parallel algorithms. They neither rely on a fixed communication schedule nor on full synchronization across the nodes. Moreover, they have the potential for producing meaningful results even in the presence of link or node failures. More generally speaking, distributed algorithms are attractive in all computations over large-scale computing systems where (<italic>i</italic>) the nodes do not have complete <italic>global</italic> information about the system, but predominantly only local information about their neighborhood and/or (<italic>ii</italic>) the system may change dynamically (e.g., due to hardware failures).</p><p>The algorithms we investigate are based on <italic>gossiping protocols</italic>. Such algorithms are attractive in such situations, because due to their randomized information exchange they do not require static or reliable hardware infrastructure. If communication is restricted to the local neighborhood of each node, the number of iterations required tends to scale logarithmically with the number of nodes in the system, which is asymptotically the same as in optimized all-to-all reduction operations. Moreover, due to their iterative structure they can deliver results at reduced accuracy levels for a communication cost which is proportional to the target accuracy.</p></sec><sec id="sec0015"><label>1.2</label><title>Related work</title><p>Approaches for achieving fault tolerance in parallel and distributed systems have been investigated at various levels. At the MPI level, FT-MPI <xref rid="bib0020" ref-type="bibr">[4]</xref> provides interfaces for improving the fault tolerance of applications. It is designed to recover from link or node failures by continuing from consistent points which have to be defined by the application developer.</p><p>A standard approach at the parallel application level is <italic>coordinated checkpointing</italic> followed by <italic>rollback recovery</italic> in case of a failure. However, it has been shown that &#x02013; depending on the checkpointing interval <xref rid="bib0025" ref-type="bibr">[5]</xref> &#x02013; the synchronization of the periodic coordinated checkpointing limits application scalability, and for large systems a dominating fraction of the runtime tends to be spent on checkpointing and restarting instead of advancing in the application <xref rid="bib0030" ref-type="bibr">[6]</xref>. Moreover, for large-scale systems it can also become expensive to provide sufficient stable storage. Several improvements of coordinated checkpointing have been proposed. Alternatives focusing on storing the checkpoints more efficiently are diskless checkpointing <xref rid="bib0035" ref-type="bibr">[7]</xref> and RAID-inspired checkpointing <xref rid="bib0040" ref-type="bibr">[8]</xref> which stores checkpoints redundantly across the nodes&#x02019; memory. <italic>Uncoordinated checkpointing</italic>
<xref rid="bib0045" ref-type="bibr">[9]</xref> does not require the synchronization of checkpointing procedures across the nodes and in case of failure only the failed process needs to be restarted, not the entire application. However, the restart tends to be much more complex, since the failed processes need to find a common checkpoint. Also, storage requirements tend to be much higher since it is not clear which checkpoint will be required for restart. Different approaches have been suggested to increase the interval between failures and thus to decrease the number of restarts. The first approach is called <italic>redundant computing</italic>. Each process is replicated across the system, and thus it can handle multiple failures without recovery overhead. An example is the rMPI library <xref rid="bib0050" ref-type="bibr">[10]</xref>, where redundancy is used to increase the interval between failures and thus to reduce the overhead caused by storing checkpoints and restarting the system. In <xref rid="bib0055" ref-type="bibr">[11]</xref> the advantages and limitations of double and triple redundancy are discussed. Another concept called <italic>live migration</italic>
<xref rid="bib0060" ref-type="bibr">[12]</xref> is a proactive approach, where processes are migrated away from unhealthy nodes to healthy nodes.</p><p>On the lower level of distributed data aggregation algorithms the algorithms discussed in this paper do not require any checkpointing or redundant computing. On the higher level of distributed matrix computations (in this paper, distributed QR factorization) our algorithms differ from the concept of checkpointing because they react to failures whenever they occur. They can be considered a combination of a redundant computing and a live migration approach, without assuming common external storage or extra hardware, though. Failed nodes do not have to be repaired or replaced, but the remaining nodes take over their responsibilities.</p><p>A purely algorithmic way of achieving fault tolerance for high level matrix operations is the technique of <italic>algorithm-based fault tolerance</italic> (<italic>ABFT</italic>) <xref rid="bib0065" ref-type="bibr">[13]</xref>. The basic idea of ABFT is to extend the input matrices by checksums and to adapt the algorithms such that these checksums get updated correctly and can consequently be used for detecting and recovering from errors. Classically, ABFT was designed for handling a prescribed amount of miscalculations with a high probability <xref rid="bib0065" ref-type="bibr">[13]</xref>, whereas more recently also fail-stop failures, where nodes entirely crash, were considered <xref rid="bib0070" ref-type="bibr">[14]</xref>. Besides the extension to fail-stop failures there are also efforts into methods with ABFT which make use of the inherent redundancy of the data distribution across the nodes to recover from failures, which results in methods where only failure situations lead to an overhead <xref rid="bib0075" ref-type="bibr">[15]</xref>.</p><p>The available ABFT literature discusses specific numerical linear algebra tasks, such as matrix&#x02013;matrix multiplications, LU decompositions or iterative linear solvers, but does not consider the elementary building blocks below the <sc>Blas</sc>-level. In contrast to ABFT, our approach does not modify the linear algebra algorithms themselves, but instead we focus on new distributed data aggregation building blocks, which also improve the resilience of the algorithms based on them. Our methods do not recover by an explicit (deterministic) recovery step as in ABFT, but they rather enter a &#x0201c;healing&#x0201d; phase once the next successful failure-free iteration can be performed (e.g., after a previously failed link has been re-established). At the distributed data aggregation algorithms-level, the overhead for higher resilience in terms of slower convergence and extra data transmission depends on the failure type and is only incurred when a failure actually happens.</p><p>DDAAs and other simple distributed algorithms based on randomized communication schedules have been discussed extensively in the literature. Important examples are algorithms based on <italic>gossiping</italic> (or <italic>epidemic</italic>) protocols <xref rid="bib0080 bib0085" ref-type="bibr">[16,17]</xref>. In the basic approach each node communicates with randomly chosen neighboring nodes <xref rid="bib0080" ref-type="bibr">[16]</xref>. In other variants the communication partners of a node are chosen from the entire network regardless of the distance <xref rid="bib0090" ref-type="bibr">[18]</xref> or its local value is broadcasted to all neighbors <xref rid="bib0095" ref-type="bibr">[19]</xref>. Most of the existing work focuses on distributed algorithms for simple network operations, such as information dissemination (rumor mongering) <xref rid="bib0100" ref-type="bibr">[20]</xref>, aggregation <xref rid="bib0105" ref-type="bibr">[21]</xref>, network organization (routing, load balancing, etc.) <xref rid="bib0110" ref-type="bibr">[22]</xref>, or computing separable functions <xref rid="bib0115" ref-type="bibr">[23]</xref>.</p><p>For our objective, DDAAs for the distributed computation of sums and averages, such as the <italic>push-sum algorithm</italic>
<xref rid="bib0120" ref-type="bibr">[24]</xref>, are most relevant. The potential of DDAAs for providing a high degree of resilience is mentioned in the literature at various places (see, e.g., <xref rid="bib0125" ref-type="bibr">[25]</xref>), but we are not aware of any work which investigates the challenges arising when trying to tap the full potential. Relevant existing DDAAs are surveyed in Section&#x000a0;<xref rid="sec0025" ref-type="sec">2</xref>.</p><p>Only recently, distributed algorithms for more complex matrix computations based on DDAAs, such as the <italic>dmGS</italic> algorithm for distributed QR factorization <xref rid="bib0130" ref-type="bibr">[26]</xref> or a distributed orthogonal iteration method <xref rid="bib0135 bib0140" ref-type="bibr">[27,28]</xref> (both based on the push-sum algorithm) have been designed and compared with state-of-the-art parallel algorithms.</p></sec><sec id="sec0020"><label>1.3</label><title>Synopsis</title><p>When trying to construct fault tolerant distributed algorithms for matrix computations based on sequences of DDAAs, resilience aspects first have to be addressed at the level of a single aggregation algorithm. Consequently, in Section&#x000a0;<xref rid="sec0025" ref-type="sec">2</xref> we survey relevant existing DDAAs and discuss their strengths and weaknesses in terms of fault tolerance. We then focus on improving fault tolerance in Section&#x000a0;<xref rid="sec0040" ref-type="sec">3</xref>. We present the new <italic>push-flow algorithm</italic>, which has superior fault tolerance properties compared to existing distributed data aggregation algorithms. As a prototypical example for matrix computations based on DDAAs, we present the new distributed orthogonalization algorithm <italic>rdmGS</italic> which is resilient to node failures. In Section&#x000a0;<xref rid="sec0070" ref-type="sec">4</xref>, we discuss the scalability of selected DDAAs and of distributed orthogonalization methods. Section&#x000a0;<xref rid="sec0085" ref-type="sec">5</xref> concludes the paper.</p></sec></sec><sec id="sec0025"><label>2</label><title>Aggregating distributed data</title><p>Over the last decade, several distributed data aggregation algorithms based on randomized communication schedules have been proposed. Those methods were traditionally motivated by networks with unreliable communication links, which precludes the usage of classical parallel algorithms with fixed communication schedules. Recently, there are also efforts in introducing self-healing mechanisms into such algorithms. For a structured discussion of the differences between the various methods we distinguish henceforth the following types of <italic>failures</italic> in the system under consideration. Note that we order the failure types according to increasing difficulty for recovering from them at the algorithmic level.<list list-type="simple"><list-item id="listitem0005"><label>(F1)</label><p>Reported temporary unavailability of links/nodes</p></list-item><list-item id="listitem0010"><label>(F2)</label><p>Unreported loss or corruption of a message</p></list-item><list-item id="listitem0015"><label>(F3)</label><p>Reported permanent node or link failures</p></list-item><list-item id="listitem0020"><label>(F4)</label><p>Unreported corruption of local data (e.g., bit flip)</p></list-item><list-item id="listitem0025"><label>(F5)</label><p>Unreported permanent node failures</p></list-item></list> While dealing with (F1) is generally easy for any randomized method with flexible communication schedules, the coverage of (F2)&#x02013;(F5) is a lot harder since those failures usually introduce a (temporary) error from which the system has to recover properly. In the case of (F5) one has to additionally define whether the initial data of a failed node should be included in the target aggregate or not, where the former is usually harder.</p><sec id="sec0030"><label>2.1</label><title>Existing methods</title><p>A basic approach for the distributed computation of aggregates is the <italic>push-sum algorithm</italic>
<xref rid="bib0120" ref-type="bibr">[24]</xref> where each node <italic>i</italic> iteratively updates a local vector <inline-formula><mml:math id="M1" altimg="si2.gif" overflow="scroll"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. The <italic>s</italic><sub><italic>i</italic></sub> are initialized with the values <italic>x</italic><sub><italic>i</italic></sub> to be aggregated, and the <inline-formula><mml:math id="M2" altimg="si3.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are weights. The initial values of the weights <inline-formula><mml:math id="M3" altimg="si4.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> determines the type of aggregation operation: For computing <inline-formula><mml:math id="M4" altimg="si5.gif" overflow="scroll"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, all weights have to be either initialized identically to <inline-formula><mml:math id="M5" altimg="si6.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula>, or to <inline-formula><mml:math id="M6" altimg="si7.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> for all nodes except one with weight <inline-formula><mml:math id="M7" altimg="si8.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. For computing the average <inline-formula><mml:math id="M8" altimg="si9.gif" overflow="scroll"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula>, all weights have to be initialized identically to <inline-formula><mml:math id="M9" altimg="si10.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. By consecutively sending fractions of the local vector <inline-formula><mml:math id="M10" altimg="si11.gif" overflow="scroll"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to randomly chosen neighbors (which add the received values to their local values), all local estimates <inline-formula><mml:math id="M11" altimg="si12.gif" overflow="scroll"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> converge linearly either to the sum or to the average of the distributed values <xref rid="bib0120" ref-type="bibr">[24]</xref>.</p><p>In <xref rid="bib0145" ref-type="bibr">[29]</xref> a more robust version of the push-sum algorithm, called <italic>LiMoSense</italic>, is derived by keeping a history (i.&#x000a0;e., the sum) of sent and received values along each communication link. By always sending the full history the receiver of a message can easily tolerate missed (or wrong) values. To keep the steadily growing histories small, a bidirectional cancellation operation is proposed in <xref rid="bib0145" ref-type="bibr">[29]</xref>.</p><p>A third approach is <italic>flow updating</italic>
<xref rid="bib0150" ref-type="bibr">[30]</xref>. The underlying idea is that the nodes keep their initial values local and only share <italic>flows</italic> with their neighbors. For each communication link, both attached nodes maintain a <italic>flow variable</italic> which represents the overall balance of communicated local values along this link. For communicating local values along a link, the sender does not transmit the local values directly, but instead adds them to the corresponding flow variable and transmits the flow variable. The receiver updates its own flow variable by the negated received flow. Consequently, as in network flow algorithms, the overall flow across each link is zero (<italic>flow conservation</italic>) if no failures occur. This flow conservation is the key idea of this method for recovering from failures, since recovering from a failure corresponds to (re-)establishing flow conservation, which is achieved after each successful communication across a link.</p><p>In flow updating, a node locally approximates the aggregate by first subtracting the sum over all flow variables it maintains from its initial value and then averaging this local approximation and the current local estimates of all its neighbors (which are exchanged together with the flows). However, it has some drawbacks in terms of convergence speed and there is no formal analysis given in <xref rid="bib0150" ref-type="bibr">[30]</xref>, whereas for the push-sum algorithm <xref rid="bib0120" ref-type="bibr">[24]</xref> and for LiMoSense <xref rid="bib0145" ref-type="bibr">[29]</xref> proofs of correctness and convergence (speed) have been given.</p><p><xref rid="tbl0005" ref-type="table">Table 1</xref> summarizes which failure types the existing DDAA can handle.</p></sec><sec id="sec0035"><label>2.2</label><title>Strengths and weaknesses</title><p>Thanks to their randomized communication schedules, all these methods will <italic>always</italic> produce at least approximate results even if hardware failures occur. The important issue in this context is <italic>mass conservation</italic>, which means that distributed data aggregation algorithms converge to the true aggregate only if all of the initial information is preserved over the whole aggregation process. Since a change of mass (usually mass loss, but also an increase of mass is theoretically possible, e.g., in the case of a bit flip) always results in a corresponding loss of achievable accuracy we are interested in methods which are able to ensure mass conservation or can recover from a change of mass even if failures of types (F2)&#x02013;(F5) occur.</p><p>Note that strict mass conservation in the sense of a network-wide invariant seems to be impossible under our assumptions since the occurrence of (F2)&#x02013;(F5) will generally result in mass loss. So the methods we aim for are designed for an autonomous full <italic>recovery</italic> from mass loss, which we denote as <italic>weak mass conservation</italic> since it weakens the strict (theoretical) form of mass conservation. Accordingly, those methods ensure strict mass conservation in failure-free scenarios and weak mass conservation if failures occur, i.&#x000a0;e., they are able to recover from a change of mass by a proper self-healing mechanism.</p><p>Although the push-sum algorithm is independent of the availability of specific communication links, it has no built-in mechanism for mass conservation and therefore it cannot recover from (F2)&#x02013;(F5). LiMoSense can recover from (F2)&#x02013;(F3) due to the redundancy in keeping and sending complete histories (cf. <xref rid="bib0145" ref-type="bibr">[29]</xref>). Flow updating does not (directly) rely on the transmission of redundant messages and it can also handle (F4), because a local error like a bit flip in a flow value is simply corrected by (re-)establishing a valid flow (cf. <xref rid="bib0150 bib0155" ref-type="bibr">[30,31]</xref>). Since LiMoSense is a direct generalization of the push-sum algorithm, it also delivers the same fast convergence speed, as opposed to flow updating, which showed an uncompetitive convergence speed in our experiments (cf. Section&#x000a0;<xref rid="sec0075" ref-type="sec">4.1</xref>).</p><p>To demonstrate the strength of the flow concept, in Section&#x000a0;<xref rid="sec0045" ref-type="sec">3.1</xref> we introduce the <italic>push-flow algorithm</italic>, which fully exploits its advantages while preserving the convergence speed of the push-sum algorithm.</p></sec></sec><sec id="sec0040"><label>3</label><title>Improving resilience</title><p>In the following, we present two new distributed algorithms which are more resilient than existing algorithms in terms of the most challenging failure types (F3)&#x02013;(F5). In Section&#x000a0;<xref rid="sec0045" ref-type="sec">3.1</xref>, we concentrate on a single instance of a distributed data aggregation algorithm and introduce the <italic>push-flow algorithm</italic>, which can recover from failure types (F1)&#x02013;(F4). In Section&#x000a0;<xref rid="sec0050" ref-type="sec">3.2</xref> we turn our attention to the level of complete matrix problems and introduce <italic>robust dmGS</italic> for distributed QR factorization or orthogonalization which can recover from failure types (F1)&#x02013;(F4), and in some scenarios even from failure type (F5).</p><sec id="sec0045"><label>3.1</label><title>The push-flow algorithm</title><p>Despite several drawbacks of the flow updating algorithm in terms of uncompetitive scalability and convergence speed (cf. Section&#x000a0;<xref rid="sec0075" ref-type="sec">4.1</xref>) as well as lack of formal analysis in <xref rid="bib0150" ref-type="bibr">[30]</xref>, the principal idea of keeping the initial mass locally and sharing flows (instead of mass) is promising and has several conceptual advantages over keeping histories, as in LiMoSense. First, a valid flow across a link can be established from any direction, while in a history-based approach a specific direction is needed to tolerate failures like (F2). Second, in contrast to the steadily increasing history values, flow variables remain bounded by definition. Third, flow-based approaches achieve higher fault tolerance, since history-based approaches are limited to transmission related failures (F1)&#x02013;(F3), whereas a flow-based approach can also recover from purely local failures of a node like (F4).</p><p>Motivated by this observation, we integrate the flow concept into the push-sum algorithm by translating each transmission of mass, i.&#x000a0;e., the direct transmission of (fractions of) local values, into a transmission of flow in a similar way as it is done in the flow updating method. The result is a variant of the push-sum algorithm, which is equivalent to the push-sum algorithm in the absence of failures and exhibits improved resilience if failures occur. The resulting <italic>push-flow algorithm</italic> is shown in <xref rid="fig0005" ref-type="fig">Fig. 1</xref>. Each node <italic>i</italic> maintains a two dimensional flow vector <italic>f</italic><sub><italic>i</italic>,<italic>j</italic></sub> for every neighbor <italic>j</italic> in its neighborhood <inline-formula><mml:math id="M12" altimg="si13.gif" overflow="scroll"><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> whose elements can be interpreted as the balance of mass which was communicated between nodes <italic>i</italic> and <italic>j</italic>. Moreover, each node <italic>i</italic> maintains a two dimensional vector <inline-formula><mml:math id="M13" altimg="si14.gif" overflow="scroll"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> which contains the local initial value <italic>x</italic><sub><italic>i</italic></sub> and the local weight <inline-formula><mml:math id="M14" altimg="si15.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. The initial values for the local weights <inline-formula><mml:math id="M15" altimg="si16.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are the same as in the push-sum algorithm.</p><p>At every point in time, the current local mass <italic>e</italic><sub><italic>i</italic></sub> at a node <italic>i</italic> is (in contrast to flow updating) computed as the difference between the initial vector <inline-formula><mml:math id="M16" altimg="si17.gif" overflow="scroll"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and the sum over all flows <italic>f</italic><sub><italic>i</italic>,<italic>j</italic></sub>, i.&#x000a0;e., <inline-formula><mml:math id="M17" altimg="si18.gif" overflow="scroll"><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Consequently, analogously to the push-sum algorithm, the local estimate of the global aggregate can be computed by dividing the first component of the vector <italic>e</italic><sub><italic>i</italic></sub> by its second component, i.&#x000a0;e., by forming <italic>e</italic><sub><italic>i</italic></sub>(1)/<italic>e</italic><sub><italic>i</italic></sub>(2).</p><p>It is easily verified that the push-flow algorithm is essentially equivalent to the push-sum algorithm in failure-free networks, since for identical communication patterns both algorithms produce identical local estimates of the global aggregate. The equivalence to push-sum also highlights that the local estimates are not computed as average over the estimates of the neighboring nodes like in flow updating. Consequently, the push-flow algorithm preserves the fast convergence of the push-sum algorithm and does not exhibit the disadvantages of flow updating in terms of convergence speed.</p><p>In the presence of failures, the push-flow algorithm benefits from the resilience and self-healing capabilities inherent in the flow concept. In particular, recovery from failures (F3) can be achieved if the neighbors of the failed node set the corresponding flow variables to zero. Therefore, the push-flow algorithm also excludes the local data of a failed node from the final aggregate similar to the resilient methods discussed in Section&#x000a0;<xref rid="sec0025" ref-type="sec">2</xref>. In the case of failures (F4) where some local flow values are corrupted, e.g., because of a bit flip, the nodes involved will recover the next time a correct communication involving this variable happens, like in the case of transmission related failures (F2). A full proof of correctness and convergence of the push-flow algorithm proceeds analogously to the ones presented in <xref rid="bib0120 bib0145" ref-type="bibr">[24,29]</xref> for the push-sum algorithm and LiMoSense, respectively.</p><p>In conclusion, the push-flow algorithm achieves the best resilience among all existing DDAAs and preserves the convergence speed of the push-sum algorithm.</p></sec><sec id="sec0050"><label>3.2</label><title>Fault tolerant orthogonalization</title><p>The resilient DDAAs we discussed so far handle permanently failed nodes by excluding their local data from the final aggregate and thus no redundancy in storing the original data is required. In situations where it is required that the original data of <italic>all</italic> nodes is aggregated, redundancy in the original data has to be introduced.</p><p>The distributed orthogonalization of the columns of the matrix <inline-formula><mml:math id="M18" altimg="si19.gif" overflow="scroll"><mml:mi>A</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mi>&#x0211d;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> over a system with <italic>N</italic> nodes is a prototypical example where the loss of original data usually resulting from a permanent node failure <italic>cannot</italic> be tolerated. Distributed modified Gram-Schmidt orthogonalization (<italic>dmGS</italic>) for computing the QR decomposition <italic>A</italic>&#x000a0;=&#x000a0;<italic>QR</italic> with <inline-formula><mml:math id="M19" altimg="si20.gif" overflow="scroll"><mml:mi>Q</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mi>&#x0211d;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M20" altimg="si21.gif" overflow="scroll"><mml:mi>R</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mi>&#x0211d;</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> has been presented in <xref rid="bib0130" ref-type="bibr">[26]</xref>. It assumes that <italic>A</italic> is distributed row wise over the <italic>N</italic> nodes. If a node permanently fails during the execution of dmGS, its local part of <italic>A</italic> is also permanently lost and as a consequence it becomes impossible to orthogonalize all original vectors. We illustrate in the following how the resilience of this approach can be improved by introducing redundancy in storing the original data. We present <italic>robust dmGS</italic> (<italic>rdmGS</italic>, see <xref rid="fig0010" ref-type="fig">Fig. 2</xref>), which produces a complete and accurate QR factorization of <italic>A</italic> even if one or several nodes fail during the computation.</p><sec id="sec0055"><label>3.2.1</label><title>Introducing redundancy</title><p>The pivotal idea of rdmGS is to maintain redundant copies of all nodes&#x02019; relevant local data at more than one active node at all times. By construction, dmGS automatically computes <italic>all</italic> entries of <italic>R</italic> at all nodes of the system <xref rid="bib0130" ref-type="bibr">[26]</xref>, and thus no specific measures are needed for backing up data of <italic>R</italic>. Every node <italic>k</italic> is responsible for a subset of the rows of <italic>A</italic> and for the parts of the corresponding rows of <italic>Q</italic> which have been computed so far. We call this data node <italic>k</italic>'s <italic>primary data</italic>. At every point in time, <italic>r</italic>&#x000a0;&#x02212;&#x000a0;1 backup copies of node <italic>k</italic>'s primary data are stored on <italic>r</italic>&#x000a0;&#x02212;&#x000a0;1 distinct other active nodes which act as backup nodes for node <italic>k</italic>. The parameter <italic>r</italic> is a measure for the resilience as well as for the overhead of rdmGS. Larger <italic>r</italic> allows for tolerating more simultaneous node failures, albeit at higher cost.</p><p>Node <italic>k</italic> may also act as a backup node for one or more other nodes in the system. The corresponding local data at node <italic>k</italic> is called node <italic>k</italic>'s <italic>backup data</italic>. We call a node <italic>k</italic>, which backs up node <italic>l</italic>'s data, <italic>l</italic>'s <italic>guardian</italic>, and <italic>l</italic> in turn <italic>k</italic>'s <italic>prot&#x000e9;g&#x000e9;</italic>.</p><p>If node <italic>k</italic> fails, its primary data is still available on its <italic>r</italic>&#x000a0;&#x02212;&#x000a0;1 guardians. One of these guardians takes over the primary responsibility for this data, and selects another active node (usually in its neighborhood) to replace itself as guardian for the primary data of the failed node <italic>k</italic>. As a result, again <italic>r</italic> copies of the data of the failed node <italic>k</italic> exist in the system.</p><p>In the process of local computation in each node not only the primary data is updated, but <italic>also</italic> all local backup data. Since the local results of the DDAA are not necessarily identical over all nodes, the <italic>r</italic>&#x000a0;&#x02212;&#x000a0;1 instances of backup data and the corresponding primary data may differ slightly, but no extra data communication is needed for the backup structure as long as no node failure occurs. Upon termination, node <italic>k</italic> considers only its primary data as part of the final result.</p><p>This concept operates successfully under the following assumptions: (<italic>i</italic>) the topology of the system stays connected despite all occurring node failures, (<italic>ii</italic>) a reliable and efficient mechanism is available for determining whether a node (usually in the neighborhood) is alive (active) or not, and (<italic>iii</italic>) in case of permanent failures nodes fail <italic>neatly</italic>, i.&#x000a0;e., if a node fails <italic>within</italic> the execution of a DDAA, this failure has to be reported immediately, and the failing node <italic>i</italic> has to send some of its local values at least to one of its neighbors in order to ensure mass conservation.</p></sec><sec id="sec0060"><label>3.2.2</label><title>Ensuring mass conservation</title><p>The concrete resilience properties of the distributed orthogonalization method depend on the choice of the DDAA (cf. <xref rid="tbl0005" ref-type="table">Table 1</xref>). If the push-sum algorithm is used as a building block for rdmGS, reliable communication is required in order to ensure mass conservation. Using the push-flow algorithm as underlying aggregation algorithm for rdmGS instead of the push-sum algorithm allows for recovering from mass loss caused by <italic>temporary</italic> node or link failures and thus increases the resilience of rdmGS to these types of failures.</p><p>The weights <inline-formula><mml:math id="M21" altimg="si22.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> play an important role, since their initial values determine the type of aggregation operation (cf. Section&#x000a0;<xref rid="sec0030" ref-type="sec">2.1</xref>). However, in the presence of node failures, both initialization variants for summing the local data across the nodes are unsuitable, because the first initialization variant depends on the system size <italic>N</italic>, which will not remain constant, and because the second initialization variant introduces a single point of failure (the node with the initial value <inline-formula><mml:math id="M22" altimg="si23.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>). Consequently, in rdmGS we initialize all <inline-formula><mml:math id="M23" altimg="si24.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> such that the DDAA computes the average <inline-formula><mml:math id="M24" altimg="si25.gif" overflow="scroll"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> across the system and we distribute the value <italic>N</italic> of the initial system size to all nodes at the beginning of the algorithm. After termination of each DDAA, each node scales its local result by <italic>N</italic> for computing the sum from the average.</p><p>Beyond the resilience properties of the DDAA used, we need to ensure that none of the original data gets lost when a node fails. For that purpose, we introduce <italic>virtual nodes</italic>. Initially, the system contains <italic>N</italic> active physical nodes, and each of them corresponds to exactly one virtual node. Whenever a physical node <italic>l</italic> fails during the computation, another active physical node <italic>k</italic> has to take over all virtual nodes which physical node <italic>l</italic> was responsible for. Thus, if node failures occur in the process of rdmGS, active physical nodes take over responsibility for more than one virtual node. In order to ensure mass conservation, the sum of the weights over all active physical nodes needs to remain equal to the initial system size <italic>N</italic>. In order to achieve this, the surviving node <italic>k</italic> from before needs to increase its local weight by the weight of the failing node <italic>l</italic> (<inline-formula><mml:math id="M25" altimg="si26.gif" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02190;</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula>). The resulting mass conservation in the system guarantees that the DDAA can converge to the average of the original values of all <italic>N</italic> initially active nodes.</p><p>The resulting algorithmic structure of rdmGS is outlined in <xref rid="fig0010" ref-type="fig">Fig. 2</xref>. Each execution of a DDAA is preceded by a fail-checking phase, where every node <italic>k</italic> checks whether all of its prot&#x000e9;g&#x000e9;s and its guardians are alive. If yes, node <italic>k</italic> can proceed. If no, the following actions have to be taken: (<italic>i</italic>) If a prot&#x000e9;g&#x000e9; <italic>l</italic> of node <italic>k</italic> has failed, node <italic>k</italic> has to take over primary responsibility for <italic>l</italic>'s data and <italic>l</italic>'s weight has to be added to <italic>k</italic>'s local weight. Note that <italic>l</italic>'s local weight represents for how many virtual nodes the physical node <italic>l</italic> has been responsible for. Moreover, all updates of local weight as well as the updated primary data of node <italic>k</italic> (parts of <italic>A</italic> and <italic>Q</italic>) have to be sent to <italic>k</italic>'s guardian. (<italic>ii</italic>) If a guardian <italic>l</italic> of node <italic>k</italic> has failed, node <italic>k</italic> has to select a new guardian and send its local weight and primary data to it for backup. Among other aspects, the selection process of a new guardian should be influenced by the objective to balance the load across the active nodes.</p><p>There are only two specific scenarios of node failures which rdmGS cannot recover from, independently of which DDAA is used. First, if a node <italic>k</italic> and all of its <italic>r</italic>&#x000a0;&#x02212;&#x000a0;1 guardians fail permanently before even a single of these failures is detected, then rows of <italic>A</italic> and <italic>Q</italic> are lost and cannot be recovered any more. Second, if a node <italic>k</italic> fails permanently after passing the fail-checks of all of its <italic>r</italic>&#x000a0;&#x02212;&#x000a0;1 guardians, but before starting the next aggregation process, mass conservation is violated because <italic>k</italic>'s primary data is not used within that aggregation process and <italic>k</italic>'s guardians are not aware of it. Note that the probability for these two scenarios to happen can be reduced by increasing the overhead in terms of fail-checking frequency.</p></sec><sec id="sec0065"><label>3.2.3</label><title>Simulation results</title><p>In order to illustrate its properties, we developed a simulation model for the rdmGS algorithm with <italic>r</italic>&#x000a0;=&#x000a0;2 (each node has one guardian) in the ns-3 network simulator <xref rid="bib0160" ref-type="bibr">[32]</xref>. Simulation results are shown for orthogonalizing a 512&#x000a0;&#x000d7;&#x000a0;32 matrix on an asynchronous wired network of 512 nodes arranged in a nine-dimensional hypercube. The times until failure of a node are exponentially distributed with mean <italic>&#x003bb;</italic>, and nodes fail neatly (cf. Section&#x000a0;<xref rid="sec0055" ref-type="sec">3.2.1</xref>). As distributed data aggregation algorithm we used the push-sum algorithm. A detailed comparison of push-sum algorithm and push-flow algorithm as building block for rdmGS is work in progress. We varied the maximum numbers <italic>t</italic><sub>max</sub> of iterations per push-sum algorithm. For a given <italic>&#x003bb;</italic>, most values <italic>t</italic><sub>max</sub>&#x000a0;&#x02208;&#x000a0;[300&#x000a0;:&#x000a0;50&#x000a0;:&#x000a0;600] have been simulated 100 times with different initializations of the random number generator, but with the same underlying topology.</p><p>In <xref rid="fig0015" ref-type="fig">Fig. 3</xref>, the accuracy of rdmGS in terms of the relative factorization error ||<italic>A</italic>&#x000a0;&#x02212;&#x000a0;<italic>QR</italic>||<sub><italic>F</italic></sub>/||<italic>A</italic>||<sub><italic>F</italic></sub> is illustrated for <italic>&#x003bb;</italic>&#x000a0;=&#x000a0;15 [s].</p><p>With this value of <italic>&#x003bb;</italic>, between 50 and 150 of the 512 nodes failed per simulation run, on average 94.95 over all values of <italic>t</italic><sub>max</sub>. The average number of failed nodes per simulation run increases with the value of <italic>t</italic><sub>max</sub>. For <italic>t</italic><sub>max</sub>&#x000a0;=&#x000a0;300 it is around 65, for <italic>t</italic><sub>max</sub>&#x000a0;=&#x000a0;450 around 95, and for <italic>t</italic><sub>max</sub>&#x000a0;=&#x000a0;600 over 120 (see <xref rid="fig0020" ref-type="fig">Fig. 4</xref>).</p><p><xref rid="fig0015" ref-type="fig">Fig. 3</xref> illustrates that for small <italic>t</italic><sub>max</sub> the low accuracy of each push-sum algorithm leads to low accuracy of rdmGS. As <italic>t</italic><sub>max</sub> increases, the factorization error decreases and machine precision is reached in almost all runs for <italic>t</italic><sub>max</sub>&#x000a0;=&#x000a0;550. However, larger <italic>t</italic><sub>max</sub> leads to longer runtimes and thus also to a higher chance of node failure constellations which rdmGS cannot recover from (cf. Section&#x000a0;<xref rid="sec0060" ref-type="sec">3.2.2</xref>). Consequently, the fraction of simulation runs where rdmGS does not produce the full matrix <italic>Q</italic> due to node failures tends to grow with <italic>t</italic><sub>max</sub>.</p><p>We observe a similar behavior for the orthogonality of <italic>Q</italic>, measured in terms of <inline-formula><mml:math id="M26" altimg="si27.gif" overflow="scroll"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mo>&#x022a4;</mml:mo></mml:msup><mml:mi>Q</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:msqrt><mml:mi>m</mml:mi></mml:msqrt></mml:math></inline-formula> in <xref rid="fig0025" ref-type="fig">Fig. 5</xref>: it gets better for larger <italic>t</italic><sub>max</sub>, but also the chance increases that rdmGS does not produce the full matrix <italic>Q</italic> due to node failure constellations which it cannot recover from.</p><p>Summarizing, <xref rid="fig0015 fig0025" ref-type="fig">Figs. 3 and 5</xref> illustrate that there is a certain range of <italic>t</italic><sub>max</sub> where factorization accuracy and orthogonality achieved by rdmGS are excellent and in most cases the rdmGS algorithm recovers successfully from the node failures. For the simulation setup considered this range is around <italic>t</italic><sub>max</sub>&#x000a0;=&#x000a0;550.</p></sec></sec></sec><sec id="sec0070"><label>4</label><title>Scalability</title><p>In this section, we discuss the scalability of the methods we developed in this paper. For distributed data aggregation algorithms, scalability in terms of number of nodes corresponds to scalability in terms of problem size, since data from all nodes is aggregated. For the distributed orthogonalization method these two aspects need to be considered separately, though.</p><sec id="sec0075"><label>4.1</label><title>Distributed data aggregation algorithms</title><p>In a failure-free environment, the push-sum and the push-flow algorithm require <inline-formula><mml:math id="M27" altimg="si28.gif" overflow="scroll"><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>log</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mo>log</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> iterations for approximating the true aggregate with an error below <italic>&#x003f5;</italic> at each node if every node can communicate with any other node in the system <xref rid="bib0120" ref-type="bibr">[24]</xref>. More generally, the convergence speed of gossip-based algorithms also depends on properties of the communication graph, such as diameter or expansion (cf. <xref rid="bib0165" ref-type="bibr">[33]</xref>).</p><p>Although a higher node degree leads to faster convergence, it may have drawbacks in terms of resilience: If a temporary failure occurs, mass conservation is violated. DDAAs based on <italic>flows</italic> (see Section&#x000a0;<xref rid="sec0025" ref-type="sec">2</xref>) in principle have the ability to recover from this violation of mass conservation at the time of the next failure-free communication along the link which was affected by the failure. Since in gossiping algorithms nodes choose their communication partners randomly (usually uniformly), a higher node degree increases the expected time until recovery from a failure. Therefore, in order to combine fast convergence with quick recovery from a violation of mass conservation, the communication graph should have small node degrees but good expansion properties. Examples of topologies with these properties are <italic>k</italic>-ary <italic>n</italic>-cubes which are often referred to as <italic>n</italic>D torus (with <italic>k</italic> nodes per edge). Besides commonly used topologies such as 3D tori, we consider in the following also hypercubes because of their interesting properties. Generally speaking, an <italic>n</italic>D torus with <italic>k</italic> nodes per edge consists of <italic>k</italic><sup><italic>n</italic></sup> nodes, has a node degree of 2<italic>n</italic> and a diameter of <italic>nk</italic>/2. According to these properties we see that diameter and node degree are antagonists in the case of <italic>n</italic>-ary <italic>k</italic>-cubes, i.&#x000a0;e., for a fixed number of nodes we can either aim for a small node degree (by decreasing <italic>k</italic>) for better fault tolerance or for a small diameter (by increasing <italic>n</italic>) for faster convergence. As we will see in the following, the concrete values of <italic>n</italic> and <italic>k</italic> have a major impact on the achieved performance.</p><p>For arbitrary fixed topologies, a theoretical framework for analyzing distributed aggregation algorithms has been developed in <xref rid="bib0080" ref-type="bibr">[16]</xref>. It has also been shown there how to derive algorithms with optimal convergence speed for arbitrary topologies. While those algorithms are not prepared to deal with failures (F2)&#x02013;(F5) it is interesting to observe that for communication graphs with good expansion the number of iterations required for convergence is still <inline-formula><mml:math id="M28" altimg="si29.gif" overflow="scroll"><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>log</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mo>log</mml:mo><mml:msup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>
<xref rid="bib0080" ref-type="bibr">[16]</xref>. Since hypercube topologies and other closely related cube-like topologies have good expansion properties, this result actually shows that on communication graphs which allow for fast reduction operations, randomized approaches scale asymptotically equally well as all-to-all reduction operations.</p><p>In the following figures, all data points shown are averages over 100 simulation runs in order to capture the randomized nature of the algorithms investigated. <xref rid="fig0030" ref-type="fig">Fig. 6</xref> illustrates simulation results of the scaling behavior (number of iterations required to reach machine precision for averaging) for increasing number of nodes of the distributed data aggregation algorithms discussed in Sections&#x000a0;<xref rid="sec0025 sec0045" ref-type="sec">2 and 3.1</xref> on a system <italic>without</italic> failures.</p><p>Note that without failures, the push-sum algorithm, LiMoSense and our new push-flow algorithm are basically equivalent and thus their scaling behavior is identical. These three algorithms clearly scale better than the flow updating algorithm. Besides the theoretically predicted <inline-formula><mml:math id="M29" altimg="si30.gif" overflow="scroll"><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>log</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> behavior if every node can communicate with any other node, we also see that even on the weaker connected hypercube topology the asymptotic behavior of the push-flow algorithm is the same as for the optimal all-to-all reduction operation, and the concrete number of iterations required only differs by a modest factor. In contrast to that, we observe a worse scaling for the 3D and 6D torus because of their weaker connectivity. In case of an <italic>n</italic>D torus we choose in general the number of nodes per edge as a power of two and we keep the number of nodes along the different dimensions as equal as possible. E.g., in the case of a 3D torus and <italic>N</italic>&#x000a0;=&#x000a0;2<sup>7</sup> we consider a 2<sup>3</sup>&#x000a0;&#x000d7;&#x000a0;2<sup>2</sup>&#x000a0;&#x000d7;&#x000a0;2<sup>2</sup> torus. This specific choice also explains the &#x0201c;staircase&#x0201d; behavior of the number of iterations observed in <xref rid="fig0030" ref-type="fig">Fig. 6</xref>, since the edge with the highest number of nodes determines the overall convergence speed.</p><p><xref rid="fig0035" ref-type="fig">Fig. 7</xref> illustrates the increase in the number of iterations required by LiMoSense and the push-flow algorithm with increasing node failure rate for different system sizes (numbers of nodes).</p><p>We see that the push-flow algorithm always requires fewer iterations for convergence than LiMoSense. Compared to a failure free environment, (<italic>i</italic>) for small systems, the push-flow algorithm hardly experiences any increase in the number of iterations, and (<italic>ii</italic>) for larger systems, the increase in the number of iterations tends to be very modest for low and medium failure rates and grows up to a factor of four for the push-flow algorithm for high failure rates. The node counts were chosen in order to allow for a rough comparison with an experimental case study for the overhead of checkpointing and restarting given in <xref rid="bib0030" ref-type="bibr">[6]</xref>: In that case study, the overhead of checkpointing and restarting was larger than a factor of two for 2<sup>16</sup>&#x000a0;&#x02248;&#x000a0;65&#x000a0;000 nodes, which corresponds to failure probabilities higher than 2<sup>&#x02212;7</sup> for the push-flow algorithm. For 2<sup>18</sup>&#x000a0;&#x02248;&#x000a0;260&#x000a0;000 nodes, the overhead of checkpointing and restarting in the case study presented in <xref rid="bib0030" ref-type="bibr">[6]</xref> was larger than a factor of eight, which corresponds to very high failure probabilities above 2<sup>&#x02212;3</sup> for the push-flow algorithm. This indicates a faster growth of the overhead caused by checkpointing and restarting.</p><p>While we studied in <xref rid="fig0035" ref-type="fig">Fig. 7</xref> the performance of different algorithms with increasing failure rate on a constant (hypercube) topology we consider in contrast to that in <xref rid="fig0040" ref-type="fig">Fig. 8</xref> only the push-flow algorithm and vary the topologies instead. More specifically, we report the number of iterations needed for convergence on a 3D, 6D and 9D (twisted) torus relative to the iterations needed on the 18D hypercube. For the 3D, 6D and 9D torus we observe exactly the behavior expected from theory that a lower dimensionality (node degree) leads to better fault tolerance properties but also to a slower convergence. For the twisted versions of the considered tori (the twists are shifted by half of the number of nodes on the edge) we also observe the expected slight improvements in terms of convergence speed and we even see that on the 9D twisted torus a higher performance is achieved than on the 18D hypercube.</p></sec><sec id="sec0080"><label>4.2</label><title>Distributed orthogonalization</title><p>The scalability of rdmGS with the number of nodes is determined by the scalability of the specific DDAA used, because all interaction with other nodes is concentrated in the data aggregation. As shown before, if it is based on the push-sum algorithm or on the push-flow algorithm, the number of iterations needed on many topologies will grow like <inline-formula><mml:math id="M30" altimg="si31.gif" overflow="scroll"><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>log</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, which is the same asymptotic behavior as parallel all-to-all reduction operations.</p><p>In terms of scalability with the problem size, we note the following: Increasing <italic>n</italic> for fixed <italic>N</italic> scales well and even improves accuracy, because it increases the local computation and does not affect the computation cost. In the version of rdmGS described in this paper, the number of DDAAs invoked grows quadratically with <italic>m</italic>, which can become a limiting factor for large <italic>m</italic>. However, we are currently developing an improvement of the underlying dmGS algorithm which requires only <inline-formula><mml:math id="M31" altimg="si32.gif" overflow="scroll"><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> DDAAs <xref rid="bib0140" ref-type="bibr">[28]</xref> and thus further improves scalability in this respect.</p></sec></sec><sec id="sec0085"><label>5</label><title>Summary and conclusions</title><p>We have shown that distributed algorithms based on randomized communication schedules can be very attractive for potentially unreliable or unstable large-scale systems, in particular in terms of fault tolerance and resilience. We have presented the new push-flow algorithm for distributed computation of sums or averages, which has better resilience properties than existing distributed data aggregation algorithms. Moreover, we have developed the distributed orthogonalization method rdmGS on top of distributed data aggregation algorithms, which is very resilient to various types of failures and capable of producing fully accurate results even if several nodes fail permanently. Simulation experiments showed that even when 30% of the nodes of the system fail on average, rdmGS produces results accurate to machine precision in at least 88% of the simulation runs.</p><p>Investigation of remaining questions in terms of the potential of these new randomized algorithms for high performance requirements as well as a quantitative investigation of the influence of asynchrony on their performance is work in progress.</p></sec></body><back><ref-list><title>References</title><ref id="bib0005"><label>1</label><element-citation publication-type="journal" id="sbref0005"><person-group person-group-type="author"><name><surname>Schumm</surname><given-names>P.</given-names></name><name><surname>Scoglio</surname><given-names>C.</given-names></name></person-group><article-title>Bloom: a stochastic growth-based fast method of community detection in networks</article-title><source>Journal of Computational Science</source><volume>3</volume><issue>5</issue><year>2012</year><fpage>356</fpage><lpage>366</lpage></element-citation></ref><ref id="bib0010"><label>2</label><element-citation publication-type="journal" id="sbref0010"><person-group person-group-type="author"><name><surname>Bliss</surname><given-names>C.A.</given-names></name><name><surname>Kloumann</surname><given-names>I.M.</given-names></name><name><surname>Harris</surname><given-names>K.D.</given-names></name><name><surname>Danforth</surname><given-names>C.M.</given-names></name><name><surname>Dodds</surname><given-names>P.S.</given-names></name></person-group><article-title>Twitter reciprocal reply networks exhibit assortativity with respect to happiness</article-title><source>Journal of Computational Science</source><volume>3</volume><issue>5</issue><year>2012</year><fpage>388</fpage><lpage>397</lpage></element-citation></ref><ref id="bib0015"><label>3</label><element-citation publication-type="journal" id="sbref0015"><person-group person-group-type="author"><name><surname>Safar</surname><given-names>M.</given-names></name><name><surname>Mahdi</surname><given-names>K.</given-names></name><name><surname>Torabi</surname><given-names>S.</given-names></name></person-group><article-title>Network robustness and irreversibility of information diffusion in complex networks</article-title><source>Journal of Computational Science</source><volume>2</volume><issue>3</issue><year>2011</year><fpage>198</fpage><lpage>206</lpage></element-citation></ref><ref id="bib0020"><label>4</label><element-citation publication-type="journal" id="sbref0020"><person-group person-group-type="author"><name><surname>Fagg</surname><given-names>G.E.</given-names></name><name><surname>Gabriel</surname><given-names>E.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Angskun</surname><given-names>T.</given-names></name><name><surname>Bosilca</surname><given-names>G.</given-names></name><name><surname>Pjesivac-Grbovic</surname><given-names>J.</given-names></name><name><surname>Dongarra</surname><given-names>J.J.</given-names></name></person-group><article-title>Process fault tolerance: semantics design and applications for high performance computing</article-title><source>International Journal of High Performance Computing Applications</source><volume>19</volume><issue>4</issue><year>2005</year><fpage>465</fpage><lpage>477</lpage></element-citation></ref><ref id="bib0025"><label>5</label><element-citation publication-type="journal" id="sbref0025"><person-group person-group-type="author"><name><surname>Daly</surname><given-names>J.T.</given-names></name></person-group><article-title>A higher order estimate of the optimum checkpoint interval for restart dumps</article-title><source>Future Generation Computer Systems</source><volume>22</volume><year>2006</year><fpage>303</fpage><lpage>312</lpage></element-citation></ref><ref id="bib0030"><label>6</label><element-citation publication-type="book" id="sbref0030"><person-group person-group-type="author"><name><surname>Varela</surname><given-names>M.</given-names></name><name><surname>Ferreira</surname><given-names>K.</given-names></name><name><surname>Riesen</surname><given-names>R.</given-names></name></person-group><chapter-title>Fault-tolerance for exascale systems</chapter-title><source>2010 IEEE International Conference on Cluster Computing Workshops and Posters (CLUSTER WORKSHOPS)</source><year>2010</year><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="bib0035"><label>7</label><element-citation publication-type="journal" id="sbref0035"><person-group person-group-type="author"><name><surname>Plank</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Puening</surname><given-names>M.</given-names></name></person-group><article-title>Diskless checkpointing</article-title><source>IEEE Transactions on Parallel and Distributed Systems</source><volume>9</volume><issue>10</issue><year>1998</year><fpage>972</fpage><lpage>986</lpage></element-citation></ref><ref id="bib0040"><label>8</label><element-citation publication-type="book" id="sbref0040"><person-group person-group-type="author"><name><surname>Plank</surname><given-names>J.</given-names></name></person-group><chapter-title>Improving the performance of coordinated checkpointers on networks of workstations using raid techniques</chapter-title><source>Proceedings of the 15th Symposium on Reliable Distributed Systems, 1996</source><year>1996</year><fpage>76</fpage><lpage>85</lpage></element-citation></ref><ref id="bib0045"><label>9</label><element-citation publication-type="book" id="sbref0045"><person-group person-group-type="author"><name><surname>Guermouche</surname><given-names>A.</given-names></name><name><surname>Ropars</surname><given-names>T.</given-names></name><name><surname>Brunet</surname><given-names>E.</given-names></name><name><surname>Snir</surname><given-names>M.</given-names></name><name><surname>Cappello</surname><given-names>F.</given-names></name></person-group><chapter-title>Uncoordinated checkpointing without domino effect for send-deterministic MPI applications</chapter-title><source>2011 IEEE International Parallel &#x00026; Distributed Processing Symposium (IPDPS)</source><year>2011</year><fpage>989</fpage><lpage>1000</lpage></element-citation></ref><ref id="bib0050"><label>10</label><element-citation publication-type="book" id="sbref0050"><person-group person-group-type="author"><name><surname>Ferreira</surname><given-names>K.</given-names></name><name><surname>Riesen</surname><given-names>R.</given-names></name><name><surname>Oldfield</surname><given-names>R.</given-names></name><name><surname>Stearley</surname><given-names>J.</given-names></name><name><surname>Laros</surname><given-names>J.</given-names></name><name><surname>Pedretti</surname><given-names>K.</given-names></name><name><surname>Brightwell</surname><given-names>T.</given-names></name></person-group><chapter-title>rMPI: increasing fault resiliency in a message-passing environment</chapter-title><source>Tech. Rep., No. SAND2011-2488</source><conf-name>Sandia National Laboratories</conf-name><year>2011</year></element-citation></ref><ref id="bib0055"><label>11</label><element-citation publication-type="book" id="sbref0055"><person-group person-group-type="author"><name><surname>Engelmann</surname><given-names>C.</given-names></name><name><surname>Ong</surname><given-names>H.H.</given-names></name><name><surname>Scott</surname><given-names>S.L.</given-names></name></person-group><chapter-title>The case for modular redundancy in large-scale high performance computing systems</chapter-title><source>Proceedings of the 27th IASTED International Conference on Parallel and Distributed Computing and Networks (PDCN) 2009</source><conf-name>ACTA Press, Calgary, AB, Canada</conf-name><year>2009</year><fpage>189</fpage><lpage>194</lpage></element-citation></ref><ref id="bib0060"><label>12</label><element-citation publication-type="book" id="sbref0060"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Mueller</surname><given-names>F.</given-names></name><name><surname>Engelmann</surname><given-names>C.</given-names></name><name><surname>Scott</surname><given-names>S.L.</given-names></name></person-group><chapter-title>Proactive process-level live migration in HPC environments</chapter-title><source>Proceedings of the 2008 ACM/IEEE Conference on Supercomputing, SC &#x02018;08</source><conf-name>IEEE Press, Piscataway, NJ, USA, 43:1&#x02013;43:12</conf-name><year>2008</year></element-citation></ref><ref id="bib0065"><label>13</label><element-citation publication-type="journal" id="sbref0065"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K.-H.</given-names></name><name><surname>Abraham</surname><given-names>J.</given-names></name></person-group><article-title>Algorithm-based fault tolerance for matrix operations</article-title><source>IEEE Transactions on Computers</source><volume>C-33</volume><issue>6</issue><year>1984</year><fpage>518</fpage><lpage>528</lpage></element-citation></ref><ref id="bib0070"><label>14</label><element-citation publication-type="journal" id="sbref0070"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Dongarra</surname><given-names>J.</given-names></name></person-group><article-title>Algorithm-based fault tolerance for fail-stop failures</article-title><source>IEEE Transactions on Parallel and Distributed Systems</source><volume>19</volume><issue>12</issue><year>2008</year><fpage>1628</fpage><lpage>1641</lpage></element-citation></ref><ref id="bib0075"><label>15</label><element-citation publication-type="book" id="sbref0075"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z.</given-names></name></person-group><chapter-title>Algorithm-based recovery for iterative methods without checkpointing</chapter-title><source>Proceedings of the 20th International Symposium on High Performance Distributed Computing, HPDC &#x02018;11, ACM, New York</source><conf-name>NY, USA</conf-name><year>2011</year><fpage>73</fpage><lpage>84</lpage></element-citation></ref><ref id="bib0080"><label>16</label><element-citation publication-type="journal" id="sbref0080"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>S.</given-names></name><name><surname>Ghosh</surname><given-names>A.</given-names></name><name><surname>Prabhakar</surname><given-names>B.</given-names></name><name><surname>Shah</surname><given-names>D.</given-names></name></person-group><article-title>Randomized gossip algorithms</article-title><source>IEEE Transactions on Information Theory</source><volume>52</volume><issue>6</issue><year>2006</year><fpage>2508</fpage><lpage>2530</lpage></element-citation></ref><ref id="bib0085"><label>17</label><element-citation publication-type="journal" id="sbref0085"><person-group person-group-type="author"><name><surname>Kermarrec</surname><given-names>A.-M.</given-names></name><name><surname>van Steen</surname><given-names>M.</given-names></name></person-group><article-title>Gossiping in distributed systems</article-title><source>SIGOPS Operating Systems Review</source><volume>41</volume><year>2007</year><fpage>2</fpage><lpage>7</lpage></element-citation></ref><ref id="bib0090"><label>18</label><element-citation publication-type="journal" id="sbref0090"><person-group person-group-type="author"><name><surname>Dimakis</surname><given-names>A.</given-names></name><name><surname>Sarwate</surname><given-names>A.</given-names></name><name><surname>Wainwright</surname><given-names>M.</given-names></name></person-group><article-title>Geographic gossip: Efficient averaging for sensor networks</article-title><source>IEEE Transactions on Signal Processing</source><volume>56</volume><issue>3</issue><year>2008</year><fpage>1205</fpage><lpage>1216</lpage></element-citation></ref><ref id="bib0095"><label>19</label><element-citation publication-type="journal" id="sbref0095"><person-group person-group-type="author"><name><surname>Aysal</surname><given-names>T.</given-names></name><name><surname>Yildiz</surname><given-names>M.</given-names></name><name><surname>Sarwate</surname><given-names>A.</given-names></name><name><surname>Scaglione</surname><given-names>A.</given-names></name></person-group><article-title>Broadcast gossip algorithms for consensus</article-title><source>IEEE Transactions on Signal Processing</source><volume>57</volume><issue>7</issue><year>2009</year><fpage>2748</fpage><lpage>2761</lpage></element-citation></ref><ref id="bib0100"><label>20</label><element-citation publication-type="book" id="sbref0100"><person-group person-group-type="author"><name><surname>Karp</surname><given-names>R.</given-names></name><name><surname>Schindelhauer</surname><given-names>C.</given-names></name><name><surname>Shenker</surname><given-names>S.</given-names></name><name><surname>Vocking</surname><given-names>B.</given-names></name></person-group><chapter-title>Randomized rumor spreading</chapter-title><source>Proceedings of the 41st Annual Symposium on Foundations of Computer Science, IEEE Computer Society</source><conf-name>Washington, DC, USA</conf-name><year>2000</year><fpage>565</fpage><lpage>574</lpage></element-citation></ref><ref id="bib0105"><label>21</label><element-citation publication-type="journal" id="sbref0105"><person-group person-group-type="author"><name><surname>Jelasity</surname><given-names>M.</given-names></name><name><surname>Montresor</surname><given-names>A.</given-names></name><name><surname>Babaoglu</surname><given-names>O.</given-names></name></person-group><article-title>Gossip-based aggregation in large dynamic networks</article-title><source>ACM Transactions on Computer Systems</source><volume>23</volume><year>2005</year><fpage>219</fpage><lpage>252</lpage></element-citation></ref><ref id="bib0110"><label>22</label><element-citation publication-type="journal" id="sbref0110"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>D.</given-names></name></person-group><article-title>Gossip algorithms</article-title><source>Found. Trends Netw.</source><volume>3</volume><year>2009</year><fpage>1</fpage><lpage>125</lpage></element-citation></ref><ref id="bib0115"><label>23</label><element-citation publication-type="book" id="sbref0115"><person-group person-group-type="author"><name><surname>Mosk-Aoyama</surname><given-names>D.</given-names></name><name><surname>Shah</surname><given-names>D.</given-names></name></person-group><chapter-title>Computing separable functions via gossip</chapter-title><source>Proceedings of the Twenty-fifth Annual ACM Symposium on Principles of Distributed Computing, PODC &#x02018;06</source><conf-name>ACM, New York, NY, USA</conf-name><year>2006</year><fpage>113</fpage><lpage>122</lpage></element-citation></ref><ref id="bib0120"><label>24</label><element-citation publication-type="book" id="sbref0120"><person-group person-group-type="author"><name><surname>Kempe</surname><given-names>D.</given-names></name><name><surname>Dobra</surname><given-names>A.</given-names></name><name><surname>Gehrke</surname><given-names>J.</given-names></name></person-group><chapter-title>Gossip-based computation of aggregate information</chapter-title><source>FOCS &#x02018;03: Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science</source><conf-name>IEEE Computer Society</conf-name><year>2003</year><fpage>482</fpage><lpage>491</lpage></element-citation></ref><ref id="bib0125"><label>25</label><element-citation publication-type="journal" id="sbref0125"><person-group person-group-type="author"><name><surname>Olshevsky</surname><given-names>A.</given-names></name><name><surname>Tsitsiklis</surname><given-names>J.N.</given-names></name></person-group><article-title>Convergence speed in distributed consensus and averaging</article-title><source>SIAM Journal on Control and Optimization</source><volume>48</volume><year>2009</year><fpage>33</fpage><lpage>55</lpage></element-citation></ref><ref id="bib0130"><label>26</label><mixed-citation publication-type="other" id="oref0130">H. Strakov&#x000e1;, W.N. Gansterer, T. Zemen, Distributed QR factorization based on randomized algorithms, in: R. Wyrzykowski, et al., (Ed.), Proceedings of the 9th International Conference on Parallel Processing and Applied Mathematics, Part I, vol. 7203 of Lecture Notes in Computer Science, Springer Verlag, 2012, pp. 235&#x02013;244.</mixed-citation></ref><ref id="bib0135"><label>27</label><element-citation publication-type="journal" id="sbref0135"><person-group person-group-type="author"><name><surname>Kempe</surname><given-names>D.</given-names></name><name><surname>McSherry</surname><given-names>F.</given-names></name></person-group><article-title>A decentralized algorithm for spectral analysis</article-title><source>Journal of Computer and System Sciences</source><volume>74</volume><issue>1</issue><year>2008</year><fpage>70</fpage><lpage>83</lpage></element-citation></ref><ref id="bib0140"><label>28</label><element-citation publication-type="book" id="sbref0140"><person-group person-group-type="author"><name><surname>Strakov&#x000e1;</surname><given-names>H.</given-names></name><name><surname>Gansterer</surname><given-names>W.N.</given-names></name></person-group><chapter-title>A distributed eigensolver for loosely coupled networks</chapter-title><source>Proceedings of the 21st Euromicro International Conference on Parallel, Distributed and Network-Based Processing</source><year>2013</year></element-citation></ref><ref id="bib0145"><label>29</label><element-citation publication-type="book" id="sbref0145"><person-group person-group-type="author"><name><surname>Eyal</surname><given-names>I.</given-names></name><name><surname>Keidar</surname><given-names>I.</given-names></name><name><surname>Rom</surname><given-names>R.</given-names></name></person-group><chapter-title>LiMoSense - Live Monitoring in Dynamic Sensor Networks</chapter-title><source>Proceedings of the 7th International Conference on Algorithms for Sensor Systems, Wireless Ad Hoc Networks and Autonomous Mobile Entities, ALGOSENSORS&#x02019;11, Springer-Verlag</source><conf-name>Berlin, Heidelberg</conf-name><year>2012</year><fpage>72</fpage><lpage>85</lpage></element-citation></ref><ref id="bib0150"><label>30</label><element-citation publication-type="book" id="sbref0150"><person-group person-group-type="author"><name><surname>Jesus</surname><given-names>P.</given-names></name><name><surname>Baquero</surname><given-names>C.</given-names></name><name><surname>Almeida</surname><given-names>P.S.</given-names></name></person-group><chapter-title>Fault-tolerant aggregation by flow updating</chapter-title><source>Proceedings of the 9th IFIP WG 6.1 International Conference on Distributed Applications and Interoperable Systems, DAIS &#x02018;09</source><conf-name>Springer-Verlag, Berlin, Heidelberg</conf-name><year>2009</year><fpage>73</fpage><lpage>86</lpage></element-citation></ref><ref id="bib0155"><label>31</label><element-citation publication-type="book" id="sbref0155"><person-group person-group-type="author"><name><surname>Jesus</surname><given-names>P.</given-names></name><name><surname>Baquero</surname><given-names>C.</given-names></name><name><surname>Almeida</surname><given-names>P.</given-names></name></person-group><chapter-title>Fault-tolerant aggregation for dynamic networks</chapter-title><source>29th IEEE Symposium on Reliable Distributed Systems</source><year>2010</year><fpage>37</fpage><lpage>43</lpage></element-citation></ref><ref id="bib0160"><label>32</label><mixed-citation publication-type="other" id="oref0160">The ns-3 network simulator. <ext-link ext-link-type="uri" xlink:href="http://www.nsnam.org/">http://www.nsnam.org/</ext-link>.</mixed-citation></ref><ref id="bib0165"><label>33</label><element-citation publication-type="journal" id="sbref0165"><person-group person-group-type="author"><name><surname>Hoory</surname><given-names>S.</given-names></name><name><surname>Linial</surname><given-names>N.</given-names></name><name><surname>Wigderson</surname><given-names>A.</given-names></name></person-group><article-title>Expander graphs and their applications</article-title><source>Bulletin of the American Mathematical Society</source><volume>43</volume><year>2006</year><fpage>439</fpage><lpage>561</lpage></element-citation></ref></ref-list><bio><p><bold>Wilfried Gansterer</bold> is associate professor at the Faculty of Computer Science of the University of Vienna. He is deputy head of the research group Theory and Applications of Algorithms and leader of the research lab Computational Technologies and Applications. He holds an M.Sc. in technical mathematics from Vienna University of Technology, an M.Sc. in scientific computing and computational mathematics from Stanford University, and a Ph.D. in scientific computing from Vienna University of Technology. After a position as senior post-doctoral research associate at the Department of Computer Science of the University of Tennessee, he joined the Faculty of Computer Science at the University of Vienna, where he successfully completed his habilitation procedure and received tenure. His research interests include parallel and distributed computing as well as high performance computing. Currently, he focuses on reliable distributed algorithms for various numerical linear algebra problems and their applications. He has co-authored two books and over 60 papers in peer-reviewed journals and conferences.</p></bio><bio><p><bold>Gerhard Niederbrucker</bold> received B.Sc. degrees in technical mathematics as well as computer science and a M.Sc. degree in computer science from the Vienna University of Technology. Currently, he is pursuing a Ph.D. degree at the Faculty of Computer Science at the University of Vienna and a M.Sc. degree in technical mathematics at the Vienna University of Technology. His present research is focused on the solution of real world computing problems and the challenges posed by existing and future computing platforms like high fault tolerance demands or extreme concurrency.</p></bio><bio><p><bold>Hana Strakov&#x000e1;</bold> holds a M.Sc. degree in computer science from the Charles University in Prague and is currently a Ph.D. student at the Faculty of Computer Science at the University of Vienna. Her research focuses on designing and analyzing fully distributed algorithms based on randomized communication schedules. In particular, she works on distributed orthogonalization methods and on distributed eigensolvers.</p></bio><bio><p><bold>Stefan Schulze Grotthoff holds</bold> a B.Sc. degree in computer science from the University of Vienna. His interests include machine learning and gossip-based algorithms.</p></bio><ack id="ack0005"><title>Acknowledgement</title><p>Financial support was provided by <funding-source>the Austrian Science Fund</funding-source> (FWF): S10608 (NFN SISE).</p></ack></back><floats-group><fig id="fig0005"><label>Fig. 1</label><caption><p>The push-flow algorithm for the local computation of a global aggregate. <inline-formula><mml:math id="M32" altimg="si1.gif" overflow="scroll"><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> denotes node <italic>i</italic>'s neighborhood.</p></caption><graphic xlink:href="gr1"/></fig><fig id="fig0010"><label>Fig. 2</label><caption><p>The rdmGS algorithm: <italic>P</italic><sub><italic>k</italic></sub> denotes the set of indices for rows of <italic>A</italic> and <italic>Q</italic> which are primary data on node <italic>k</italic>. <italic>B</italic><sub><italic>k</italic></sub> denotes the set of indices for rows of <italic>A</italic> and <italic>Q</italic> which are backup data on node <italic>k</italic>. &#x0201c;DDAA(x)&#x0201d; denotes the execution of a distributed data aggregation algorithm on the distributed vector <italic>x</italic>.</p></caption><graphic xlink:href="gr2"/></fig><fig id="fig0015"><label>Fig. 3</label><caption><p>Relative factorization error of rdmGS for <italic>&#x003bb;</italic>&#x000a0;=&#x000a0;15 [s]. &#x0201c;no full result&#x0201d; refers to scenarios where failure of a node <italic>and</italic> its backup node between checks for node failures (within the call of a DDAA) causes the complete loss of information and thus an incomplete result.</p></caption><graphic xlink:href="gr3"/></fig><fig id="fig0020"><label>Fig. 4</label><caption><p>Average number of node failures per simulation for <italic>&#x003bb;</italic>&#x000a0;=&#x000a0;15 [s] and different <italic>t</italic><sub>max</sub>. Averaged over all simulation runs and all <italic>t</italic><sub>max</sub>, 95.13 of the 512 nodes failed per simulation run.</p></caption><graphic xlink:href="gr4"/></fig><fig id="fig0025"><label>Fig. 5</label><caption><p>Orthogonality of rdmGS for <italic>&#x003bb;</italic>&#x000a0;=&#x000a0;15 [s]. &#x0201c;no full result&#x0201d; refers to scenarios where failure of a node <italic>and</italic> its backup node between checks for node failures (within the call of a DDAA) causes the complete loss of information and thus an incomplete result.</p></caption><graphic xlink:href="gr5"/></fig><fig id="fig0030"><label>Fig. 6</label><caption><p>Scalability of DDAAs on fully connected, hypercube and torus topologies (no failures; push-flow, push-sum and LiMoSense are equivalent).</p></caption><graphic xlink:href="gr6"/></fig><fig id="fig0035"><label>Fig. 7</label><caption><p>Resilience and scalability of LiMoSense and push-flow for averaging on a hypercube topology relative to failure-free system with 2<sup>8</sup> nodes.</p></caption><graphic xlink:href="gr7"/></fig><fig id="fig0040"><label>Fig. 8</label><caption><p>Resilience and scalability of push-flow for averaging on different torus topologies with 2<sup>18</sup> nodes relative to the execution on an 18D hypercube.</p></caption><graphic xlink:href="gr8"/></fig><table-wrap id="tbl0005" position="float"><label>Table 1</label><caption><p>Resilience properties of existing DDAAs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Push-sum</th><th align="left">LiMoSense</th><th align="left">Flow updating</th></tr></thead><tbody><tr><td align="left">(F1)</td><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td></tr><tr><td align="left">(F2)</td><td align="left">&#x02013;</td><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td></tr><tr><td align="left">(F3)</td><td align="left">&#x02013;</td><td align="left">&#x0221a;</td><td align="left">&#x0221a;</td></tr><tr><td align="left">(F4)</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x0221a;</td></tr><tr><td align="left">(F5)</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr></tbody></table></table-wrap></floats-group></article>
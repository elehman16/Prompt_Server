<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23637687</article-id><article-id pub-id-type="pmc">3636464</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2013.00213</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Processing of Fear and Anger Facial Expressions: The Role of Spatial Frequency</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Comfort</surname><given-names>William E.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Meng</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Benton</surname><given-names>Christopher P.</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Zana</surname><given-names>Yossi</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Centro de Matem&#x000e1;tica, Computa&#x000e7;&#x000e3;o e Cogni&#x000e7;&#x000e3;o, Universidade Federal do ABC</institution><country>Santo Andr&#x000e9;, Brazil</country></aff><aff id="aff2"><sup>2</sup><institution>Institute of Neuroscience, Guangzhou Medical University</institution><country>Guangzhou, China</country></aff><aff id="aff3"><sup>3</sup><institution>School of Experimental Psychology, University of Bristol</institution><country>Bristol, UK</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Linda Isaac, Palo Alto VA and Stanford University, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Linda Isaac, Palo Alto VA and Stanford University, USA; Charles Alain Collin, University of Ottawa, Canada</p></fn><corresp id="fn001">*Correspondence: William E. Comfort, Centro de Matem&#x000e1;tica, Computa&#x000e7;&#x000e3;o e Cogni&#x000e7;&#x000e3;o, Universidade Federal do ABC, Rua Santa Ad&#x000e9;lia 166, Santo Andr&#x000e9;, S&#x000e3;o Paulo 09.210-170, Brazil. e-mail: <email xlink:type="simple">william.comfort@ufabc.edu.br</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Frontiers in Cognitive Science, a specialty of Frontiers in Psychology.</p></fn></author-notes><pub-date pub-type="epub"><day>26</day><month>4</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>4</volume><elocation-id>213</elocation-id><history><date date-type="received"><day>15</day><month>11</month><year>2012</year></date><date date-type="accepted"><day>07</day><month>4</month><year>2013</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2013 Comfort, Wang, Benton and Zana.</copyright-statement><copyright-year>2013</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in other forums, provided the original authors and source are credited and subject to any copyright notices concerning any third-party graphics etc.</license-p></license></permissions><abstract><p>Spatial frequency (SF) components encode a portion of the affective value expressed in face images. The aim of this study was to estimate the relative weight of specific frequency spectrum bandwidth on the discrimination of anger and fear facial expressions. The general paradigm was a classification of the expression of faces morphed at varying proportions between anger and fear images in which SF adaptation and SF subtraction are expected to shift classification of facial emotion. A series of three experiments was conducted. In Experiment 1 subjects classified morphed face images that were unfiltered or filtered to remove either low (&#x0003c;8 cycles/face), middle (12&#x02013;28 cycles/face), or high (&#x0003e;32 cycles/face) SF components. In Experiment 2 subjects were adapted to unfiltered or filtered prototypical (non-morphed) fear face images and subsequently classified morphed face images. In Experiment 3 subjects were adapted to unfiltered or filtered prototypical fear face images with the phase component randomized before classifying morphed face images. Removing mid frequency components from the target images shifted classification toward fear. The same shift was observed under adaptation condition to unfiltered and low- and middle-range filtered fear images. However, when the phase spectrum of the same adaptation stimuli was randomized, no adaptation effect was observed. These results suggest that medium SF components support the perception of fear more than anger at both low and high level of processing. They also suggest that the effect at high-level processing stage is related more to high-level featural and/or configural information than to the low-level frequency spectrum.</p></abstract><kwd-group><kwd>spatial frequency</kwd><kwd>face adaptation</kwd><kwd>fear</kwd><kwd>anger</kwd><kwd>facial expressions</kwd><kwd>face processing</kwd></kwd-group><counts><fig-count count="9"/><table-count count="0"/><equation-count count="0"/><ref-count count="47"/><page-count count="12"/><word-count count="7573"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Face perception includes the integration of complex visual input across a spectrum of varying spatial frequencies (SFs) (Sowden and Schyns, <xref ref-type="bibr" rid="B40">2006</xref>). Different SFs are transmitted to higher cortical regions through separate neurological channels broadly related to the magnocellular and parvocellular pathways originating in the retina. High spatial frequency (HSF) information is processed primarily through the ventral visual processing stream from V1 and encodes the fine-grained texture of objects such as faces. In contrast, low spatial frequency (LSF) information is encoded through faster, more direct networks in subcortical and early visual areas and communicates rapid, coarse signals concerning the configuration or spatial relationship between facial features (for an overview, see Ruiz-Soler and Beltran, <xref ref-type="bibr" rid="B33">2006</xref>). Subsequent outputs from these pathways typically converge in higher-level regions, including the amygdala, fusiform gyrus, and orbitofrontal cortex (Vuilleumier et al., <xref ref-type="bibr" rid="B43">2003</xref>; Bar et al., <xref ref-type="bibr" rid="B2">2006</xref>; Rotshtein et al., <xref ref-type="bibr" rid="B32">2007</xref>). Previous studies have suggested that information from high and low SF bandwidths may be effectively integrated at these sites, producing a neural representation of the face to guide subsequent recognition (Eger et al., <xref ref-type="bibr" rid="B7">2004</xref>; Gauthier et al., <xref ref-type="bibr" rid="B12">2005</xref>; Rotshtein et al., <xref ref-type="bibr" rid="B32">2007</xref>). While the integration of facial identity through SF information has been well documented (Costen et al., <xref ref-type="bibr" rid="B4">1996</xref>; N&#x000e4;s&#x000e4;nen, <xref ref-type="bibr" rid="B29">1999</xref>; Goffaux et al., <xref ref-type="bibr" rid="B15">2003</xref>; Rotshtein et al., <xref ref-type="bibr" rid="B32">2007</xref>; Gao and Bentin, <xref ref-type="bibr" rid="B11">2011</xref>), there is less information about how our neural representation of a facial emotion is related to specific SF ranges. Studies which have focused on this issue (Vuilleumier et al., <xref ref-type="bibr" rid="B43">2003</xref>; Deruelle and Fagot, <xref ref-type="bibr" rid="B6">2004</xref>; Aguado et al., <xref ref-type="bibr" rid="B1">2010</xref>; Kumar and Srinivasan, <xref ref-type="bibr" rid="B23">2011</xref>) have examined the effect of high and low SF on the perception of emotional expressions using removal of specific SF components from the target stimuli. For example, Kumar and Srinivasan (<xref ref-type="bibr" rid="B23">2011</xref>) showed that low (&#x0003c;8 cycles/face) and high-frequencies (&#x0003e;32 cycles/face) are more critical for the representation of happy and sad expressions, respectively. Aguado et al. (<xref ref-type="bibr" rid="B1">2010</xref>) compared reaction time to classification of happy and anger faces and found an advantage of LSF (&#x0003c;12 cycles/degree) over HSF (&#x0003e;3 cycle/degree). Vuilleumier et al. (<xref ref-type="bibr" rid="B43">2003</xref>), on the other hand, found no differences between reaction time and accuracy for discrimination of fear over neutral expression for low (2&#x02013;8 cycles/face) and middle (8&#x02013;16 cycles/face) SF band-pass filtered images. Similarly, Deruelle and Fagot (<xref ref-type="bibr" rid="B6">2004</xref>) found no differences in adult subjects between discrimination of low (&#x0003c;12) and high-pass (&#x0003e;36) filtered faces with a smiling or grimacing expression (participants aged below nine showed a bias toward high-pass filtered images). Although important, none of these results advance our current knowledge regarding the SF tuning at different phases of facial expression processing. In this study we are aim to discriminate between low and high levels of processing SF content in the representation of facial expressions. Potentially, this will reveal further details of how and at what stage differing SF bandwidths are integrated. Visual adaptation, a psychophysical method commonly associated with higher-order visual processing (Fox and Barton, <xref ref-type="bibr" rid="B9">2007</xref>; Webster and MacLeod, <xref ref-type="bibr" rid="B46">2011</xref>) may provide answers as to how visual SFs are processed at relatively late stages of facial expression analysis.</p><p>Visual adaptation, in the context of face processing, takes the form of prolonged exposure to a target face which can affect the subsequent perception of facial attributes and provide valuable insights into the pattern of visual coding used in facial processing (for an overview, see Webster and MacLeod, <xref ref-type="bibr" rid="B46">2011</xref>). In a typical experimental setting, a subject is exposed to a particular face, causing faces which are presented afterward to be perceived as less similar to the adapting face in comparison to a condition with no adaptation. Such face adaptation aftereffects are robust across changes in the size, retinal position, and orientation of facial images and exhibit similar patterns of adaptation across low-level features such as contrast, color, and SF (Leopold et al., <xref ref-type="bibr" rid="B24">2001</xref>; Watson and Clifford, <xref ref-type="bibr" rid="B44">2003</xref>; Yamashita et al., <xref ref-type="bibr" rid="B47">2005</xref>). Adaptation is also specific to facial attributes: aftereffects have been documented for facial properties such as identity (Leopold et al., <xref ref-type="bibr" rid="B24">2001</xref>, <xref ref-type="bibr" rid="B25">2005</xref>), gender, race, and expression (Webster et al., <xref ref-type="bibr" rid="B45">2004</xref>; Furl et al., <xref ref-type="bibr" rid="B10">2007</xref>). As these adaptation effects are uninfluenced by variations in low-level visual features (Webster and MacLeod, <xref ref-type="bibr" rid="B46">2011</xref>), it is reasonable to assume they are associated with higher-level processing (Clifford et al., <xref ref-type="bibr" rid="B3">2007</xref>). Electrophysiological evidence (Kov&#x000e1;cs et al., <xref ref-type="bibr" rid="B22">2007</xref>) supports this hypothesis: ERP responses associated with facial aftereffects were more correlated with detailed encoding of the facial attribute rather than processing of low-level visual features.</p><p>As facial stimuli filtered to include only specific SF bandwidths have been shown to selectively activate higher-level areas associated with the semantic/functional content of faces (Vuilleumier et al., <xref ref-type="bibr" rid="B43">2003</xref>; Rotshtein et al., <xref ref-type="bibr" rid="B32">2007</xref>), the adaptation effects associated with certain facial stimuli are expected to vary according to their SF content. Yamashita et al. (<xref ref-type="bibr" rid="B47">2005</xref>) found that featural distortions in one SF range (e.g., LSF) of a facial stimulus failed to produce subsequent aftereffects in its opposing SF range (e.g., HSF), showing less transfer of adaptation effects across SF than other visual features such as image size. In addition, pairs of faces varying solely by SF content were rated as less similar than those differing according to other low-level features such as size, contrast, and color. Thus, SF information manipulation appears to drive face adaptation effects at a relatively high-level of visual processing. It should also be noted that spatially filtering a facial image may alter configural and featural properties of the face which are important for facial representation (Goffaux et al., <xref ref-type="bibr" rid="B14">2005</xref>; Yamashita et al., <xref ref-type="bibr" rid="B47">2005</xref>).</p><p>Clearly, evidence from face adaptation can offer key insights into how facial expressions are represented in the brain. Two alternative views predominate in this domain: one that posits that facial emotions are represented as discrete categories, with expression aftereffects serving to highlight the differential relationships between emotion categories (e.g., Rutherford et al., <xref ref-type="bibr" rid="B34">2008</xref>). The other views face adaptation as equivalent to a shift in the prototype of a multidimensional &#x0201c;face space,&#x0201d; where the facial attribute is encoded in terms of its deviation from an average (e.g., Robbins et al., <xref ref-type="bibr" rid="B31">2007</xref>). In the present study the latter continuum-based approach was taken.</p><p>Given a reference point <italic>x<sub>i</sub></italic> at which a certain morphed face image is perceived as expressing fear and anger at equal probability (a balance point), manipulation of frequency content and adaptation state is expected to shift that point to one of the two extremes. For example, if low frequency components are used to encode fear more than anger, removing low SF content from the stimuli will increase the probability of the face image at point <italic>x<sub>i</sub></italic> to be classified as expressing anger. Fear and anger expressions were chosen due to the fact that these form opposites along such a continuum in the feature dimension (e.g., distance between eyebrows and eyes, aperture of mouth) and they are negatively correlated in terms of their diagnostic information (Smith et al., <xref ref-type="bibr" rid="B38">2005</xref>) and evolutionary perspective (anger as dominant, fear as submissive; see Lepp&#x000e4;nen and Nelson, <xref ref-type="bibr" rid="B26">2009</xref>). Adaptation to either facial expression is expected to cause the morphed face to appear more similar to the opposite expression. As an alternative to methods adopted by other studies examining adaptation to facial expressions (Hsu and Young, <xref ref-type="bibr" rid="B18">2004</xref>; Fox and Barton, <xref ref-type="bibr" rid="B9">2007</xref>; Juricevic and Webster, <xref ref-type="bibr" rid="B20">2012</xref>), this design evaluates adaptation aftereffects in the context of ambivalent expressions instead of neutral expressions (Webster et al., <xref ref-type="bibr" rid="B45">2004</xref>).</p><p>Experiment 1 looks at the classification of emotional expressions under conditions of limited SF information; specifically, when components of high, medium, or low frequency are missing from the target face image. This method probes the use of specific SF information for encoding facial expression at a low and/or high level of processing, and was used to determine what SF range will be likely recruited in later adaptation. In Experiment 2 we tested the combined adaptation effect of magnitude and phase components on classification of morphed facial expressions of fear and anger, through adaptation to filtered and unfiltered images of prototypical fear. In Experiment 3 filtered and unfiltered adaptation stimuli were constructed and presented such that only the amplitude spectrum from a specific spectral bandwidth was retained, while its corresponding phase spectrum was randomized. Thus, we tested the working hypothesis that fear and anger expression processing is tuned to specific SF bandwidths, independent of the featural or configural information in the face. By comparison of the results from Experiment 2 and 3, we can estimate the relative use of SF content for high-level representation of facial expressions along a fear-anger axis. The results of this study demonstrate for the first time, to the best of our knowledge, differential SF tuning of facial fear and anger expression at low and high visual processing levels through behavioral paradigms.</p></sec><sec><title>Experiment 1: Removing Specific SF Bandwidths from Test Stimuli</title><sec sec-type="materials|methods"><title>Materials and methods</title><sec><title>Participants and apparatus</title><p>Seven volunteers from the Universidade Federal do ABC (four male and four female between 18 and 37&#x02009;years of age) with normal or corrected-to-normal vision participated in the experiment. Images were displayed on a Samsung SyncMaster 997MB monitor with screen dimensions of 368&#x02009;&#x000d7;&#x02009;276&#x02009;mm, 1024&#x02009;&#x000d7;&#x02009;768 resolution, and refresh rate of 100&#x02009;Hz; Gamma was corrected to produce a linear luminance-modulated image using a photometer (Tektronix, model J18, sensor model J1803) for luminance measurements. The monitor was driven by a Pentium D 3.40&#x02009;GHz PC. PsychToolBox software (Pelli, <xref ref-type="bibr" rid="B30">1997</xref>) was used to display all images in MATLAB<sup>&#x000ae;</sup>.</p></sec><sec><title>Stimuli</title><p>A total of 50 images with equal numbers of male and female faces were drawn from the Karolinska Directed Emotional Faces (KDEF) database (Lundqvist et al., <xref ref-type="bibr" rid="B28">1998</xref>) to form a subset of 25 male and female images representing fear and a subset of 25 male and female images representing anger. The emotional content of the images were rated on intensity and arousal scales and it was shown that they comprise a valid set of affective facial pictures (Goeleven et al., <xref ref-type="bibr" rid="B13">2008</xref>). Each of these subsets was gray-scaled and averaged across gender and identity to produce a prototypical fearful and angry face, measuring 512&#x02009;&#x000d7;&#x02009;512 pixels, using the methods described in Tiddeman et al. (<xref ref-type="bibr" rid="B42">2001</xref>).</p><p>A sequence of 101 images was created by morphing between the prototypical fear and anger expression face (Tiddeman et al., <xref ref-type="bibr" rid="B42">2001</xref>), such that the 0 and 100 morph level corresponded to prototypical anger or fear, respectively, and the 50 morph level corresponded to a morph image with equal weighting of both expressions (Figure <xref ref-type="fig" rid="F1">1</xref>). Next, the set of morphed images was filtered to remove specific SF components of either low (&#x0003c;8 cycles/face), medium (12&#x02013;28 cycles/face), or high (&#x0003e;32 cycles/face) bandwidth. This filtering procedure was conducted after the morphing procedure to minimize SF distortion (Tiddeman et al., <xref ref-type="bibr" rid="B42">2001</xref>). Butterworth filters were applied to produce three sets of morphed continua: HF-subtracted condition, MF-subtracted condition, and LF-subtracted condition (Figure <xref ref-type="fig" rid="F1">1</xref>). Cut-off frequencies were 8 cycles/face, 12 and 28 cycles/face, and 32 cycles/face, respectively. Thus, the bandwidths of the low and high-pass filters are 3-octaves wide, while the middle-range notch-filter is approximately 1.2 octave-wide. The fourth-order Butterworth filters guaranteed a minimum of 4.3&#x02009;dB attenuation before any frequency crossover occurred.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Image examples from the four morph continua (Full-bandwidth, -HSF, -MSF, and -LSF)</bold>. The abscissa represent the degree of morphing of the corresponding images.</p></caption><graphic xlink:href="fpsyg-04-00213-g001"/></fig><p>Next, an oval mask subtending 289&#x02009;&#x000d7;&#x02009;416 pixels was applied, producing an object area of 7.43&#x000b0;&#x02009;&#x000d7;&#x02009;10.68&#x000b0; of visual angle. All images were presented at a mean luminance of 25&#x02009;cd/m<sup>2</sup>. To standardize the presentation times of stimuli across all three experiments, an adaptation image of white noise, subtending an area of 13.1&#x000b0;&#x02009;&#x000d7;&#x02009;13.1&#x000b0;, was displayed prior to the test images.</p></sec><sec><title>Procedure</title><p>In each trial subjects were required to classify the emotional expression of the test image by making a binary choice (&#x0201c;anger&#x0201d; or &#x0201c;fear&#x0201d;) on a standard keyboard. Subjects were required to maintain their attention on a white fixation point, which remained onscreen during the presentation of adaptation and test stimuli, the interstimulus interval and the response phase of the experiment. The white noise adaptor was presented for 30&#x02009;s in the first trial and thereafter for 5&#x02009;s duration intervals prior to the test image. After an interstimulus interval of 500&#x02009;ms (containing a central fixation point only), the test image was displayed for 1&#x02009;s (see Figure <xref ref-type="fig" rid="F2">2</xref> for details). In the control sequence, a test image drawn from the unfiltered morph dataset was presented. In the experimental conditions (-HSF, -MSF, -LSF), a test image was drawn from one of the filtered morph ranges and presented. Test images appeared at a random location in a circular trajectory with a diameter of 1&#x000b0; of visual angle around the central fixation point and continued moving in a randomly determined directional rotation (clockwise or counter-clockwise) around the center of the screen. Subjects were instructed to maintain fixation on the cross location while the test image rotated in order to avoid retinotopic adaptation (Skinner and Benton, <xref ref-type="bibr" rid="B36">2010</xref>).</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Procedure for Experiment 1</bold>. At the beginning of each trial, a central fixation point is presented for 500&#x02009;ms. Next, a white noise adaptor is presented for 30&#x02009;s in the first trial of the block, and for 5&#x02009;s in every subsequent trial. After the noise stimulus, the central fixation point is displayed for another 500&#x02009;ms. A test image (from either the full-bandwidth, -HSF, -MSF, or -LSF morph continua) is then displayed for 1&#x02009;s, and the screen presents a gray background until a binary response (anger or fear) is made. All adaptors and test images are rotated in a randomized direction with a diameter of 1&#x000b0; of visual angle around the center of the screen.</p></caption><graphic xlink:href="fpsyg-04-00213-g002"/></fig><p>After viewing the test image there was an interstimulus interval with the central fixation point only, and subjects were required to make a binary response classifying the expression in the image. Only once this response had been made, did the experiment progress to the next trial. The order of test image presentation was determined using Bayesian entropy estimation (Kontsevich and Tyler, <xref ref-type="bibr" rid="B21">1999</xref>), updating previous probabilities in the estimated psychophysical function by selecting the morph level on each trial. The morph level in each trial was selected to yield the maximum expected information for prediction of the expected mean threshold.</p><p>The ocular distance to the screen was set to 80&#x02009;cm. The order and design of the experimental blocks was based on a full Latin squares design. Prior to each experimental session, participants completed a training phase to minimize subsequent variability within the experiment. The training phase consisted of blocks using the control condition of full-bandwidth images. Participants only progressed to the experiment if the standard deviation of the last 10 thresholds in the block was lower than 3. In total, each participant completed 16 experimental blocks, spaced evenly into 4 experimental sessions, with 4 blocks per session. Each block contained a total of 40 trials. The block order and session order was pseudo-randomized.</p><p>Subjects&#x02019; responses for the 40 trials in each block were collected and threshold and slope parameters were estimated by fitting a Gaussian cumulative function. Each participant yielded a total of 16 thresholds. The Chi-square statistic of goodness-of-fit was used to assess the fit of the function and blocks containing a score of &#x003c7;<sup>2</sup>&#x02009;&#x0003c;&#x02009;&#x02212;18 were discarded. The data from one participant was excluded from the analysis, due to lack of adequate fitting of their data (the majority of blocks were discarded).</p><p>All experimental procedures were conducted in accordance with the Conselho Nacional de &#x000c9;tica em Pesquisa rules and were approved by the Ethical Committee of the Federal University of ABC. An informed consent was obtained from all subjects.</p></sec></sec><sec><title>Results and discussion</title><p>Figure <xref ref-type="fig" rid="F3">3</xref> shows the mean threshold results of the participants for each condition (full-bandwidth, -HSF, -MSF, and -LSF). For all subjects, the balance point between fear and anger in the full-bandwidth condition was at a morph level weighted toward anger (34&#x02013;39). There was a relative shift toward anger for participants 1&#x02013;4 when LF components were removed in comparison with the conditions in which medium or high-frequency components were removed. Thresholds for the middle- and high-frequency subtracted conditions were similar (participants 1&#x02013;5) and/or shifted toward fear (participants 1, 2, and 4) in comparison with the control condition.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Mean thresholds of morph level for the four conditions (full-bandwidth, -HSF, -MSF, and -LSF) for participants in Experiment 1</bold>. Error bars show &#x000b1;1 standard error of the mean.</p></caption><graphic xlink:href="fpsyg-04-00213-g003"/></fig><p>To investigate the between-subject effect of the different conditions, the threshold of each participant for each experimental condition was subtracted from his mean threshold of the control condition (Figure <xref ref-type="fig" rid="F4">4</xref>). Results show that removing high or middle-frequency components shifts the balance point toward fear, i.e., without these components, there is an increase in responses classifying the expression as anger in comparison to the control condition. The opposite effect is observed when low frequency components are removed from the face images, where there is an increase in the reported perception of fear.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Mean difference in threshold for each condition of test image type (full-bandwidth, -HSF, -MSF, and -LSF); with respect to the mean threshold of the control condition (full-bandwidth)</bold>. Error bars show &#x000b1;1 standard error of the mean. Asterisk represents significant difference at <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05 level from full-bandwidth condition.</p></caption><graphic xlink:href="fpsyg-04-00213-g004"/></fig><p>A one-way repeated-measures ANOVA was conducted to determine statistical significance of the differences in thresholds. There was a significant main effect of filtering condition [<italic>F</italic>(3, 69)&#x02009;=&#x02009;4.423, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01]. <italic>Post hoc</italic> tests using the Dunnett correction revealed a significant difference between the -MSF (<italic>M</italic>&#x02009;=&#x02009;39.59, &#x003c3;<sub>M</sub>&#x02009;=&#x02009;0.688) and full-bandwidth condition (<italic>M</italic>&#x02009;=&#x02009;36.96, &#x003c3;<sub>M</sub>&#x02009;=&#x02009;0.688), <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05. All other differences between conditions were non-significant (<italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05).</p><p>The results indicate that middle-frequency spatial information is more critical for anger encoding than for fear, while high-frequency components might also dominate anger representation. Considering the literature reviewed in the Section <xref ref-type="sec" rid="s1">&#x0201c;Introduction,&#x0201d;</xref> by itself, this result reflects processing at low-level facial expression processing, high-level, or both. In selecting between angry and fearful expressions, SF information may be flexibly used (as suggested by Schyns and Oliva, <xref ref-type="bibr" rid="B35">1999</xref>), where the higher level of threat associated with anger is preferentially encoded through low SF information, while medium and high SF information is recruited for the perception of fear. To test the effect of SF information at a higher level of processing facial expressions, an adaptation paradigm was adopted and forms the basis of Experiment 2 and 3.</p></sec></sec><sec><title>Experiment 2: Adaptation to Filtered Expressions of Fear</title><sec><title>Materials and methods</title><p>Experiment 2 shares partially the methods already described in Experiment 1 and will be described only briefly. Five volunteers from the Federal University of ABC (two male and two female between 18 and 29&#x02009;years of age) with normal or corrected-to-normal vision participated in the experiment. Only subjects who had not participated in Experiment 1 were included in Experiment 2. The apparatus used was identical to that used in Experiment 1.</p><p>A 100-level morph stimulus (prototypical fear) was used as the template for all adaptation stimuli. To test the combined adaptation effect of the SF magnitude and the phase component present in the expression, four adaptation images were generated by Fourier transforming the 100-level morph image. Images were then either filtered through a low-pass, band-pass, or high-pass Butterworth filter, or left unfiltered. This operation produced four adaptor stimuli: low SF (&#x0003c;8 cycle/image), medium SF (12&#x02013;28 cycles/image), high SF (&#x0003e;&#x02009;32 cycles/image), and full-bandwidth SF fear images. A white noise control adaptor was used as a baseline condition. Images from the full-bandwidth morph range, described in Experiment 1, were used as test images. All adaptation and test images had a size of 7.43&#x000b0;&#x02009;&#x000d7;&#x02009;10.7&#x000b0;.</p><p>Experimental instructions, stimuli presentation intervals, response collection, and the method of threshold estimation were identical to those of Experiment 1. In this experiment only the adaptor and test images were altered. There were five adaptation conditions: white noise, LF fear, MF fear, HF fear, and full-bandwidth fear. Forty full-bandwidth test images were used (Figure <xref ref-type="fig" rid="F5">5</xref>).</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Procedure for Experiment 2</bold>. At the beginning of each trial, a central fixation point is presented for 500&#x02009;ms. Next, an unfiltered or filtered face adaptor is presented for 30&#x02009;s in the first trial of the block, and for 5&#x02009;s in every subsequent trial. After the noise stimulus, the central fixation point is displayed for another 500&#x02009;ms. A test image (from the full-bandwidth morph range) is then displayed for 1&#x02009;s, and the screen presents a gray background until a binary response (anger or fear) is made. All adaptors and test images are rotated in a randomized direction with a diameter of 1&#x000b0; of visual angle around the center of the screen.</p></caption><graphic xlink:href="fpsyg-04-00213-g005"/></fig><p>The procedure and design of the training phase was identical to that used in Experiment 1. The order and design of blocks were based on a partial Latin squares design. In total, each participant completed 12 experimental blocks, spaced evenly into three experimental sessions, with four blocks per session. Each block contained a total of 40 trials. The block order and session order was pseudo-randomized for all participants. The data from one participant was excluded from the analysis, due to lack of adequate fitting of their data (the majority of blocks were discarded).</p></sec><sec><title>Results and discussion</title><p>Figure <xref ref-type="fig" rid="F6">6</xref> shows the mean threshold results of the participants for each condition (white noise, LF fear, MF fear, HF fear, and full-bandwidth fear). Adaptation to the MF fear and full fear conditions produces consistently stronger aftereffects than adaptation to the LF fear and HF fear conditions. All subjects show adaptation effects for the MF and full-bandwidth fear conditions compared with the control condition. Participants 2 and 3 also show adaptation effects for the LF and HF fear conditions. To calculate a measure of the adaptation strength for all condition, thresholds were subtracted from the mean threshold of the control condition, producing mean difference scores representing adaptation strength (Figure <xref ref-type="fig" rid="F9">9</xref>).</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Mean thresholds of morph level for the five conditions (white noise, LF fear, MF fear, HF fear, and full-bandwidth fear) for the participants in Experiment 3</bold>. Error bars show &#x000b1;1 standard error of the mean.</p></caption><graphic xlink:href="fpsyg-04-00213-g006"/></fig><p>A one-way repeated-measures ANOVA was conducted to determine statistical significance of threshold difference. There was a significant main effect of adaptation condition [<italic>F</italic>(4, 24)&#x02009;=&#x02009;16.360, <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01]. <italic>Post hoc</italic> tests using the Dunnett correction revealed a significant difference between white noise condition (<italic>M</italic>&#x02009;=&#x02009;43.65, &#x003c3;<sub>M</sub>&#x02009;=&#x02009;5.57) and LF fear condition (<italic>M</italic>&#x02009;=&#x02009;50.63, &#x003c3;<sub>M</sub>&#x02009;=&#x02009;9.02), <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05, MF fear condition (<italic>M</italic>&#x02009;=&#x02009;61.60, &#x003c3;<sub>M</sub>&#x02009;=&#x02009;5.36), <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01, and full-bandwidth fear condition (<italic>M</italic>&#x02009;=&#x02009;61.79, &#x003c3;<sub>M</sub>&#x02009;=&#x02009;7.77), <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.01. HF fear condition (<italic>M</italic>&#x02009;=&#x02009;48.35, &#x003c3;<sub>M</sub>&#x02009;=&#x02009;7.83) was not significantly different from the white noise condition (<italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05).</p><p>The full-bandwidth fear adaptor shifted perception toward anger, a result that corroborates the literature by suggesting a polarity between anger and fear face processing, but which does not suffice to answer whether it is related to the SF content or not. The results of the other conditions further indicate that the adaptation effect is specific to low- and middle-frequency components, but not high-frequency. In consideration of the literature presented in the Section <xref ref-type="sec" rid="s1">&#x0201c;Introduction,&#x0201d;</xref> it can be argued that these results reflect characteristics of high-level processing. To test the adaptation effect of facial expression without phase information, we conducted Experiment 3.</p></sec></sec><sec><title>Experiment 3: Adaptation to Filtered Expressions of Fear at Random-Phase</title><sec><title>Materials and methods</title><p>Experiment 3 shares partially the methods already described in Experiment 1 and will be described only briefly. Six volunteers from the Federal University of ABC (all male between 18 and 45&#x02009;years of age) with normal or corrected-to-normal vision participated in the experiment, including authors YZ and WC (participants 5 and 6, respectively). Only subjects who had not participated in Experiment 1 and 2 were included. The apparatus used was identical to that of Experiment 1.</p><p>A 100-level morph stimulus (prototypical fear) was used as the template for all adaptation stimuli (same as in Experiment 2). To test adaptation effects related to specific SF bandwidths, four adaptors were generated by first Fourier transforming the images. Images were then either filtered through a low-pass, band-pass, or high-pass Butterworth filter, or left unfiltered. Next, the phase part of the frequency components was randomized and an inverse Fourier transformation was applied (Hansen et al., <xref ref-type="bibr" rid="B17">2008</xref>). This operation produced four adaptor stimuli: low SF (0&#x02013;8 cycle/image), medium SF (12&#x02013;28 cycles/image), high SF (&#x0003e;32 cycles/image), and full-bandwidth SF random-phase images. A white noise control adaptor was used as a baseline condition. Images from the full-bandwidth morph range, described in Experiment 1, were used as test images. Adaptation images subtended over an object area of 13.1&#x000b0;&#x02009;&#x000d7;&#x02009;13.1&#x000b0;, while all test images spanned 7.43&#x000b0;&#x02009;&#x000d7;&#x02009;10.7&#x000b0;.</p><p>Experimental instructions, stimuli presentation intervals, response collection, and the method of threshold estimation were identical to those of Experiment 1. In this experiment only the adaptor and test images were altered. There were five adaptation conditions: white noise, LF random-phase, MF random-phase, HF random-phase, and full-bandwidth random-phase. Forty full-bandwidth test images were used (Figure <xref ref-type="fig" rid="F7">7</xref>).</p><fig id="F7" position="float"><label>Figure 7</label><caption><p><bold>Procedure for Experiment 3</bold>. At the beginning of each trial, a central fixation point is presented for 500&#x02009;ms. Next, a random-phase adaptor is presented for 30&#x02009;s in the first trial of the block, and for 5&#x02009;s in every subsequent trial. After the adapting stimulus, the central fixation point is displayed for another 500&#x02009;ms. The test image (from the full-bandwidth morph range) is displayed for 1&#x02009;s, and the screen presents a gray background until a binary response (anger or fear) is made. All adaptors and test images are rotated in a randomized direction with a diameter of 1&#x000b0; of visual angle around the center of the screen.</p></caption><graphic xlink:href="fpsyg-04-00213-g007"/></fig><p>The procedure and design of the training phase were identical to that used in Experiment 1. The order and design of blocks were based on a partial Latin squares design. Each participant completed 15 experimental blocks, spaced evenly into three experimental sessions, with five blocks per session. Each block contained a total of 40 trials. The block order and session order were pseudo-randomized for all participants.</p></sec><sec><title>Results and discussion</title><p>Figure <xref ref-type="fig" rid="F8">8</xref> shows the mean threshold results of the participants for each condition (white noise, LF random-phase, MF random-phase, HF random-phase, and full-bandwidth random-phase). There appear to be no consistent differences between the control condition (white noise) and the adaptor conditions. The balance point varied strongly between participants, in the 20&#x02013;50 range. That variation probably represents differences in criteria level, while the main effect is calculated via the deviation from the control condition.</p><fig id="F8" position="float"><label>Figure 8</label><caption><p><bold>Mean thresholds of morph level for the five conditions (white noise, LF random-phase, MF random-phase, HF random-phase, and full-bandwidth random-phase) for participants in Experiment 3</bold>. Error bars show &#x000b1;1 standard error of the mean.</p></caption><graphic xlink:href="fpsyg-04-00213-g008"/></fig><p>To compare the difference in threshold shift between the control condition (white noise) and the experimental conditions (LF random-phase, MF random-phase, HF random-phase, full-bandwidth random-phase) the thresholds for each condition were subtracted from the mean threshold of the control condition, producing mean difference scores for every condition (Figure <xref ref-type="fig" rid="F9">9</xref>). A one-way repeated-measures ANOVA was conducted, revealing no significant main effect of adaptation condition, <italic>F</italic>(4, 55)&#x02009;=&#x02009;0.003, <italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05.</p><fig id="F9" position="float"><label>Figure 9</label><caption><p><bold>Mean difference in threshold for each condition of adaptor image (white noise, LF fear, MF fear, HF fear, full-bandwidth fear) with respect to the mean threshold of the control condition (white noise)</bold>. Blue triangles represent results from adaptor images with magnitude and phase spectra (Experiment 2). Red diamonds represent results from adaptors with phase spectrum only (Experiment 3). Error bars show &#x000b1;1 standard error of the mean. Asterisks represent significant difference at <italic>p</italic>&#x02009;&#x0003c;&#x02009;0.05 level from white noise condition.</p></caption><graphic xlink:href="fpsyg-04-00213-g009"/></fig><p>The absence of significant differences between the balance-points at different adaptation conditions indicates that the information content of the phase spectrum is critical for encoding fear and anger expressions.</p></sec></sec><sec><title>General Discussion</title><p>The experiments reported in this study examined the effect of specific SF bandwidth manipulation on the classification of facial expressions under two alternative paradigms: SF subtraction and SF adaptation. Shifts along the anger-fear dimension as a result of subtracting specific SF components can indicate the critical information used for representing the expressions. Here we show that subtracting medium-frequency components significantly shifts the balance point toward the fear prototype in comparison to the control condition, indicating that these components are used for encoding fear more than for anger. As previously suggested, different SF ranges may be recruited when presenting both expressions simultaneously as opposed to presenting either expression in isolation. Accordingly, the lesser degree of threat present in fear may result in increased recruitment of medium SF channels, and not low SF channels. Subtracting low frequency components, on the other hand, did not produce a significant shift of the balance point, suggesting that they are equally critical for anger and fear classification. However, although the results are significant, the effect is weak, indicating that anger and fear classification can rely, although with partial accuracy, on any SF bandwidth.</p><p>Previous studies indicate that this paradigm predominantly probes low-level processing of face stimuli (Halit et al., <xref ref-type="bibr" rid="B16">2006</xref>; Flevaris et al., <xref ref-type="bibr" rid="B8">2008</xref>), consequently these results suggest that fear encoding, in comparison with anger, depends more on medium-frequency sources at the initial processing stages. To probe higher-level processing, an adaptation approach was taken. The results (Figure <xref ref-type="fig" rid="F9">9</xref>) show that adaptation to full-spectrum fear shifted the balance point toward the fear prototype, indicating a relative increase in sensitivity to perception of anger. This result is in accordance with previous studies (Webster et al., <xref ref-type="bibr" rid="B45">2004</xref>; Juricevic and Webster, <xref ref-type="bibr" rid="B20">2012</xref>). Experiment 2 investigated the specific spatial components associated with this effect, revealing that low- and middle-frequency components, with a predominance of the latter, are the most influential when processing fearful face images. Thus, middle-frequency components seem to be the most critical for fear encoding at both low and high-level processing. However, the adaptation effect of filtered images in Experiment 2 could be due to the effect of (1) low-level SF content, (2) high-level phase information, or (3) both. Experiment 3 was designed to exclude any featural and/or configural information present in the phase spectrum, leaving untouched the magnitude of the frequency components by randomizing the phase spectrum. The results indicate that the adaptation effect observed in Experiment 2 is dependent on the phase information. As a whole, the results suggest that (1) mid frequency components are the most critical for encoding the fearful facial expression on the anger-fear axis at both low- and high level of processing and that (2) high-level processing of fear relies more on facial information content present in the phase spectrum which may include featural and configural information.</p><p>The adaptation technique used in the present study to investigate the effect of different SFs on face representations has visible antecedents: Vuilleumier et al. (<xref ref-type="bibr" rid="B43">2003</xref>) discovered an asymmetrical habituation effect in the fusiform gyrus. Participants habituated to the same face identity when the facial image was shown first in a high-pass version and later as a low-pass version but not vice versa. If a neural face representation were independent of the type of SF information processed, we would expect to see a symmetrical habituation effect instead (Gauthier et al., <xref ref-type="bibr" rid="B12">2005</xref>). Kumar and Srinivasan (<xref ref-type="bibr" rid="B23">2011</xref>) recently examined the effect of SF specificity on reaction time and error rate in the discrimination of facial expressions of happiness and sadness. They concluded that low SF bandwidths (&#x0003c;8 cycles/face) were diagnostic for classification of happy expressions, while high SF bandwidths (&#x0003e;32 cycles/face) were critical for processing of sad expressions. However their dependent measures, used also in other studies (Deruelle and Fagot, <xref ref-type="bibr" rid="B6">2004</xref>; Aguado et al., <xref ref-type="bibr" rid="B1">2010</xref>), give no indication as to the level of SF processing. The limitations of calculating a &#x0201c;critical bandwidth&#x0201d; for processing individual facial expressions are becoming increasingly clear, as can be seen from the findings that SF information is flexibly used depending on the task type (Schyns and Oliva, <xref ref-type="bibr" rid="B35">1999</xref>) and/or viewing distance (Smith and Schyns, <xref ref-type="bibr" rid="B37">2009</xref>).</p><p>Visual channels within the brain appear to be tuned to specific SF bandwidths, varying by octave (e.g., 2&#x02013;4, 32&#x02013;64), and correspond to changes along a contrast sensitivity function (Ruiz-Soler and Beltran, <xref ref-type="bibr" rid="B33">2006</xref>). Low and high SF information is processed in parallel through separate neural pathways, with fast, unconscious processing of coarse low SF information occurring through the pulvinar, superior colliculi, amygdala and other subcortical structures, and slower, more refined analysis of detailed high SF information occurring through cortical visual areas including the fusiform gyrus (Vuilleumier et al., <xref ref-type="bibr" rid="B43">2003</xref>; Rotshtein et al., <xref ref-type="bibr" rid="B32">2007</xref>). Sowden and Schyns (<xref ref-type="bibr" rid="B40">2006</xref>) have argued that flexible use of diagnostic SF information depends on top-down selection between different SF channels which may be a product of learning the relevance of available SF content, the viewing environment and task-based expectations. The evidence for top-down selection of SF channels is inconclusive: studies of SF uncertainty show improved performance on blocks where the target SF is the same as opposed to intercalated with other SFs. Moreover, these uncertainty effects on intermixed blocks are reduced or suppressed if the target SF is cued by a sound or number (Davis et al., <xref ref-type="bibr" rid="B5">1983</xref>; H&#x000fc;bner, <xref ref-type="bibr" rid="B19">1996</xref>), yet critical-band noise-masking studies show no evidence of top-down selection between SF channels (Solomon and Pelli, <xref ref-type="bibr" rid="B39">1994</xref>; Lu and Dosher, <xref ref-type="bibr" rid="B27">2004</xref>; Talgar et al., <xref ref-type="bibr" rid="B41">2004</xref>). Our results indicate that SF information is processed alongside other diagnostic facial information to show selectivity in the higher-level representation of facial stimuli. This conclusion offers some clarification to the inconclusive findings of top-down selection of SF channels and goes some way to identifying at which stage SF and other forms of visual information are integrated in facial processing.</p><p>This study offers a timely and effective means of bridging face adaptation and SF research. Future research in this area should go beyond determining &#x0201c;critical values&#x0201d; for identity and expression, and investigate how specific SF bandwidths are integrated through multiple channels and at different stages of processing, how selection of SF information changes in relation to the scarcity of visual input or noise distraction and the role of learning in the selection of diagnostic or relevant information in response to task stimuli.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>Author William E. Comfort was supported by a CAPES grant under CPF number 233.889.298-40.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguado</surname><given-names>L.</given-names></name><name><surname>Serrano-Pedraza</surname><given-names>I.</given-names></name><name><surname>Rodrigues</surname><given-names>S.</given-names></name><name><surname>Rom&#x000e1;n</surname><given-names>F. J.</given-names></name></person-group> (<year>2010</year>). <article-title>Effects of spatial frequency content on classification of face gender and expression</article-title>. <source>Span. J. Psychol.</source>
<volume>13</volume>, <fpage>525</fpage>&#x02013;<lpage>537</lpage><pub-id pub-id-type="doi">10.1017/S1138741600002225</pub-id><pub-id pub-id-type="pmid">20977005</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M.</given-names></name><name><surname>Kassam</surname><given-names>K. S.</given-names></name><name><surname>Ghuman</surname><given-names>A. S.</given-names></name><name><surname>Boshyan</surname><given-names>J.</given-names></name><name><surname>Schmid</surname><given-names>A. M.</given-names></name><name><surname>Dale</surname><given-names>A. M.</given-names></name><etal/></person-group> (<year>2006</year>). <article-title>Top-down facilitation of visual recognition</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>103</volume>, <fpage>449</fpage>&#x02013;<lpage>454</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507062103</pub-id><pub-id pub-id-type="pmid">16407167</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clifford</surname><given-names>C. W. G.</given-names></name><name><surname>Webster</surname><given-names>M. A.</given-names></name><name><surname>Stanley</surname><given-names>G. B.</given-names></name><name><surname>Stocker</surname><given-names>A. A.</given-names></name><name><surname>Kohn</surname><given-names>A.</given-names></name><name><surname>Sharpee</surname><given-names>T. O.</given-names></name><etal/></person-group> (<year>2007</year>). <article-title>Visual adaptation: neural, psychological and computational aspects</article-title>. <source>Vis. Res.</source>
<volume>47</volume>, <fpage>3125</fpage>&#x02013;<lpage>3131</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.08.023</pub-id><pub-id pub-id-type="pmid">17936871</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costen</surname><given-names>N. P.</given-names></name><name><surname>Parker</surname><given-names>D. M.</given-names></name><name><surname>Craw</surname><given-names>I.</given-names></name></person-group> (<year>1996</year>). <article-title>Effects of high-pass and low-pass spatial filtering on face identification</article-title>. <source>Percept. Psychophys.</source>
<volume>58</volume>, <fpage>602</fpage>&#x02013;<lpage>612</lpage><pub-id pub-id-type="doi">10.3758/BF03213093</pub-id><pub-id pub-id-type="pmid">8934690</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>E. T.</given-names></name><name><surname>Kramer</surname><given-names>P.</given-names></name><name><surname>Graham</surname><given-names>N.</given-names></name></person-group> (<year>1983</year>). <article-title>Uncertainty about spatial frequency, spatial position, or contrast of visual patterns</article-title>. <source>Percept. Psychophys.</source>
<volume>33</volume>, <fpage>20</fpage>&#x02013;<lpage>28</lpage><pub-id pub-id-type="doi">10.3758/BF03205862</pub-id><pub-id pub-id-type="pmid">6844089</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deruelle</surname><given-names>C.</given-names></name><name><surname>Fagot</surname><given-names>J.</given-names></name></person-group> (<year>2004</year>). <article-title>Categorizing facial identities, emotions, and genders: attention to high- and low-spatial frequencies by children and adults</article-title>. <source>J. Exp. Child Psychol.</source>
<volume>90</volume>, <fpage>172</fpage>&#x02013;<lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.jecp.2004.09.001</pub-id><pub-id pub-id-type="pmid">15683861</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eger</surname><given-names>E.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Kleinschmidt</surname><given-names>A.</given-names></name></person-group> (<year>2004</year>). <article-title>Scale invariant adaptation in fusiform face-responsive regions</article-title>. <source>Neuroimage</source>
<volume>22</volume>, <fpage>232</fpage>&#x02013;<lpage>242</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.12.028</pub-id><pub-id pub-id-type="pmid">15110013</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flevaris</surname><given-names>A. V.</given-names></name><name><surname>Robertson</surname><given-names>L. C.</given-names></name><name><surname>Bentin</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>Using spatial frequency scales for processing facial features and face configuration: an ERP analysis</article-title>. <source>Brain Res.</source>
<volume>1194</volume>, <fpage>100</fpage>&#x02013;<lpage>109</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2007.11.071</pub-id><pub-id pub-id-type="pmid">18190897</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>C. J.</given-names></name><name><surname>Barton</surname><given-names>J. J. S.</given-names></name></person-group> (<year>2007</year>). <article-title>What is adapted in face adaptation? The neural representations of expression in the human visual system</article-title>. <source>Brain Res.</source>
<volume>1127</volume>, <fpage>80</fpage>&#x02013;<lpage>89</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.09.104</pub-id><pub-id pub-id-type="pmid">17109830</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furl</surname><given-names>N.</given-names></name><name><surname>van Rijsbergen</surname><given-names>N. J.</given-names></name><name><surname>Treves</surname><given-names>A.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>2007</year>). <article-title>Face adaptation aftereffects reveal anterior medial temporal cortex role in high level category representation</article-title>. <source>Neuroimage</source>
<volume>37</volume>, <fpage>300</fpage>&#x02013;<lpage>310</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.057</pub-id><pub-id pub-id-type="pmid">17561416</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Z.</given-names></name><name><surname>Bentin</surname><given-names>S.</given-names></name></person-group> (<year>2011</year>). <article-title>Coarse-to-fine encoding of spatial frequency information into visual short-term memory for faces but impartial decay</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>37</volume>, <fpage>1051</fpage>&#x02013;<lpage>1064</lpage><pub-id pub-id-type="doi">10.1037/a0023091</pub-id><pub-id pub-id-type="pmid">21500938</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I.</given-names></name><name><surname>Curry</surname><given-names>K. M.</given-names></name><name><surname>Skudlarski</surname><given-names>P.</given-names></name><name><surname>Epstein</surname><given-names>R. A.</given-names></name></person-group> (<year>2005</year>). <article-title>Individual differences in FFA activity suggest independent processing at different spatial scales</article-title>. <source>Cogn. Affect. Behav. Neurosci.</source>
<volume>5</volume>, <fpage>222</fpage>&#x02013;<lpage>234</lpage><pub-id pub-id-type="doi">10.3758/CABN.5.2.222</pub-id><pub-id pub-id-type="pmid">16180628</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goeleven</surname><given-names>E.</given-names></name><name><surname>De Raedt</surname><given-names>R.</given-names></name><name><surname>Leyman</surname><given-names>L.</given-names></name><name><surname>Verschuere</surname><given-names>B.</given-names></name></person-group> (<year>2008</year>). <article-title>The Karolinska directed emotional faces: a validation study</article-title>. <source>Cogn. Emot.</source>
<volume>22</volume>, <fpage>1094</fpage>&#x02013;<lpage>1118</lpage><pub-id pub-id-type="doi">10.1080/02699930701626582</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goffaux</surname><given-names>V.</given-names></name><name><surname>Hault</surname><given-names>B.</given-names></name><name><surname>Michel</surname><given-names>C.</given-names></name><name><surname>Vuong</surname><given-names>Q. C.</given-names></name><name><surname>Rossion</surname><given-names>B.</given-names></name></person-group> (<year>2005</year>). <article-title>The respective role of low and high spatial frequencies in supporting configural and featural processing of faces</article-title>. <source>Perception</source>
<volume>34</volume>, <fpage>77</fpage>&#x02013;<lpage>86</lpage><pub-id pub-id-type="doi">10.1068/p5370</pub-id><pub-id pub-id-type="pmid">15773608</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goffaux</surname><given-names>V.</given-names></name><name><surname>Jemel</surname><given-names>B.</given-names></name><name><surname>Jacques</surname><given-names>C.</given-names></name><name><surname>Rossion</surname><given-names>B.</given-names></name><name><surname>Schyns</surname><given-names>P.</given-names></name></person-group> (<year>2003</year>). <article-title>ERP evidence for task modulations on face perceptual processing at different spatial scales</article-title>. <source>Cogn. Sci.</source>
<volume>27</volume>, <fpage>313</fpage>&#x02013;<lpage>325</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog2702_8</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halit</surname><given-names>H.</given-names></name><name><surname>de Haan</surname><given-names>M.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Johnson</surname><given-names>M. H.</given-names></name></person-group> (<year>2006</year>). <article-title>Is high-spatial-frequency information used in the early stages of face detection?</article-title>
<source>Brain Res.</source>
<volume>1117</volume>, <fpage>154</fpage>&#x02013;<lpage>161</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.07.059</pub-id><pub-id pub-id-type="pmid">16999942</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>B. C.</given-names></name><name><surname>Farivar</surname><given-names>R.</given-names></name><name><surname>Thompson</surname><given-names>B.</given-names></name><name><surname>Hess</surname><given-names>R. F.</given-names></name></person-group> (<year>2008</year>). <article-title>A critical band of phase alignment for discrimination but not recognition of human faces</article-title>. <source>Vision Res.</source>
<volume>48</volume>, <fpage>2523</fpage>&#x02013;<lpage>2536</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.07.009</pub-id><pub-id pub-id-type="pmid">18801383</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>S. M.</given-names></name><name><surname>Young</surname><given-names>A. W.</given-names></name></person-group> (<year>2004</year>). <article-title>Adaptation effects in facial expression recognition</article-title>. <source>Vis. Cogn.</source>
<volume>11</volume>, <fpage>871</fpage>&#x02013;<lpage>899</lpage><pub-id pub-id-type="doi">10.1080/13506280444000030</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000fc;bner</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>). <article-title>Specific effects of spatial-frequency uncertainty and different cue types on contrast detection: data and models</article-title>. <source>Vision Res.</source>
<volume>36</volume>, <fpage>3429</fpage>&#x02013;<lpage>3439</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(96)00088-0</pub-id><pub-id pub-id-type="pmid">8977010</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juricevic</surname><given-names>I.</given-names></name><name><surname>Webster</surname><given-names>M. A.</given-names></name></person-group> (<year>2012</year>). <article-title>Selectivity of face aftereffects for expressions and anti-expressions</article-title>. <source>Front. Psychol.</source>
<volume>3</volume>:<fpage>4</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00004</pub-id><pub-id pub-id-type="pmid">22291677</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kontsevich</surname><given-names>L. L.</given-names></name><name><surname>Tyler</surname><given-names>C. W.</given-names></name></person-group> (<year>1999</year>). <article-title>Bayesian adaptive estimation of psychometric slope and threshold</article-title>. <source>Vision Res.</source>
<volume>39</volume>, <fpage>2729</fpage>&#x02013;<lpage>2737</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(98)00286-7</pub-id><pub-id pub-id-type="pmid">10492833</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kov&#x000e1;cs</surname><given-names>G.</given-names></name><name><surname>Zimmer</surname><given-names>M.</given-names></name><name><surname>Harza</surname><given-names>I.</given-names></name><name><surname>Vidny&#x000e1;nszky</surname><given-names>Z.</given-names></name></person-group> (<year>2007</year>). <article-title>Adaptation duration affects the spatial selectivity of facial aftereffects</article-title>. <source>Vision Res.</source>
<volume>47</volume>, <fpage>3141</fpage>&#x02013;<lpage>3149</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.08.019</pub-id><pub-id pub-id-type="pmid">17935749</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>D.</given-names></name><name><surname>Srinivasan</surname><given-names>N.</given-names></name></person-group> (<year>2011</year>). <article-title>Emotion perception is mediated by spatial frequency content</article-title>. <source>Emotion</source>
<volume>11</volume>, <fpage>1144</fpage>&#x02013;<lpage>1151</lpage><pub-id pub-id-type="doi">10.1037/a0025453</pub-id><pub-id pub-id-type="pmid">21942699</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>D. A.</given-names></name><name><surname>O&#x02019;Toole</surname><given-names>A. J.</given-names></name><name><surname>Vetter</surname><given-names>T.</given-names></name><name><surname>Blanz</surname><given-names>V.</given-names></name></person-group> (<year>2001</year>). <article-title>Prototype-referenced shape encoding revealed by high-level aftereffects</article-title>. <source>Nat. Neurosci.</source>
<volume>4</volume>, <fpage>89</fpage>&#x02013;<lpage>94</lpage><pub-id pub-id-type="doi">10.1038/82947</pub-id><pub-id pub-id-type="pmid">11135650</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>D. A.</given-names></name><name><surname>Rhodes</surname><given-names>G.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K.-M.</given-names></name><name><surname>Jeffery</surname><given-names>L.</given-names></name></person-group> (<year>2005</year>). <article-title>The dynamics of visual adaptation to faces</article-title>. <source>Proc. R. Soc. Lond. B Biol. Sci.</source>
<volume>272</volume>, <fpage>897</fpage>&#x02013;<lpage>904</lpage><pub-id pub-id-type="doi">10.1098/rspb.2004.3022</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lepp&#x000e4;nen</surname><given-names>J. M.</given-names></name><name><surname>Nelson</surname><given-names>C. A.</given-names></name></person-group> (<year>2009</year>). <article-title>Tuning the developing brain to social signals of emotions</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>10</volume>, <fpage>37</fpage>&#x02013;<lpage>47</lpage><pub-id pub-id-type="doi">10.1038/nrn2554</pub-id><pub-id pub-id-type="pmid">19050711</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z. L.</given-names></name><name><surname>Dosher</surname><given-names>B. A.</given-names></name></person-group> (<year>2004</year>). <article-title>Spatial attention excludes external noise without changing the spatial frequency tuning of the perceptual template</article-title>. <source>Vision Res.</source>
<volume>4</volume>, <fpage>955</fpage>&#x02013;<lpage>966</lpage></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lundqvist</surname><given-names>D.</given-names></name><name><surname>Flykt</surname><given-names>A.</given-names></name><name><surname>&#x000d6;hman</surname><given-names>A.</given-names></name></person-group> (<year>1998</year>). <source>The Karolinska Directed Emotional Faces (KDEF) [CD-ROM]</source>. <publisher-loc>Stockholm</publisher-loc>: <publisher-name>Psychology Section, Department of Clinical Neuroscience, Karolinska Institute</publisher-name></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>N&#x000e4;s&#x000e4;nen</surname><given-names>R.</given-names></name></person-group> (<year>1999</year>). <article-title>Spatial frequency bandwidth used in the recognition of facial images</article-title>. <source>Vision Res.</source>
<volume>39</volume>, <fpage>3824</fpage>&#x02013;<lpage>3833</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(99)00096-6</pub-id><pub-id pub-id-type="pmid">10748918</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>D. G.</given-names></name></person-group> (<year>1997</year>). <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spat. Vis.</source>
<volume>10</volume>, <fpage>437</fpage>&#x02013;<lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robbins</surname><given-names>R.</given-names></name><name><surname>McKone</surname><given-names>E.</given-names></name><name><surname>Edwards</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>Aftereffects for face attributes with different natural variability: adapter position effects and neural models</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>33</volume>, <fpage>570</fpage>&#x02013;<lpage>592</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.33.3.570</pub-id><pub-id pub-id-type="pmid">17563222</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rotshtein</surname><given-names>P.</given-names></name><name><surname>Vuilleumier</surname><given-names>P.</given-names></name><name><surname>Winston</surname><given-names>J.</given-names></name><name><surname>Driver</surname><given-names>J.</given-names></name><name><surname>Dolan</surname><given-names>R.</given-names></name></person-group> (<year>2007</year>). <article-title>Distinct and convergent visual processing of high and low spatial frequency information in faces</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>2713</fpage>&#x02013;<lpage>2724</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl180</pub-id><pub-id pub-id-type="pmid">17283203</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruiz-Soler</surname><given-names>M.</given-names></name><name><surname>Beltran</surname><given-names>F. S.</given-names></name></person-group> (<year>2006</year>). <article-title>Face perception: an integrative review of the role of spatial frequencies</article-title>. <source>Psychol. Res.</source>
<volume>70</volume>, <fpage>273</fpage>&#x02013;<lpage>292</lpage><pub-id pub-id-type="doi">10.1007/s00426-005-0215-z</pub-id><pub-id pub-id-type="pmid">16075260</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutherford</surname><given-names>M. D.</given-names></name><name><surname>Chattha</surname><given-names>H. M.</given-names></name><name><surname>Krysko</surname><given-names>K. M.</given-names></name></person-group> (<year>2008</year>). <article-title>The use of aftereffects in the study of relationships among emotion categories</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>34</volume>, <fpage>27</fpage>&#x02013;<lpage>40</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.34.1.27</pub-id><pub-id pub-id-type="pmid">18248138</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name></person-group> (<year>1999</year>). <article-title>Dr. Angry and Mr. smile: when categorization flexibly modifies the perception of faces in rapid visual presentations</article-title>. <source>Cognition</source>
<volume>69</volume>, <fpage>243</fpage>&#x02013;<lpage>265</lpage><pub-id pub-id-type="doi">10.1016/S0010-0277(98)00069-9</pub-id><pub-id pub-id-type="pmid">10193048</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skinner</surname><given-names>A. L.</given-names></name><name><surname>Benton</surname><given-names>C. P.</given-names></name></person-group> (<year>2010</year>). <article-title>Anti-expression aftereffects reveal prototype-referenced coding of facial expressions</article-title>. <source>Psychol. Sci.</source>
<volume>21</volume>, <fpage>1248</fpage>&#x02013;<lpage>1253</lpage><pub-id pub-id-type="doi">10.1177/0956797610380702</pub-id><pub-id pub-id-type="pmid">20713632</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>F. W.</given-names></name><name><surname>Schyns</surname><given-names>P.</given-names></name></person-group> (<year>2009</year>). <article-title>Smile through your fear and sadness: transmitting and identifying facial expression signals over a range of viewing distances</article-title>. <source>Psychol. Sci.</source>
<volume>20</volume>, <fpage>1202</fpage>&#x02013;<lpage>1208</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02427.x</pub-id><pub-id pub-id-type="pmid">19694983</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>M. L.</given-names></name><name><surname>Cottrell</surname><given-names>G. W.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name><name><surname>Schyns</surname><given-names>P.</given-names></name></person-group> (<year>2005</year>). <article-title>Transmitting and decoding facial expressions</article-title>. <source>Psychol. Sci.</source>
<volume>16</volume>, <fpage>184</fpage>&#x02013;<lpage>189</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.00773.x</pub-id><pub-id pub-id-type="pmid">15733197</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname><given-names>J. A.</given-names></name><name><surname>Pelli</surname><given-names>D. G.</given-names></name></person-group> (<year>1994</year>). <article-title>The visual filter mediating letter identification</article-title>. <source>Nature</source>
<volume>369</volume>, <fpage>395</fpage>&#x02013;<lpage>397</lpage><pub-id pub-id-type="doi">10.1038/369395a0</pub-id><pub-id pub-id-type="pmid">8196766</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sowden</surname><given-names>P. T.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2006</year>). <article-title>Channel surfing in the visual brain</article-title>. <source>Trends Cogn. Sci.</source>
<volume>10</volume>, <fpage>538</fpage>&#x02013;<lpage>545</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.10.007</pub-id><pub-id pub-id-type="pmid">17071128</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talgar</surname><given-names>C. P.</given-names></name><name><surname>Pelli</surname><given-names>D. G.</given-names></name><name><surname>Carrasco</surname><given-names>M.</given-names></name></person-group> (<year>2004</year>). <article-title>Covert attention enhances letter identification without affecting channel tuning</article-title>. <source>J. Vis.</source>
<volume>4</volume>, <fpage>22</fpage>&#x02013;<lpage>31</lpage><pub-id pub-id-type="doi">10.1167/4.8.22</pub-id><pub-id pub-id-type="pmid">14995896</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tiddeman</surname><given-names>B.</given-names></name><name><surname>Burt</surname><given-names>D. M.</given-names></name><name><surname>Perrett</surname><given-names>D.</given-names></name></person-group> (<year>2001</year>). <article-title>Prototyping and transforming facial texture for perception research</article-title>. <source>IEEE Comput. Graph.</source>
<volume>21</volume>, <fpage>42</fpage>&#x02013;<lpage>50</lpage><pub-id pub-id-type="doi">10.1109/38.933523</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuilleumier</surname><given-names>P.</given-names></name><name><surname>Armony</surname><given-names>J. L.</given-names></name><name><surname>Driver</surname><given-names>J.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>2003</year>). <article-title>Distinct spatial frequency sensitivities for processing faces and emotional expressions</article-title>. <source>Nat. Neurosci.</source>
<volume>6</volume>, <fpage>624</fpage>&#x02013;<lpage>631</lpage><pub-id pub-id-type="doi">10.1038/nn1057</pub-id><pub-id pub-id-type="pmid">12740580</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>T. L.</given-names></name><name><surname>Clifford</surname><given-names>C. W. G.</given-names></name></person-group> (<year>2003</year>). <article-title>Pulling faces: an investigation of the face-distortion aftereffect</article-title>. <source>Perception</source>
<volume>32</volume>, <fpage>1109</fpage>&#x02013;<lpage>1116</lpage><pub-id pub-id-type="doi">10.1068/p5082</pub-id><pub-id pub-id-type="pmid">14651323</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>M. A.</given-names></name><name><surname>Kaping</surname><given-names>D.</given-names></name><name><surname>Mizokami</surname><given-names>Y.</given-names></name><name><surname>Duhamel</surname><given-names>P.</given-names></name></person-group> (<year>2004</year>). <article-title>Adaptation to natural facial categories</article-title>. <source>Nature</source>
<volume>428</volume>, <fpage>557</fpage>&#x02013;<lpage>561</lpage><pub-id pub-id-type="doi">10.1038/nature02420</pub-id><pub-id pub-id-type="pmid">15058304</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>M. A.</given-names></name><name><surname>MacLeod</surname><given-names>D. I. A.</given-names></name></person-group> (<year>2011</year>). <article-title>Visual adaptation and face perception</article-title>. <source>Philos. Trans. R. Soc. B</source>
<volume>366</volume>, <fpage>1702</fpage>&#x02013;<lpage>1725</lpage><pub-id pub-id-type="doi">10.1098/rstb.2010.0360</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamashita</surname><given-names>J. A.</given-names></name><name><surname>Hardy</surname><given-names>J. L.</given-names></name><name><surname>De Valois</surname><given-names>K. K.</given-names></name><name><surname>Webster</surname><given-names>M. A.</given-names></name></person-group> (<year>2005</year>). <article-title>Stimulus selectivity of figural aftereffects for faces</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>31</volume>, <fpage>420</fpage>&#x02013;<lpage>437</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.31.3.420</pub-id><pub-id pub-id-type="pmid">15982123</pub-id></mixed-citation></ref></ref-list></back></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Dement Neuropsychol</journal-id><journal-id journal-id-type="iso-abbrev">Dement Neuropsychol</journal-id><journal-id journal-id-type="publisher-id">dn</journal-id><journal-title-group><journal-title>Dementia &#x00026; Neuropsychologia</journal-title></journal-title-group><issn pub-type="ppub">1980-5764</issn><publisher><publisher-name>Associa&#x000e7;&#x000e3;o de Neurologia Cognitiva e do
Comportamento</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">29213914</article-id><article-id pub-id-type="pmc">5619405</article-id><article-id pub-id-type="doi">10.1590/S1980-57642014DN83000012</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Articles</subject></subj-group></article-categories><title-group><article-title>Discourse intervention strategies in Alzheimer's disease:
Eye-tracking and the effect of visual cues in conversation</article-title><trans-title-group xml:lang="pt"><trans-title>INTERVEN&#x000c7;&#x000c3;O DISCURSIVA NA DOEN&#x000c7;A DE ALZHEIMER:
RASTREIO OCULAR E EFEITO DE PISTAS VISUAIS NA
CONVERSA&#x000c7;&#x000c3;O</trans-title></trans-title-group></title-group><contrib-group><contrib contrib-type="author"><name><surname>Brand&#x000e3;o</surname><given-names>Lenisa</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="c1"/></contrib><contrib contrib-type="author"><name><surname>Mon&#x000e7;&#x000e3;o</surname><given-names>Ana Maria</given-names></name><xref ref-type="aff" rid="aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Andersson</surname><given-names>Richard</given-names></name><xref ref-type="aff" rid="aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Holmqvist</surname><given-names>Kenneth</given-names></name><xref ref-type="aff" rid="aff4">4</xref></contrib></contrib-group><aff id="aff1"><label>1</label>Professor in Speech-Language Pathology, Departamento de
Sa&#x000fa;de e Comunica&#x000e7;&#x000e3;o Humana, Instituto de Psicologia,
Universidade Federal do Rio Grande do Sul, RS, Brazil.</aff><aff id="aff2"><label>2</label>Professor in Linguistics, Centro de Lingu&#x000ed;stica,
Universidade Nova de Lisboa, Lisbon, Portugal.</aff><aff id="aff3"><label>3</label>Doctor in Cognitive Science, Humanities Lab, Lund
University, Lund, Sweden.</aff><aff id="aff4"><label>4</label>Professor in Psychology, Humanities Lab, Lund University,
Lund, Sweden.</aff><author-notes><corresp id="c1"><bold>Lenisa Brand&#x000e3;o</bold>. Rua Gon&#x000e7;alves Dias, 810
/ ap 102 &#x02013; 90130-061 Porto Alegre RS &#x02013; Brazil. E-mail:
<email>lenisabrandao@hotmail.com</email></corresp></author-notes><pub-date pub-type="epub-ppub"><season>Jul-Sep</season><year>2014</year></pub-date><!--Fake ppub date generated by PMC from publisher
							pub-date/@pub-type='epub-ppub' --><pub-date pub-type="ppub"><season>Jul-Sep</season><year>2014</year></pub-date><volume>8</volume><issue>3</issue><fpage>278</fpage><lpage>284</lpage><history><date date-type="received"><day>09</day><month>5</month><year>2014</year></date><date date-type="accepted"><day>10</day><month>7</month><year>2014</year></date></history><permissions><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an Open Access article distributed under the terms of the
Creative Commons Attribution License, which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is
properly cited.</license-p></license></permissions><abstract><sec><title>Objective</title><p>The goal of this study was to investigate whether on-topic visual cues can
serve as aids for the maintenance of discourse coherence and informativeness
in autobiographical narratives of persons with Alzheimer's disease (AD).</p></sec><sec><title>Methods</title><p>The experiment consisted of three randomized conversation conditions: one
without prompts, showing a blank computer screen; an on-topic condition,
showing a picture and a sentence about the conversation; and an off-topic
condition, showing a picture and a sentence which were unrelated to the
conversation. Speech was recorded while visual attention was examined using
eye tracking to measure how long participants looked at cues and the face of
the listener.</p></sec><sec><title>Results</title><p>Results suggest that interventions using visual cues in the form of images
and written information are useful to improve discourse informativeness in
AD.</p></sec><sec><title>Conclusion</title><p>This study demonstrated the potential of using images and short written
messages as means of compensating for the cognitive deficits which underlie
uninformative discourse in AD. Future studies should further investigate the
efficacy of language interventions based in the use of these compensation
strategies for AD patients and their family members and friends.</p></sec></abstract><trans-abstract xml:lang="pt"><sec><title>Objetivo</title><p>O objetivo deste estudo foi o de investigar se pistas visuais sobre o
t&#x000f3;pico discursivo podem servir como auxiliares para a
manuten&#x000e7;&#x000e3;o da coer&#x000ea;ncia e da informatividade em
narrativas autobiogr&#x000e1;ficas de pessoas com doen&#x000e7;a de Alzheimer
(DA).</p></sec><sec><title>M&#x000e9;todos</title><p>O experimento apresentou tr&#x000ea;s condi&#x000e7;&#x000f5;es: uma conversa
tendo &#x000e0; vista uma tela de computador em branco; uma
condi&#x000e7;&#x000e3;o de conversa&#x000e7;&#x000e3;o com pista dentro do
t&#x000f3;pico, em que a tela do computador mostrava uma foto e uma frase
sobre o evento tema; e uma condi&#x000e7;&#x000e3;o fora de t&#x000f3;pico, em
que a tela mostrava uma foto e uma frase que n&#x000e3;o apresentavam
rela&#x000e7;&#x000e3;o com o tema da conversa. O discurso foi gravado
enquanto a aten&#x000e7;&#x000e3;o visual foi examinada atrav&#x000e9;s de
rastreamento ocular para medir a aten&#x000e7;&#x000e3;o dos participantes
para as pistas apresentadas na tela e para o rosto do ouvinte.</p></sec><sec><title>Resultados</title><p>Os resultados sugerem que as interven&#x000e7;&#x000f5;es por meio de pistas
visuais na forma de imagens e informa&#x000e7;&#x000f5;es escritas s&#x000e3;o
&#x000fa;teis para melhorar a informatividade discursiva na AD.</p></sec><sec><title>Conclus&#x000e3;o</title><p>Este estudo demonstrou o potencial do uso de imagens e mensagens escritas
curtas como meios de compensa&#x000e7;&#x000e3;o para aumentar a
informatividade do discurso em pacientes com DA. Estudos futuros devem
investigar a efic&#x000e1;cia de interven&#x000e7;&#x000f5;es de linguagem
baseadas no uso dessas estrat&#x000e9;gias de compensa&#x000e7;&#x000e3;o para
a comunica&#x000e7;&#x000e3;o de pacientes com DA com seus familiares e
amigos.</p></sec></trans-abstract><kwd-group><kwd>Alzheimer's disease</kwd><kwd>discourse</kwd><kwd>attention</kwd><kwd>visual cues</kwd></kwd-group></article-meta></front><body><sec sec-type="intro"><title>INTRODUCTION</title><p>The use of external aids to improve the conversational skills of patients with
Alzheimer's disease (AD) were first reported in the 1990s.<sup><xref rid="r2" ref-type="bibr">2</xref>-<xref rid="r3" ref-type="bibr">3</xref></sup> Studies on
memory wallets seem to have been discontinued, but research on the use of recipe
books with images and short sentences has suggested that visual information
facilitates access to memory.<sup><xref rid="r4" ref-type="bibr">4</xref></sup>
These authors emphasized the need for future studies to investigate the effect of
relevant visual stimuli in the discourse production of AD patients.</p><p>Eye tracking is widely used to investigate visual perception and attention during
different cognitive and linguistic tasks.<sup><xref rid="r5" ref-type="bibr">5</xref></sup> However, eye-tracking studies with AD participants have not
focused specifically on the communication skills of this population. Instead, there
has been an emphasis on demonstrating impairment in a range of cognitive abilities
related to visual attention.<sup><xref rid="r6" ref-type="bibr">6</xref></sup>
However, there is evidence that persons with AD are able to successfully focus their
attention on targets flanked by distractors on a screen.<sup><xref rid="r7" ref-type="bibr">7</xref></sup> Visual orientation and sustained attention to
emotionally-arousing scenes are also preserved in AD.<sup><xref rid="r8" ref-type="bibr">8</xref></sup></p><p>Nevertheless, there is evidence that, as AD progresses, a loss of interest in the
listener can be observed.<sup><xref rid="r9" ref-type="bibr">9</xref></sup>
Monitoring gaze direction provides information about the listener's engagement in or
disengagement from tasks that require shared attention.<sup><xref rid="r10" ref-type="bibr">10</xref></sup> Attention to faces seems to be reduced in this
population. A study about visual attention to faces demonstrated that AD patients
dwelled on faces in pictures for shorter periods of time than the control
group.<sup><xref rid="r11" ref-type="bibr">11</xref></sup> Patients in the
severe stage of AD may not lose their ability to care about the nonverbal reaction
of their communication partners,<sup><xref rid="r12" ref-type="bibr">12</xref></sup>
but whether attention to faces is well preserved in communication contexts has yet
to be investigated.</p><p>The goal of the present study was to investigate the discourse and visual attention
of AD participants and healthy older adults to on- and off-topic cues. The
hypothesis was that participants with AD produce a more coherent and informative
discourse while fixating on-topic visual cues.</p></sec><sec sec-type="methods"><title>METHOD</title><p><bold>Participants.</bold> Participants comprised:</p><p><list list-type="simple"><list-item><p>(a) a control group of 10 older adults without dementia, who attended the
Universidade Senior de Oeiras; and</p></list-item><list-item><p>(b) five older adults with moderate-stage AD, who were outpatients of the
Cl&#x000ed;nica S&#x000e3;o Jos&#x000e9; located in Lisbon. The selection
of participants with AD was performed by two neurologists of the
Cl&#x000ed;nica S&#x000e3;o Jos&#x000e9; (Lisbon, Portugal), who
established the diagnosis of probable Alzheimer's dementia.<sup><xref rid="r13" ref-type="bibr">13</xref></sup></p></list-item></list></p><p>The control group comprised predominantly women (70%); and, in the AD group, three
out of five participants were women. The mean age of the control group was 78.31
years (6.65). In the AD group, the mean age was 80.92 (5.51) (U=89.00; p=0.35). In
terms of educational level, there were no significant differences between groups
(control: 6.12 (1.58); AD: 6.00 (1.66); U=108.00; p=0.88). The exclusion criteria of
dementia for controls were: score ranging from 27 to 30 on the Mini-Mental State
Examination (MMSE) (14; 15); autonomy in daily life; and absence of cognitive
complaints, neurological, and psychiatric disorders based on data provided in an
interview about health-related and sociodemographic data. The mean score of controls
on the MMSE was 28.37 (1.02). The mean score of participants with AD on the MMSE
differed significantly from the control sample (M=20.91; SD=4.25; p&#x0003c;0.05). Most
of the participants with AD had moderate decline (MMSE=16). The study was approved
by the Research Ethics Committee of Cl&#x000ed;nica S&#x000e3;o Jos&#x000e9; in
Portugal, where data collection took place. Participants included in the AD group
and their relatives, as well as the control group, all signed a written consent form
explaining the tasks planned for the study.</p><p><bold>Study design.</bold> This was a cross-sectional, quasi-experimental study,
based on mixed analysis using comparison between groups, cases and controls, as well
as comparison between conditions. Three different conditions were used:</p><p><list list-type="simple"><list-item><p>[A] conversations during which on-topic visual cues were displayed on a
screen,</p></list-item><list-item><p>[B] conversations during which off-topic visual cues were displayed on a
screen, and</p></list-item><list-item><p>[C] conversations during which a blank screen was displayed. All
conditions were focused on the participants' personal reports about
their youth years (between 20 and 30 years old). A family member of each
participant previously selected two important events in the
participant's life (one to be used in conditions A and B and the other
for condition C). Family members were sent a confidential letter
containing instructions on how to select pictures and create sentences
to be used in the study.</p></list-item></list></p><p>Youth years were established based on studies demonstrating special focus on
autobiographical memory during this period, which is known as the memory bump for
older adults.<sup><xref rid="r17" ref-type="bibr">17</xref></sup> The pictures and
sentences provided by family members were reviewed and selected to decide which
event would be told during the experiment. The same event was used in conditions A
and B to keep memory differences from interfering with the results of visual
attention and discourse. Because of this, the events were used in a randomized order
to prevent effects of practice from interfering with the results. Two researchers
participated in the conversation experiment to ensure that the participants did not
report the event twice to the same listener, which would increase context
artificiality. A different important life event was used in condition C to avoid
excessive effects of practice. With this in mind, each examiner's participation was
balanced to avoid greater participation by any one of the examiners in a given
condition, which could also interfere with the differences between the conditions.
Discourse data of condition C were used to provide a means of comparison between the
samples in a condition without visual cues.</p><p>Pictures were chosen so that the image cues of conditions A and B were not
significantly different in terms of those visual characteristics that tend to have
an influence on the fixation time of pictures, such as number of people in the
picture, size of the faces, presence of children and animals, and emotional arousal
(pleasantness and facial expression arousal). Characteristics such as color,
brightness, and size of the pictures were controlled using the computer program
<italic>Photoshop</italic> by Microsoft, so that all characteristics were
similar in conditions A and B. The sentences provided by family members were revised
to be simple sentences containing approximately the same number of words and
relevant information in order to be similar to the title of the event. The schedule
of the experiment was set using the computer program<italic>
E-prime.</italic><sup><xref rid="r18" ref-type="bibr">18</xref></sup></p><p><bold>Procedures.</bold> A mobile, head-mounted eye tracker (SMI HED 50Hz incl.
Polhemus head tracking) was used for data collection. Adjustments were made aimed at
placing the camera and the lens in a position to capture appropriate images of the
eye, including the analysis of corneal and pupillary reflexes on a computer used to
record eye-tracking data. Describing the procedures prevented the AD participants
from being afraid of wearing the reflective lens, which was positioned at a safe
distance from the eye. Once an appropriate image was achieved, calibration was
done.</p><p>After calibration, the researcher pressed the key to start the presentation of the
instructions on the screen, while the first examiner entered the room. The examiner
sat facing the participant and explained the instructions that were being displayed
on the screen:</p><p>Next, you'll be asked to tell me about certain life events. Please tell the
communicative partner sitting in front of you about the life event suggested. A
picture and a sentence about the event might be displayed on the screen, or a
picture and a sentence about a different event might be displayed, or the screen
might be blank. Feel free to look at the screen for as long as you want while you
are talking about the event. The next interviewer will not listen to your report.
The second interviewer will substitute the first one and you will be asked to tell
the same story again.</p><p>Each participant was randomly exposed to the three conditions. In all the conditions,
the picture and the sentence were displayed on the screen during the period of time
each participant took to complete the story. Examiners were graduate students
trained to limit their participation to certain speech acts during the
conversation:</p><p><list list-type="simple"><list-item><p>(a) offering signs of attention, interest, and emotional reactions;</p></list-item><list-item><p>(b) when participants interrupted their discourse, examiners provided
verbal clues such as "What else"; and</p></list-item><list-item><p>(c) when participants switched topic, the examiner provided a verbal clue
to lead back to the topic.</p></list-item></list></p><p><bold>Data analysis.</bold> Total fixation time, known as &#x02018;dwell time'<sup><xref rid="r19" ref-type="bibr">19</xref></sup> on regions of interest (picture,
sentence, and interlocutor's face) was analyzed by checking possible differences
between samples and between conditions.</p><p>All discourse samples were transcribed verbatim and segmented into propositions. The
coded variables were analyzed using the computer program <italic>CHAT</italic> of
the <italic>CHILDS</italic> project.<sup><xref rid="r20" ref-type="bibr">20</xref></sup> Discourse was divided into propositions and classified in terms
of global coherence and informativeness according to a method adapted from Laine,
Laakso, Vuorinen and Rinne.<sup><xref rid="r21" ref-type="bibr">21</xref></sup>
Twenty percent of the corpus was randomly selected and coded by two examiners, a
speech therapist and a linguist blind to the conditions and to the group of
participants. The Kappa test was used to evaluate the reliability of the discursive
analysis. Global coherence showed 77% agreement; whereas informativeness had 81%
agreement. According to Linell, Gustavsson and Juvonen,<sup><xref rid="r22" ref-type="bibr">22</xref></sup> a 75% agreement is realistic for a conversation
analysis, considering that interaction is often obscured by ambiguity.
Carletta<sup><xref rid="r23" ref-type="bibr">23</xref></sup> recommends that
Kappa agreement should not be &#x0003c; 0.67 in order to allow reliable conclusions.</p><p><bold>Statistical analysis.</bold> Because of the objective characteristic of the
eye-tracking data, small groups were compared using non parametric tests with the
statistical package SPSS.<sup><xref rid="r24" ref-type="bibr">24</xref></sup> The
Mann-Whitney test was used to compare the groups, whereas the Wilcoxon test was
applied to compare the visual tracking data between each of the conditions (within
each sample). Regarding discursive data, each of the AD cases was compared with the
control group to ensure a more accurate analysis. For this purpose, a special
statistical program (SINGLIMS.EXE) was used for the comparison of cases with a small
control group. According to this modified t-test,<sup><xref rid="r25" ref-type="bibr">25</xref></sup> t values suggested significant differences between
each AD case and the control group. The closer to zero the t values, the greater the
likelihood of proving the null hypothesis that the case is part of the control
group.</p></sec><sec sec-type="results"><title>RESULTS</title><p>AD patients looked longer (ms) at the sentence on the screen than controls, both in
the condition displaying on-topic visual cues (U=54.50; p&#x0003c;0.01) and in the
condition showing off-topic visual cues (U=65.00; p&#x0003c;0.05). With regard to the
total fixation time on the interlocutor's face, we found a statistically significant
difference between the groups in the off-topic condition: the control group looked
longer at the interlocutor's face than AD participants (U=62.50; p&#x0003c;0.05). This
was not true in the on-topic condition (U=71.00; p=0.09). As regards the total
fixation time on the picture, there were no significant differences between the
groups both in the on-topic condition (U=91.00; p=0.4) and in the off-topic
condition (U=109.0; p=0.91). In addition to fixating the listener's face for a
longer period of time in the off-topic condition, the control group also looked
longer at the screen than AD participants in the blank screen condition (U=64.50;
p&#x0003c;0.05).</p><p>The analysis of the differences between fixation time on the screen and fixation time
on the face revealed that the control group demonstrated fixation time on the face
that was not statistically different from fixation time on the screen (Z= -1.86;
p=0.06), although it was longer. Conversely, in the AD group, this difference was
significant because AD participants looked longer at the listener's face than at the
screen (Z= -2.35; p&#x0003c;0.01). Descriptive data are shown in <xref ref-type="table" rid="t1">Table 1</xref>.</p><table-wrap id="t1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Total fixation time on visual input in each condition.</p></caption><table frame="hsides" rules="rows"><colgroup span="1"><col width="25%" span="1"/><col width="25%" span="1"/><col width="25%" span="1"/><col width="25%" span="1"/></colgroup><thead style="background-color:#d2d3d5"><tr><th align="left" rowspan="1" colspan="1">Visual input </th><th align="left" rowspan="1" colspan="1">Condition</th><th align="center" rowspan="1" colspan="1">Control group Mean (SD)</th><th align="center" rowspan="1" colspan="1">AD group Mean (SD)</th></tr></thead><tbody><tr><td rowspan="2" align="left" valign="top" colspan="1">Picture</td><td align="left" valign="top" rowspan="1" colspan="1">On-topic</td><td align="center" rowspan="1" colspan="1">51955.70 (46684.06)</td><td align="center" rowspan="1" colspan="1">28525.63 (18830.30)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Off-topic</td><td align="center" rowspan="1" colspan="1">25145.24 (37429.97)</td><td align="center" rowspan="1" colspan="1">18020.18 (17869.98)</td></tr><tr><td rowspan="2" align="left" valign="top" colspan="1">Sentence</td><td align="left" valign="top" rowspan="1" colspan="1">On-topic</td><td align="center" rowspan="1" colspan="1">929.50 (1628.17)</td><td align="center" rowspan="1" colspan="1">4211.30 (5987.18)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Off-topic</td><td align="center" rowspan="1" colspan="1">931.99 (1675.70)<xref ref-type="table-fn" rid="TFN1">*</xref></td><td align="center" rowspan="1" colspan="1">4108.49 (5718.17)<xref ref-type="table-fn" rid="TFN1">*</xref></td></tr><tr><td rowspan="3" align="left" valign="top" colspan="1">Interlocutor&#x02019;s face</td><td align="left" valign="top" rowspan="1" colspan="1">On-topic</td><td align="center" rowspan="1" colspan="1">34476.99 (37341.64)</td><td align="center" rowspan="1" colspan="1">29880.66 (63644.09)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Off-topic</td><td align="center" rowspan="1" colspan="1">49384.57 (45755.71)<xref ref-type="table-fn" rid="TFN1">*</xref></td><td align="center" rowspan="1" colspan="1">21510.70 (32514.94)<xref ref-type="table-fn" rid="TFN1">*</xref></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Blank screen</td><td align="center" rowspan="1" colspan="1">60248.32 (74996.44)</td><td align="center" rowspan="1" colspan="1">25245.84 (24115.81)</td></tr><tr><td rowspan="3" align="left" valign="top" colspan="1">Screen</td><td align="left" valign="top" rowspan="1" colspan="1">On-topic</td><td align="center" rowspan="1" colspan="1">52885.20 (47352.45)</td><td align="center" rowspan="1" colspan="1">32736.93 (21938.80)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Off-topic</td><td align="center" rowspan="1" colspan="1">26069.16 (38781.59)</td><td align="center" rowspan="1" colspan="1">21485.82 (23065.10)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Blank screen</td><td align="center" rowspan="1" colspan="1">22566.28 (37513.37)<xref ref-type="table-fn" rid="TFN1">*</xref></td><td align="center" rowspan="1" colspan="1">2925.24 (3587.05)<xref ref-type="table-fn" rid="TFN1">*</xref></td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><label>*</label><p>Represents significant difference between the groups, p&#x0003c;0.05.</p></fn></table-wrap-foot></table-wrap><p>The control group looked for a significantly longer time at the screen in the
on-topic condition than in the off-topic condition (Z= -2.53; p&#x0003c;0.01). Although
the AD group showed a tendency to look longer at the screen in the on-topic
condition than in the off-topic condition, the difference between the conditions was
not significant (Z= -1.85; p=0.06). When comparing the on-topic and off-topic
conditions with the blank screen condition, the control group was found to have
looked longer at the screen in the on-topic condition than in the blank screen
condition (Z= -2.66; p&#x0003c;0.01). This was not true for the control group when the
blank screen condition was compared with the off-topic condition (Z= -0.90; p=0.36).
Conversely, the AD group looked longer at the screen in both conditions including
cues, with significant differences when comparing the blank screen condition both
with the on-topic condition (Z= -3.29; p&#x0003c;0.01) and with the off-topic condition
(Z= -2.66; p&#x0003c;0.01). Therefore, the AD group paid much more attention to the
screen in the conditions displaying visual cues than in the blank screen condition,
showing particular interest in the cues on the screen, regardless of their relevance
to the context of the discourse.</p><p>When comparing the total fixation time on the interlocutor's face, the control group
showed a tendency to look longer at the interlocutor's face in the blank screen
condition than in the conditions displaying on-topic and off-topic cues. However,
this difference was not significant (&#x003c7;<sup>2</sup>=5.2; p=0.07). AD
participants did not show this tendency, having similar fixation times on the face
under all three conditions (&#x003c7;<sup>2</sup>=1.28; p=0.52).</p><p>In the control group, the picture was the cue at which the participants looked
longer, having looked much longer at the on-topic picture than at the off-topic
picture (Z= -2.63; p&#x0003c;0.01). The picture was also the cue at which the AD group
looked longer; however, in this group, there were no significant differences between
the fixation time on the picture for on-topic and off-topic conditions (Z= -1.6;
p=0.1). With regard to the total fixation time for the sentence, there were no
differences between the conditions both in the control group (Z= -0.03; p=0.97) and
in the AD group (-0.59; p=0.55). The fixation time for the listener's face also
showed no significant difference between the conditions in the control group (Z=
-1.59; p=0.11) and in the AD group (Z= -0.91; p=0.36).</p><p>Two AD participants had significantly lower global coherence scores than controls in
the on-topic condition. The discourse of the other three AD participants did not
differ from that of controls in terms of global coherence under the same condition.
Conversely, in the off-topic condition and in the blank screen condition, the
discourses of the three AD participants were significantly less coherent than that
of controls (<xref ref-type="table" rid="t2">Table 2</xref>).</p><table-wrap id="t2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Differences between cases and controls in global coherence scores.</p></caption><table frame="hsides" rules="rows"><colgroup span="1"><col width="25%" span="1"/><col width="25%" span="1"/><col width="25%" span="1"/><col width="25%" span="1"/></colgroup><thead style="background-color:#d2d3d5"><tr><th align="left" rowspan="1" colspan="1">Participants<break/>AD cases</th><th align="center" rowspan="1" colspan="1">On-topic condition<break/>Score</th><th align="center" rowspan="1" colspan="1">Off-topic condition<break/>Score</th><th align="center" rowspan="1" colspan="1">Blank screen condition<break/>Score</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">A</td><td align="center" rowspan="1" colspan="1">83.45</td><td align="center" rowspan="1" colspan="1">57.27*</td><td align="center" rowspan="1" colspan="1">54.10*</td></tr><tr><td align="left" rowspan="1" colspan="1">B</td><td align="center" rowspan="1" colspan="1">43.22*</td><td align="center" rowspan="1" colspan="1">21.87*</td><td align="center" rowspan="1" colspan="1">32.71*</td></tr><tr><td align="left" rowspan="1" colspan="1">C</td><td align="center" rowspan="1" colspan="1">70.20</td><td align="center" rowspan="1" colspan="1">65.15</td><td align="center" rowspan="1" colspan="1">71.31</td></tr><tr><td align="left" rowspan="1" colspan="1">D</td><td align="center" rowspan="1" colspan="1">87.23</td><td align="center" rowspan="1" colspan="1">66.66</td><td align="center" rowspan="1" colspan="1">52.23</td></tr><tr><td align="left" rowspan="1" colspan="1">E</td><td align="center" rowspan="1" colspan="1">46.36*</td><td align="center" rowspan="1" colspan="1">28.57*</td><td align="center" rowspan="1" colspan="1">34.29*</td></tr><tr><td align="left" rowspan="1" colspan="1">M (SD) controls</td><td align="center" rowspan="1" colspan="1">80.55 (15.62)</td><td align="center" rowspan="1" colspan="1">82.34 (14.85)</td><td align="center" rowspan="1" colspan="1">80.20 (11.35)</td></tr></tbody></table><table-wrap-foot><fn><p>Differences between each AD case and the control group are marked for
p&#x0003c;0.05. P values indicate an estimated percentage of the control
population that would have lower scores than the cases.</p></fn></table-wrap-foot></table-wrap><p>As shown in <xref ref-type="table" rid="t3">Table 3</xref>, in terms of
informativeness, all AD participants had similar scores to those of controls in the
on-topic condition. Conversely, in the off-topic condition, the discourses of two AD
participants were significantly less informative than those of controls. However,
three AD participants had similar scores of informativeness to those of controls in
this condition. In the blank screen condition, three AD participants demonstrated
significantly less informative discourses than controls.</p><table-wrap id="t3" orientation="portrait" position="float"><label>Table 3</label><caption><p>Differences between cases and controls in terms of informativeness</p></caption><table frame="hsides" rules="rows"><colgroup span="1"><col width="25%" span="1"/><col width="25%" span="1"/><col width="25%" span="1"/><col width="25%" span="1"/></colgroup><thead style="background-color:#d2d3d5"><tr><th align="left" rowspan="1" colspan="1">Participants<break/>AD cases</th><th align="center" rowspan="1" colspan="1">On-topic<break/>condition<break/>Score</th><th align="center" rowspan="1" colspan="1">Off-topic<break/>condition<break/>Score</th><th align="center" rowspan="1" colspan="1">Blank screen <break/>condition<break/>Score</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">A</td><td align="center" rowspan="1" colspan="1">84.25</td><td align="center" rowspan="1" colspan="1">65.24</td><td align="center" rowspan="1" colspan="1">26.33*</td></tr><tr><td align="left" rowspan="1" colspan="1">B</td><td align="center" rowspan="1" colspan="1">67.42</td><td align="center" rowspan="1" colspan="1">29.71*</td><td align="center" rowspan="1" colspan="1">35.30*</td></tr><tr><td align="left" rowspan="1" colspan="1">C</td><td align="center" rowspan="1" colspan="1">76.32</td><td align="center" rowspan="1" colspan="1">70.33</td><td align="center" rowspan="1" colspan="1">73.11</td></tr><tr><td align="left" rowspan="1" colspan="1">D</td><td align="center" rowspan="1" colspan="1">82.54</td><td align="center" rowspan="1" colspan="1">72.12</td><td align="center" rowspan="1" colspan="1">84.37</td></tr><tr><td align="left" rowspan="1" colspan="1">E</td><td align="center" rowspan="1" colspan="1">63.22</td><td align="center" rowspan="1" colspan="1">39.37*</td><td align="center" rowspan="1" colspan="1">47.14*</td></tr><tr><td align="left" rowspan="1" colspan="1">M (SD) controls</td><td align="center" rowspan="1" colspan="1">86.49 (16.53)</td><td align="center" rowspan="1" colspan="1">82.91 (11.96)</td><td align="center" rowspan="1" colspan="1">85.27 (15.10)</td></tr></tbody></table><table-wrap-foot><fn><p>Differences between each AD case and the control group are marked for
p&#x0003c;0.05. P values indicate an estimated percentage of the control
population that would have lower scores than the cases.</p></fn></table-wrap-foot></table-wrap></sec><sec sec-type="discussion"><title>DISCUSSION</title><p>In general, the results of this study are in agreement with, and can be understood in
the light of, related findings from studies investigating visual attention and
discourse abilities of individuals with AD and healthy controls. Concerning visual
attention, eye-tracking measures showed that, unlike the control group, AD
participants did not differ in the time spent dwelling for on- and off-topic cues.
This finding is in agreement with studies showing that the inhibition deficits of
patients with AD affect their ability to disengage attention from irrelevant
stimuli.<sup><xref rid="r26" ref-type="bibr">26</xref></sup> AD participants
paid attention to visual cues, but did not have sufficiently preserved attentional
skills to independently choose to focus exclusively on relevant visual stimuli. This
result demonstrates that individuals with AD may benefit from interventions
conducted by therapists and family members trained to select and display only those
cues directly relevant to the context of conversations. According to
Bourgeois,<sup><xref rid="r3" ref-type="bibr">3</xref></sup> discourse
interventions designed for AD patients should consider that direct external aids are
more effective than self-monitored strategies.</p><p>Eye-tracking data demonstrated that when the screen was displaying irrelevant
information, controls looked instead at the examiner's face. On the other hand, in
the blank screen condition, controls used the gaze averting strategy by looking more
frequently to the screen. This did not occur with AD participants, who were much
more attracted to the listener's face in the blank screen condition. This condition
required participants to recall and recount an event without the display of visual
cues that could possibly help them. It can be said that this situation demands
greater independence and the ability of the speaker to rely on his or her own
resources in order to retrieve information for discourse processing. Since the
classic study of Argyle and Cook,<sup><xref rid="r27" ref-type="bibr">27</xref></sup> it is known that during complex discourse planning, healthy
adults frequently avoid looking at the listener's face in an attempt to focus on
information processing. AD participants rarely looked at the screen in this task and
were drawn to their listener's face for a much longer period, possibly in an attempt
to obtain help and as a strategy for switching turns. This finding reinforces the
idea that AD participants rely greatly on their communicative partners. In fact,
strategies in which AD speakers attempt to obtain the listener's help and try to
switch turns confirm the preservation of certain pragmatic abilities in moderate and
moderate-severe stages of AD.<sup><xref rid="r28" ref-type="bibr">28</xref></sup>
Looking longer at the listener's face also confirms the idea that in moderate stages
of AD there is no significant decrease in interest in the interlocutor.<sup><xref rid="r9" ref-type="bibr">9</xref></sup> There is evidence that important
non-verbal pragmatic abilities necessary for enjoying the context of communication
are present even in the latest stages of AD.<sup><xref rid="r12" ref-type="bibr">12</xref></sup> Thus, independently of the severity of AD and of the
difficulties in communicating verbally, there is undoubtedly interest and need for
communication.</p><p>Although pictures were the most focused cues, written information was dwelled on much
longer by AD participants than by controls. Mahendra and Bayles<sup><xref rid="r29" ref-type="bibr">29</xref></sup> demonstrated that written visual
information is more efficient as a mnemonic cue for AD patients than auditory
information. The authors argued that, whereas sentences produced by an interlocutor
disappear from an AD person's working memory, written sentences can be read again
many times as a mnemonic aid while performing a task. Therefore, it is possible that
the AD group in the present study looked longer at the written information in an
attempt to use it as a retrieval aid and a cue to keep up with the conversation.</p><p>We found no significant differences between the discourse of the participants with AD
and controls in terms of informativeness in the on-topic condition. Conversely, in
the condition including off-topic cues and in the blank screen condition, the
discourse of two and three cases, respectively, was significantly less informative
and less coherent. These findings should be regarded with caution given the
heterogeneity of the patterns demonstrated by the cases in each condition. Due to
their very nature, this kind of data tend to show more individual differences, but
the heterogeneity of discourse results was probably greater because of the small
sample size, which represents a limitation of the present study. Nevertheless, most
of the results are in agreement with previous findings<sup><xref rid="r3" ref-type="bibr">3</xref></sup> demonstrating that individuals with AD are able to
take advantage of visual cues to express more factual information and fewer
ambiguous and repetitive sentences. The advantage of using autobiographical visual
cues may be that they seem to work as sensory aids that do not require cognitive
effort. Dijkstra et al.<sup><xref rid="r4" ref-type="bibr">4</xref></sup> suggested
that the visual information provided by recipe books facilitated access to semantic
memory. With regard to the discursive task proposed in the present study, visual
cues seemed to help AD participants to access information from autobiographical
memory and to produce new propositions. It is possible that the impaired executive
system responsible for retrieving relevant autobiographical information is benefited
by the use of a direct channel created by visual input based on implicit activation
mechanisms that do not require great cognitive effort. Taken together, our results
suggest that on-topic pictures and short sentences may be helpful in prompting AD
participants to activate relevant ideas in conversations. Future studies should
investigate the efficacy of such interventions on a larger scale.</p><p><bold>Acknowledgements.</bold> The first received Post-doctorate scholarship awarded
by the Portuguese Science and Technology Foundation (<italic>Funda&#x000e7;&#x000e3;o
para a Ci&#x000ea;ncia e Tecnologia - FCT</italic>) for the execution of this
study at the Center of Linguistics of Universidade Nova de Lisboa. The authors would
like to thank the collaboration of the senior university of Oeiras
(<italic>Universidade Senior de Oeiras</italic>) and its students, who
enthusiastically volunteered as controls. We also thank the participants with AD and
their family members, as well as Dr. Manuel Goncalves Pereira for his kind
collaboration in recruiting the patients from the Clinica Sao Jose.</p></sec></body><back><fn-group><fn fn-type="COI-statement"><p>Disclosure: The authors report no conflicts of interest.</p></fn></fn-group><ref-list><title>REFERENCES</title><ref id="r1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourgeois</surname><given-names>MS</given-names></name></person-group><article-title>Evaluating memory wallets in conversations with persons with
dementia</article-title><source>J Speech Hear Res</source><year>1992</year><volume>35</volume><fpage>1344</fpage><lpage>1357</lpage><pub-id pub-id-type="pmid">1494276</pub-id></element-citation></ref><ref id="r2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McPherson</surname><given-names>A</given-names></name><name><surname>Furniss</surname><given-names>FG</given-names></name><name><surname>Sdogati</surname><given-names>C</given-names></name><name><surname>Cesaroni</surname><given-names>F</given-names></name><name><surname>Tartaglini</surname><given-names>B</given-names></name><name><surname>Lindesay</surname><given-names>J</given-names></name></person-group><article-title>Effects of individualized memory aids on the conversation of
persons with severe dementia: a pilot study</article-title><source>Aging Ment Health</source><year>2001</year><volume>5</volume><fpage>289</fpage><lpage>294</lpage><pub-id pub-id-type="pmid">11575069</pub-id></element-citation></ref><ref id="r3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourgeois</surname><given-names>MS</given-names></name><name><surname>Camp</surname><given-names>C</given-names></name><name><surname>Rose</surname><given-names>M</given-names></name><etal/></person-group><article-title>A comparison of training strategies to enhance use of external
aids by persons with dementia</article-title><source>J Commun Disord</source><year>2003</year><volume>36</volume><fpage>361</fpage><lpage>378</lpage><pub-id pub-id-type="pmid">12927944</pub-id></element-citation></ref><ref id="r4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>K</given-names></name><name><surname>Bourgeois</surname><given-names>M</given-names></name><name><surname>Youmans</surname><given-names>G</given-names></name><name><surname>Hancock</surname><given-names>A</given-names></name></person-group><article-title>Implications of an advice-giving and teacher role on language
production in adults with dementia</article-title><source>Gerontologist</source><year>2006</year><volume>46</volume><fpage>357</fpage><lpage>366</lpage><pub-id pub-id-type="pmid">16731874</pub-id></element-citation></ref><ref id="r5"><label>5</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holsanova</surname><given-names>J</given-names></name><name><surname>Johansson</surname><given-names>R</given-names></name><name><surname>Holmqvist</surname><given-names>K</given-names></name></person-group><chapter-title>To tell and to show: the interplay of language and visualisations
in communication</chapter-title><person-group person-group-type="editor"><name><surname>G&#x000e4;rdenfors</surname><given-names>P</given-names></name><name><surname>Wallin</surname><given-names>A</given-names></name></person-group><source>A smorgasbord of cognitive science</source><publisher-loc>Nora, Sweden</publisher-loc><publisher-name>Nya Doxa</publisher-name><year>2008</year><fpage>215</fpage><lpage>229</lpage></element-citation></ref><ref id="r6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tales</surname><given-names>A</given-names></name><name><surname>Haworth</surname><given-names>J</given-names></name><name><surname>Nelson</surname><given-names>S</given-names></name><name><surname>Snowden</surname><given-names>RJ</given-names></name><name><surname>Wilcock</surname><given-names>G</given-names></name></person-group><article-title>Abnormal visual search in mild cognitive impairment and
Alzheimer's disease</article-title><source>Neurocase</source><year>2005</year><volume>11</volume><fpage>80</fpage><lpage>84</lpage><pub-id pub-id-type="pmid">15804928</pub-id></element-citation></ref><ref id="r7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez-Duque</surname><given-names>D</given-names></name><name><surname>Black</surname><given-names>S</given-names></name></person-group><article-title>Attentional Networks in normal aging and Alzheimer's
disease</article-title><source>Neuropsychology</source><year>2006</year><volume>20</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="pmid">16594774</pub-id></element-citation></ref><ref id="r8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LaBar</surname><given-names>K</given-names></name><name><surname>Mesulam</surname><given-names>M</given-names></name><name><surname>Gitelman</surname><given-names>DR</given-names></name><name><surname>Weintraub</surname><given-names>S</given-names></name></person-group><article-title>Emotional curiosity: modulation of visuospatial attention by
arousal is preserved in aging and early-stage Alzheimer's
disease</article-title><source>Neuropsychologia</source><year>2000</year><volume>38</volume><fpage>1734</fpage><lpage>1740</lpage><pub-id pub-id-type="pmid">11099731</pub-id></element-citation></ref><ref id="r9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhys</surname><given-names>C</given-names></name><name><surname>Schmidt-Renfree</surname><given-names>N</given-names></name></person-group><article-title>Facework, social politeness and the Alzheimer's
patient</article-title><source>Clin Linguist Phon</source><year>2000</year><volume>14</volume><fpage>533</fpage><lpage>543</lpage></element-citation></ref><ref id="r10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langton</surname><given-names>SRH</given-names></name><name><surname>Watt</surname><given-names>RJ</given-names></name><name><surname>Bruce</surname><given-names>V</given-names></name></person-group><article-title>Do the eyes have it? Cues to the direction of social
attention</article-title><source>Trends Cogn Sci</source><year>2000</year><volume>4</volume><fpage>50</fpage><lpage>59</lpage><pub-id pub-id-type="pmid">10652522</pub-id></element-citation></ref><ref id="r11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogrocki</surname><given-names>PK</given-names></name><name><surname>Hills</surname><given-names>AC</given-names></name><name><surname>Strauss</surname><given-names>ME</given-names></name></person-group><article-title>Visual exploration of facial emotion by healthy older adults and
patients with Alzheimer disease</article-title><source>Neuropsychiatry Neuropsychol Behav Neurol</source><year>2000</year><volume>13</volume><fpage>271</fpage><lpage>278</lpage><pub-id pub-id-type="pmid">11186163</pub-id></element-citation></ref><ref id="r12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astell</surname><given-names>AJ</given-names></name><name><surname>Ellis</surname><given-names>MP</given-names></name></person-group><article-title>The social function of imitation in severe
dementia</article-title><source>Inf Child Dev</source><year>2006</year><volume>15</volume><fpage>311</fpage><lpage>319</lpage></element-citation></ref><ref id="r13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKhann</surname><given-names>G</given-names></name><name><surname>Drachman</surname><given-names>D</given-names></name><name><surname>Folstein</surname><given-names>M</given-names></name><name><surname>Katzman</surname><given-names>R</given-names></name><name><surname>Price</surname><given-names>D</given-names></name><name><surname>Stadlan</surname><given-names>EM</given-names></name></person-group><article-title>Clinical diagnosis of Alzheimer's disease: report of the
NINCDS-ADRDA Work Group under the auspices of Department of Health and Human
Services Task Force on Alzheimer's disease</article-title><source>Neurology</source><year>1984</year><volume>34</volume><fpage>939</fpage><lpage>944</lpage><pub-id pub-id-type="pmid">6610841</pub-id></element-citation></ref><ref id="r14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Folstein</surname><given-names>MF</given-names></name><name><surname>Folstein</surname><given-names>SE</given-names></name><name><surname>Mchugh</surname><given-names>PR</given-names></name></person-group><article-title>"Mini-mental state": a practical method for grading the cognitive
status of patients for the clinician</article-title><source>J Psychiatr Res</source><year>1975</year><volume>12</volume><fpage>189</fpage><lpage>198</lpage><pub-id pub-id-type="pmid">1202204</pub-id></element-citation></ref><ref id="r15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerreiro</surname><given-names>M</given-names></name><name><surname>Silva</surname><given-names>AP</given-names></name><name><surname>Botelho</surname><given-names>MA</given-names></name><name><surname>Leit&#x000e3;o</surname><given-names>O</given-names></name><name><surname>Castro-Caldas</surname><given-names>A</given-names></name><name><surname>Garcia</surname><given-names>C</given-names></name></person-group><article-title>Adapta&#x000e7;&#x000e3;o &#x000e0; popula&#x000e7;&#x000e3;o
portuguesa na tradu&#x000e7;&#x000e3;o do "Mini Mental State Examination"
(MMSE)</article-title><source>Rev Portug Neurol</source><year>1994</year><volume>1</volume><fpage>9</fpage><lpage>10</lpage></element-citation></ref><ref id="r16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reisberg</surname><given-names>B</given-names></name><name><surname>Ferris</surname><given-names>SH</given-names></name><name><surname>De Leon</surname><given-names>MJ</given-names></name><name><surname>Crook</surname><given-names>T</given-names></name></person-group><article-title>The Global Deterioration Scale for assessment of primary
degenerative dementia</article-title><source>Am J Psychiatr</source><year>1982</year><volume>139</volume><fpage>1136</fpage><lpage>1139</lpage><pub-id pub-id-type="pmid">7114305</pub-id></element-citation></ref><ref id="r17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DC</given-names></name><name><surname>Schulkind</surname><given-names>MD</given-names></name></person-group><article-title>Distribution of important and word-cued autobiographical memories
in 20-, 35-, and 70-year-old adults</article-title><source>Psychol Aging</source><year>1997</year><volume>12</volume><fpage>524</fpage><lpage>535</lpage><pub-id pub-id-type="pmid">9308099</pub-id></element-citation></ref><ref id="r18"><label>18</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>W</given-names></name><name><surname>Eschman</surname><given-names>A</given-names></name><name><surname>Zuccolotto</surname><given-names>A</given-names></name></person-group><source>E-Prime User's Guide</source><publisher-loc>Pittsburgh</publisher-loc><publisher-name>Psychology Software Tools, Inc</publisher-name><year>2002</year></element-citation></ref><ref id="r19"><label>19</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holmqvist</surname><given-names>K</given-names></name><name><surname>Nystr&#x000f6;m</surname><given-names>M</given-names></name><name><surname>Andersson</surname><given-names>R</given-names></name><name><surname>Dewhurst</surname><given-names>R</given-names></name><name><surname>Jarodzka</surname><given-names>H</given-names></name><name><surname>van de Weijer</surname><given-names>J</given-names></name></person-group><source>Eye tracking: A comprehensive guide to methods and measures</source><publisher-name>Oxford University Press</publisher-name><year>2011</year></element-citation></ref><ref id="r20"><label>20</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macwhinney</surname><given-names>B</given-names></name></person-group><source>The CHILDS Project: Tools for Analyzing Talk</source><edition>3rd ed</edition><publisher-loc>Mahwah, NJ</publisher-loc><publisher-name>Lawrence Erlbaum Associates</publisher-name><year>2000</year></element-citation></ref><ref id="r21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laine</surname><given-names>M</given-names></name><name><surname>Laakso</surname><given-names>M</given-names></name><name><surname>Vuorinen</surname><given-names>E</given-names></name><name><surname>Rinne</surname><given-names>J</given-names></name></person-group><article-title>Coherence and informativeness of discourse in two dementia
types</article-title><source>J Neurolinguistics</source><year>1998</year><volume>11</volume><fpage>79</fpage><lpage>87</lpage></element-citation></ref><ref id="r22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linell</surname><given-names>P</given-names></name><name><surname>Gustavsson</surname><given-names>L</given-names></name><name><surname>Juvonen</surname><given-names>P</given-names></name></person-group><article-title>Interactional dominance in dyadic communication: a presentation
of initiative-response analysis</article-title><source>Linguistics</source><year>1988</year><volume>26</volume><fpage>415</fpage><lpage>442</lpage></element-citation></ref><ref id="r23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carletta</surname><given-names>J</given-names></name></person-group><article-title>Assessing agreement on classification tasks: the kappa
statistic</article-title><source>Computational Linguistics</source><year>1996</year><volume>22</volume><fpage>249</fpage><lpage>254</lpage></element-citation></ref><ref id="r24"><label>24</label><element-citation publication-type="book"><source>Statistical Package for the Social Sciences (SPSS). Version 18.0
[Computer program]</source><publisher-loc>Chicago: USA</publisher-loc><year>2009</year></element-citation></ref><ref id="r25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname><given-names>JR</given-names></name><name><surname>Howell</surname><given-names>DC</given-names></name></person-group><article-title>Comparing an individual's test score against norms derived from
small samples</article-title><source>Clin Neuropsychol</source><year>1998</year><volume>12</volume><fpage>482</fpage><lpage>486</lpage></element-citation></ref><ref id="r26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobbins</surname><given-names>IG</given-names></name><name><surname>Hans</surname><given-names>S</given-names></name></person-group><article-title>Cue-versus probe-dependent prefrontal cortex activity during
contextual remembering</article-title><source>J Cogn Neurosci</source><year>2006</year><volume>18</volume><fpage>1439</fpage><lpage>1452</lpage><pub-id pub-id-type="pmid">16989546</pub-id></element-citation></ref><ref id="r27"><label>27</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Argyle</surname><given-names>M</given-names></name><name><surname>Cook</surname><given-names>M</given-names></name></person-group><source>Gaze and mutual gaze</source><publisher-loc>London</publisher-loc><publisher-name>Cambridge University Press</publisher-name><year>1976</year></element-citation></ref><ref id="r28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brand&#x000e3;o</surname><given-names>L</given-names></name><name><surname>Parente</surname><given-names>MAMP</given-names></name><name><surname>Pena-Casanova</surname><given-names>J</given-names></name></person-group><article-title>Estrat&#x000e9;gias comunicativas de pessoas com doen&#x000e7;a de
Alzheimer</article-title><source>Psicol Reflex Crit</source><year>2010</year><volume>23</volume><fpage>308</fpage><lpage>316</lpage></element-citation></ref><ref id="r29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahendra</surname><given-names>N</given-names></name><name><surname>Bayles</surname><given-names>K</given-names></name></person-group><article-title>Effect of presentation modality on immediate and delayed recall
in individuals with Alzheimer's disease</article-title><source>Am J Speech Lang Pathol</source><year>2005</year><volume>14</volume><fpage>144</fpage><lpage>155</lpage><pub-id pub-id-type="pmid">15989389</pub-id></element-citation></ref></ref-list></back></article>
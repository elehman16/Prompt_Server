<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Biomed Res Int</journal-id><journal-id journal-id-type="iso-abbrev">Biomed Res Int</journal-id><journal-id journal-id-type="publisher-id">BMRI</journal-id><journal-title-group><journal-title>BioMed Research International</journal-title></journal-title-group><issn pub-type="ppub">2314-6133</issn><issn pub-type="epub">2314-6141</issn><publisher><publisher-name>Hindawi Publishing Corporation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">24236293</article-id><article-id pub-id-type="pmc">3819885</article-id><article-id pub-id-type="doi">10.1155/2013/758491</article-id><article-categories><subj-group subj-group-type="heading"><subject>Clinical Study</subject></subj-group></article-categories><title-group><article-title>Design and Validation of an Augmented Reality System for Laparoscopic Surgery in a Real Environment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>L&#x000f3;pez-Mir</surname><given-names>F.</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">*</xref></contrib><contrib contrib-type="author"><name><surname>Naranjo</surname><given-names>V.</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Fuertes</surname><given-names>J. J.</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Alca&#x000f1;iz</surname><given-names>M.</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Bueno</surname><given-names>J.</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Pareja</surname><given-names>E.</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib></contrib-group><aff id="I1"><sup>1</sup>Instituto Interuniversitario de Investigaci&#x000f3;n en Bioingenier&#x000ed;a y Tecnolog&#x000ed;a Orientada al Ser Humano (I3BH), Universitat Polit&#x000e8;cnica de Val&#x000e8;ncia, I3BH/LabHuman, Camino de Vera s/n, 46022 Valencia, Spain</aff><aff id="I2"><sup>2</sup>CIBER, Fisiopatolog&#x000ed;a de Obesidad y Nutrici&#x000f3;n (CB06/03), Instituto de Salud Carlos III, 28029 Madrid, Spain</aff><aff id="I3"><sup>3</sup>Unidad de Cirug&#x000ed;a Hepatobiliopancre&#x000e1;tica y Trasplante Hep&#x000e1;tico, Hospital Universitario y Polit&#x000e9;cnico La Fe, 46026 Valencia, Spain</aff><author-notes><corresp id="cor1">*F. L&#x000f3;pez-Mir: <email>ferlomir@labhuman.i3bh.es</email></corresp><fn fn-type="other"><p>Academic Editor: Surinder K. Jindal</p></fn></author-notes><pub-date pub-type="ppub"><year>2013</year></pub-date><pub-date pub-type="epub"><day>23</day><month>10</month><year>2013</year></pub-date><volume>2013</volume><elocation-id>758491</elocation-id><history><date date-type="received"><day>20</day><month>5</month><year>2013</year></date><date date-type="rev-recd"><day>12</day><month>9</month><year>2013</year></date><date date-type="accepted"><day>16</day><month>9</month><year>2013</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2013 F. L&#x000f3;pez-Mir et al.</copyright-statement><copyright-year>2013</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/3.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>
<italic>Purpose</italic>. This work presents the protocol carried out in the development and validation of an augmented reality system which was installed in an operating theatre to help surgeons with trocar placement during laparoscopic surgery. The purpose of this validation is to demonstrate the improvements that this system can provide to the field of medicine, particularly surgery. <italic>Method</italic>. Two experiments that were noninvasive for both the patient and the surgeon were designed. In one of these experiments the augmented reality system was used, the other one was the control experiment, and the system was not used. The type of operation selected for all cases was a cholecystectomy due to the low degree of complexity and complications before, during, and after the surgery. The technique used in the placement of trocars was the French technique, but the results can be extrapolated to any other technique and operation. <italic>Results and Conclusion</italic>. Four clinicians and ninety-six measurements obtained of twenty-four patients (randomly assigned in each experiment) were involved in these experiments. The final results show an improvement in accuracy and variability of 33% and 63%, respectively, in comparison to traditional methods, demonstrating that the use of an augmented reality system offers advantages for trocar placement in laparoscopic surgery.</p></abstract></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>Laparoscopic surgery has proven to be an alternative to traditional open surgery since smaller incisions are made in the abdomen of the patient [<xref rid="B1" ref-type="bibr">1</xref>]. The laparoscopic camera and the different endoscopic instruments are introduced through trocars, the hollow cylindrical instruments that are placed into these incisions. Thanks to these smaller incisions, this surgery offers many advantages to the patient, such as less chance of infection, less subsequent operations to remake the abdominal muscle, and so forth. Consequently, the recovery time for the patient is faster both physically and psychologically, which means a lower postoperative cost for the hospital [<xref rid="B2" ref-type="bibr">2</xref>].</p><p>The main drawbacks of laparoscopic surgery in contrast to open surgery are the lack of direct vision, the need for hand-eye coordination, and the lack of tactile feedback to the surgeon. Another problem is related to the trocar placement since it may result in more invasive surgery. Currently, the incisions are made by palpation based on the experience and skill of the surgeon. The improper placement of trocars in an operation, such as in the lymph node dissection in the hepatoduodenal ligament, can complicate the operation. In these cases, a relocation of the trocar might be necessary (and more incisions than strictly necessary will be made) thereby limiting the advantages of laparoscopic surgery mentioned above [<xref rid="B3" ref-type="bibr">3</xref>].</p><p>Augmented reality (AR) is a 3D computer vision technique that is characterized by the real-time fusion of virtual elements on a real space [<xref rid="B4" ref-type="bibr">4</xref>]. Currently, augmented reality offers enormous potential in many fields such as education, simulation, architecture, advertising, navigation devices, medicine, and rehabilitation [<xref rid="B4" ref-type="bibr">4</xref>]. Surgery is the branch of medicine where augmented reality has more potential for application because it can provide surgeons with preoperative information (magnetic resonance imaging or MRI, radiography, 3D reconstructions, etc.) in the same place and at the same time that they are operating. Thus, some of the drawbacks previously cited are alleviated [<xref rid="B5" ref-type="bibr">5</xref>, <xref rid="B6" ref-type="bibr">6</xref>].</p><p>In [<xref rid="B7" ref-type="bibr">7</xref>], a taxonomy of augmented reality systems in image-guided surgery is proposed. The work compares and analyzes several systems which use augmented reality technology in surgery applications. The analysis is based on the type of input data, the visualization format, and the way in which data is displayed in the operating theatre. The objectives of this comparison are to establish the syntax for defining a system of these characteristics and to show the principal components of an AR system for image-guided applications. In our case, following the analysis suggested in the work [<xref rid="B7" ref-type="bibr">7</xref>], three components have been chosen.<list list-type="roman-lower"><list-item><p>Specific data of the patient: our system uses MRI of the patient and generates a 3D model of internal structures.</p></list-item><list-item><p>A visualization format based on color coding for anatomical structures: transparency has been used in the models to give more realistic depth in the virtual model.</p></list-item><list-item><p>A full HD monitor for displaying data.</p></list-item></list>
</p><p>The main limitation of AR systems is the registration technique employed. In our case, the registration and fusion are done between a 3D volume from the segmented magnetic resonance images of the patient and the real-time image that is recorded by a webcam placed over the patient in the operating theater, specifically above of the patient's abdomen (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Some authors have developed techniques to improve and automate preoperative placement of trocars. Based on 3D information extracted from computed tomography (CT) images or MRI, the surgeons must remember this information once they are in the operating theatre. In [<xref rid="B8" ref-type="bibr">8</xref>], an optimal access system with virtual endoscopic views is proposed, making the simulation with a phantom. In [<xref rid="B9" ref-type="bibr">9</xref>, <xref rid="B10" ref-type="bibr">10</xref>], the problem is addressed in image-guided surgery, and trocar placement is optimized from a robotic point of view. The validation is performed on animals. In [<xref rid="B11" ref-type="bibr">11</xref>], the system requires the use of fiducials that have to be in the same position as when the CT was acquired. In addition, the position and orientation of the patient have to be the same in the operating theater.</p><p>Other authors deal with the problem in the operating theater similarly to the method presented in this work, but they are focused on the navigation during the intervention using the image that the endoscopic camera provides. In [<xref rid="B12" ref-type="bibr">12</xref>], a registration with fiducials is carried out to monitor the camera. These fiducials must be placed over the patient in the same positions in the operating theater and when the CT is acquired. In [<xref rid="B13" ref-type="bibr">13</xref>], 3D information is merged into the laparoscopic video. In [<xref rid="B14" ref-type="bibr">14</xref>], the validation was done in animals, and the registration and fusion processes were done manually thanks to the surgeon's anatomical knowledge. In [<xref rid="B15" ref-type="bibr">15</xref>], a head-mounted display is used, and the validation was carried out in a commercial phantom.</p><p>The experiments carried out in this work were performed in laparoscopic cholecystectomies. This type of intervention is a common solution for diseases such as symptomatic gallstones [<xref rid="B16" ref-type="bibr">16</xref>]. It is a common operation with low probability of pre- and postoperative complications. The placement of trocars is usually performed using either French or American techniques [<xref rid="B17" ref-type="bibr">17</xref>]. The choice of one technique or another does not determine the outcome of the experiments, which can be extrapolated from each other. For our experiments, the surgeons chose the French technique because they are accustomed to using it. Both techniques are based on placing four abdominal trocars. Three of them are placed in the same positions in both cases; the fourth trocar differs in the American and the French technique from the area below the sternum in the American and in the opposite side of the liver in the French. In both cases, the surgeon draws four marks with a biocompatible pen taking into account external anatomical references. These marks serve as the initial references of where to make the final incisions. The first trocar (which is called the Veress needle or Hasson cannula and is different from others) is always located at the same position that has been marked with the pen (one centimeter above the navel after making an incision of about 10&#x02009;mm with the scalpel). By means of this trocar, the pneumoperitoneum technique is performed, and the abdominal cavity is deformed [<xref rid="B18" ref-type="bibr">18</xref>]. Subsequently, the endoscopic camera is inserted through this same trocar to visualize the abdominal cavity (keeping the insufflator hose connected to maintain pneumoperitoneum throughout the entire surgery). The other three incisions are made with the internal vision of the camera and palpation, correcting the position of the marks made with the pen. For two of the incisions, the primary surgeon inserts the surgical instruments (scalpel, forceps, scissors, etc.), and the other incision is used by the secondary surgeon according to the principal surgeon instructions.</p><p>The goodness of our system using augmented reality in the operating theatre was determined by measuring the precision offered by the system compared to not using it. Four distances relating to the four incisions made in the patient (||<italic>d</italic>
<sub><italic>i</italic></sub>||, <italic>i</italic> = {1, 2, 3, 4})&#x02009;&#x02009;were obtained. In this work, the position of the incision (<italic>P</italic>
<sub><italic>i</italic></sub>* = &#x02329;<italic>x</italic>
<sub><italic>i</italic></sub>*, <italic>y</italic>
<sub><italic>i</italic></sub>*, <italic>z</italic>
<sub><italic>i</italic></sub>*&#x0232a;)&#x02009;&#x02009;is equal to the position of the pen mark (<italic>P</italic>
<sub><italic>i</italic></sub> = &#x02329;<italic>x</italic>
<sub><italic>i</italic></sub>, <italic>y</italic>
<sub><italic>i</italic></sub>, <italic>z</italic>
<sub><italic>i</italic></sub>&#x0232a;) plus an offset (<italic>d</italic>
<sub><italic>i</italic></sub> = &#x02329;<italic>d</italic>
<sub><italic>xi</italic></sub>, <italic>d</italic>
<sub><italic>yi</italic></sub>, <italic>d</italic>
<sub><italic>zi</italic></sub>&#x0232a;) (<xref ref-type="disp-formula" rid="EEq1">1</xref>). The measured distance <inline-formula><mml:math id="M1"><mml:mrow><mml:mo stretchy="false">||</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">||</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo>+</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:math></inline-formula>, <italic>i</italic> = {1, 2, 3, 4} is due to different factor such as displacements and deformities of the pneumoperitoneum technique (<italic>d</italic>
<sub>neum</sub>), the distinctive features of the internal anatomy of the patient (<italic>d</italic>
<sub>patient</sub>) that the surgeon does not notice at the time of making the marks with the pen, and the experience and skill of the surgeon (<italic>d</italic>
<sub>surgeon</sub>):
<disp-formula id="EEq1"><label>(1)</label><mml:math id="M2"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02217;</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="EEq2"><label>(2)</label><mml:math id="EEq2EAAAAEDCA"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>neum</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>patient</mml:mtext><mml:mi>&#x02009;&#x02009;</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>surgeon</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>other</mml:mtext><mml:mi>&#x02009;&#x02009;</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>The distance <italic>d</italic>
<sub>other</sub> is related to any error not taking into account by the other variables, for example, operating theatre characteristics (light, position of the patient in the stretcher). When an augmented reality system is used, <italic>d</italic>
<sub><italic>i</italic></sub> may be decomposed into the elements of (<xref ref-type="disp-formula" rid="EEq3">3</xref>) (similar to (<xref ref-type="disp-formula" rid="EEq2">2</xref>), but decomposing <italic>d</italic>
<sub>other&#x02009;&#x02009;</sub> into three new corrections). The augmented reality system introduces an error due to the applied registration method (<italic>d</italic>
<sub>ra</sub>) which is related to the precision offered by the AR system (<xref ref-type="sec" rid="sec2.2.1">Section 2.2.1</xref>). There is also another error related to the accuracy in the segmentation procedure (<italic>d</italic>
<sub>segment</sub>) of the 3D model which will be projected onto the patient. In our experiments, this segmentation was previously done by an expert and then reviewed by a second expert to check it. This assumption leads us to conclude that the segmentation error can be considered zero or equal to the pixel resolution. The distance <italic>d</italic>
<sub>other</sub>&#x02032; is similar to <italic>d</italic>
<sub>other</sub> (and other errors as the manual alignment of the marker are also included in this distance). In any case, the hypothesis of this work is that the errors introduced by the augmented reality system will be compensated by the global improvement in <italic>d</italic>
<sub><italic>i</italic></sub>:
<disp-formula id="EEq3"><label>(3)</label><mml:math id="M3"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>neum</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>patient</mml:mtext><mml:mi>&#x02009;&#x02009;</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>surgeon</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>ra</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>segment</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>other</mml:mtext></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>The purpose of this work is to measure the ||<italic>d</italic>
<sub><italic>i</italic></sub>|| distance for the four trocars which <italic>P</italic>
<sub><italic>i</italic></sub>* (incisions where the trocar is inserted) and <italic>P</italic>
<sub><italic>i</italic></sub> (marks made with the pen) are known. These distances are measured in some patients when the system is used and on other patients when the system is not used. The goal is to verify if the use of augmented reality system minimizes ||<italic>d</italic>
<sub><italic>i</italic></sub>||.</p><p>Several authors have attempted to measure the error caused by an augmented reality system in an operating theater. Most validate it on phantoms and in the maxillofacial and neurosurgery fields. The high resolution of these images and the rigidity of these structures indicate that this error can be explained mainly by (but not limited to) a registration error associated with augmented reality algorithms [<xref rid="B19" ref-type="bibr">19</xref>&#x02013;<xref rid="B26" ref-type="bibr">26</xref>]. This error is measured qualitatively [<xref rid="B22" ref-type="bibr">22</xref>] or quantitatively as being on the order of several millimeters. Other authors validate their algorithms using abdominal operations. In [<xref rid="B27" ref-type="bibr">27</xref>], an AR system is applied in a liver phantom limiting the measured error. The AR system presented in [<xref rid="B11" ref-type="bibr">11</xref>] is validated for liver surgery on pigs where registration with 4 fiducials is used to measure its accuracy.</p><p>The rest of this paper is organized as follows. <xref ref-type="sec" rid="sec2"> Section 2</xref> is divided into two parts, the first part explains the AR system, and the second part describes the protocol of the experiments that were carried out. <xref ref-type="sec" rid="sec3"> Section 3</xref> presents the results, and <xref ref-type="sec" rid="sec4">Section 4</xref> presents conclusions and discussions. The primary contribution of this paper is the design, performance, and validation of an augmented reality system, the ergonomic study of the visualization devices and the protocol definition for its validation on real patients in an operating theatre.</p></sec><sec id="sec2"><title>2. Methodology</title><sec id="sec2.1"><title>2.1. Augmented Reality System</title><sec id="sec2.1.1"><title>2.1.1. Virtual 3D Model</title><p>When MR images are acquired, the patient must lie on a stretcher with his/her back straight and centered on both sides to calculate the position and the orientation relative to an initial coordinate system. A virtual model of the patient's organs is extracted from these images using techniques of digital image processing, especially our own image segmentation algorithms [<xref rid="B28" ref-type="bibr">28</xref>] and others developed in [<xref rid="B29" ref-type="bibr">29</xref>]. With this model, the clinician selects the patient's navel in the MRI images to establish the origin of 3D space at that point in order to perform the registration with the real-time image (<xref ref-type="disp-formula" rid="EEq4">4</xref>). The new coordinate system is
<disp-formula id="EEq4"><label>(4)</label><mml:math id="M4"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mo>&#x02003;</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mo>&#x02003;</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>&#x003b1;</italic>, <italic>&#x003b2;</italic>, and <italic>&#x003b3;</italic> are the coordinates of the center of the patient's navel with respect to the initial coordinate system (<italic>x</italic>, <italic>y</italic>, <italic>z</italic>), (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec><sec id="sec2.1.2"><title>2.1.2. Camera Calibration and Real-Time Image</title><p>The real-time images are recorded with a camera that shows the area of interest throughout the entire surgery. Initially, the intrinsic parameters of the camera are obtained to calibrate it. To do this, it is necessary to have different captures of planar checkerboard patterns (see <xref ref-type="fig" rid="fig3">Figure 3</xref>), which should be different for each calibration image. Zhang's method is used for the calibration step, taking the correspondence between 2D image points and 3D scene points over a number of images [<xref rid="B30" ref-type="bibr">30</xref>].</p><p>The 3 &#x000d7; 3 intrinsic matrix <italic>K</italic> and the vector <italic>&#x003b3;</italic> of the camera with the distortion parameters have the following form:
<disp-formula id="EEq5"><label>(5)</label><mml:math id="M5"><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>f</mml:mi></mml:mtd><mml:mtd><mml:mi>s</mml:mi></mml:mtd><mml:mtd><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>a</mml:mi><mml:mi>f</mml:mi></mml:mtd><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mo>&#x02003;</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>f</italic> is the focal length, (<italic>u</italic>, <italic>v</italic>) is the optical center of the camera, <italic>a</italic> is the aspect ratio, <italic>s</italic> is the camera skew between the <italic>x</italic>- and <italic>y</italic>-axes, <italic>&#x003b1;</italic>
<sub>1</sub> and <italic>&#x003b1;</italic>
<sub>2</sub> are the radial distortion parameters, and <italic>&#x003b2;</italic>
<sub>1</sub> and <italic>&#x003b2;</italic>
<sub>2</sub> are the tangential distortion parameters. The values of these parameters for camera calibration were (all in mm)
<disp-formula id="eq6"><label>(6)</label><mml:math id="M6"><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>518.55</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>349.727</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1.33</mml:mn><mml:mi>&#x02217;</mml:mi><mml:mn>517.48</mml:mn></mml:mtd><mml:mtd><mml:mn>279.897</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">0.323276</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn mathvariant="normal">0.112309</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">0.000341309</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">0.00175445</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Then, a hexadecimal mark is placed on the navel and centered and oriented as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. It is advisable to keep the camera parallel to the patient's trunk in order to improve the accuracy of the system, but it is not mandatory (as explained in <xref ref-type="sec" rid="sec2.1.3">Section 2.1.3</xref>) because the system takes into account the inclination between the patient and the camera position. The next steps are the hexadecimal mark detection and the registration and fusion of the real image with the virtual model of the patient.</p></sec><sec id="sec2.1.3"><title>2.1.3. Registration, Fusion, and Hexadecimal Mark Detection</title><p>A binary hexadecimal code marker of 8.45 &#x000d7; 8.45&#x02009;centimeters is used in this step. First, the RGB captured image is converted to a binary image, and the edge of the marker is detected thanks to an adaptive threshold algorithm based on the technique of Pintaric [<xref rid="B31" ref-type="bibr">31</xref>]. Basically, &#x0201c;<italic>this technique evaluates the mean pixel luminance over a thresholding region of interest, which is defined as a bounding rectangle around the marker axis-aligned corner vertices in the screen-space.</italic>&#x0201d;</p><p>Afterwards, the relative marker position and orientation with respect to the camera (view point) can also be estimated from a planar structure when the internal parameters are known, in order to apply them to the virtual model. First, a 3D/2D homography matrix must be calculated to later obtain the projective matrix, as detailed in [<xref rid="B32" ref-type="bibr">32</xref>].</p><p>A 3D/2D correspondence (<italic>m</italic>, <italic>M</italic>) includes a 3D point <italic>M</italic> and a 2D pixel point <italic>m</italic>, which are represented as (<italic>X</italic>, <italic>Y</italic>, <italic>Z</italic>, 1) and (<italic>x</italic>,<italic>y</italic>,1)<sup><italic>T</italic></sup>, respectively. (<italic>m</italic>, <italic>M</italic>) is related by the 3 &#x000d7; 3 projective matrix <italic>P</italic>
<sub><italic>i</italic></sub> as [<xref rid="B33" ref-type="bibr">33</xref>] shows:
<disp-formula id="EEq6"><label>(7)</label><mml:math id="M7"><mml:mtable><mml:mtr><mml:mtd><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02003;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>R</italic>
<sub><italic>i</italic></sub> is a 3 &#x000d7; 3 rotation matrix, <italic>t</italic>
<sub><italic>i</italic></sub> is the translation vector of the camera, and <italic>&#x003bb;</italic>
<sub><italic>i</italic></sub> is the homogeneous scale factor that is dependent on <italic>P</italic>
<sub><italic>i</italic></sub>
<italic>M</italic>. Specifically, considering the <italic>z</italic> = 0 plane, the expression of the homography that maps a point onto this plane, and its corresponding 2D point <italic>m</italic> under the perspective can be recovered by writing
<disp-formula id="EEq7"><label>(8)</label><mml:math id="M8"><mml:mtable><mml:mtr><mml:mtd><mml:maligngroup/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:malignmark/><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>X</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mi>Y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>X</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>R</italic>
<sub>1</sub>, <italic>R</italic>
<sub>2</sub>, and <italic>R</italic>
<sub>3</sub> are the columns of the matrix <italic>R</italic>. Thus, (<italic>m</italic>, <italic>M</italic>) is related by a 3 &#x000d7; 3 matrix <italic>H</italic>
<sub><italic>w</italic></sub>
<sup><italic>i</italic></sup> that is called homography matrix:
<disp-formula id="EEq8"><label>(9)</label><mml:math id="M9"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="normal">1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>X</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>Y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn mathvariant="normal">1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x02003;</mml:mo><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Conversely, once <italic>H</italic>
<sub><italic>w</italic></sub>
<sup><italic>i</italic></sup>&#x02009;&#x02009;and <italic>K</italic> are known, the patient's pose can be recovered from (<xref ref-type="disp-formula" rid="EEq6">7</xref>) and (<xref ref-type="disp-formula" rid="EEq8">9</xref>), because <italic>R</italic> is a unit orthogonal matrix, as is explained in [<xref rid="B34" ref-type="bibr">34</xref>] (&#x0201c;the last column <italic>R</italic>
<sub>3</sub> is given by the cross-product <italic>R</italic>
<sub>1</sub> &#x000d7; <italic>R</italic>
<sub>2</sub>&#x0201d;):
<disp-formula id="EEq9"><label>(10)</label><mml:math id="M10"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi/><mml:mo>&#x02003;</mml:mo><mml:mo>&#x02003;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Generally, the patient's pose can be refined by nonlinear minimization, since the anterior processes are sensitive to noise, and, therefore, a lack of precision and the &#x0201c;jitter&#x0201d; phenomenon are produced.</p><p>In this case, the sum of the reprojection errors is minimized, which is the squared distance between the projection of the 3D points and their measured 2D coordinates. We can therefore write that
<disp-formula id="EEq10"><label>(11)</label><mml:math id="M11"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow/></mml:mrow><mml:mrow><mml:msub><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mo>||</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>PM</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>||</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p> This equation will be solved using the Levenberg-Marquardt (LM) algorithm proposed by [<xref rid="B35" ref-type="bibr">35</xref>], providing a solution for the problem &#x0201c;Nonlinear Least Squares Minimization.&#x0201d;</p><p>In this way, the 3D virtual model and the patient's image can be registered and fused. Just then, it is important for the patient to maintain his/her position to avoid possible registration errors.</p></sec></sec><sec id="sec2.2"><title>2.2. Experimentation</title><sec id="sec2.2.1"><title>2.2.1. Error Introduced by the AR System</title><p>Before the system was validated by the surgeon in the hospital, to test how the AR module works and to determine its accuracy (<italic>d</italic>
<sub>ra</sub>), the following experiment was performed. Initially, 512 &#x000d7; 512 CT images with a spacing-resolution of 0.488 &#x000d7; 0.488 &#x000d7; 0.625&#x02009;mm per pixel were extracted from a jar by means of a GE LightSpeed VCT-5124069 machine. The model used was a 500&#x02009;mL DURAN GLS 80 jar with a diameter of 101&#x02009;mm. The 3D virtual model was obtained by applying a region growing algorithm taking the pixels between thresholds 150 and 2200 Hounsfield Units (HU).</p><p>The camera was placed at a 90&#x000b0; degree angle relative to the real jar. Then, the middle point of the jar was selected in the CT images as the new origin, and the marker was centered on the jar. The registration and fusion were performed at that moment, taking an image of the real jar and the virtual jar to validate the system's accuracy. A full graphic example of the experiment is shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p><p>Different positions of the camera and measures were taken. Finally, it was proved that if the camera was placed at a 90&#x000b0; degree angle relative to the real jar, the system was introduced an error of 3 pixels (the minima of all cases). The real width of the jar and the image width provided by the camera were known, so a direct correspondence was made, and a measurement of <italic>d</italic>
<sub>ra</sub> = 2.91&#x02009;mm, was obtained [<xref rid="B26" ref-type="bibr">26</xref>]. As mentioned in the introduction, the augmented reality systems introduce an error in the virtual pose calculation with respect to the real space. This error is irrelevant in most of the domains where an augmented reality system is used; however, it is of great importance in medicine. The main causes of this error (but not the only ones) are related to the following: the camera (the internal configuration and different lighting conditions that produce different behaviors), the accuracy of registration algorithms, and the accuracy of segmentation methods. Since it is difficult to give the results for each of these errors separately, this measure is usually given as a whole, and in our case it is defined as <italic>d</italic>
<sub>ra</sub>.</p></sec><sec id="sec2.2.2"><title>2.2.2. Real Patient Experiments</title><p>We carried out two experiments out on real patients. All the documents required for the adoption of the experiments were presented to the research ethics committee of the hospital. This documentation included the following.<list list-type="roman-lower"><list-item><p>A certificate of commitment related to the ethical principles of clinical trials: it includes the fundamental human rights and ethical principles related to biomedical research on humans of the Helsinki and Tokyo declaration.</p></list-item><list-item><p>A certificate of commitment from the researchers that take part in these experiments: in this certificate, the researchers agree to follow the rules and the protocol approved by the research ethics committee of the hospital.</p></list-item><list-item><p>Informed consent, which is delivered to the patient: this document explains the purpose of study, the procedure, confidentiality, the cost, and the right to leave the study at any time without their final treatment being affected.</p></list-item><list-item><p>A manual of the developed research: this document specifies the sample selection, the protocol used for the randomization of the sample into the two experiments, the protocol of the whole experiment, and the collection and analysis of the data.</p></list-item><list-item><p>A validation and data collection protocol: this document has the templates and protocols necessary for the data collection of these studies.</p></list-item><list-item><p>A request to the hospital committee for approval of the protocol: this document summarizes all the information explained above and is mandatory in order to apply for approval of the experiments.</p></list-item></list>
</p><p>Initially, the experiments were to be performed through a segmentation of gadolinium contrast MR images. The use of this agent improves the image contrast and facilitates the segmentation of different organs to extract the patient's 3D model. Even though it is safe, there is always the possibility of small allergic reactions in the patient. For this reason and since this contrast agent is not commonly used for this type of pathology, the committee rejected its use in the MRI acquisition. This change caused more difficulties in the segmentation procedure of abdominal organs, but it did not affect the results or conclusions of the experiments. After making this change, the clinical research committee approved the study, and the experiments were carried out.</p><p>In the first experiment, the augmented reality system is not used. The selected sample consists of 12 patients chosen randomly (eight women and four men). The following protocol was used.<list list-type="roman-lower"><list-item><p>Before the operation (the first time the surgeon visits the patient), the informed consent approved by the research ethics committee of the hospital and common information related to the MRI exam are given to the patient.</p></list-item><list-item><p>The day of the surgery, the patient goes to the presurgery room and then passes to the operating theater.</p></list-item><list-item><p>The surgeon performs the usual protocol until the operation ends. This protocol can be summarized as follows.</p><p>
<list list-type="order"><list-item><p>First, with a biocompatible pen, the surgeon marks the points where he/she will make the four incisions through which trocars will be inserted (<xref ref-type="fig" rid="fig6">Figure 6</xref>, left).</p></list-item><list-item><p>Second, the surgeon performs the four incisions based on his/her skill, experience, and traditional palpation techniques as explained in <xref ref-type="sec" rid="sec1">Section 1</xref> (<xref ref-type="fig" rid="fig6">Figure 6</xref>, right).</p></list-item><list-item><p>When the four trocars are placed, the surgeon begins the operation according to the specific protocol for this type of surgery.</p></list-item><list-item><p>Once the gallbladder has been extracted and the four incisions are sutured, the surgeon measures the four values or distances <italic>d</italic>
<sub><italic>i</italic></sub> (<xref ref-type="fig" rid="fig6">Figure 6</xref>, right). These four distances measure the difference between the initial pen marks and the real incisions or, in other words, the correction that has to be made for the technique of pneumoperitoneum, the anatomical differences of patients, and the skill of the surgeon.</p></list-item><list-item><p>Finally, the four incisions are bandaged.</p></list-item></list>
</p></list-item><list-item><p>The surgery ends, and the patient leaves the operating theatre and goes to the postoperation room where he/she wakes up and continues with the recovery protocol.</p></list-item></list>
</p><p>In the second experiment, the augmented reality system was used. The system hardware, as shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> (left), is composed of a display device and a camera. The goal of the camera is to capture the image in real-time in order to register and merge this sequence with the 3D virtual model of the patient. The display device is responsible for showing the fusion of the video and the virtual object. In this experiment, different display devices were evaluated. <xref ref-type="table" rid="tab1"> Table 1</xref> summarizes the advantages and the disadvantages that different displays offer; this information was obtained by a usability study taking in account operating theatre restrictions.</p><p>We chose a 23 inch full HD monitor as the display device based on the criteria of minimal interaction with the patient, minimal discomfort to the surgeon, and low cost. A dual core i3 computer with graphics card &#x0201c;Nvidia GT 240&#x0201d; was used. The screen and the camera were mounted on a stand as shown in <xref ref-type="fig" rid="fig7">Figure 7</xref> (left). The stretcher with the patient was positioned between the stand and the surgeon. The actual image of the abdomen of the patient was captured by the camera which was positioned perpendicularly to the patient as shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p><p>The software (<xref ref-type="fig" rid="fig7">Figure 7</xref>, right) loads preoperative imaging and 3D models. A mark is placed in the navel for registration and fusion, and the final result is shown in one window of the software [<xref rid="B26" ref-type="bibr">26</xref>].</p><p>The sample selected for this experiment also consisted of 12 patients chosen randomly (seven men and five women). The protocol used was similar to the one used in Experiment&#x02009;&#x02009;1.<list list-type="roman-lower"><list-item><p>Before the operation, the same informed consent as in the first experiment is given to the patient. Then, the MRI is acquired.</p></list-item><list-item><p>Thanks to different segmentation algorithms, a 3D model of the patient's organs is obtained with the MR images. Specifically, in all cases, the liver and kidneys were segmented; in some cases the gallbladder and aorta were extracted (for surgeon requirements). The tool to perform the segmentation was made ad hoc [<xref rid="B28" ref-type="bibr">28</xref>, <xref rid="B29" ref-type="bibr">29</xref>].</p></list-item><list-item><p>On the day of the surgery, all the steps were similar to the first experiment, with only one difference: when the surgeon marks with the pen (<xref ref-type="fig" rid="fig8">Figure 8</xref>), he/she used the AR system that registers and merges the 3D model with the real-time image (<xref ref-type="fig" rid="fig9">Figure 9</xref>). The result of this process is shown on the screen that is directly in front of the surgeon.</p></list-item><list-item><p>Once the 4 marks are drawn, the system is removed, and the surgeon continues the usual protocol until the surgery ends.</p></list-item><list-item><p>Finally, the same four values or distances (<italic>d</italic>
<sub><italic>i</italic></sub>) as in the first experiment are measured, and the patient goes to the postoperation room to wake up and continue the recovery protocol.</p></list-item></list>
</p></sec></sec></sec><sec id="sec3"><title>3. Results</title><p>Ninety-six distances/measures were obtained (four per patient) half of them using the system and the other half without it. The protocol described in both experiments has been followed without major problems. The usual procedure for cholecystectomy surgery was only to be modified when the four distances were measured, after the operation had been completed and before the incisions were bandaged. If any unexpected complication appeared, these distances would not be measured in the patient. During the twenty-four surgeries, no complications occurred, so the measures were taken in all cases. <xref ref-type="table" rid="tab2"> Table 2</xref> shows the mean and standard deviation of the four distances measured in Experiment&#x02009;&#x02009;1 on twelve patients. In this case, the traditional procedures (palpation and the skill of the surgeon) were used in the placement of trocars.</p><p>
<xref ref-type="table" rid="tab3">Table 3</xref> shows the mean and standard deviation of the twelve cases in Experiment&#x02009;&#x02009;2, that is, when the augmented reality system was used (a new procedure was added to the traditional protocol).</p><p>The Mann-Whitney <italic>U</italic> test was used to validate the null hypothesis of equal medians at the default 5% significance level relating to the distance measures of both experiments. A <italic>P</italic> value higher than 0.05 indicates that there is a nonsignificance difference, and therefore, the measures can be added. In Experiment&#x02009;&#x02009;1, the <italic>P</italic> values between the four distances were <italic>P</italic>
<sub><italic>d</italic>1&#x02013;<italic>d</italic>3</sub> = 0.56, <italic>P</italic>
<sub><italic>d</italic>1&#x02013;<italic>d</italic>4</sub> = 0.93, <italic>P</italic>
<sub><italic>d</italic>3-<italic>d</italic>4</sub> = 0.5, <italic>P</italic>
<sub><italic>d</italic>1-<italic>d</italic>2</sub> = 0.003, <italic>P</italic>
<sub><italic>d</italic>3-<italic>d</italic>2</sub> = 0.048, and <italic>P</italic>
<sub><italic>d</italic>4&#x02013;<italic>d</italic>2</sub> = 0.03. If these values are analyzed the distance <italic>d</italic>
<sub>2</sub> has significance differences with the other three distances. However, the median of the other three measurements has nonsignificance changes, and it can be assumed that the three distances has the same distribution. These conclusions are the same for the Experiment&#x02009;&#x02009;2 distance measures.</p><p>
<xref ref-type="table" rid="tab4">Table 4</xref> shows the average of the three distances (<italic>d</italic>
<sub>1</sub>, <italic>d</italic>
<sub>3</sub>, <italic>d</italic>
<sub>4</sub>) as a global measure for both experiments. It represents the required correction when an augmented reality system is used and when it is not used.</p></sec><sec id="sec4"><title>4. Conclusions</title><p>This paper shows the protocol followed for the validation of an augmented reality system to help surgeons in the placement of trocars on patients in a real environment. First of all, the documentation that is normally required for patient involvement was presented to the hospital. The difference with other experiments, where new drugs or therapies are applicable, lies in the nonimpact that this process has on the patient or on the clinical staff because it does not introduce any additional risk to the surgery and no protocols are changed.</p><p>The hypothesis of this paper is that an augmented reality system can improve the placement of trocars in laparoscopic surgery. The results confirm this hypothesis since the average accuracy improved and its variability decreased when the AR system was used.</p><p>The Experiment&#x02009;&#x02009;2 of our work was validated with a 3D model extracted from MR images. The reason for this modeling is that MR images are acquired in the normal protocol of the hospital where we performed the experiments for the laparoscopic cholecystectomies. However, the 3D model can be segmented from other types of images such as CT images. We used the navel as the anatomical structure of reference in the registration procedure of our AR system, but other external reference may be used for other surgeries if it is visible in the CT/MR images.</p><p>When Tables <xref ref-type="table" rid="tab1">1</xref> and <xref ref-type="table" rid="tab2">2</xref> are analyzed, the first trocar that is introduced in the patient (<italic>d</italic>
<sub>2</sub>) shows a null improvement. This result is consistent since the surgeon has no additional information (i.e., laparoscopic camera view) to change the position since the first mark is drawn with the pen until the first incision is made as explained in the introduction; it is usually located one centimeter above the navel, so a minimal correction is made with or without the system.</p><p>As shown in <xref ref-type="table" rid="tab3">Table 3</xref>, the system improved the trocar placement accuracy by 33%, while variability was reduced by 65%. The use of an augmented reality system can be helpful in complex situations by providing additional information, where even the use of an internal camera view is not enough for the required accuracy (and more incisions than necessary may be made as mentioned in <xref ref-type="sec" rid="sec1">Section 1</xref>). Another advantage of using the augmented reality system is its low cost and its applicability. When the surgeon has the internal information provided by the laparoscopic camera which has already been introduced into the patient, they have a finite and limited time to make the rest of the incisions. The longer the decision time, the higher the costs and the greater the risks for the patient. The augmented reality system is useful even in the hours before the operation (when the patient is awake and out of risk) making it possible to plan and reduce the time spent in the placement of the trocar in the operating theater. The augmented reality system also has direct application for automating and optimizing the trocar placement for guided surgery.</p><p>When (<xref ref-type="disp-formula" rid="EEq2">2</xref>) and (<xref ref-type="disp-formula" rid="EEq3">3</xref>) are analyzed, errors introduced by the augmented reality system (<italic>d</italic>
<sub>ra</sub>, <italic>d</italic>
<sub>segment</sub>, and <italic>d</italic>
<sub>other</sub>) are much lower than the correction offered by the system (<italic>d</italic>
<sub>i</sub>). The correction (in absolute value) achieved by the system in this work was on the order of <italic>d</italic> = 25&#x02009;mm, while our AR algorithms introduced an error <italic>d</italic>
<sub>ra</sub> = 2.91&#x02009;mm. If the correction or distance &#x0201c;<italic>d</italic>
<sub><italic>i</italic></sub>&#x0201d; is the consequence of displacement produced by the technique of pneumoperitoneum and/or the subjectivity of the surgeon and/or the patient anatomical particularities, the system helps to correct the subjectivity and the particularities. The deformity and displacement that the pneumoperitoneum technique (<italic>d</italic>
<sub>neum</sub>) produces could be solved if the 3D model was deformed the same way as the real deformations using a biomechanical or predictive model [<xref rid="B36" ref-type="bibr">36</xref>]. Since the augmented reality system offers an internal view of the patient's organs, it is hoped that the system can help to accurately determinate the displacement (<italic>d</italic>
<sub>patient</sub>) that is currently corrected with the laparoscopic camera by introducing references that are not visible when the initial marks are made.</p><p>It is difficult to make a direct comparison with the literature results because different scenario particularities, the methods, the surgeries, and the type of &#x0201c;patient&#x0201d; involved in each validation (phantom, animals, or humans) as it was introduced in <xref ref-type="sec" rid="sec1">Section 1</xref>. Most of the authors validate their systems using numerical methods and/or using phantoms but few of them are evaluated in clinical settings with real patients, showing that the integration of augmented reality technology into the clinical environment and workflow is not common [<xref rid="B9" ref-type="bibr">9</xref>&#x02013;<xref rid="B15" ref-type="bibr">15</xref>, <xref rid="B19" ref-type="bibr">19</xref>&#x02013;<xref rid="B26" ref-type="bibr">26</xref>]. It is often not feasible to evaluate a system based on surgical outcome or the impact of the system on the patient, but it is possible to evaluate these systems indirectly in phantoms and/or controlled environments [<xref rid="B7" ref-type="bibr">7</xref>]. The contribution of our work is that the system is validated and evaluated in a real environment with patients, and the benefits of using an AR system are demonstrated in a more realistic manner.</p></sec></body><back><ack><title>Acknowledgments</title><p>This work has been supported by <italic>Centro para el Desarrollo Tecnol&#x000f3;gico Industrial</italic> (CDTI) under the project Oncotic (IDI-20101153) and the <italic>Hospital Clinica Benidorm</italic> (HCB) and partially supported by the Ministry of Education and Science of Spain (TIN2010-20999-C04-01), the project Consolider-C (SEJ2006-14301/PSIC) and the &#x0201c;CIBER of Physiopathology of Obesity Nutrition, an initiative of ISCIII&#x0201d; Prometheus and Excellence Research Program (<italic>Generalitat Valenciana</italic>, Department of Education, 2008-157). The authors would like to express their gratitude to the <italic>Hospital Clinica Benidorm</italic> and to the <italic>Hospital Univeritari i Polit&#x000e8;cnic la Fe</italic> (especially the surgical team) for their participation and involvement in this work.</p></ack><ref-list><ref id="B1"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>AG</given-names></name><name><surname>Taylor</surname><given-names>PJ</given-names></name><name><surname>Royston</surname><given-names>C</given-names></name></person-group><source><italic>Practical Laparoscopy</italic></source><year>1993</year><publisher-name>Blackwell Scientific</publisher-name></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rowe</surname><given-names>CK</given-names></name><name><surname>Pierce</surname><given-names>MW</given-names></name><name><surname>Tecci</surname><given-names>KC</given-names></name><etal/></person-group><article-title>A comparative direct cost analysis of pediatric urologic robot-assisted laparoscopic surgery versus open surgery: could robot-assisted surgery be less expensive</article-title><source><italic>Journal of Endourology</italic></source><year>2012</year><volume>26</volume><issue>2</issue><fpage>871</fpage><lpage>877</lpage><pub-id pub-id-type="pmid">22283146</pub-id></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Feuerstein</surname><given-names>M</given-names></name></person-group><source><italic>Augmented reality in laparoscopic surgery new concepts for intraoperative multimodal imaging [Ph.D. thesis]</italic></source><year>2007</year><publisher-name>Fakult&#x000e4;t f&#x000fc;r Informatik Technische Universit&#x000e4;t M&#x000fc;nchen</publisher-name></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azuma</surname><given-names>RT</given-names></name></person-group><article-title>A survey of augmented reality</article-title><source><italic>Presence</italic></source><year>1997</year><volume>6</volume><issue>4</issue><fpage>355</fpage><lpage>385</lpage><pub-id pub-id-type="other">2-s2.0-0039560342</pub-id></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Samset</surname><given-names>E</given-names></name><name><surname>Schmalstieg</surname><given-names>D</given-names></name><name><surname>Sloten</surname><given-names>JV</given-names></name><etal/></person-group><article-title>Augmented reality in surgical procedures</article-title><conf-name>Human Vision and Electronic Imaging</conf-name><conf-date>January 2008</conf-date><series>Proceedings of SPIE</series><pub-id pub-id-type="other">2-s2.0-41149124404</pub-id></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shuhaiber</surname><given-names>JH</given-names></name></person-group><article-title>Augmented reality in surgery</article-title><source><italic>Archives of Surgery</italic></source><year>2004</year><volume>139</volume><issue>2</issue><fpage>170</fpage><lpage>174</lpage><pub-id pub-id-type="other">2-s2.0-0842309828</pub-id><pub-id pub-id-type="pmid">14769575</pub-id></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten-Oertel</surname><given-names>M</given-names></name><name><surname>Jannin</surname><given-names>P</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><article-title>DVV: A taxonomy for mixed reality visualization in image guided surgery</article-title><source><italic>IEEE Transactions on Visualization and Computer Graphics</italic></source><year>2012</year><volume>18</volume><issue>2</issue><fpage>332</fpage><lpage>352</lpage><pub-id pub-id-type="other">2-s2.0-83855163506</pub-id><pub-id pub-id-type="pmid">21383411</pub-id></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>AM</given-names></name><name><surname>Dey</surname><given-names>D</given-names></name><name><surname>Drangova</surname><given-names>M</given-names></name><name><surname>Boyd</surname><given-names>WD</given-names></name><name><surname>Peters</surname><given-names>TM</given-names></name></person-group><article-title>3-D image guidance for minimally invasive robotic coronary artery bypass</article-title><source><italic>The Heart Surgery Forum</italic></source><year>2000</year><volume>3</volume><issue>3</issue><fpage>224</fpage><lpage>231</lpage><pub-id pub-id-type="other">2-s2.0-0001286623</pub-id><pub-id pub-id-type="pmid">11074977</pub-id></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>JW</given-names></name><name><surname>Stoll</surname><given-names>JA</given-names></name><name><surname>Selha</surname><given-names>SD</given-names></name><name><surname>Dupont</surname><given-names>PE</given-names></name><name><surname>Howe</surname><given-names>RD</given-names></name><name><surname>Torchiana</surname><given-names>DF</given-names></name></person-group><article-title>Port placement planning in robot-assisted coronary artery bypass</article-title><source><italic>IEEE Transactions on Robotics and Automation</italic></source><year>2003</year><volume>19</volume><issue>5</issue><fpage>912</fpage><lpage>917</lpage><pub-id pub-id-type="other">2-s2.0-0142227187</pub-id><pub-id pub-id-type="pmid">22287831</pub-id></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adhami</surname><given-names>L</given-names></name><name><surname>Coste-Mani&#x000e8;re</surname><given-names>&#x000c8;</given-names></name></person-group><article-title>Optimal planning for minimally invasive surgical robots</article-title><source><italic>IEEE Transactions on Robotics and Automation</italic></source><year>2003</year><volume>19</volume><issue>5</issue><fpage>854</fpage><lpage>863</lpage><pub-id pub-id-type="other">2-s2.0-0142227191</pub-id></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Scheuering</surname><given-names>M</given-names></name><name><surname>Schenk</surname><given-names>A</given-names></name><name><surname>Schneider</surname><given-names>A</given-names></name><name><surname>Preim</surname><given-names>B</given-names></name><name><surname>Greiner</surname><given-names>G</given-names></name></person-group><article-title>Intraoperative augmented reality for minimally invasive liver interventions</article-title><conf-name>Medical Imaging: Visualization, Image-Guided Procedures and Display</conf-name><conf-date>February 2003</conf-date><fpage>407</fpage><lpage>417</lpage><series>Proceedings of SPIE</series><pub-id pub-id-type="other">2-s2.0-0041624102</pub-id></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bichlmeier</surname><given-names>C</given-names></name><name><surname>Heining</surname><given-names>SM</given-names></name><name><surname>Feuerstein</surname><given-names>M</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name></person-group><article-title>The virtual mirror: a new interaction paradigm for augmented reality environments</article-title><source><italic>IEEE Transactions on Medical Imaging</italic></source><year>2009</year><volume>28</volume><issue>9</issue><fpage>1498</fpage><lpage>1510</lpage><pub-id pub-id-type="other">2-s2.0-77953084095</pub-id><pub-id pub-id-type="pmid">19336291</pub-id></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feuerstein</surname><given-names>M</given-names></name><name><surname>Mussack</surname><given-names>T</given-names></name><name><surname>Heining</surname><given-names>SM</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name></person-group><article-title>Intraoperative laparoscope augmentation for port placement and resection planning in minimally invasive liver resection</article-title><source><italic>IEEE Transactions on Medical Imaging</italic></source><year>2008</year><volume>27</volume><issue>3</issue><fpage>355</fpage><lpage>369</lpage><pub-id pub-id-type="other">2-s2.0-40149097958</pub-id><pub-id pub-id-type="pmid">18334431</pub-id></element-citation></ref><ref id="B14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Volonte</surname><given-names>F</given-names></name><name><surname>Bucher</surname><given-names>P</given-names></name><name><surname>Pugin</surname><given-names>F</given-names></name><etal/></person-group><article-title>Mixed reality for laparoscopic distal pancreatic resection</article-title><source><italic>International Journal of Computer Assisted Radiology and Surgery</italic></source><year>2010</year><volume>5</volume><issue>1</issue><fpage>122</fpage><lpage>130</lpage></element-citation></ref><ref id="B15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrari</surname><given-names>V</given-names></name><name><surname>Megali</surname><given-names>G</given-names></name><name><surname>Troia</surname><given-names>E</given-names></name><name><surname>Pietrabissa</surname><given-names>A</given-names></name><name><surname>Mosca</surname><given-names>F</given-names></name></person-group><article-title>A 3-D mixed-reality system for stereoscopic visualization of medical dataset</article-title><source><italic>IEEE Transactions on Biomedical Engineering</italic></source><year>2009</year><volume>56</volume><issue>11</issue><fpage>2627</fpage><lpage>2633</lpage><pub-id pub-id-type="other">2-s2.0-70449378573</pub-id><pub-id pub-id-type="pmid">19651551</pub-id></element-citation></ref><ref id="B16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McSherry</surname><given-names>CK</given-names></name></person-group><article-title>Cholecystectomy: the gold standard</article-title><source><italic>The American Journal of Surgery</italic></source><year>1989</year><volume>158</volume><issue>3</issue><fpage>174</fpage><lpage>178</lpage><pub-id pub-id-type="other">2-s2.0-0024342388</pub-id><pub-id pub-id-type="pmid">2672837</pub-id></element-citation></ref><ref id="B17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kum</surname><given-names>C-K</given-names></name><name><surname>Eypasch</surname><given-names>E</given-names></name><name><surname>Aljaziri</surname><given-names>A</given-names></name><name><surname>Troidl</surname><given-names>H</given-names></name></person-group><article-title>Randomized comparison of pulmonary function after the &#x02019;French&#x02019; and &#x02019;American&#x02019; techniques of laparoscopic cholecystectomy</article-title><source><italic>British Journal of Surgery</italic></source><year>1996</year><volume>83</volume><issue>7</issue><fpage>938</fpage><lpage>941</lpage><pub-id pub-id-type="other">2-s2.0-0029931560</pub-id><pub-id pub-id-type="pmid">8813779</pub-id></element-citation></ref><ref id="B18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mart&#x000ed;nez-Mart&#x000ed;nez</surname><given-names>F</given-names></name><name><surname>Rup&#x000e9;rez</surname><given-names>MJ</given-names></name><name><surname>Lago</surname><given-names>MA</given-names></name><name><surname>L&#x000f3;pez-Mir</surname><given-names>F</given-names></name><name><surname>Monserrat</surname><given-names>C</given-names></name><name><surname>Alca&#x000f1;&#x000ed;z</surname><given-names>M</given-names></name></person-group><article-title>Pneumoperitoneum technique simulation in laparoscopic surgery on lamb liver samples and 3D reconstruction</article-title><source><italic>Studies in Health Technology and Information</italic></source><year>2010</year><volume>18</volume><fpage>348</fpage><lpage>350</lpage></element-citation></ref><ref id="B19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sch&#x000f6;nfelder</surname><given-names>C</given-names></name><name><surname>Stark</surname><given-names>T</given-names></name><name><surname>Kahrs</surname><given-names>L</given-names></name><etal/></person-group><article-title>Port visualization for laparoscopic surgery-setup and first intraoperative evaluation</article-title><source><italic>International Journal of Computer Assisted Radiology and Surgery</italic></source><year>2008</year><volume>3</volume><issue>1</issue><fpage>141</fpage><lpage>142</lpage></element-citation></ref><ref id="B20"><label>20</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simitopoulos</surname><given-names>D</given-names></name><name><surname>Kosaka</surname><given-names>A</given-names></name></person-group><article-title>An augmented reality system for surgical navigation</article-title><conf-name>Proceedings of the International Conference on Augmented, Virtual Environments and Three-dimensional Imaging</conf-name><conf-date>May 2001</conf-date><fpage>152</fpage><lpage>156</lpage></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glossop</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wedlake</surname><given-names>C</given-names></name><name><surname>Moore</surname><given-names>J</given-names></name><name><surname>Peters</surname><given-names>T</given-names></name></person-group><article-title>Augmented reality laser projection device for surgery</article-title><source><italic>Studies in Health Technology and Informatics</italic></source><year>2004</year><volume>98</volume><fpage>104</fpage><lpage>110</lpage><pub-id pub-id-type="other">2-s2.0-16544390019</pub-id><pub-id pub-id-type="pmid">15544252</pub-id></element-citation></ref><ref id="B22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mischkowski</surname><given-names>RA</given-names></name><name><surname>Zinser</surname><given-names>MJ</given-names></name><name><surname>K&#x000fc;bler</surname><given-names>AC</given-names></name><name><surname>Krug</surname><given-names>B</given-names></name><name><surname>Seifert</surname><given-names>U</given-names></name><name><surname>Z&#x000f6;ller</surname><given-names>JE</given-names></name></person-group><article-title>Application of an augmented reality tool for maxillary positioning in orthognathic surgery: a feasibility study</article-title><source><italic>Journal of Cranio-Maxillofacial Surgery</italic></source><year>2006</year><volume>34</volume><issue>8</issue><fpage>478</fpage><lpage>483</lpage><pub-id pub-id-type="other">2-s2.0-33845526039</pub-id><pub-id pub-id-type="pmid">17157519</pub-id></element-citation></ref><ref id="B23"><label>23</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>King</surname><given-names>BW</given-names></name><name><surname>Reisner</surname><given-names>LA</given-names></name><name><surname>Klein</surname><given-names>MD</given-names></name><name><surname>Auner</surname><given-names>GW</given-names></name><name><surname>Pandya</surname><given-names>AK</given-names></name></person-group><article-title>Registered, sensor-integrated virtual reality for surgical applications</article-title><conf-name>Proceedings of the IEEE Virtual Reality Conference (VR &#x02019;07)</conf-name><conf-date>March 2007</conf-date><conf-loc>Charlotte, Calif, USA</conf-loc><fpage>277</fpage><lpage>278</lpage><pub-id pub-id-type="other">2-s2.0-34547660091</pub-id></element-citation></ref><ref id="B24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawamata</surname><given-names>T</given-names></name><name><surname>Iseki</surname><given-names>H</given-names></name><name><surname>Shibasaki</surname><given-names>T</given-names></name><etal/></person-group><article-title>Endoscopic augmented reality navigation system for endonasal transsphenoidal surgery to treat pituitary tumors: technical note</article-title><source><italic>Neurosurgery</italic></source><year>2002</year><volume>50</volume><issue>6</issue><fpage>1393</fpage><lpage>1397</lpage><pub-id pub-id-type="other">2-s2.0-0036626677</pub-id><pub-id pub-id-type="pmid">12015864</pub-id></element-citation></ref><ref id="B25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogt</surname><given-names>S</given-names></name><name><surname>Khamene</surname><given-names>A</given-names></name><name><surname>Sauer</surname><given-names>F</given-names></name></person-group><article-title>Reality augmentation for medical procedures: system architecture, single camera marker tracking, and system evaluation</article-title><source><italic>International Journal of Computer Vision</italic></source><year>2006</year><volume>70</volume><issue>2</issue><fpage>179</fpage><lpage>190</lpage><pub-id pub-id-type="other">2-s2.0-33746471660</pub-id></element-citation></ref><ref id="B26"><label>26</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fuertes</surname><given-names>JJ</given-names></name><name><surname>L&#x000f3;pez-Mir</surname><given-names>F</given-names></name><name><surname>Naranjo</surname><given-names>V</given-names></name><name><surname>Ortega</surname><given-names>M</given-names></name><name><surname>Villanueva</surname><given-names>E</given-names></name><name><surname>Alca&#x000f1;iz</surname><given-names>M</given-names></name></person-group><article-title>Augmented reality system for keyhole surgery: performance and accuracy validation</article-title><conf-name>Proceedings of the International Conference on Computer Graphics Theory and Applications (GRAPP '11)</conf-name><conf-date>March 2011</conf-date><conf-loc>Algarve, Portugal</conf-loc><fpage>273</fpage><lpage>279</lpage><pub-id pub-id-type="other">2-s2.0-79960234827</pub-id></element-citation></ref><ref id="B27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolau</surname><given-names>S</given-names></name><name><surname>Soler</surname><given-names>L</given-names></name><name><surname>Mutter</surname><given-names>D</given-names></name><name><surname>Marescaux</surname><given-names>J</given-names></name></person-group><article-title>Augmented reality in laparoscopic surgical oncology</article-title><source><italic>Surgical Oncology</italic></source><year>2011</year><volume>20</volume><issue>3</issue><fpage>189</fpage><lpage>201</lpage><pub-id pub-id-type="other">2-s2.0-80051872997</pub-id><pub-id pub-id-type="pmid">21802281</pub-id></element-citation></ref><ref id="B28"><label>28</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>L&#x000f3;pez-Mir</surname><given-names>F</given-names></name><name><surname>Naranjo</surname><given-names>V</given-names></name><name><surname>Angulo</surname><given-names>J</given-names></name><name><surname>Villanueva</surname><given-names>E</given-names></name><name><surname>Alca&#x000f1;iz</surname><given-names>M</given-names></name><name><surname>L&#x000f3;pez-Celada</surname><given-names>S</given-names></name></person-group><article-title>Aorta segmentation using the watershed algorithm for an augmented reality system in laparoscopic surgery</article-title><conf-name>Proceedings of the IEEE International Conference on Image Processing</conf-name><conf-date>September 2011</conf-date><conf-loc>Brussels, Belgium</conf-loc><fpage>2705</fpage><lpage>2708</lpage></element-citation></ref><ref id="B29"><label>29</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Iba&#x000f1;ez</surname><given-names>L</given-names></name><name><surname>Schroeder</surname><given-names>W</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Cates</surname><given-names>J</given-names></name></person-group><source><italic>The ITK Sotware Guide</italic></source><year>2005</year><publisher-name>Kitware</publisher-name></element-citation></ref><ref id="B30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><article-title>A flexible new technique for camera calibration</article-title><source><italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic></source><year>2000</year><volume>22</volume><issue>11</issue><fpage>1330</fpage><lpage>1334</lpage><pub-id pub-id-type="other">2-s2.0-0000879124</pub-id></element-citation></ref><ref id="B31"><label>31</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pintaric</surname><given-names>T</given-names></name></person-group><article-title>An adaptive thresholding algorithm for augmented reality toolkit</article-title><conf-name>Proceedings of the 2nd IEEE International Augmented Reality Toolkit Workshop (ART '03)</conf-name><conf-date>2003</conf-date></element-citation></ref><ref id="B32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin-Gutierrez</surname><given-names>J</given-names></name><name><surname>Saorin</surname><given-names>JL</given-names></name><name><surname>Contero</surname><given-names>M</given-names></name><name><surname>Alca&#x000f1;iz</surname><given-names>M</given-names></name><name><surname>P&#x000e9;rez-L&#x000f3;pez</surname><given-names>D</given-names></name><name><surname>Ortega</surname><given-names>M</given-names></name></person-group><article-title>Education: design and validation of an augmented book for spatial abilities development in engineering students</article-title><source><italic>Computers and Graphics</italic></source><year>2010</year><volume>34</volume><issue>1</issue><fpage>77</fpage><lpage>91</lpage></element-citation></ref><ref id="B33"><label>33</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hartley</surname><given-names>R</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Multiple View Geometry</article-title><source><italic>Computer Vision</italic></source><year>2003</year><edition>2nd edition</edition><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="B34"><label>34</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>G</given-names></name><name><surname>Fitzgibbon</surname><given-names>AW</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Markerless tracking using planar structures in the scene</article-title><conf-name>Proceedings of the IEEE and ACM International Symposium on Augmented Reality (ISAR '00)</conf-name><conf-date>2000</conf-date><fpage>120</fpage><lpage>128</lpage></element-citation></ref><ref id="B35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marquardt</surname><given-names>DW</given-names></name></person-group><article-title>An algorithm for least-squares estimation of nonlinear parameters</article-title><source><italic>Journal of the Society for Industrial and Applied Mathematics</italic></source><year>1963</year><volume>11</volume><issue>2</issue><fpage>431</fpage><lpage>441</lpage></element-citation></ref><ref id="B36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mart&#x000ed;nez-Mart&#x000ed;nez</surname><given-names>F</given-names></name><name><surname>Lago</surname><given-names>MA</given-names></name><name><surname>Rup&#x000e9;rez</surname><given-names>MJ</given-names></name><name><surname>Monserrat</surname><given-names>C</given-names></name></person-group><article-title>Analysis of several biomechanical models for the simulation of lamb liver behavior using similarity coefficients from medical image</article-title><source><italic>Computer Methods in Biomechanics and Biomedical Engineering</italic></source><year>2012</year><fpage>1</fpage><lpage>11</lpage></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Magnetic resonance machine. Magnetic resonance images and 3D model of abdominal organs. Registration and fusion with real-time video in a phantom.</p></caption><graphic xlink:href="BMRI2013-758491.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Change in the coordinate system.</p></caption><graphic xlink:href="BMRI2013-758491.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Checkerboard that is used to calibrate the camera. A Logitech QuickCam Pro 9000 webcam.</p></caption><graphic xlink:href="BMRI2013-758491.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>(a) Hexadecimal binary mark and (b) fusion of virtual model in a phantom.</p></caption><graphic xlink:href="BMRI2013-758491.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>CT Jar images, 3D Jar model, marker, and registration and fusion of 3D virtual jar and real jar.</p></caption><graphic xlink:href="BMRI2013-758491.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Marks in a simulation patient and marks made with the biocompatible pen (red) and real incisions (blue).</p></caption><graphic xlink:href="BMRI2013-758491.006"/></fig><fig id="fig7" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Hardware device and software system.</p></caption><graphic xlink:href="BMRI2013-758491.007"/></fig><fig id="fig8" orientation="portrait" position="float"><label>Figure 8</label><caption><p> Performing the marks in two different simulation patients.</p></caption><graphic xlink:href="BMRI2013-758491.008"/></fig><fig id="fig9" orientation="portrait" position="float"><label>Figure 9</label><caption><p>Surgeon view and image fusion in a real environment with a simulation patient.</p></caption><graphic xlink:href="BMRI2013-758491.009"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Advantages and disadvantages of different visualization devices.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Device</th><th align="left" rowspan="1" colspan="1">Advantages</th><th align="left" rowspan="1" colspan="1">Disadvantages</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Projector</td><td align="left" rowspan="1" colspan="1">Direct vision of 3D model onto the patient, free movement of the surgeon in the scene, and easy hand-eye training. </td><td align="left" rowspan="1" colspan="1">Low resolution, ambient light in the operating theater that reduces projection sharpness, heat and air ventilation that are dangerous in sterile environments, shadow effect, and setup just above the scene that entails an associated risk. </td></tr><tr><td align="left" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">AR glasses</td><td align="left" rowspan="1" colspan="1">High grade of immersion and good mobility of the surgeon in the scene but more reduced in comparison to the projector. </td><td align="left" rowspan="1" colspan="1">Possibility of dizziness, decrease of reality perception and loss of depth, additional material in the surgeon's field of view, and feeling of stress until surgeon gets used to it.</td></tr><tr><td align="left" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Monitor </td><td align="left" rowspan="1" colspan="1">Harmless for patient and surgeon, free movement in the scene (the screen can be placed farther from the doctor), easy to sterilize, and high resolution.</td><td align="left" rowspan="1" colspan="1">Hand-eye training since the surgeon must look up to see the monitor.</td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Results of Experiment 1 (without the system).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>1</sub>
</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>2</sub>
</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>3</sub>
</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>4</sub>
</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Average (cm)</td><td align="center" rowspan="1" colspan="1">1.13 </td><td align="center" rowspan="1" colspan="1">0.13</td><td align="center" rowspan="1" colspan="1">1.88</td><td align="center" rowspan="1" colspan="1">1.00</td></tr><tr><td align="left" rowspan="1" colspan="1">Stan. deviat. (cm)</td><td align="center" rowspan="1" colspan="1">1.13</td><td align="center" rowspan="1" colspan="1">0.35</td><td align="center" rowspan="1" colspan="1">1.96</td><td align="center" rowspan="1" colspan="1">1.07</td></tr></tbody></table></table-wrap><table-wrap id="tab3" orientation="portrait" position="float"><label>Table 3</label><caption><p>Results of Experiment 2 (with the system).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>1</sub>
</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>2</sub>
</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>3</sub>
</th><th align="center" rowspan="1" colspan="1">
<italic>d</italic>
<sub>4</sub>
</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Average (cm)</td><td align="center" rowspan="1" colspan="1">
<bold>0.87</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.12</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>1.00</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.98</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Stan. deviat. (cm)</td><td align="center" rowspan="1" colspan="1">
<bold>0.53</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.35</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.53</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>0.35</bold>
</td></tr></tbody></table></table-wrap><table-wrap id="tab4" orientation="portrait" position="float"><label>Table 4</label><caption><p>Global result of the experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1">Without system</th><th align="center" rowspan="1" colspan="1">With system </th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Average (cm)</td><td align="center" rowspan="1" colspan="1">1.33</td><td align="center" rowspan="1" colspan="1">
<bold>0.87</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Stan. deviat. (cm)</td><td align="center" rowspan="1" colspan="1">1.43</td><td align="center" rowspan="1" colspan="1">
<bold>0.53</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>
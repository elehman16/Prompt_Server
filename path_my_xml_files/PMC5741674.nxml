<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Integr Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Integr Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Integr. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Integrative Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1662-5145</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">29326564</article-id><article-id pub-id-type="pmc">5741674</article-id><article-id pub-id-type="doi">10.3389/fnint.2017.00036</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Audio Motor Training at the Foot Level Improves Space Representation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Aggius-Vella</surname><given-names>Elena</given-names></name><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/387955/overview"/><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author"><name><surname>Campus</surname><given-names>Claudio</given-names></name><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/295039/overview"/><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author"><name><surname>Finocchietti</surname><given-names>Sara</given-names></name><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/228698/overview"/><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author"><name><surname>Gori</surname><given-names>Monica</given-names></name><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/25629/overview"/><xref ref-type="aff" rid="aff1"/></contrib></contrib-group><aff id="aff1"><institution>Unit for Visually Impaired People (U-VIP), Center for Human Technologies, Fondazione Istituto Italiano di Tecnologia</institution>, <addr-line>Genoa</addr-line>, <country>Italy</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Nadia Bolognini, Universit&#x000e0; degli Studi di Milano Bicocca, Italy</p></fn><fn fn-type="edited-by"><p>Reviewed by: Andrew Joseph Kolarik, University of London, United Kingdom; Caterina Bertini, Universit&#x000e0; di Bologna, Italy</p></fn><corresp id="fn001">*Correspondence: Monica Gori <email xlink:type="simple">monica.gori@iit.it</email></corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>12</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>11</volume><elocation-id>36</elocation-id><history><date date-type="received"><day>14</day><month>9</month><year>2017</year></date><date date-type="accepted"><day>05</day><month>12</month><year>2017</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2017 Aggius-Vella, Campus, Finocchietti and Gori.</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Aggius-Vella, Campus, Finocchietti and Gori</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Spatial representation is developed thanks to the integration of visual signals with the other senses. It has been shown that the lack of vision compromises the development of some spatial representations. In this study we tested the effect of a new rehabilitation device called ABBI (Audio Bracelet for Blind Interaction) to improve space representation. ABBI produces an audio feedback linked to body movement. Previous studies from our group showed that this device improves the spatial representation of space in early blind adults around the upper part of the body. Here we evaluate whether the audio motor feedback produced by ABBI can also improve audio spatial representation of sighted individuals in the space around the legs. Forty five blindfolded sighted subjects participated in the study, subdivided into three experimental groups. An audio space localization (front-back discrimination) task was performed twice by all groups of subjects before and after different kind of training conditions. A group (experimental) performed an audio-motor training with the ABBI device placed on their foot. Another group (control) performed a free motor activity without audio feedback associated with body movement. The other group (control) passively listened to the ABBI sound moved at foot level by the experimenter without producing any body movement. Results showed that only the experimental group, which performed the training with the audio-motor feedback, showed an improvement in accuracy for sound discrimination. No improvement was observed for the two control groups. These findings suggest that the audio-motor training with ABBI improves audio space perception also in the space around the legs in sighted individuals. This result provides important inputs for the rehabilitation of the space representations in the lower part of the body.</p></abstract><kwd-group><kwd>rehabilitation</kwd><kwd>spatial cognition</kwd><kwd>blind</kwd><kwd>multisensory integration</kwd><kwd>hearing</kwd></kwd-group><counts><fig-count count="5"/><table-count count="1"/><equation-count count="3"/><ref-count count="69"/><page-count count="9"/><word-count count="6926"/></counts></article-meta></front><body><sec sec-type="introduction" id="s1"><title>Introduction</title><p>From childhood, the brain is constantly stimulated by different sensory information coming from the external world. Vision has a predominant role in the development of spatial cognition (Pasqualotto and Proulx, <xref rid="B51" ref-type="bibr">2012</xref>; Gori, <xref rid="B27" ref-type="bibr">2015</xref>). In agreement with this idea, it has been found that blind people are impaired in some aspects of sound localization, such as the localization of end point of a dynamic sound (Finocchietti et al., <xref rid="B21" ref-type="bibr">2015b</xref>), the audio space bisection (Gori et al., <xref rid="B29" ref-type="bibr">2014</xref>), the evaluation of the absolute distance (Kolarik et al., <xref rid="B37" ref-type="bibr">2013b</xref>), the auditory spatial representations of the extrapersonal space in both: reverberant and anechoic environments, for speech, music and noise signals (Kolarik et al., <xref rid="B38" ref-type="bibr">2017</xref>) and the vertical localization of a sound source (Zwiers et al., <xref rid="B69" ref-type="bibr">2001</xref>). On the other hand, it has been shown that the absence of vision, as in blind individuals, improves other auditory skills, such as horizontal sound localization (Lessard et al., <xref rid="B41" ref-type="bibr">1998</xref>; King and Parsons, <xref rid="B34" ref-type="bibr">1999</xref>; R&#x000f6;der et al., <xref rid="B53" ref-type="bibr">1999</xref>; Gougoux et al., <xref rid="B30" ref-type="bibr">2004</xref>; Doucet et al., <xref rid="B18" ref-type="bibr">2005</xref>; Lewald, <xref rid="B43" ref-type="bibr">2007</xref>) and relative distance discrimination (Voss et al., <xref rid="B63" ref-type="bibr">2004</xref>; Kolarik et al., <xref rid="B36" ref-type="bibr">2013a</xref>). The reason why some auditory spatial skills are enhanced and other impaired in blind individuals is still an open question. Similarly, the effect of sensory loss on cortical activity is still matter of debate. Some studies reported that when the most appropriate sense for a specific ability is lacking, such as vision in spatial cognition, the silent pre-existing connection is revealed and leads to new strong connections (Amedi and Meijer, <xref rid="B2" ref-type="bibr">2005</xref>; Dahmen and King, <xref rid="B16" ref-type="bibr">2007</xref>). This thesis is supported by several imaging studies (Paus, <xref rid="B52" ref-type="bibr">1996</xref>; Gougoux et al., <xref rid="B31" ref-type="bibr">2005</xref>; Voss et al., <xref rid="B62" ref-type="bibr">2006</xref>; Martuzzi et al., <xref rid="B46" ref-type="bibr">2007</xref>; Eckert et al., <xref rid="B19" ref-type="bibr">2008</xref>; Frasnelli et al., <xref rid="B25" ref-type="bibr">2011</xref>). However, other imaging studies provided an evidence for reduced connectivity between visual and auditory systems, as well as between visual and somatosensory systems (Liu et al., <xref rid="B44" ref-type="bibr">2007</xref>; Yu et al., <xref rid="B68" ref-type="bibr">2012</xref>; Burton et al., <xref rid="B11" ref-type="bibr">2014</xref>), supporting instead the idea that these heightened abilities reflect re-programming of visual cortex for &#x0201c;metamodal&#x0201d; purpose (Burton et al., <xref rid="B11" ref-type="bibr">2014</xref>).</p><p>These findings support the idea indicating that the lack of visual experience interferes with the development of some spatial representations. Multiple rehabilitation procedures and devices have been developed to date to improve inclusion of blind individuals by exploiting audio and tactile channels. We have recently developed a new device called ABBI (The Audio Bracelet for Blind Interaction; Finocchietti et al., <xref rid="B23" ref-type="bibr">2015a</xref>; Gori et al., <xref rid="B28" ref-type="bibr">2016</xref>; Ben Porquis et al., <xref rid="B26" ref-type="bibr">2017</xref>). ABBI is an audio bracelet that provides audio feedback to body movement. Recent results from our group suggest that the use of ABBI improves mobility and spatial cognition in visually impaired children and adults (Cappagli et al., <xref rid="B13" ref-type="bibr">2017</xref>; Finocchietti et al., <xref rid="B22" ref-type="bibr">2017</xref>). This result is in agreement with previous works which have shown that sensory-motor learning is not sensory-modality-specific, but that a novel sensory-motor information can be transferred between sensory modalities (Levy-Tzedek et al., <xref rid="B42" ref-type="bibr">2012</xref>). We can speculate that the use of ABBI could allow the creation of a strict link between auditory and motor signals. The new sensory (audio) feedback to body movement might create a bridge between body and external representations in blind individuals by helping the creation of more complex spatial representations of the environment. This idea is in agreement with recent studies showing that in blind individuals the body can be used as a spatial reference to improve audio spatial representations (Vercillo et al., <xref rid="B61" ref-type="bibr">2017</xref>).</p><p>While previous works from our group mainly focused on the recalibration of spatial representations around the upper body portion of space in blind individuals (Finocchietti et al., <xref rid="B22" ref-type="bibr">2017</xref>), no studies have investigated whether the use of this device can be also useful to improve spatial representations around the lower body part in sighted individuals. Improvement of space representation at the lower body part would be important for the rehabilitation of locomotion and legs mobility functions in individuals with motor disabilities. With the aim of improving space representation around the lower body portion in sighted individuals, here we studied their audio space representation before and after a training with ABBI positioned on the subject&#x02019;s foot.</p><p>In order to investigate an improvement of audio spatial precision, we used an audio task for humans that is the front-back sound discrimination. Front-back spatial perceptual ambiguity is known as the cone of confusion (Wallach, <xref rid="B64" ref-type="bibr">1938</xref>), an imaginary cone extending outward from each ear, representing sound source locations producing the same interaural differences. It has been shown that head movements help in discriminating front from back sounds, as it affects inter temporal delay (ITD) and inter level difference (ILD; Wightman and Kistler, <xref rid="B67" ref-type="bibr">1999</xref>). An audio front-back discrimination task around the legs was performed in all subjects before and after the training by asking the subjects to judge if a sound was delivered in the frontal or back space. Forty five sighted subjects, split into three groups, performed two sessions of an audio localization task. The experimental group performed 2 min of audio motor training with ABBI between the two audio tests, while no audio motor training was performed by the control groups, where subjects completed just 2 min of free leg movement without sound, or 20 min of passive sound&#x02019;s hearing. We expected that only the experimental group improve in localizing sounds after the training with the ABBI device, suggesting that an audio-motor training delivered at foot level improves the spatial representation around the legs. Our results support our hypothesis by showing an improvement only in the experimental group. These results suggest that, as hypothesized, the integration of self-generated sounds with proprioceptive-motor information could be used by our brain to improve spatial representation around the legs. These findings open new possibilities for the use of sensory motor trainings in people with spatial and mobility impairments at the leg level.</p></sec><sec sec-type="materials and methods" id="s2"><title>Materials and Methods</title><sec id="s2-1"><title>Subjects</title><p>Forty five participants were enrolled in the study. Subjects were randomly split into three age (<italic>F</italic><sub>(2,42)</sub> = 0.13, <italic>P</italic> = 0.87) and height (<italic>F</italic><sub>(2,42)</sub> = 1.35, <italic>P</italic> = 0.37) matched groups: experimental group, which did the audio motor training (<italic>N</italic> = 15; 11 females, age: 26 &#x000b1; 5, years old, height: 165 &#x000b1; 9) cm; motor control group, which did only motor training (<italic>N</italic> = 15; 5 females, age: 27 &#x000b1; 6 years old, height: 170 &#x000b1; 2) cm and audio control group, which did only audio training (<italic>N</italic> = 15; 7 females, age 26 &#x000b1; 3 years old, height:170 &#x000b1; 1) cm. All the participants had a similar educational background, no cognitive impairments, were right handed, and they reported to haven&#x02019;t any hearing impairment (we administer an online hearing test to be sure all participants had the same hearing perception). The participants provided written informed consent in accordance with the Declaration of Helsinki. The study was approved by the ethics committee of the local health service (Comitato Etico, ASL3 Genovese, Italy).</p></sec><sec id="s2-2"><title>Set-Up and Sound Localization Task</title><p>The experiment was performed in the center of the same dark reverberant room. All participants were positioned in the middle of the room, far from each wall, so that reverberant noise was the same across subjects. As shown in Figure <xref ref-type="fig" rid="F1">1</xref>, the apparatus consisted of 14 speakers split into two arrays of seven speakers each, vertically oriented; the lowest speaker of each array was positioned at 4 cm from the floor, while the others were situated at: 19 cm, 34 cm, 49 cm, 63 cm, 78 cm, the highest being at 85 cm. There were therefore seven equivalent sound elevations in the frontal and rear space. The two arrays were positioned facing each other; one array of speakers was placed in the frontal space (at 40&#x000b0; in relation to the face) and the other one in the rear space (at 160&#x000b0; in relation to the face); both arrays were situated at a distance of 50 cm from the subject&#x02019;s position. During each trial, pink noise lasting 1 s was randomly delivered from one of the 14 speakers. Each speaker delivered the sound in six trials, for a total of 84 trials for each session (42 trials in the frontal space and 42 in the rear space). As our goal was to clarify the representation of auditory space around the legs, we split the seven equivalent speakers into two areas: above the knee space and below the knee space, as shown in Figure <xref ref-type="fig" rid="F1">1</xref>. Above the knee space referred to speakers (numbers 5, 6 and 7-up to 34 cm), while below the knee space (speaker number 4) was represented by (speakers 1, 2 and 3-under 34 cm). We decided to use the knee because it divides the leg into two separate segments, allowing free movement. The knee is also involved in walking and leg actions, and so could influence spatial representation of the two leg segments.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p>Sound localization task: 14 speakers split into two arrays of seven speakers each, vertically oriented; speakers of each array were positioned at 4 cm, 19 cm, 34 cm, 49 cm, 63 cm, 78 cm and 85 cm from the floor, creating seven equivalent sound elevations in the frontal and rear space. The two arrays were positioned facing each other, one in the frontal space and the other in the rear space.</p></caption><graphic xlink:href="fnint-11-00036-g0001"/></fig><p>Participants were blindfolded and led into the experimental room, where they remained standing for the entire session (they were allowed to rest before the training). They were asked to keep their head straight and not to direct it toward the sound. They had to verbally report if sounds were delivered in the frontal or in the back area, without considering their spatial elevations. Subject position and posture were continuously monitored and corrected when necessary by the experimenter. Sounds were administered by a custom-made code in Matlab (R2013a, The Math Works, USA); the experimenter recorded on text the oral answer given by the subject (&#x0201c;Front&#x0201d; or &#x0201c;Back&#x0201d;) for the consequent analysis. The entire experiment was performed at the participant&#x02019;s own pace and each trial started after the subject&#x02019;s answer, without any time constraints.</p></sec><sec id="s2-3"><title>Protocol</title><p>The auditory localization task, as previously described, was performed in two sessions (about 20 min each), spaced out by 2 min of training (Figure <xref ref-type="fig" rid="F1">1</xref>).</p><p>The experimental group underwent audio-motor training with the sound source (digital metronome with single pulse 500 Hz, intermittent sound at 180 bpm), delivered by ABBI, placed on the left ankle; they were asked to move their left leg and consequently the sound, from the frontal position to the rear and vice versa, to freely explore space around the body. It was required a continuous and constant movement. The short timing for the audio-motor training was chosen because a previous study (Finocchietti et al., <xref rid="B22" ref-type="bibr">2017</xref>) showed that the spatial recalibration is fast, thanks to the association of the auditory feedback with a voluntary movement. Two control conditions were performed. One control group (motor control group) performed only the same free leg movement, by repeatedly moving the leg from the front position to the back position (as in the group trained with ABBI) but without audio feedback associated to the movement. The second control group (audio control group) listened to the ABBI sound moved by the experimenter with known position. In this case, the experimenter provided before the sound in front and afterwards in the back (random order) by communicating to the subject the spatial position of the sound: the sound was presented for 1 min in the front and for 1 min in the back. The subjects of this group received the same amount of sound feedback as the experimental group. After the training, all groups performed the second session of the sound localization task.</p></sec><sec id="s2-4"><title>Data Analysis and Statistics</title><p>Localization data were post-processed and analyzed by a custom made program in R (R Development Core Team, New Zealand). We removed speaker number 4 (49 cm) from the analysis, as it was at the edge between space below and above the knee level. The six sound sources remained were grouped into two spatial levels: below the knee (speaker numbers 1, 2 and 3) and above the knee (speaker numbers 5, 6 and 7), <italic>t</italic> test confirmed no differences inside these two spatial portions (Figure <xref ref-type="fig" rid="F2">2</xref>). In order to evaluate the relation between sound localization and the role of senses in representing spaces, we analyzed the pool of single trials using generalized linear mixed models (GLMMs). In this way, we could estimate the variability of fixed and random effects (Moscatelli et al., <xref rid="B48" ref-type="bibr">2012</xref>). We applied GLMM with a logit link function and a binomial distribution. Our model was random-slope (or maximal) following Barr guidelines (Barr et al., <xref rid="B7" ref-type="bibr">2013</xref>; Barr, <xref rid="B6" ref-type="bibr">2013</xref>) and was set for all subjects, taking into account the individual variability in the responses. We set the model to the choices from the localization task using the lme4 package (Bates et al., <xref rid="B8" ref-type="bibr">2015</xref>) in the R statistical language. The model took into account the correct response; to do this, we regressed, in each trial, the answers of each subject considered the correct answer (1 = correct, 0 = incorrect), as a function of sound level (above the knee vs. below the knee), longitudinal position (front vs. back space) and session (pre vs. post) as factors within subjects, while group (experimental vs. motor control and vs. audio control) as factor between subjects. These factors are included in our model as fixed effects. We calculated Analysis of Deviance Tables (using Type II Wald chi-square tests) for the models using the analysis of variance (ANOVA) function of the car package (Fox and Weisberg, <xref rid="B24" ref-type="bibr">2011</xref>). For significant effects, we performed <italic>post hoc</italic> comparisons using lsmeans package (Lenth, <xref rid="B40" ref-type="bibr">2016</xref>), which computes and contrasts least-squares means (predicted marginal means). We adopted Holm P adjustment. Contrasts with <italic>P</italic> &#x0003c; 0.05 were considered as significant (P corrected are reported). Data are presented as mean &#x000b1; standard error.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p>Percentage of correct answers on each speaker: red squares represents frontal speakers, blue squares represents back speakers. Plots represent percentage of correct answers (<italic>x</italic> axis) in the pre (left column) and post (right column) session.</p></caption><graphic xlink:href="fnint-11-00036-g0002"/></fig></sec></sec><sec sec-type="results" id="s3"><title>Results</title><p>Results on the analysis of deviance showed a multiple interaction between longitudinal space (front vs. back), sound level (above the knee vs. below the knee), session (pre vs. post) and groups (experimental, motor and audio control) <inline-formula><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 11.86, <italic>P</italic> = 0.002. Figure <xref ref-type="fig" rid="F3">3</xref> shows this interaction in terms of performance&#x02019;s variations, i.e., the difference (post session-pre session) of the probability to respond correct calculated by the lsmens function for the <italic>post hoc</italic> contrasts based on the GLMM model (Prob in Table <xref ref-type="table" rid="T1">1</xref>). Table <xref ref-type="table" rid="T1">1</xref> reports standard errors and confidence intervals estimated for the pre and post session. Green bars represent sounds delivered above the knee level, red bars represent sounds delivered below the knee level; light colors denote sounds delivered in the frontal space, while dark colors denote sounds delivered in the back space. Positive values of the bars represent improvement in performance in the post session compared to the pre session, and negative values represent decrement in performance. As can be seen, only the experimental group showed performance&#x02019;s variations after the training. Specifically, considering the back area, an improvement is present in space above the knee (dark green bar; (OR) = 1.7 &#x000b1; 0.36, z.ratio = 2.9, <italic>P</italic> = 0.01) and below the knee (dark red bar; (OR) = 1.91 &#x000b1; 0.4, z.ratio = 2.8, <italic>P</italic> = 0.01). Instead, in the frontal space, an improvement is visible above the knee (light green bar; (OR) = 2.04 &#x000b1; 0.5, z.ratio = 2.5, <italic>P</italic> = 0.02), while a performance worsened below the knee (light red; (OR) = 0.48 &#x000b1; 0.09, z.ratio = 3.8, <italic>P</italic> = 0.0006). Therefore, performance&#x02019;s variations in the frontal but not in the back space were strongly dependent on the elevation at which sounds were delivered.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Performance&#x02019;s variation in each space after the training. Green bars represent space above the knee, red bars denote space above the knee. Dark colors are used for the back space, while light colors are used for the frontal area. As can be seen only the experimental group shows performance&#x02019;s variations (post&#x02014;pre) after the training, leading to an improvement in the back space and to a worsened performance in the frontal space under the knee. *Indicates <italic>P</italic> &#x0003c; 0.05.</p></caption><graphic xlink:href="fnint-11-00036-g0003"/></fig><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Interaction between longitudinal position, sound level (above the knee, below the knee), groups (experimental, motor control and audio control) and session (pre, post): table reports probability to be correct, in the pre and post session, in each space for each group.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="7" rowspan="1">EXPERIMENTAL</th></tr><tr><th align="left" rowspan="1" colspan="1">Session</th><th align="center" rowspan="1" colspan="1">Sound level</th><th align="center" rowspan="1" colspan="1">Position</th><th align="center" rowspan="1" colspan="1">Prob</th><th align="center" rowspan="1" colspan="1">SE</th><th align="center" rowspan="1" colspan="1">asymp.LCL</th><th align="center" rowspan="1" colspan="1">asymp.UCL</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.63</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.74</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.49</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.62</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.68</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.78</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.53</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.62</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.83</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.90</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.71</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.81</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">40.0</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.49</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.58</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.67</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>MOTOR CONTROL</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.62</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">0.72</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.55</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.42</td><td align="center" rowspan="1" colspan="1">0.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.70</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.59</td><td align="center" rowspan="1" colspan="1">0.79</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.65</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.56</td><td align="center" rowspan="1" colspan="1">0.73</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.72</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.59</td><td align="center" rowspan="1" colspan="1">0.82</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.72</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.60</td><td align="center" rowspan="1" colspan="1">0.81</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.37</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.29</td><td align="center" rowspan="1" colspan="1">0.45</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.37</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.29</td><td align="center" rowspan="1" colspan="1">0.45</td></tr><tr><td align="left" rowspan="1" colspan="1"><bold>AUDIO CONTROL</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.62</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.50</td><td align="center" rowspan="1" colspan="1">0.72</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.65</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.52</td><td align="center" rowspan="1" colspan="1">0.76</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.64</td><td align="center" rowspan="1" colspan="1">0.05</td><td align="center" rowspan="1" colspan="1">0.53</td><td align="center" rowspan="1" colspan="1">0.74</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Back</td><td align="center" rowspan="1" colspan="1">0.68</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.59</td><td align="center" rowspan="1" colspan="1">0.76</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.66</td><td align="center" rowspan="1" colspan="1">0.07</td><td align="center" rowspan="1" colspan="1">0.52</td><td align="center" rowspan="1" colspan="1">0.78</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Above knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.66</td><td align="center" rowspan="1" colspan="1">0.06</td><td align="center" rowspan="1" colspan="1">0.54</td><td align="center" rowspan="1" colspan="1">0.77</td></tr><tr><td align="left" rowspan="1" colspan="1">Post</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.55</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.46</td><td align="center" rowspan="1" colspan="1">0.63</td></tr><tr><td align="left" rowspan="1" colspan="1">Pre</td><td align="center" rowspan="1" colspan="1">Below knee</td><td align="center" rowspan="1" colspan="1">Front</td><td align="center" rowspan="1" colspan="1">0.52</td><td align="center" rowspan="1" colspan="1">0.04</td><td align="center" rowspan="1" colspan="1">0.43</td><td align="center" rowspan="1" colspan="1">0.60</td></tr></tbody></table><table-wrap-foot><p><italic>Asymp.LCL and asymp.UCL report the control limits (lower and upper respectively). They represent the range of expected variation</italic>.</p></table-wrap-foot></table-wrap><p>A second interaction was found between longitudinal position, session and groups <inline-formula><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 10.90, <italic>P</italic> = 0.004. Figure <xref ref-type="fig" rid="F4">4</xref> explains this interaction in terms of performance&#x02019;s variations. Blue bars represent back space, while red bars frontal space. As can be seen, an improvement is present only in the experimental group and only in the back space ((OR) = 1.85 &#x000b1; 0.2, z.ratio = 3.94, <italic>P</italic> = 0.0005).</p><fig id="F4" position="float"><label>Figure 4</label><caption><p>Influences of training on longitudinal position: represent performance&#x02019;s variation (post&#x02014;pre) in discriminating front-back location without considering body elevation. Red bars denote frontal sounds, blue bars represent back sounds. As can be seen only the experimental group improved. The improvement is present only in the back space. *Indicates <italic>P</italic> &#x0003c; 0.05.</p></caption><graphic xlink:href="fnint-11-00036-g0004"/></fig><p>Moreover, a third interaction was found between sound level, session and groups <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 8.02, <italic>P</italic> = 0.01. Figure <xref ref-type="fig" rid="F5">5</xref> describes this interaction in terms of performance&#x02019;s variations. Red bars represent sounds delivered above the knee and green bars sounds delivered below the knee. An improvement is present only in the experimental group and only for stimuli presented above the knee ((OR) = 1.91 &#x000b1; 0.32, z.ratio = 3.77, <italic>P</italic> = 0.0009).</p><fig id="F5" position="float"><label>Figure 5</label><caption><p>Influences of training on sound elevation: represent performance&#x02019;s variation (post&#x02014;pre) in discriminating front-back location at different body elevation. Red bars denote sounds above the knee, green bars represent sounds below the knee. As can be seen only the experimental group improved. The improvement is confined to sounds above the knee. *Indicates <italic>P</italic> &#x0003c; 0.05.</p></caption><graphic xlink:href="fnint-11-00036-g0005"/></fig></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>In this study, we tested whether a sensory-motor training, with a new rehabilitative device called ABBI, could be useful to improve the space representation around the lower body part in sighted individuals. Previous studies in blind individuals have shown that a training of few minutes with the ABBI device, a bracelet that produces audio feedback of body movements, improves the audio spatial representation of space around the upper part of the body (Finocchietti et al., <xref rid="B22" ref-type="bibr">2017</xref>). Here we show that a training with ABBI can also improve audio spatial representation in sighted individuals in the lower body space (around the legs). Subjects were asked to perform a front-back sound discrimination task before and after different training conditions. Front-back sound discrimination is a difficult task due to the presence of the cone of confusion. In agreement with previous results (Wenzel et al., <xref rid="B66" ref-type="bibr">1993</xref>), all subjects were around chance level for the front-back sound localization before the trainings. After the training only the experimental group that received the audio motor feedback, by using ABBI, improved spatial performance. No improvement was observed in the other two groups of subjects which performed the training by moving the leg without sound or by listening to the sound moved by the experimenter. These results suggest that only the combination of audio feedback associated with body movement is useful to improve audio spatial representation around the legs in sighted individuals. Future studies will be necessary in order to investigate whether the same is true also for the upper body part since at the moment, in this body region, ABBI has been tested just considering the availability of both audio and motor signals available (Finocchietti et al., <xref rid="B23" ref-type="bibr">2015a</xref>, <xref rid="B22" ref-type="bibr">2017</xref>).</p><p>An interesting result is that the effect of training with ABBI varies according to the body parts (above the knee vs. below the knee) and spatial areas (front vs. back) considered. As regard the body parts considered, we observed an improvement above but not below the knee for frontal sounds. The different impact of the audio-motor training on high and low spatial body representation can be explained by considering how often the auditory feedback is linked to those body parts. Indeed when walking, only body space around the foot is mapped by hearing, thanks to the audio feedback produced by the foot reaching the floor: this might be automatically linked to the tactile and proprioceptive information used to encode the leg spatial position. The training with the ABBI device might be less beneficial in the lower portion of the body because at the foot level a natural audio-motor association is already present and it is mediated by locomotion. Another possible speculation is that different multisensory processing act above and below the knee. During locomotion we usually look in front of our feet so visual experience occurs independently respect to the audio-proprioceptive integration related to feet. Since experience can modulate audio-visual integration (Meredith and Stein, <xref rid="B47" ref-type="bibr">1996</xref>), it is plausible that the audio information associated with walking is integrated with proprioceptive feedback on the same spatial area but with visual information congruent in time and not in space. Thus a possible speculation is that this sensory misalignment could lead to distorted or less automatic sensory integration. The training with ABBI might reinforce this misaligned association. The same distortion is not present above the knee because in this body zone the audio feedback of movements isn&#x02019;t present and multisensory integration is similar to the upper body part where the sensory-motor training with ABBI is useful for spatial recalibration (Finocchietti et al., <xref rid="B22" ref-type="bibr">2017</xref>). As regard the spatial areas considered, several studies indicate that space is processed differently depending both: the body part considered (Serino et al., <xref rid="B57" ref-type="bibr">2015</xref>; di Pellegrino and L&#x000e0;davas, <xref rid="B17" ref-type="bibr">2015</xref>) and on the distance from the body (L&#x000e0;davas and Farn&#x000e8;, <xref rid="B39" ref-type="bibr">2004</xref>; Aimola et al., <xref rid="B1" ref-type="bibr">2012</xref>; Tomasino et al., <xref rid="B59" ref-type="bibr">2012</xref>; Ca&#x000e7;ola et al., <xref rid="B12" ref-type="bibr">2013</xref>; Mahayana et al., <xref rid="B45" ref-type="bibr">2014</xref>). Other studies have found a difference between frontal and rear space (K&#x000f3;bor et al., <xref rid="B35" ref-type="bibr">2006</xref>; Occelli et al., <xref rid="B49" ref-type="bibr">2011</xref>; Van der Stoep et al., <xref rid="B60" ref-type="bibr">2015</xref>), showing a higher saliency of sounds in the back (Farn&#x000e8; and L&#x000e0;davas, <xref rid="B20" ref-type="bibr">2002</xref>). Nonetheless only few studies investigated spatial perception around the foot (Schicke et al., <xref rid="B56" ref-type="bibr">2009</xref>; Smid and den Otter, <xref rid="B58" ref-type="bibr">2013</xref>) even if auditory feedback is most commonly perceived at the foot level, for example during locomotion. For different regions around the lower body part (e.g., the foot and upper leg portion), we found different audio performance and recalibration after the audio-motor training supporting the idea that space around the legs is split into sub regions, probably based on the different sensory and motor feedback commonly available in these zones. Indeed, we found an improvement in the back space at both elevation: above and below the knee, suggesting that in space where vision is not available, an audio motor training is useful to recalibrate auditory space. This result is in agreement with other evidences showing the beneficial effect of ABBI in improving auditory space in blind people (Finocchietti et al., <xref rid="B23" ref-type="bibr">2015a</xref>, <xref rid="B22" ref-type="bibr">2017</xref>). For what regards the frontal space, we found a different performance above and below the knee, supporting our hypothesis that these two regions rely on different mechanisms of audio visual integration.</p><p>Why the training with the ABBI device is useful to improve audio spatial representations? ABBI favors the association of sensorimotor association and thus it facilitates multisensory integration. Previous studies have shown that audio-motor associations are easily encoded by our brain and transferred across senses (Levy-Tzedek et al., <xref rid="B42" ref-type="bibr">2012</xref>). The flow of information between auditory and motor cortex seems to be bidirectional, and arbitrary sounds (without a previous motor or verbal meaning) can be rapidly mapped onto the motor system (Ticini et al., <xref rid="B14" ref-type="bibr">2012</xref>). Importantly, we observed that the audio-motor training with ABBI improved audio spatial performances. This result seems to be supported by previous studies showing that self-produced stimuli are processed differently than not self-produced stimuli. For example at a behavioral level, self-produced tactile stimulation is perceived as less intense compared with identical tactile stimulation produced by an external source (Blakemore et al., <xref rid="B10" ref-type="bibr">1999</xref>). Similarly, in the auditory modality, when subjects compare the volume of two identical sounds, one self-generated (by actively pressing a button) and the other perceived passively, the self-generated sound is reported as being less loud (Weiss et al., <xref rid="B65" ref-type="bibr">2011</xref>). At the cortical level, self-generated sound activate the sensory cortex differently respect to external sounds (Sato, <xref rid="B54" ref-type="bibr">2008</xref>; Baess et al., <xref rid="B5" ref-type="bibr">2009</xref>, <xref rid="B4" ref-type="bibr">2011</xref>). Crucially, the intention and voluntary aspect of the movement are needed to modulate activity in auditory cortex (Haggard and Whitford, <xref rid="B33" ref-type="bibr">2004</xref>); in other words, the modulation of neural responses to sensory consequences of self-generated actions are influenced by volition and the sense of agency (Haggard, <xref rid="B32" ref-type="bibr">2008</xref>). Since some studies suggest that in case of sensory loss, the silent pre-existing connections are revealed (Amedi et al., <xref rid="B3" ref-type="bibr">2005</xref>; Dahmen and King, <xref rid="B16" ref-type="bibr">2007</xref>), a possible speculation to interpret the improvement observed with ABBI is that the use of self-generated sounds may enhance audio motor integration by unmasking and training pre-existing silent connections, leading to greater effectiveness of this feedback in perceiving space.</p><p>The natural feedback provided by the ABBI device has the advantage of being immediate as the subject doesn&#x02019;t need to use codes for interpreting the sensory feedback he receives, contrarily to what is required by most of the sensory substitution devices developed to date (Cuturi et al., <xref rid="B15" ref-type="bibr">2016</xref>; Gori et al., <xref rid="B28" ref-type="bibr">2016</xref>). Previous results showed that intentional movement has an influence on spatial cognition (Paillard, <xref rid="B50" ref-type="bibr">1991</xref>; Berti and Frassinetti, <xref rid="B9" ref-type="bibr">1996</xref>; Scandola et al., <xref rid="B55" ref-type="bibr">2016</xref>). Our results further confirmed this hypothesis by demonstrating that by adding an auditory feedback to self-generated movements spatial skills improve in spaces unexplored by leg movements, such as above the knee space. Importantly, in this study the improvement is observed also in sighted individuals suggesting that not only blind individuals can benefit from this form of audio-motor feedback (as previously shown in Finocchietti et al., <xref rid="B22" ref-type="bibr">2017</xref>). This effect could be explained by the fact that the sound is integrated with the part of the body that is producing the body movement (i.e., the tight), hence the portion of space calibrated is around the effector driving the motor execution.</p><p>To conclude, we showed that an audio motor training below the knee modifies the representation of space around the leg, probably by impacting on different multisensory integration processes. This could explain the improvement and decrement in performance in different zones around the legs. Future experiments will be performed to explore the brain plasticity of the recalibration mediated by the use of ABBI and its application in people with motor disability.</p></sec><sec id="s5"><title>Author Contributions</title><p>EA-V, MG, CC and SF conceived the studies and designed the experiments, wrote the manuscript. EA-V carried out experiments. EA-V and CC analyzed data. All authors reviewed the manuscript.</p></sec><sec id="s6"><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>We thank all the participants for their kind willing contribution and Giulia Cappagli for her help in manuscript revision.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aimola</surname><given-names>L.</given-names></name><name><surname>Schindler</surname><given-names>I.</given-names></name><name><surname>Simone</surname><given-names>A. M.</given-names></name><name><surname>Venneri</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Near and far space neglect: task sensitivity and anatomical substrates</article-title>. <source>Neuropsychologia</source>
<volume>50</volume>, <fpage>1115</fpage>&#x02013;<lpage>1123</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.01.022</pub-id><pub-id pub-id-type="pmid">22306826</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Amedi</surname><given-names>A.</given-names></name><name><surname>Meijer</surname><given-names>P.</given-names></name></person-group> (<year>2005</year>). &#x0201c;<article-title>Neural correlates of visual-to-auditory sensory substitution in proficient blind users</article-title>,&#x0201d; in <source>6th Annual Meeting of the International Multisensory Research Forum (IMRF)</source>, <conf-loc>Roverto, Italy</conf-loc>.</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname><given-names>A.</given-names></name><name><surname>Merabet</surname><given-names>L. B.</given-names></name><name><surname>Bermpohl</surname><given-names>F.</given-names></name><name><surname>Pascual-leone</surname><given-names>A.</given-names></name></person-group> (<year>2005</year>). <article-title>The occipital cortex in the blind: lessons about plasticity and vision</article-title>. <source>Curr. Dir. Psychol. Sci.</source>
<volume>14</volume>, <fpage>306</fpage>&#x02013;<lpage>311</lpage>. <pub-id pub-id-type="doi">10.1111/j.0963-7214.2005.00387.x</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baess</surname><given-names>P.</given-names></name><name><surname>Horv&#x000e1;th</surname><given-names>J.</given-names></name><name><surname>Jacobsen</surname><given-names>T.</given-names></name><name><surname>Schr&#x000f6;ger</surname><given-names>E.</given-names></name></person-group> (<year>2011</year>). <article-title>Selective suppression of self-initiated sounds in an auditory stream: an ERP study</article-title>. <source>Psychophysiology</source>
<volume>48</volume>, <fpage>1276</fpage>&#x02013;<lpage>1283</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.2011.01196.x</pub-id><pub-id pub-id-type="pmid">21449953</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baess</surname><given-names>P.</given-names></name><name><surname>Widmann</surname><given-names>A.</given-names></name><name><surname>Roye</surname><given-names>A.</given-names></name><name><surname>Schr&#x000f6;ger</surname><given-names>E.</given-names></name><name><surname>Jacobsen</surname><given-names>T.</given-names></name></person-group> (<year>2009</year>). <article-title>Attenuated human auditory middle latency response and evoked 40-Hz response to self-initiated sounds</article-title>. <source>Eur. J. Neurosci.</source>
<volume>29</volume>, <fpage>1514</fpage>&#x02013;<lpage>1521</lpage>. <pub-id pub-id-type="doi">10.1111/j.1460-9568.2009.06683.x</pub-id><pub-id pub-id-type="pmid">19323693</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barr</surname><given-names>D. J.</given-names></name></person-group> (<year>2013</year>). <article-title>Random effects structure for testing interactions in linear mixed-effects models</article-title>. <source>Front. Psychol.</source>
<volume>4</volume>:<fpage>328</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00328</pub-id><pub-id pub-id-type="pmid">23761778</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barr</surname><given-names>D. J.</given-names></name><name><surname>Levy</surname><given-names>R.</given-names></name><name><surname>Scheepers</surname><given-names>C.</given-names></name><name><surname>Tily</surname><given-names>H. J.</given-names></name></person-group> (<year>2013</year>). <article-title>Random effects structure for confirmatory hypothesis testing: keep it maximal</article-title>. <source>J. Mem. Lang.</source>
<volume>68</volume>, <fpage>255</fpage>&#x02013;<lpage>278</lpage>. <pub-id pub-id-type="doi">10.1016/j.jml.2012.11.001</pub-id><pub-id pub-id-type="pmid">24403724</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D.</given-names></name><name><surname>M&#x000e4;chler</surname><given-names>M.</given-names></name><name><surname>Bolker</surname><given-names>B.</given-names></name><name><surname>Walker</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>Fitting linear mixed-effects models using lme4</article-title>. <source>J. Stat. Softw.</source>
<volume>67</volume>, <fpage>1</fpage>&#x02013;<lpage>48</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ben Porquis</surname><given-names>L.</given-names></name><name><surname>Finocchietti</surname><given-names>S.</given-names></name><name><surname>Zini</surname><given-names>G.</given-names></name><name><surname>Cappagli</surname><given-names>G.</given-names></name><name><surname>Gori</surname><given-names>M.</given-names></name><name><surname>Baud-Bovy</surname><given-names>G.</given-names></name></person-group> (<year>2017</year>). &#x0201c;<article-title>ABBI: a wearable device for improving spatial cognition in visually impaired children</article-title>,&#x0201d; in <source>IEEE Biomed Circuits System Conference</source> (<publisher-loc>Genoa</publisher-loc>).</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berti</surname><given-names>A.</given-names></name><name><surname>Frassinetti</surname><given-names>F.</given-names></name></person-group> (<year>1996</year>). <article-title>When far becomes near: remapping of space</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>12</volume>, <fpage>415</fpage>&#x02013;<lpage>420</lpage>. <pub-id pub-id-type="doi">10.1162/089892900562237</pub-id><pub-id pub-id-type="pmid">10931768</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blakemore</surname><given-names>S. J.</given-names></name><name><surname>Frith</surname><given-names>C. D.</given-names></name><name><surname>Wolpert</surname><given-names>D. M.</given-names></name></person-group> (<year>1999</year>). <article-title>Spatio-temporal prediction modulates the perception of self-produced stimuli</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>11</volume>, <fpage>551</fpage>&#x02013;<lpage>559</lpage>. <pub-id pub-id-type="doi">10.1162/089892999563607</pub-id><pub-id pub-id-type="pmid">10511643</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>H.</given-names></name><name><surname>Snyder</surname><given-names>A. Z.</given-names></name><name><surname>Raichle</surname><given-names>M. E.</given-names></name></person-group> (<year>2014</year>). <article-title>Resting state functional connectivity in early blind humans</article-title>. <source>Front. Syst. Neurosci.</source>
<volume>8</volume>:<fpage>51</fpage>. <pub-id pub-id-type="doi">10.3389/fnsys.2014.00051</pub-id><pub-id pub-id-type="pmid">24778608</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ca&#x000e7;ola</surname><given-names>P.</given-names></name><name><surname>Martinez</surname><given-names>A.</given-names></name><name><surname>Ray</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>The ability to modulate peripersonal and extrapersonal reach space via tool use among the elderly</article-title>. <source>Arch. Gerontol. Geriatr.</source>
<volume>56</volume>, <fpage>383</fpage>&#x02013;<lpage>388</lpage>. <pub-id pub-id-type="doi">10.1016/j.archger.2012.08.010</pub-id><pub-id pub-id-type="pmid">22980778</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cappagli</surname><given-names>G.</given-names></name><name><surname>Cocchi</surname><given-names>E.</given-names></name><name><surname>Gori</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Auditory and proprioceptive spatial impairments in blind children and adults</article-title>. <source>Dev. Sci.</source>
<volume>20</volume>:<fpage>e12374</fpage>. <pub-id pub-id-type="doi">10.1111/desc.12374</pub-id><pub-id pub-id-type="pmid">26613827</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuturi</surname><given-names>L. F.</given-names></name><name><surname>Aggius-Vella</surname><given-names>E.</given-names></name><name><surname>Campus</surname><given-names>C.</given-names></name><name><surname>Parmiggiani</surname><given-names>A.</given-names></name><name><surname>Gori</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>From science to technology: orientation and mobility in blind children and adults</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>71</volume>, <fpage>240</fpage>&#x02013;<lpage>251</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.08.019</pub-id><pub-id pub-id-type="pmid">27608959</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahmen</surname><given-names>J. C.</given-names></name><name><surname>King</surname><given-names>A. J.</given-names></name></person-group> (<year>2007</year>). <article-title>Learning to hear: plasticity of auditory cortical processing</article-title>. <source>Curr. Opin. Neurobiol.</source>
<volume>17</volume>, <fpage>456</fpage>&#x02013;<lpage>464</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2007.07.004</pub-id><pub-id pub-id-type="pmid">17714932</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>di Pellegrino</surname><given-names>G.</given-names></name><name><surname>L&#x000e0;davas</surname><given-names>E.</given-names></name></person-group> (<year>2015</year>). <article-title>Peripersonal space in the brain</article-title>. <source>Neuropsychologia</source>
<volume>66</volume>, <fpage>126</fpage>&#x02013;<lpage>133</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.11.011</pub-id><pub-id pub-id-type="pmid">25448862</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doucet</surname><given-names>M. E.</given-names></name><name><surname>Guillemot</surname><given-names>J. P.</given-names></name><name><surname>Lassonde</surname><given-names>M.</given-names></name><name><surname>Gagn&#x000e9;</surname><given-names>J. P.</given-names></name><name><surname>Leclerc</surname><given-names>C.</given-names></name><name><surname>Lepore</surname><given-names>F.</given-names></name></person-group> (<year>2005</year>). <article-title>Blind subjects process auditory spectral cues more efficiently than sighted individuals</article-title>. <source>Exp. Brain Res.</source>
<volume>160</volume>, <fpage>194</fpage>&#x02013;<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-004-2000-4</pub-id><pub-id pub-id-type="pmid">15309355</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckert</surname><given-names>M. A.</given-names></name><name><surname>Kamdar</surname><given-names>N. V.</given-names></name><name><surname>Chang</surname><given-names>C. E.</given-names></name><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Greicius</surname><given-names>M. D.</given-names></name><name><surname>Menon</surname><given-names>V.</given-names></name></person-group> (<year>2008</year>). <article-title>A cross-modal system linking primary auditory and visual cortices: evidence from intrinsic fMRI connectivity analysis</article-title>. <source>Hum. Brain Mapp.</source>
<volume>29</volume>, <fpage>848</fpage>&#x02013;<lpage>857</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20560</pub-id><pub-id pub-id-type="pmid">18412133</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farn&#x000e8;</surname><given-names>A.</given-names></name><name><surname>L&#x000e0;davas</surname><given-names>E.</given-names></name></person-group> (<year>2002</year>). <article-title>Auditory peripersonal space in humans</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>14</volume>, <fpage>1030</fpage>&#x02013;<lpage>1043</lpage>. <pub-id pub-id-type="doi">10.1162/089892902320474481</pub-id><pub-id pub-id-type="pmid">12419126</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Finocchietti</surname><given-names>S.</given-names></name><name><surname>Cappagli</surname><given-names>G.</given-names></name><name><surname>Ben Porquis</surname><given-names>L.</given-names></name><name><surname>Baud-Bovy</surname><given-names>G.</given-names></name><name><surname>Cocchi</surname><given-names>E.</given-names></name></person-group> (<year>2015a</year>). &#x0201c;<article-title>Evaluation of the audio bracelet for blind interaction for improving mobility and spatial cognition in early blind children&#x02014;A pilot study</article-title>,&#x0201d; in <source>37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source> (<conf-loc>Milan</conf-loc>), <fpage>7998</fpage>&#x02013;<lpage>8001</lpage>. </mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finocchietti</surname><given-names>S.</given-names></name><name><surname>Cappagli</surname><given-names>G.</given-names></name><name><surname>Gori</surname><given-names>M.</given-names></name></person-group> (<year>2015b</year>). <article-title>Encoding audio motion: spatial impairment in early blind individuals</article-title>. <source>Front. Psychol.</source>
<volume>6</volume>:<fpage>1357</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.01357</pub-id><pub-id pub-id-type="pmid">26441733</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finocchietti</surname><given-names>S.</given-names></name><name><surname>Cappagli</surname><given-names>G.</given-names></name><name><surname>Gori</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Auditory spatial recalibration in congenital blind individuals</article-title>. <source>Front. Neurosci.</source>
<volume>11</volume>:<fpage>76</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2017.00268</pub-id><pub-id pub-id-type="pmid">28261053</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>J.</given-names></name><name><surname>Weisberg</surname><given-names>S.</given-names></name></person-group> (<year>2011</year>). <source>An R Companion to Applied Regression.</source>
<edition>2nd Edn.</edition>
<publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage Publications</publisher-name> Available online at: <ext-link ext-link-type="uri" xlink:href="http://socserv.socsci.mcmaster.ca/jfox/Books/Companion">http://socserv.socsci.mcmaster.ca/jfox/Books/Companion</ext-link></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frasnelli</surname><given-names>J.</given-names></name><name><surname>Collignon</surname><given-names>O.</given-names></name><name><surname>Voss</surname><given-names>P.</given-names></name><name><surname>Lepore</surname><given-names>F.</given-names></name></person-group> (<year>2011</year>). <article-title>Crossmodal plasticity in sensory loss</article-title>. <source>Prog. Brain Res.</source>
<volume>191</volume>, <fpage>233</fpage>&#x02013;<lpage>249</lpage>. <pub-id pub-id-type="doi">10.1016/B978-0-444-53752-2.00002-3</pub-id><pub-id pub-id-type="pmid">21741555</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gori</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Multisensory integration and calibration in children and adults with and without sensory and motor disabilities</article-title>. <source>Multisens. Res.</source>
<volume>28</volume>, <fpage>71</fpage>&#x02013;<lpage>99</lpage>. <pub-id pub-id-type="doi">10.1163/22134808-00002478</pub-id><pub-id pub-id-type="pmid">26152053</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gori</surname><given-names>M.</given-names></name><name><surname>Cappagli</surname><given-names>G.</given-names></name><name><surname>Tonelli</surname><given-names>A.</given-names></name><name><surname>Baud-Bovy</surname><given-names>G.</given-names></name><name><surname>Finocchietti</surname><given-names>S.</given-names></name></person-group> (<year>2016</year>). <article-title>Devices for visually impaired people: high technological devices with low user acceptance and no adaptability for children</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>69</volume>, <fpage>79</fpage>&#x02013;<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.06.043</pub-id><pub-id pub-id-type="pmid">27484870</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gori</surname><given-names>M.</given-names></name><name><surname>Sandini</surname><given-names>G.</given-names></name><name><surname>Martinoli</surname><given-names>C.</given-names></name><name><surname>Burr</surname><given-names>D. C.</given-names></name></person-group> (<year>2014</year>). <article-title>Impairment of auditory spatial localization in congenitally blind human subjects</article-title>. <source>Brain</source>
<volume>137</volume>, <fpage>288</fpage>&#x02013;<lpage>293</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awt311</pub-id><pub-id pub-id-type="pmid">24271326</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gougoux</surname><given-names>F.</given-names></name><name><surname>Lepore</surname><given-names>F.</given-names></name><name><surname>Lassonde</surname><given-names>M.</given-names></name><name><surname>Voss</surname><given-names>P.</given-names></name><name><surname>Zatorre</surname><given-names>R. J.</given-names></name><name><surname>Belin</surname><given-names>P.</given-names></name></person-group> (<year>2004</year>). <article-title>Neuropsychology: pitch discrimination in the early blind</article-title>. <source>Nature</source>
<volume>430</volume>:<fpage>309</fpage>. <pub-id pub-id-type="doi">10.1038/430309a</pub-id><pub-id pub-id-type="pmid">15254527</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gougoux</surname><given-names>F.</given-names></name><name><surname>Zatorre</surname><given-names>R. J.</given-names></name><name><surname>Lassonde</surname><given-names>M.</given-names></name><name><surname>Voss</surname><given-names>P.</given-names></name><name><surname>Lepore</surname><given-names>F.</given-names></name></person-group> (<year>2005</year>). <article-title>A functional neuroimaging study of sound localization: visual cortex activity predicts performance in early-blind individuals</article-title>. <source>PLoS Biol.</source>
<volume>3</volume>:<fpage>e27</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.0030027</pub-id><pub-id pub-id-type="pmid">15678166</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haggard</surname><given-names>P.</given-names></name></person-group> (<year>2008</year>). <article-title>Human volition: towards a neuroscience of will</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>9</volume>, <fpage>934</fpage>&#x02013;<lpage>946</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2497</pub-id><pub-id pub-id-type="pmid">19020512</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haggard</surname><given-names>P.</given-names></name><name><surname>Whitford</surname><given-names>B.</given-names></name></person-group> (<year>2004</year>). <article-title>Supplementary motor area provides an efferent signal for sensory suppression</article-title>. <source>Cogn. Brain Res.</source>
<volume>19</volume>, <fpage>52</fpage>&#x02013;<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1016/j.cogbrainres.2003.10.018</pub-id><pub-id pub-id-type="pmid">14972358</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>A. J.</given-names></name><name><surname>Parsons</surname><given-names>C. H.</given-names></name></person-group> (<year>1999</year>). <article-title>Improved auditory spatial acuity in visually deprived ferrets</article-title>. <source>Eur. J. Neurosci.</source>
<volume>11</volume>, <fpage>3945</fpage>&#x02013;<lpage>3956</lpage>. <pub-id pub-id-type="doi">10.1046/j.1460-9568.1999.00821.x</pub-id><pub-id pub-id-type="pmid">10583483</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>K&#x000f3;bor</surname><given-names>I.</given-names></name><name><surname>F&#x000fc;redi</surname><given-names>L.</given-names></name><name><surname>Kov&#x000e1;cs</surname><given-names>G.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name><name><surname>Vidny&#x000e1;nszky</surname><given-names>Z.</given-names></name></person-group> (<year>2006</year>). <article-title>Back-to-front: improved tactile discrimination performance in the space you cannot see</article-title>. <source>Neurosci. Lett.</source>
<volume>400</volume>, <fpage>163</fpage>&#x02013;<lpage>167</lpage>. <pub-id pub-id-type="doi">10.1016/j.neulet.2006.02.037</pub-id><pub-id pub-id-type="pmid">16516383</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>A. J.</given-names></name><name><surname>Cirstea</surname><given-names>S.</given-names></name><name><surname>Pardhan</surname><given-names>S.</given-names></name></person-group> (<year>2013a</year>). <article-title>Evidence for enhanced discrimination of virtual auditory distance among blind listeners using level and direct-to-reverberant cues</article-title>. <source>Exp. Brain Res.</source>
<volume>224</volume>, <fpage>623</fpage>&#x02013;<lpage>633</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-012-3340-0</pub-id><pub-id pub-id-type="pmid">23178908</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>A. J.</given-names></name><name><surname>Cirstea</surname><given-names>S.</given-names></name><name><surname>Pardhan</surname><given-names>S.</given-names></name><name><surname>Moore</surname><given-names>B. C.</given-names></name></person-group> (<year>2013b</year>). &#x0201c;<article-title>An assessment of virtual auditory distance judgments among blind and sighted listeners</article-title>,&#x0201d; in <source>Proceedings of Meetings on Acoustics: Acoustical Society of America</source> (<conf-loc>Montreal, QC</conf-loc>), <volume>19</volume>:<fpage>050043</fpage>
<pub-id pub-id-type="doi">10.1121/1.4799570</pub-id>
</mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>A. J.</given-names></name><name><surname>Pardhan</surname><given-names>S.</given-names></name><name><surname>Cirstea</surname><given-names>S.</given-names></name><name><surname>Moore</surname><given-names>B. C. J.</given-names></name></person-group> (<year>2017</year>). <article-title>Auditory spatial representations of the world are compressed in blind humans</article-title>. <source>Exp. Brain Res.</source>
<volume>235</volume>, <fpage>597</fpage>&#x02013;<lpage>606</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-016-4823-1</pub-id><pub-id pub-id-type="pmid">27837259</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>L&#x000e0;davas</surname><given-names>E.</given-names></name><name><surname>Farn&#x000e8;</surname><given-names>A.</given-names></name></person-group> (<year>2004</year>). <article-title>Visuo-tactile representation of near-the-body space</article-title>. <source>J. Physiol. Paris</source>
<volume>98</volume>, <fpage>161</fpage>&#x02013;<lpage>170</lpage>. <pub-id pub-id-type="doi">10.1016/j.jphysparis.2004.03.007</pub-id><pub-id pub-id-type="pmid">15477030</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenth</surname><given-names>R. V.</given-names></name></person-group> (<year>2016</year>). <article-title>Least-squares means: the R package lsmeans</article-title>. <source>J. Stat. Softw.</source>
<volume>69</volume>,<fpage>1</fpage>&#x02013;<lpage>33</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v069.i01</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lessard</surname><given-names>N.</given-names></name><name><surname>Par&#x000e9;</surname><given-names>M.</given-names></name><name><surname>Lepore</surname><given-names>F.</given-names></name><name><surname>Lassonde</surname><given-names>M.</given-names></name></person-group> (<year>1998</year>). <article-title>Early-blind human subjects localize sound sources better than sighted subjects</article-title>. <source>Nature</source>
<volume>395</volume>, <fpage>278</fpage>&#x02013;<lpage>280</lpage>. <pub-id pub-id-type="doi">10.1038/26228</pub-id><pub-id pub-id-type="pmid">9751055</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy-Tzedek</surname><given-names>S.</given-names></name><name><surname>Novick</surname><given-names>I.</given-names></name><name><surname>Arbel</surname><given-names>R.</given-names></name><name><surname>Abboud</surname><given-names>S.</given-names></name><name><surname>Maidenbaum</surname><given-names>S.</given-names></name><name><surname>Vaadia</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Cross-sensory transfer of sensory-motor information: visuomotor learning affects performance on an audiomotor task, using sensory-substitution</article-title>. <source>Sci. Rep.</source>
<volume>2</volume>:<fpage>949</fpage>. <pub-id pub-id-type="doi">10.1038/srep00949</pub-id><pub-id pub-id-type="pmid">23230514</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>More accurate sound localization induced by short-term light deprivation</article-title>. <source>Neuropsychologia</source>
<volume>45</volume>, <fpage>1215</fpage>&#x02013;<lpage>1222</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.10.006</pub-id><pub-id pub-id-type="pmid">17113113</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>C.</given-names></name><name><surname>Liang</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Tian</surname><given-names>L.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2007</year>). <article-title>Whole brain functional connectivity in the early blind</article-title>. <source>Brain</source>
<volume>130</volume>, <fpage>2085</fpage>&#x02013;<lpage>2096</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awm121</pub-id><pub-id pub-id-type="pmid">17533167</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahayana</surname><given-names>I.</given-names></name><collab>Hartono</collab><name><surname>Tcheang</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>C.-Y.</given-names></name><name><surname>Juan</surname><given-names>C.-H.</given-names></name><name><surname>Muggleton</surname><given-names>N.</given-names></name></person-group> (<year>2014</year>). <article-title>Posterior parietal cortex and visuospatial control in near and far space</article-title>. <source>Transl. Neurosci.</source>
<volume>5</volume>, <fpage>269</fpage>&#x02013;<lpage>274</lpage>. <pub-id pub-id-type="doi">10.2478/s13380-014-0229-3</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martuzzi</surname><given-names>R.</given-names></name><name><surname>Murray</surname><given-names>M. M.</given-names></name><name><surname>Michel</surname><given-names>C. M.</given-names></name><name><surname>Thiran</surname><given-names>J.-P.</given-names></name><name><surname>Maeder</surname><given-names>P. P.</given-names></name><name><surname>Clarke</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2007</year>). <article-title>Multisensory interactions within human primary cortices revealed by BOLD dynamics</article-title>. <source>Cereb. Cortex</source>
<volume>17</volume>, <fpage>1672</fpage>&#x02013;<lpage>1679</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhl077</pub-id><pub-id pub-id-type="pmid">16968869</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meredith</surname><given-names>M. A.</given-names></name><name><surname>Stein</surname><given-names>B. E.</given-names></name></person-group> (<year>1996</year>). <article-title>Spatial determinants of multisensory integration in cat superior colliculus neurons</article-title>. <source>J. Neurophysiol.</source>
<volume>75</volume>, <fpage>1843</fpage>&#x02013;<lpage>1857</lpage>. <pub-id pub-id-type="pmid">8734584</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moscatelli</surname><given-names>A.</given-names></name><name><surname>Mezzetti</surname><given-names>M.</given-names></name><name><surname>Lacquaniti</surname><given-names>F.</given-names></name></person-group> (<year>2012</year>). <article-title>Modeling psychophysical data at the population-level: the generalized linear mixed model</article-title>. <source>J. Vis.</source>
<volume>12</volume>:<fpage>26</fpage>. <pub-id pub-id-type="doi">10.1167/12.11.26</pub-id><pub-id pub-id-type="pmid">23104819</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Occelli</surname><given-names>V.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name><name><surname>Zampini</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Audiotactile interactions in front and rear space</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>35</volume>, <fpage>589</fpage>&#x02013;<lpage>598</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2010.07.004</pub-id><pub-id pub-id-type="pmid">20621120</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paillard</surname><given-names>J.</given-names></name></person-group> (<year>1991</year>). <article-title>Motor and representational framing of space</article-title>. <source>Brain Sp.</source>
<volume>182</volume>, <fpage>147</fpage>&#x02013;<lpage>162</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://jacquespaillard.apinc.org/pdf/207-framing-of-space-91.pdf">http://jacquespaillard.apinc.org/pdf/207-framing-of-space-91.pdf</ext-link></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasqualotto</surname><given-names>A.</given-names></name><name><surname>Proulx</surname><given-names>M. J.</given-names></name></person-group> (<year>2012</year>). <article-title>The role of visual experience for the neural basis of spatial cognition</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>36</volume>, <fpage>1179</fpage>&#x02013;<lpage>1187</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2012.01.008</pub-id><pub-id pub-id-type="pmid">22330729</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paus</surname><given-names>T.</given-names></name></person-group> (<year>1996</year>). <article-title>Modulation of cerebral blood flow in the human auditory cortex during speech: role of motor-to-sensory discharges</article-title>. <source>Eur. J. Neurosci.</source>
<volume>8</volume>, <fpage>2236</fpage>&#x02013;<lpage>2246</lpage>. <pub-id pub-id-type="doi">10.1111/j.1460-9568.1996.tb01187.x</pub-id><pub-id pub-id-type="pmid">8950088</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;der</surname><given-names>B.</given-names></name><name><surname>Teder-S&#x000e4;lej&#x000e4;rvi</surname><given-names>W.</given-names></name><name><surname>Sterr</surname><given-names>A.</given-names></name><name><surname>R&#x000f6;sler</surname><given-names>F.</given-names></name><name><surname>Hillyard</surname><given-names>S. A.</given-names></name><name><surname>Neville</surname><given-names>H. J.</given-names></name></person-group> (<year>1999</year>). <article-title>Improved auditory spatial tuning in blind humans</article-title>. <source>Nature</source>
<volume>400</volume>, <fpage>162</fpage>&#x02013;<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1038/22106</pub-id><pub-id pub-id-type="pmid">10408442</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>Action observation modulates auditory perception of the consequence of others&#x02019; actions</article-title>. <source>Conscious. Cogn.</source>
<volume>17</volume>, <fpage>1219</fpage>&#x02013;<lpage>1227</lpage>. <pub-id pub-id-type="doi">10.1016/j.concog.2008.01.003</pub-id><pub-id pub-id-type="pmid">18299207</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scandola</surname><given-names>M.</given-names></name><name><surname>Aglioti</surname><given-names>S. M.</given-names></name><name><surname>Bonente</surname><given-names>C.</given-names></name><name><surname>Avesani</surname><given-names>R.</given-names></name><name><surname>Moro</surname><given-names>V.</given-names></name></person-group> (<year>2016</year>). <article-title>Spinal cord lesions shrink peripersonal space around the feet, passive mobilization of paraplegic limbs restores it</article-title>. <source>Sci. Rep.</source>
<volume>6</volume>:<fpage>24126</fpage>. <pub-id pub-id-type="doi">10.1038/srep24126</pub-id><pub-id pub-id-type="pmid">27049439</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schicke</surname><given-names>T.</given-names></name><name><surname>Bauer</surname><given-names>F.</given-names></name><name><surname>R&#x000f6;der</surname><given-names>B.</given-names></name></person-group> (<year>2009</year>). <article-title>Interactions of different body parts in peripersonal space: how vision of the foot influences tactile perception at the hand</article-title>. <source>Exp. Brain Res.</source>
<volume>192</volume>, <fpage>703</fpage>&#x02013;<lpage>715</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-008-1587-2</pub-id><pub-id pub-id-type="pmid">18841353</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serino</surname><given-names>A.</given-names></name><name><surname>Noel</surname><given-names>J.-P.</given-names></name><name><surname>Galli</surname><given-names>G.</given-names></name><name><surname>Canzoneri</surname><given-names>E.</given-names></name><name><surname>Marmaroli</surname><given-names>P.</given-names></name><name><surname>Lissek</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Body part-centered and full body-centered peripersonal space representations</article-title>. <source>Sci. Rep.</source>
<volume>5</volume>:<fpage>18603</fpage>. <pub-id pub-id-type="doi">10.1038/srep18603</pub-id><pub-id pub-id-type="pmid">26690698</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smid</surname><given-names>K. A.</given-names></name><name><surname>den Otter</surname><given-names>A. R.</given-names></name></person-group> (<year>2013</year>). <article-title>Why you need to look where you step for precise foot placement: the effects of gaze eccentricity on stepping errors</article-title>. <source>Gait Posture</source>
<volume>38</volume>, <fpage>242</fpage>&#x02013;<lpage>246</lpage>. <pub-id pub-id-type="doi">10.1016/j.gaitpost.2012.11.019</pub-id><pub-id pub-id-type="pmid">23266044</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomasino</surname><given-names>B.</given-names></name><name><surname>Weiss</surname><given-names>P. H.</given-names></name><name><surname>Fink</surname><given-names>G. R.</given-names></name></person-group> (<year>2012</year>). <article-title>Imagined tool-use in near and far space modulates the extra-striate body area</article-title>. <source>Neuropsychologia</source>
<volume>50</volume>, <fpage>2467</fpage>&#x02013;<lpage>2476</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.06.018</pub-id><pub-id pub-id-type="pmid">22749971</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ticini</surname><given-names>L. F.</given-names></name><name><surname>Sch&#x000fc;tz-Bosbach</surname><given-names>S.</given-names></name><name><surname>Weiss</surname><given-names>C.</given-names></name><name><surname>Casile</surname><given-names>A.</given-names></name><name><surname>Waszak</surname><given-names>F.</given-names></name></person-group> (<year>2012</year>). <article-title>When sounds become actions: higher-order representation of newly learned action sounds in the human motor system</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>24</volume>, <fpage>464</fpage>&#x02013;<lpage>474</lpage>. <pub-id pub-id-type="doi">10.1162/jocn_a_00134</pub-id><pub-id pub-id-type="pmid">21916562</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Stoep</surname><given-names>N.</given-names></name><name><surname>Nijboer</surname><given-names>T. C. W.</given-names></name><name><surname>Van der Stigchel</surname><given-names>S.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Multisensory interactions in the depth plane in front and rear space: a review</article-title>. <source>Neuropsychologia</source>
<volume>70</volume>, <fpage>335</fpage>&#x02013;<lpage>349</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.12.007</pub-id><pub-id pub-id-type="pmid">25498407</pub-id></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vercillo</surname><given-names>T.</given-names></name><name><surname>Tonelli</surname><given-names>A.</given-names></name><name><surname>Gori</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Intercepting a sound without vision</article-title>. <source>PLoS One</source>
<volume>12</volume>:<fpage>e0177407</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0177407</pub-id><pub-id pub-id-type="pmid">28481939</pub-id></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname><given-names>M.</given-names></name><name><surname>Ingram</surname><given-names>J. N.</given-names></name><name><surname>Haggard</surname><given-names>P.</given-names></name><name><surname>Wolpert</surname><given-names>D. M.</given-names></name></person-group> (<year>2006</year>). <article-title>Sensorimotor attenuation by central motor command signals in the absence of movement</article-title>. <source>Nat. Neurosci.</source>
<volume>9</volume>, <fpage>26</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1038/nn1592</pub-id><pub-id pub-id-type="pmid">16311591</pub-id></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname><given-names>P.</given-names></name><name><surname>Lassonde</surname><given-names>M.</given-names></name><name><surname>Gougoux</surname><given-names>F.</given-names></name><name><surname>Fortin</surname><given-names>M.</given-names></name><name><surname>Guillemot</surname><given-names>J.-P. P.</given-names></name><name><surname>Lepore</surname><given-names>F.</given-names></name></person-group> (<year>2004</year>). <article-title>Early- and late-onset blind individuals show supra-normal auditory abilities in far-space</article-title>. <source>Curr. Biol.</source>
<volume>14</volume>, <fpage>1734</fpage>&#x02013;<lpage>1738</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2004.09.051</pub-id><pub-id pub-id-type="pmid">15458644</pub-id></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallach</surname><given-names>H.</given-names></name></person-group> (<year>1938</year>). <article-title>On sound localization</article-title>. <source>J. Acoust. Soc. Am.</source>
<volume>10</volume>, <fpage>83</fpage>&#x02013;<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1121/1.1902063</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>C.</given-names></name><name><surname>Herwig</surname><given-names>A.</given-names></name><name><surname>Sch&#x000fc;tz-Bosbach</surname><given-names>S.</given-names></name></person-group> (<year>2011</year>). <article-title>The self in action effects: selective attenuation of self-generated sounds</article-title>. <source>Cognition</source>
<volume>121</volume>, <fpage>207</fpage>&#x02013;<lpage>218</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2011.06.011</pub-id><pub-id pub-id-type="pmid">21784422</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wenzel</surname><given-names>E. M.</given-names></name><name><surname>Arruda</surname><given-names>M.</given-names></name><name><surname>Kistler</surname><given-names>D. J.</given-names></name><name><surname>Wightman</surname><given-names>F. L.</given-names></name></person-group> (<year>1993</year>). <article-title>Localization using nonindividualized head-related transfer functions</article-title>. <source>J. Acoust. Soc. Am.</source>
<volume>94</volume>, <fpage>111</fpage>&#x02013;<lpage>123</lpage>. <pub-id pub-id-type="doi">10.1121/1.407089</pub-id><pub-id pub-id-type="pmid">8354753</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wightman</surname><given-names>F. L.</given-names></name><name><surname>Kistler</surname><given-names>D. J.</given-names></name></person-group> (<year>1999</year>). <article-title>Resolution of front-back ambiguity in spatial hearing by listener and source movement</article-title>. <source>J. Acoust. Soc. Am.</source>
<volume>105</volume>, <fpage>2841</fpage>&#x02013;<lpage>2853</lpage>. <pub-id pub-id-type="doi">10.1121/1.426899</pub-id><pub-id pub-id-type="pmid">10335634</pub-id></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X.</given-names></name><name><surname>Chung</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>D.-Y.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Dodd</surname><given-names>S. J.</given-names></name><name><surname>Walters</surname><given-names>J. R.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Thalamocortical inputs show post-critical-period plasticity</article-title>. <source>Neuron</source>
<volume>74</volume>, <fpage>731</fpage>&#x02013;<lpage>742</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.024</pub-id><pub-id pub-id-type="pmid">22632730</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwiers</surname><given-names>M. P.</given-names></name><name><surname>Van Opstal</surname><given-names>A. J.</given-names></name><name><surname>Cruysberg</surname><given-names>J. R. M.</given-names></name></person-group> (<year>2001</year>). <article-title>A spatial hearing deficit in early-blind humans</article-title>. <source>J. Neurosci.</source>
<volume>21</volume>,<fpage>RC142</fpage>&#x02013;<lpage>RC145</lpage>. <pub-id pub-id-type="pmid">11312316</pub-id></mixed-citation></ref></ref-list></back></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Adv Health Sci Educ Theory Pract</journal-id><journal-title-group><journal-title>Advances in Health Sciences Education</journal-title></journal-title-group><issn pub-type="ppub">1382-4996</issn><issn pub-type="epub">1573-1677</issn><publisher><publisher-name>Springer Netherlands</publisher-name><publisher-loc>Dordrecht</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">20054648</article-id><article-id pub-id-type="pmc">2964459</article-id><article-id pub-id-type="publisher-id">9215</article-id><article-id pub-id-type="doi">10.1007/s10459-009-9215-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>The effects of performance-based assessment criteria on student performance and self-assessment skills</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Fastr&#x000e9;</surname><given-names>Greet Mia Jos</given-names></name><address><email>g.fastre@educ.unimaas.nl</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>van der Klink</surname><given-names>Marcel R.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>van Merri&#x000eb;nboer</surname><given-names>Jeroen J. G.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label>Open University of the Netherlands, Heerlen, The Netherlands </aff><aff id="Aff2"><label>2</label>Maastricht University, Maastricht, The Netherlands </aff></contrib-group><pub-date pub-type="epub"><day>7</day><month>1</month><year>2010</year></pub-date><pub-date pub-type="pmc-release"><day>7</day><month>1</month><year>2010</year></pub-date><pub-date pub-type="ppub"><month>10</month><year>2010</year></pub-date><volume>15</volume><issue>4</issue><fpage>517</fpage><lpage>532</lpage><history><date date-type="received"><day>16</day><month>6</month><year>2009</year></date><date date-type="accepted"><day>16</day><month>12</month><year>2009</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2010</copyright-statement></permissions><abstract><p>This study investigated the effect of performance-based versus competence-based assessment criteria on task performance and self-assessment skills among 39 novice secondary vocational education students in the domain of nursing and care. In a performance-based assessment group students are provided with a preset list of performance-based assessment criteria, describing what students should do, for the task at hand. The performance-based group is compared to a competence-based assessment group in which students receive a preset list of competence-based assessment criteria, describing what students should be able to do. The test phase revealed that the performance-based group outperformed the competence-based group on test task performance. In addition, higher performance of the performance-based group was reached with lower reported mental effort during training, indicating a higher instructional efficiency for novice students.</p></abstract><kwd-group><title>Keywords</title><kwd>Competence-based assessment criteria</kwd><kwd>Mental effort</kwd><kwd>Performance-based assessment criteria</kwd><kwd>Self-assessment skills</kwd><kwd>Task performance</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Science+Business Media B.V. 2010</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p>In competence-based education, authentic learning tasks based on real-life problems are the driving force behind training, simultaneously encouraging the development of professional skills and more general competences like being self-directed. Competence-based education is a dominant trend in vocational education in many European countries (Wesselink et al. <xref ref-type="bibr" rid="CR31">2007</xref>). The aim is to prepare students for the workplace where people are expected to be broadly educated while stimulating lifelong learning (van Merri&#x000eb;nboer et al. <xref ref-type="bibr" rid="CR30">2002</xref>, <xref ref-type="bibr" rid="CR28">2009</xref>). Because competences are context-bound and the aim of vocational education is preparing students for the workplace, students should always develop competences in the context of a profession (Biemans et al. <xref ref-type="bibr" rid="CR2">2004</xref>). When teachers want to judge the competence development of their students, student assessments performed in a real-life context can support their findings.</p><p>Assessment criteria and standards are key clues for students to know what is essential in their study program. Fastr&#x000e9; et al. (<xref ref-type="bibr" rid="CR7">2009</xref>) show that drawing students&#x02019; attention to the assessment criteria that are relevant for a particular learning task improves their understanding of the criteria and subsequently leads to better test task performance and better self-assessment skills. The following citation of Otter (<xref ref-type="bibr" rid="CR17">1995</xref>) emphasizes the importance of being familiar with the relevant assessment criteria:<disp-quote><p>Describing and making clear and public what the learner is intended to achieve changes the nature of assessment from a tutor-led system with fuzzy objectives and undisclosed criteria, to a student-led system with greater emphasis on formative development and personal responsibility. (p. 45).</p></disp-quote></p><p>In the behavioural tradition of instruction and instructional design, assessment criteria were performance-based, meaning that they described the desired performance in terms of what the student has to do (e.g. Mager <xref ref-type="bibr" rid="CR13">1984</xref>). With the introduction of competence-based education, assessment criteria are often formulated as competences, in terms of what the student is able to do. However, no research so far has investigated the effects of this introduction of competence-based assessment criteria. The main goal of this study is to investigate the effects of competence-based versus performance-based assessment criteria on learning, test task performance and students&#x02019; self-assessment skills.</p><p>The difference between performance-based and competence-based assessment criteria should be seen as a continuum, where on the one end assessment criteria are formulated as competences, which are an integration of knowledge, skills and attitudes; and on the other end assessment criteria are formulated as performance indicators. Performance-based criteria can be linked directly to competence-based criteria and vice versa as they complement each other. When discussing the continuum, the two extremes and their underlying connection will be tackled. The discussion will be coupled to the level of experience students have as it can be assumed that students with different levels of experience will have different needs concerning assessment criteria (Kalyuga <xref ref-type="bibr" rid="CR10">2007</xref>). In this article the focus is on the needs of novice students.</p><p>Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> presents a summary of the continuum between competence-based and performance-based assessment criteria: (1) What is assessed, (2) the nature of the criteria, (3) holistic versus analytic, and (4) the level of mental effort.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>Continuum of performance-based to competence-based assessment criteria</p></caption><graphic xlink:href="10459_2009_9215_Fig1_HTML" id="MO1"/></fig></p><p>First, with regard to what is assessed, when assessing with competence-based criteria, the competences underlying the performance are the focus of the assessment. What is assessed is the student&#x02019;s ability to perform a certain task. However, competences as a whole are not directly observable (Gr&#x000e9;goire <xref ref-type="bibr" rid="CR8">1997</xref>). Certain aspects of competences are observable, like particular skills the students demonstrate, but certain aspects are hidden, like their self-concept and personal characteristics that influence their performance (Spencer and Spencer <xref ref-type="bibr" rid="CR22">1993</xref>).</p><p>When assessing with performance-based criteria, the observable behaviours produced by the students are the heart of the assessment. The question is not if the student is able to perform the task, but if the student shows good performance (Gr&#x000e9;goire <xref ref-type="bibr" rid="CR8">1997</xref>). In order to show this good performance, students probably also know how to perform and consequently master the underlying competences necessary for performing the task (Miller <xref ref-type="bibr" rid="CR15">1990</xref>). For example, in the case of stoma care, the student shows he can remove the stoma in a correct way. An underlying competence is supporting the patient according to protocols, regulations and the vision of the organisation but the performance criterion is removing the stoma in a correct way. This means there is a direct link between what students show (performance) and what students are able to do (competence). Every performance shown involves one or more competences the student has to possess to perform well, and every competence can be shown in several behaviours of the student.</p><p>Because for novice students it is important in an early stage to obtain an idea of how well they are doing, the directly observable character of the performance-based criteria may be expected to be more beneficial to assess their task performance. Based on these performance-based criteria, the development of the students from the beginning on can be monitored. In order to improve novice students&#x02019; self-assessment skills, it is easier to assess what they are actually doing because this is more objective than their ability to do so. Therefore, with regard to what is assessed, performance-based criteria are expected to be more beneficial for supporting novice students&#x02019; learning than competence-based criteria. In later stages, it is important for students to learn to see the link with the underlying abilities they are developing.</p><p>Second, with regard to the nature of the criteria, to uncover competence development, consistency of proof of competence level across different tasks is needed (Albanese et al. <xref ref-type="bibr" rid="CR1">2008</xref>; Gr&#x000e9;goire <xref ref-type="bibr" rid="CR8">1997</xref>). It is therefore important to formulate competence-based assessment criteria in a way that they can be used across different tasks and thus are task-independent. For example, a nurse has to be able to conduct nursing technical skills. In one situation this means replacing a stoma bag while in another situation this means washing a patient.</p><p>To judge student performance on a certain task, performance-based assessment criteria should be formulated on task-level as for each task a different set of criteria is relevant. Performance-based criteria are thus task-dependent. As is shown by Fastr&#x000e9; et al. (<xref ref-type="bibr" rid="CR7">2009</xref>), for novice students it is important to know the relevant criteria in every task. For example, when a nurse has to conduct stoma care, some of the relevant criteria are to remove the old stoma bag and apply a new one.</p><p>It is likely that when students know exactly what to do, their motivation, learning and performance will increase significantly (see for example Ecclestone <xref ref-type="bibr" rid="CR5">2001</xref>). Moreover, Miller (<xref ref-type="bibr" rid="CR16">2003</xref>) argues that having task-specific assessment criteria leads to a better quantitative differentiation of performance levels. This more detailed view on students&#x02019; performance, would argue for the use of performance-based assessment criteria. Following the results of Fastr&#x000e9; et al. (<xref ref-type="bibr" rid="CR7">2009</xref>), it can be concluded that the use of performance-based criteria is especially beneficial for novice students because of their task-specific character.</p><p>Third, the competence-based assessment model currently used in Europe, starts from a fixed set of competences that are categorically divided (e.g. communication skills, nursing technical skills). No further decomposition of the competences is made. The formulation of the competence-based assessment criteria is therefore holistic (Gr&#x000e9;goire <xref ref-type="bibr" rid="CR8">1997</xref>). This does not mean that when working with competence-based assessment criteria only a holistic judgment on the end result is given, but the criteria are more holistically formulated than the performance-based criteria.</p><p>In a performance-based assessment model, the whole task is hierarchically analysed by developing a skills hierarchy (van Merri&#x000eb;nboer <xref ref-type="bibr" rid="CR26">1997</xref>; Sadler <xref ref-type="bibr" rid="CR20">1985</xref>). In this way, criteria are expressed as a component of a higher-level criterion or a number of lower-level criteria. After the student performed the task, the teacher gives separate judgments on each of the preset criteria. Then, these judgments are combined to compose a final judgment which is often converted into a grade. As an example, Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> shows a part of this decomposition for performing the task of stoma care.<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>Part of the skill hierarchy of stoma care</p></caption><graphic xlink:href="10459_2009_9215_Fig2_HTML" id="MO2"/></fig></p><p>Gulikers et al. (<xref ref-type="bibr" rid="CR9">2008</xref>) discuss the notions of analytic versus holistic grading from the perspective of the level of experience of students. They argue that novice students need analytic criteria as guidelines in a step-by-step process leading to the desired behaviour. In future tasks, this helps to set appropriate learning goals (Eva and Regehr <xref ref-type="bibr" rid="CR6">2005</xref>). For more experienced students, analytic criteria may hamper their learning process because they have to be stimulated to keep their focus on a certain outcome level and they do not need the step-by-step approach any more (Scheffer et al. <xref ref-type="bibr" rid="CR21">2008</xref>). Following these ideas, for novice students it would be better to receive performance-based assessment criteria.</p><p>Finally, with regard to mental effort, when designing a study program, including assessment, it is important to strive for the optimal level of using students&#x02019; cognitive capacity (van Gog and Paas <xref ref-type="bibr" rid="CR25">2008</xref>). Cognitive load theory presupposes that people have a limited working memory capacity (Sweller et al. <xref ref-type="bibr" rid="CR24">1998</xref>; van Merri&#x000eb;nboer and Sweller <xref ref-type="bibr" rid="CR29">2005</xref>). Because of this limited capacity, it is essential for learning to properly allocate the available cognitive resources (Kalyuga et al. <xref ref-type="bibr" rid="CR11">2003</xref>).</p><p>An important difference can be distinguished here between novice students and more experienced students. For novice students, it is important to provide sufficient guidance that compensates for the limited knowledge they have on the task at hand (e.g. stoma care) by providing them performance-based assessment criteria because this requires less cognitive capacity for the assessment and most of their working memory capacity can be devoted to the task of stoma care. For more experienced students, who already have some knowledge on the task at hand (e.g. stoma care), competence-based assessment criteria can provide them with an extra stimulus to think about the task in another way and thereby make the extra cognitive capacity beneficial for them. In addition, providing these students with performance-based assessment criteria would give them redundant information on the task which may hamper their learning. This is called the expertise reversal effect (Kalyuga <xref ref-type="bibr" rid="CR10">2007</xref>).</p><p>Summarising, it appears that for novice students, performance-based criteria have more advantages than competence-based criteria because: (1) They are directly observable, (2) they lead to a better quantitative differentiation of levels of performance, (3) they stimulate a step-by-step process leading to desired performance, and (4) they require less cognitive capacity for assessment leaving more capacity for learning the task at hand. The following section describes the hypotheses following this comparison.</p></sec><sec id="Sec50"><title>Hypotheses</title><p>The first hypothesis is that students who receive the performance-based criteria during learning will show superior test task performance compared to students who receive the competence-based criteria because they know better what is expected from their performance. The second hypothesis is that students who receive the performance-based criteria will experience a lower mental effort during assessment than students who receive the competence-based criteria. The third hypothesis is that students who receive the performance-based criteria will be better self-assessors than students who receive the competence-based criteria because they are better able to assess their performance.</p></sec><sec id="Sec3" sec-type="methods"><title>Method</title><sec id="Sec4"><title>Participants</title><p>Thirty-nine second-year students of a school for Secondary Vocational Education, attending a Nursing and Care program (Level 3 and 4 in the European Qualifications Framework, 2 males and 37 females) participated in this study as part of their regular training on the nursing task of stoma care. Their mean age was 18.07&#x000a0;years (SD&#x000a0;=&#x000a0;1.05). Participants were randomly assigned to one of the two conditions: competence-based criteria (<italic>n</italic>&#x000a0;=&#x000a0;20) and performance-based criteria (<italic>n</italic>&#x000a0;=&#x000a0;19).</p></sec><sec id="Sec5" sec-type="materialsandmethods"><title>Materials</title><p>The whole task of stoma care, addressing the psychosocial needs of the patient, analysing the situation of the patient, changing the stoma bag, and the evaluation afterwards are included in the task. This means students did not only practise the technical skill of changing the stoma bag, but also needed knowledge on the stoma (e.g. possible problems with stomas), and an appropriate attitude towards the patient. The task was set up in accordance with the theory of experiential learning by Steinaker and Bell (<xref ref-type="bibr" rid="CR23">1979</xref>) which distinguishes four important steps: (a) exposure, (b) participation, (c) identification, and (d) internalisation. Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> summarises the materials described below.<fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>Summary of materials and measures</p></caption><graphic xlink:href="10459_2009_9215_Fig3_HTML" id="MO3"/></fig></p><sec id="Sec6"><title>Lecture</title><p>A lecture was developed that provided students with the theoretical background of stoma care. The two teachers who were responsible for this lecture set up the lecture together.</p></sec><sec id="Sec7"><title>Video examples and video assessment</title><p>An electronic learning environment was developed including six video fragments (&#x000b1;3&#x000a0;min each) in which an expert nurse shows good stoma care behavior. All fragments are subsequent parts of the whole task of stoma care: (1) Introduction, (2) preparation, (3) removing the old stoma bag, (4) applying the new stoma bag, (5) finishing off care, (6) evaluation and reporting. Students individually watched the video fragments on a computer screen. They were not allowed to put the fragment on hold, and they could watch the video a maximum of three times. On average, students watched the video 1.14 times (SD&#x000a0;=&#x000a0;.29). No differences between conditions were found.</p><p>After students watched the video, they had to assess the performance of the nurse in the video on an electronic list of preset criteria. A distinctive feature was made for the two conditions. In the competence-based condition, the assessment criteria were formulated as competences of stoma care as used previously in the study program (VA-C). Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> shows some examples, competence-based criteria as shown in the electronic learning environment.<fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>Screendump of competence-based assessment criteria</p></caption><graphic xlink:href="10459_2009_9215_Fig4_HTML" id="MO4"/></fig></p><p>In the performance-based condition, the assessment criteria were formulated as the underlying skills of a skill hierarchy of stoma care (VA-P). Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows some examples of performance-based criteria as shown in the learning environment.<fig id="Fig5"><label>Fig.&#x000a0;5</label><caption><p>Screendump of performance-based assessment criteria</p></caption><graphic xlink:href="10459_2009_9215_Fig5_HTML" id="MO5"/></fig></p><p>In order to encourage students to make the assessment criteria more concrete, students in both groups had to indicate the manner in which the nurse in the fragment showed good behaviour on the criteria by typing their answer in the text boxes.</p></sec><sec id="Sec8"><title>Practical lesson, peer assessment, and self-assessment</title><p>A practical training session was developed in which students had to practice in pairs or groups of three the task of stoma care with a fellow student being the patient. After students had performed the task, they had to score their peers&#x02019; task performance on the same list of criteria as in the assessment of the video examples. The students in the competence-based condition received the list with competence-based criteria (PA-C) and students in the performance-based condition received the list with performance-based criteria (PA-P). They had to indicate how well their peers mastered the criteria on a four-point scale: (1) behaviour not shown, (2) behaviour shown but insufficient, (3) behaviour shown and sufficient, (4) behaviour shown and good. In addition to this peer assessment, students had to self-asses their task performance using the identical list of competence-based criteria (SA-C) or performance based criteria (SA-P), using the same four-point scale. While practising the task, students also received oral feedback on their task performance from the instructor in the room.</p></sec><sec id="Sec9"><title>Examination and self-assessment</title><p>An examination was developed in which students individually had to perform the task of stoma care with a simulation patient. Afterwards they had to assess their own performance on that particular task by filling in a blank paper with the question: assess your own performance on this task and indicate what went well and what went wrong.</p></sec></sec><sec id="Sec10"><title>Measures</title><sec id="Sec11"><title>Background questionnaire</title><p>A short questionnaire measured the background of the students on demographical factors such as age, sex and prior education. Student perceptions of the relevance of self-assessment and their perceptions of their ability to self-assess were measured by the self-directed learning skills questionnaire adapted from Kicken et al. (<xref ref-type="bibr" rid="CR12">2006</xref>). This questionnaire proved reliable for the population in this study (Fastr&#x000e9; et al. <xref ref-type="bibr" rid="CR7">2009</xref>). Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> shows the Cronbach&#x02019;s alpha scores of the perception scales; internal consistencies ranged from .70 to .75 and are thus quite acceptable.<table-wrap id="Tab1"><label>Table&#x000a0;1</label><caption><p>Reliability of the self-directed learning skills questionnaire</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Scale</th><th align="left">Cronbach&#x02019;s alpha</th><th align="left"># Items</th><th align="left">Example item<sup>a</sup></th></tr></thead><tbody><tr><td align="left" colspan="4">Self-directed learning skills questionnaire</td></tr><tr><td align="left">&#x000a0;Relevance of self-assessment</td><td char="." align="char">.70</td><td char="." align="char">3</td><td align="left">I think the opinion of the teacher is more important than self-assessment</td></tr><tr><td align="left">&#x000a0;Ability to self-assess</td><td char="." align="char">.75</td><td char="." align="char">12</td><td align="left">I can assess to what extent my performance fits the assessment criteria</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>Items have been translated from Dutch</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec12"><title>Knowledge test</title><p>At the end of the lecture, a 15-item multiple choice test was taken to test the students&#x02019; knowledge on this subject.</p></sec><sec id="Sec13"><title>Judgment scheme for video assessment</title><p>To measure the accuracy of the video assessment, judgment schemes specified the quality of the video assessments. The overall score for quality of video assessment was the sum of the <italic>z</italic>-scores of the following aspects: how many words the students used because it is expected that performance-based criteria stimulate students more to elaborate on their answers (count of the number of words), if they gave concrete examples of the nurse&#x02019;s behaviour (0&#x000a0;=&#x000a0;no concrete behaviour, 1&#x000a0;=&#x000a0;concrete behaviour), and if they gave a judgment on the behaviour of the nurse (0&#x000a0;=&#x000a0;no judgment, 1&#x000a0;=&#x000a0;judgment). The higher the sum of the <italic>z</italic>-scores, the better the score for quality of video assessment as it is important that the combination of these factors is of a high quality. The quality of the video assessments was judged by two raters, with a high interrater reliability of <italic>r</italic>&#x000a0;=&#x000a0;.82, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.00.</p></sec><sec id="Sec14"><title>Mental effort rating scale</title><p>After the assessment of each video fragment, students were required to fill in the rating scale of Paas (<xref ref-type="bibr" rid="CR18">1992</xref>) that measured their mental effort as the &#x02018;effort required to perform the assessment&#x02019;, ranging from a very small amount of effort (1) to a very high amount of effort (7).</p></sec><sec id="Sec15"><title>Peer assessment of task performance</title><p>The peer assessments during the practical lesson indicated the task performance of the students assessed by the peers, using the competence-based criteria in one group and performance-based criteria in the other group. Peer assessed task performance was the average score on all the assessment criteria.</p></sec><sec id="Sec16"><title>Self-assessment of task performance</title><p>The self-assessments during the practical lesson indicated the task performance of the students by the students&#x02019; own opinion, using the competence-based criteria in one group and performance based criteria in the other group. Self-assessed task performance was the average score on all the assessment criteria.</p></sec><sec id="Sec17"><title>Teacher assessment of test task performance</title><p>During the examination, the teachers observed and assessed the test task performance of the students, who took care of the stoma of a simulation patient, on the list of performance-based criteria. A second assessor co-assessed with each of the teachers to measure the reliability of the assessments. The correlation between the scores of the teacher and the second assessor, <italic>r</italic>&#x000a0;=&#x000a0;.77, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.01, appeared to be acceptable.</p></sec><sec id="Sec18"><title>Judgment scheme for self-assessment</title><p>The overall score for quality of the self-assessments during examination was the sum of the <italic>z</italic>-scores of the following aspects: how many words the students used because it is expected that performance-based criteria stimulate students more to elaborate when self-assessing (count of the number of words), how many criteria the students came up with (count of the number of criteria), if students had a critical attitude to their own performance (0&#x000a0;=&#x000a0;no critical attitude, 1&#x000a0;=&#x000a0;critical attitude), and if they formulated points of improvement (0&#x000a0;=&#x000a0;no points of improvement, 1&#x000a0;=&#x000a0;points of improvement). The higher the sum of the <italic>z</italic>-scores, the better the score for quality of self-assessment because it is important that the combination of these factors is of a high quality. The quality of the self-assessments was judged by two raters, with an interrater reliability of <italic>r</italic>&#x000a0;=&#x000a0;.82, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.00.</p></sec><sec id="Sec19"><title>Perception questionnaire</title><p>The following aspects of perception were measured to evaluate the learning experience: Motivation for the study, regulation strategies, interesting course material, task orientation, pleasure and interest, pleasure and interest in reflection, and usefulness. All aspects were measured with the use of four-point Likert scales. Higher scores indicate a more positive perception of the learning experience. Two scales (interesting course material and task orientation) of the inventory of perceived study environment (IPSE; Wierstra et al. <xref ref-type="bibr" rid="CR32">1999</xref>) measured students&#x02019; perceptions of the learning environment. Three scales (interest and pleasure, interest and pleasure in reflection, and usefulness) of the intrinsic motivation inventory by Deci et al. (<xref ref-type="bibr" rid="CR3">1994</xref>), translated into Dutch by Martens and Kirschner (<xref ref-type="bibr" rid="CR14">2004</xref>), were included in the questionnaire. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> shows the Cronbach&#x02019;s alpha scores of the perception scales; internal consistencies ranged from .69 to .89 and are thus acceptable to high.<table-wrap id="Tab2"><label>Table&#x000a0;2</label><caption><p>Reliability of the perception measures</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Scale</th><th align="left">Cronbach&#x02019;s alpha</th><th align="left"># Items</th><th align="left">Example item<sup>a</sup></th></tr></thead><tbody><tr><td align="left" colspan="4">Inventory of perceived study environment</td></tr><tr><td align="left">&#x000a0;Interesting course materials</td><td char="." align="char">.72</td><td char="." align="char">8</td><td align="left">The learning task is interesting</td></tr><tr><td align="left">&#x000a0;Task orientation</td><td char="." align="char">.69</td><td char="." align="char">3</td><td align="left">I know what is expected of me when performing the task</td></tr><tr><td align="left" colspan="4">Intrinsic motivation inventory</td></tr><tr><td align="left">&#x000a0;Interest and pleasure in learning tasks</td><td char="." align="char">.69</td><td char="." align="char">7</td><td align="left">I enjoy working on the learning task</td></tr><tr><td align="left">&#x000a0;Interest and pleasure in reflection</td><td char="." align="char">.89</td><td char="." align="char">6</td><td align="left">I find it interesting to reflect</td></tr><tr><td align="left">&#x000a0;Usefulness</td><td char="." align="char">.69</td><td char="." align="char">4</td><td align="left">I would like to conduct more learning tasks because they are useful</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>Items have been translated from Dutch</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec20"><title>Measure of agreement</title><p>For the peer assessments and the self-assessments during the practical lesson, the agreement of the scores between the self- and peer assessments was measured by computing the Pearson&#x02019;s correlation.</p></sec><sec id="Sec21"><title>Instructional efficiency</title><p>Instructional efficiency is calculated by relating task performance in the test task and experienced mental effort during training (Paas and van Merri&#x000eb;nboer <xref ref-type="bibr" rid="CR19">1993</xref>; van Gog and Paas <xref ref-type="bibr" rid="CR25">2008</xref>). Performance and mental effort scores are first standardized, and then the <italic>z</italic>-scores are entered into the formula:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ E = {\frac{{Z_{\text{Performance}} - Z_{\text{Mentaleffort}} }}{\sqrt 2 }}. $$\end{document}</tex-math><graphic xlink:href="10459_2009_9215_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p><p>High efficiency indicates that with a relatively low mental effort during training a relatively high task performance in the examination is accomplished, while a low efficiency indicates that with a relatively high mental effort during training a relatively low task performance is accomplished. For example, instructional efficiency is higher for an instructional condition in which participants attain a certain performance level with a minimum investment of mental effort than for an instructional condition in which participants attain the same level of performance with a maximum investment of mental effort.</p></sec></sec><sec id="Sec22"><title>Procedure</title><p>At the start of the lecture, the background questionnaire was administered. After students had filled in the questionnaire, the lecture was given and the multiple choice test was taken. This phase lasted for 90&#x000a0;min.</p><p>After the lecture students were instructed to assess the video examples. While doing this, students were exposed to the stoma care by watching video examples of an expert nurse showing the intended behaviour, which is the first step in the taxonomy of Steinaker and Bell (<xref ref-type="bibr" rid="CR23">1979</xref>). Students were split up in the two experimental groups to work on the assessment of video examples. Students could work on the assessment of video examples for maximum 90&#x000a0;min. After the assessment of video examples, the practical lesson with peer and self-assessments took place for 90&#x000a0;min. In this lesson, students could participate in stoma care by practicing on a doll (second step).</p><p>One&#x000a0;week after the practical lesson, students had to conduct the examination after which they had to assess their own performance. In this examination, they could identify with the stoma care because they were exposed to a simulation patient in performing the care (third step). Student performance was assessed by a teacher. At the end of the examination the evaluation questionnaire was filled in by the students. The examination including self-assessment lasted for 40&#x000a0;min. After the whole experiment, students were sufficiently prepared for further practice during internships which leads them to internalise the competence of stoma care (fourth step).</p></sec></sec><sec id="Sec23"><title>Results</title><p>This section describes the results on prior measurements, the dependent variables in the learning and test phase, and the student perceptions. Mann&#x02013;whitney <italic>U</italic> tests were performed to test for differences between the two conditions. For all analyses, the significance level is set to .05. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> presents the means and standard deviations for all dependent variables.<table-wrap id="Tab3"><label>Table&#x000a0;3</label><caption><p>Means and standard deviations for dependent variables</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">Competence-based (<italic>n</italic>&#x000a0;=&#x000a0;20)</th><th align="left" colspan="2">Performance-based (<italic>n</italic>&#x000a0;=&#x000a0;19)</th></tr><tr><th align="left"><italic>M</italic></th><th align="left">SD</th><th align="left"><italic>M</italic></th><th align="left">SD</th></tr></thead><tbody><tr><td align="left" colspan="5">Pretest</td></tr><tr><td align="left">&#x000a0;Score on prior knowledge test</td><td align="left">7.72</td><td align="left">1.27</td><td align="left">7.85</td><td align="left">1.35</td></tr><tr><td align="left" colspan="5">Learning phase</td></tr><tr><td align="left">&#x000a0;Score quality of video assessment</td><td align="left">.08</td><td align="left">1.50</td><td align="left">.22</td><td align="left">1.65</td></tr><tr><td align="left">&#x000a0;Number of words</td><td align="left">889</td><td align="left">143</td><td align="left">855</td><td align="left">248</td></tr><tr><td align="left">&#x000a0;Concreteness of answer</td><td align="left">.50</td><td align="left">.52</td><td align="left">.80</td><td align="left">.41</td></tr><tr><td align="left">&#x000a0;Judgment</td><td align="left">.56</td><td align="left">.51</td><td align="left">.43</td><td align="left">.51</td></tr><tr><td align="left">&#x000a0;Mental effort</td><td align="left">3.33</td><td align="left">.75</td><td align="left">2.36</td><td align="left">.65</td></tr><tr><td align="left">&#x000a0;Task performance scored by peer</td><td align="left">2.95</td><td align="left">.34</td><td align="left">3.14</td><td align="left">.43</td></tr><tr><td align="left">&#x000a0;Task performance scored by self</td><td align="left">3.12</td><td align="left">.31</td><td align="left">3.25</td><td align="left">.48</td></tr><tr><td align="left" colspan="5">Test phase</td></tr><tr><td align="left">&#x000a0;Test task performance scored by teacher</td><td align="left">2.89</td><td align="left">.44</td><td align="left">3.18</td><td align="left">.45</td></tr><tr><td align="left">&#x000a0;Score quality of self-assessment</td><td align="left">&#x02212;.53</td><td align="left">2.73</td><td align="left">.56</td><td align="left">2.92</td></tr><tr><td align="left">&#x000a0;Number of words</td><td align="left">93</td><td align="left">45</td><td align="left">111</td><td align="left">67</td></tr><tr><td align="left">&#x000a0;Number of criteria</td><td align="left">6.35</td><td align="left">3.80</td><td align="left">6.32</td><td align="left">3.86</td></tr><tr><td align="left">&#x000a0;Critical attitude</td><td align="left">.60</td><td align="left">.50</td><td align="left">.74</td><td align="left">.45</td></tr><tr><td align="left">&#x000a0;Points of improvement</td><td align="left">.15</td><td align="left">.37</td><td align="left">.37</td><td align="left">.50</td></tr><tr><td align="left">&#x000a0;Instructional efficiency</td><td align="left">&#x02212;.65</td><td align="left">.81</td><td align="left">.68</td><td align="left">.91</td></tr></tbody></table></table-wrap></p><sec id="Sec24"><title>Prior measurements</title><p>On the background questionnaire, no significant difference between the conditions was found, indicating that students did not differ in background at the end of the lecture.</p><p>On the knowledge test, no significant difference between the conditions was found, indicating that all students had the same level of knowledge at the end of the lecture. Thus, students had the same background and prior knowledge before they started to study the video examples.</p></sec><sec id="Sec25"><title>Learning phase</title><p>On the overall score for quality of video assessment, a significant difference between the conditions was found, <italic>z</italic>&#x000a0;=&#x000a0;&#x02212;1.964, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.05. Students in the performance-based condition had an average rank of 18.21, while students in the competence-based condition had an average rank of 12.00. More specifically, on number of words no difference was found. In concreteness of answers, a significant difference was found, <italic>z</italic>&#x000a0;=&#x000a0;&#x02212;1.716, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.05. Students in the performance-based condition had an average rank of 18.40, while students in the competence-based condition had an average rank of 13.75. No significant difference in judgment was found. A further qualitative analysis of the data reveals that students in the competence-based condition often decoded the competence-based assessment criteria into the performance-based criteria as an answer but were not able to describe the concrete behaviour.</p><p>Mental effort during assessment of the video examples is an average score of the scores during assessment of the six film fragments. On mental effort, a significant difference between conditions was found, <italic>z</italic>&#x000a0;=&#x000a0;&#x02212;3.964, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.001, indicating that students in the performance-based condition had an average rank of 12.61, while students in the competence-based condition had an average rank of 27.03.</p><p>On peer assessment and self-assessment of task performance in the practical lesson, no significant differences between conditions was found. Yet, a moderate agreement between peer and self-assessment was found, <italic>r</italic>&#x000a0;=&#x000a0;.65, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.00, indicating that students&#x02019; self-assessment scores corresponded with the scores of their peers. For the performance-based condition <italic>r</italic>&#x000a0;=&#x000a0;.66, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.01, and for the competence-based condition <italic>r</italic>&#x000a0;=&#x000a0;.63, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.01.</p></sec><sec id="Sec26"><title>Test phase</title><p>On test task performance, a significant difference between conditions was found, <italic>z</italic>&#x000a0;=&#x000a0;&#x02212;2.037, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.05. Students in the performance-based condition had an average rank of 23.82, while students in the competence-based condition had an average rank of 16.38. On the overall score for quality of self-assessment, no significant differences between both conditions were found. Although not significant, the direction of the differences was in line with the expectations. On instructional efficiency, a significant difference between conditions was found, <italic>z</italic>&#x000a0;=&#x000a0;&#x02212;3.962, <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;.001, indicating that students in the performance-based condition had an average rank of 27.42, while students in the competence-based condition had an average rank of 12.95.</p></sec><sec id="Sec27"><title>Evaluation questionnaire</title><p>Overall, students perceived the learning environment as interesting and useful. Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> shows the means and standard deviations for all scales.<table-wrap id="Tab4"><label>Table&#x000a0;4</label><caption><p>Means and standard deviations for evaluation questionnaire</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">Competence-based (<italic>n</italic>&#x000a0;=&#x000a0;20)</th><th align="left" colspan="2">Performance-based (<italic>n</italic>&#x000a0;=&#x000a0;19)</th></tr><tr><th align="left"><italic>M</italic></th><th align="left">SD</th><th align="left"><italic>M</italic></th><th align="left">SD</th></tr></thead><tbody><tr><td align="left">Interesting course material</td><td char="." align="char">3.13</td><td char="." align="char">.43</td><td char="." align="char">3.07</td><td char="." align="char">.39</td></tr><tr><td align="left">Task orientation</td><td char="." align="char">2.83</td><td char="." align="char">.67</td><td char="." align="char">2.51</td><td char="." align="char">.70</td></tr><tr><td align="left">Interest and pleasure</td><td char="." align="char">3.47</td><td char="." align="char">.39</td><td char="." align="char">3.35</td><td char="." align="char">.37</td></tr><tr><td align="left">Interest and pleasure in reflection</td><td char="." align="char">2.59</td><td char="." align="char">.43</td><td char="." align="char">2.51</td><td char="." align="char">.58</td></tr><tr><td align="left">Usefulness</td><td char="." align="char">3.45</td><td char="." align="char">.52</td><td char="." align="char">3.50</td><td char="." align="char">.33</td></tr></tbody></table></table-wrap></p><p>No significant differences were found between conditions. Being in the performance-based or competence-based condition did not influence students&#x02019; perceptions of the learning task.</p></sec></sec><sec id="Sec28"><title>Discussion</title><p>The goal of this study was to investigate the effects of competence-based versus performance-based assessment criteria on students&#x02019; test task performance and self-assessment skills. The first hypothesis, stating that students who receive the performance-based criteria will be better task performers than students who receive the competence-based criteria is confirmed by the data. It seems that novice students who receive the performance-based criteria during training know better what is expected from their task performance and are better able to show desired performance than students who receive the competence-based criteria. A possible explanation is the finding that students who receive the performance-based criteria had a higher quality of video assessments in the learning phase. They were especially better in being concrete on the desired behaviour, which may have led to better task performance in the test phase. This is in line with the ideas of Eva and Regehr (<xref ref-type="bibr" rid="CR6">2005</xref>), who state that performance-based criteria make it easier to distinguish levels of performance, enabling a step-by-step process of performance improvement.</p><p>The second hypothesis, stating that students who receive the performance-based criteria experience a lower mental effort during assessment than students who receive the competence-based criteria is also confirmed by the data. It appears that by providing novice students with performance-based assessment criteria, they have to invest less mental effort to assess their task performance. This effect is positive when it leads to a better test task performance because this would mean that during training the reduced load of assessment permits more cognitive capacity for learning to perform the task of stoma care.</p><p>Indeed, the findings concerning the first and second hypotheses together allow to conclude that the performance-based assessment criteria result into a higher instructional efficiency, since students in the performance-based condition experience a lower cognitive load during the learning phase, followed by a higher performance on the test task (Paas and van Merri&#x000eb;nboer <xref ref-type="bibr" rid="CR19">1993</xref>; van Gog and Paas <xref ref-type="bibr" rid="CR25">2008</xref>). Providing novice students with performance-based assessment criteria thus leads to more efficient learning.</p><p>The third hypothesis, stating that students who receive the performance-based criteria become better self-assessors than students who receive the competence-based criteria, is not confirmed by the results. This finding is, however, in line with the findings of Dunning et al. (<xref ref-type="bibr" rid="CR4">2004</xref>), who also found that for novice students knowing the assessment criteria does not necessarily imply the ability to assess their own performance on those criteria. As self-assessment can be seen as a complex cognitive skill, one of the key words in developing this skill is sufficient practice (van Merri&#x000eb;nboer and Kirschner <xref ref-type="bibr" rid="CR27">2007</xref>). It is likely that students need considerably more practice than provided in the current study to improve their self-assessment skills.</p><p>Finally, students did not differ in their perceptions of the learning environment. Receiving competence-based or performance-based criteria thus did not influence their appreciation of the learning task. The findings indicate that both groups were positive about the learning task as a whole and especially valued the provided video examples.</p><p>The results of this study show that for novice students performance-based assessment criteria do lead to a lower mental effort during learning and a higher test task performance, which is in line with our theoretical assumption that for novice learners it is better to use performance-based criteria than competence-based criteria. The question remains, however, what causes the observed effects. The relative importance of the separate dimensions of Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> was not investigated in this study and further research is required to determine the contribution of the various dimensions to the reported effects on mental effort during learning and test task performance. Is it because these criteria refer to directly observable behaviour? Or is it because the criteria are more task-dependent? Maybe the analytic character of the criteria is the driving force behind these effects? These insights could serve as a guideline for teachers in the development of performance-based assessment criteria and should be further examined.</p><p>Furthermore, the effects of providing students with performance-based assessment criteria should be examined with students in later years of the educational program to explore differences between novice and more experienced students as it is expected that students in later phases of their educational program have to learn to think on a higher level and thus work more efficient with competence-based criteria.</p><p>A shortcoming of this study is the limited duration of the intervention. Because this intervention was restricted to only one learning task (i.e. stoma care), students did not get the opportunity to practice extensively on their skill development. This was most visible for the complex cognitive skill of self-assessment. According to van Merri&#x000eb;nboer and Kirschner (<xref ref-type="bibr" rid="CR27">2007</xref>), more training is needed to develop this kind of skill. Furthermore, only a small sample was used in the study. The question remains if the results are transferable to larger groups of students or students in other domains. Nevertheless, the fact that this intervention yielded some important results concerning mental effort expenditure during learning and test task performance is a sound basis for further research on this topic.</p><p>The findings yield the clear guideline that novice students should be provided with performance-based assessment criteria in order to improve their learning process, and reach higher test task performance. For instructing young nurses in the beginning of their study, performance-based assessment criteria are a necessity to guide their learning process. It should be noted, however, that formulating such performance-based criteria is a demanding task. To assure a sound implementation, training should be provided to teachers to increase their skills in formulating performance-based assessment criteria, based on a systematic process of drawing up a skills hierarchy with related criteria. When students progress in the study program, explicit attention should be paid to training students to interpret their own behaviours in terms of the underlying competences. In this way, students learn to see the link between performance and competence development. If this is not explicitly in the program, students remain on a lower level of thinking.</p><p>To conclude, the introduction of competence-based education primarily consisting of authentic learning tasks based on real-life problems, leads educators to solve the issue of how to redesign their assessment programs. Our results show that stating that competence-based assessment criteria are the answer to this problem is a step too far. Whereas competences seem to be a good starting point to develop professional education, they do not always serve this purpose for assessment. At least for novice students, providing them with performance-based assessment criteria is more beneficial than providing them with competence-based criteria. This study shows that novice students need less mental effort to assess their task performance and show higher test task performance, that is, they learn more efficiently when being provided with performance-based assessment criteria.</p></sec></body><back><ack><p>We would like to thank the participants in this study and the staff of ROC A12 for all their help in conducting this research. Participants were offered confidentiality. Ethical approval was not necessary as this study was part of the normal education program.</p><p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><mixed-citation publication-type="other">Albanese, M. A., Mejicano, G., &#x00026; Anderson, W. M. (2008, March 15). Building a competency-based curriculum: The agony and the ecstasy. Advances in Health Sciences Education. Retrieved 26 May 2009, from <ext-link ext-link-type="uri" xlink:href="http://www.springerlink.com/content/g1w11258v173u6r6/fulltext.pdf">http://www.springerlink.com/content/g1w11258v173u6r6/fulltext.pdf</ext-link>.</mixed-citation></ref><ref id="CR2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biemans</surname><given-names>H</given-names></name><name><surname>Nieuwenhuis</surname><given-names>L</given-names></name><name><surname>Poell</surname><given-names>R</given-names></name><name><surname>Mulder</surname><given-names>M</given-names></name><name><surname>Wesselink</surname><given-names>R</given-names></name></person-group><article-title>Competence-based VET in the Netherlands: Background and pitfalls</article-title><source>Journal of Vocational Education and Training</source><year>2004</year><volume>36</volume><issue>4</issue><fpage>523</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1080/13636820400200268</pub-id></mixed-citation></ref><ref id="CR3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deci</surname><given-names>EL</given-names></name><name><surname>Eghrari</surname><given-names>H</given-names></name><name><surname>Patrick</surname><given-names>BC</given-names></name><name><surname>Leone</surname><given-names>D</given-names></name></person-group><article-title>Facilitating internalization: The self-determination theory perspective</article-title><source>Journal of Personality</source><year>1994</year><volume>62</volume><fpage>119</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1111/j.1467-6494.1994.tb00797.x</pub-id><pub-id pub-id-type="pmid">8169757</pub-id></mixed-citation></ref><ref id="CR4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunning</surname><given-names>D</given-names></name><name><surname>Heath</surname><given-names>C</given-names></name><name><surname>Suls</surname><given-names>JM</given-names></name></person-group><article-title>Flawed self-assessment: Implications for health, education, and the workplace</article-title><source>Psychological science in the public interest</source><year>2004</year><volume>5</volume><issue>3</issue><fpage>69</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1111/j.1529-1006.2004.00018.x</pub-id></mixed-citation></ref><ref id="CR5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecclestone</surname><given-names>K</given-names></name></person-group><article-title>&#x02018;I know a 2:1 when I see it&#x02019;: Understanding criteria for degree classifications in franchised university programmes</article-title><source>Journal of Further and Higher Education</source><year>2001</year><volume>25</volume><issue>3</issue><fpage>301</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1080/03098770126527</pub-id></mixed-citation></ref><ref id="CR6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eva</surname><given-names>KW</given-names></name><name><surname>Regehr</surname><given-names>G</given-names></name></person-group><article-title>Self-assessment in the health professions: A reformulation and research agenda</article-title><source>Academic Medicine</source><year>2005</year><volume>80</volume><issue>10</issue><fpage>46</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1097/00001888-200510001-00015</pub-id></mixed-citation></ref><ref id="CR7"><mixed-citation publication-type="other">Fastr&#x000e9;, G. M. J., Van der Klink, M., van Merri&#x000eb;nboer, J. J. G. &#x00026; Sluijsmans, D. (2009). Drawing students&#x02019; attention to relevant assessment criteria: Effects on self-assessment skills and performance. Manuscript submitted for publication.</mixed-citation></ref><ref id="CR8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gr&#x000e9;goire</surname><given-names>J</given-names></name></person-group><article-title>Diagnostic assessment of learning disabilities. From assessment of performance to assessment of competence</article-title><source>European Journal of Psychological Assessment</source><year>1997</year><volume>13</volume><issue>1</issue><fpage>10</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1027/1015-5759.13.1.10</pub-id></mixed-citation></ref><ref id="CR9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulikers</surname><given-names>JTM</given-names></name><name><surname>Kester</surname><given-names>L</given-names></name><name><surname>Kirschner</surname><given-names>PA</given-names></name><name><surname>Bastiaens</surname><given-names>Th J</given-names></name></person-group><article-title>The influence of practical experience on perceptions, study approach and learning outcomes in authentic assessment</article-title><source>Learning and Instruction</source><year>2008</year><volume>18</volume><issue>2</issue><fpage>172</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.learninstruc.2007.02.012</pub-id></mixed-citation></ref><ref id="CR10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalyuga</surname><given-names>S</given-names></name></person-group><article-title>Expertise reversal effect and its implications for learner-tailored instruction</article-title><source>Educational Psychology Review</source><year>2007</year><volume>19</volume><fpage>509</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1007/s10648-007-9054-3</pub-id></mixed-citation></ref><ref id="CR11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalyuga</surname><given-names>S</given-names></name><name><surname>Ayres</surname><given-names>P</given-names></name><name><surname>Chandler</surname><given-names>P</given-names></name><name><surname>Sweller</surname><given-names>J</given-names></name></person-group><article-title>The expertise reversal effect</article-title><source>Educational Psychologist</source><year>2003</year><volume>38</volume><issue>1</issue><fpage>23</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1207/S15326985EP3801_4</pub-id></mixed-citation></ref><ref id="CR12"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kicken</surname><given-names>W</given-names></name><name><surname>Brand-Gruwel</surname><given-names>S</given-names></name><name><surname>Merri&#x000eb;nboer</surname><given-names>JJG</given-names></name></person-group><source>Self-directed learning skills questionnaire</source><year>2006</year><publisher-loc>Heerlen</publisher-loc><publisher-name>Open Universiteit Nederland</publisher-name></mixed-citation></ref><ref id="CR13"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mager</surname><given-names>RF</given-names></name></person-group><source>Preparing instructional objectives</source><year>1984</year><edition>2</edition><publisher-loc>Belmont</publisher-loc><publisher-name>David S. Lake</publisher-name></mixed-citation></ref><ref id="CR14"><mixed-citation publication-type="other">Martens, R. L., &#x00026; Kirschner, P. A. (2004). What predicts intrinsic motivation? Paper presented at the 2004 AECT convention, Chicago.</mixed-citation></ref><ref id="CR15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>GE</given-names></name></person-group><article-title>The assessment of clinical skills/competence/performance</article-title><source>Academic Medicine</source><year>1990</year><volume>65</volume><issue>Suppl.</issue><fpage>S63</fpage><lpage>S67</lpage><pub-id pub-id-type="doi">10.1097/00001888-199009000-00045</pub-id><pub-id pub-id-type="pmid">2400509</pub-id></mixed-citation></ref><ref id="CR16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>PJ</given-names></name></person-group><article-title>The effect of scoring criteria specificity on peer and self-assessment</article-title><source>Assessment and Evaluation in Higher Education</source><year>2003</year><volume>28</volume><issue>4</issue><fpage>383</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1080/0260293032000066218</pub-id></mixed-citation></ref><ref id="CR17"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Otter</surname><given-names>S</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Edwards</surname><given-names>A</given-names></name><name><surname>Knight</surname><given-names>P</given-names></name></person-group><article-title>Assessing competence: The experience of the enterprise in higher education initiative</article-title><source>Assessing competence in higher education</source><year>1995</year><publisher-loc>London</publisher-loc><publisher-name>Kogan Page</publisher-name></mixed-citation></ref><ref id="CR18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paas</surname><given-names>F</given-names></name></person-group><article-title>Training strategies for attaining transfer of problem-solving skills in statistics: A cognitive-load approach</article-title><source>Journal of Educational Psychology</source><year>1992</year><volume>84</volume><fpage>429</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1037/0022-0663.84.4.429</pub-id></mixed-citation></ref><ref id="CR19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paas</surname><given-names>F</given-names></name><name><surname>Merri&#x000eb;noer</surname><given-names>JJG</given-names></name></person-group><article-title>The efficiency of instructional conditions: An approach to combine mental effort and performance measures</article-title><source>Human Factors</source><year>1993</year><volume>35</volume><fpage>737</fpage><lpage>743</lpage></mixed-citation></ref><ref id="CR20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadler</surname><given-names>DR</given-names></name></person-group><article-title>The origins and functions of evaluative criteria</article-title><source>Educational Theory</source><year>1985</year><volume>35</volume><issue>3</issue><fpage>285</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1111/j.1741-5446.1985.00285.x</pub-id></mixed-citation></ref><ref id="CR21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheffer</surname><given-names>S</given-names></name><name><surname>Muehlinghaus</surname><given-names>I</given-names></name><name><surname>Froehmel</surname><given-names>A</given-names></name><name><surname>Ortwein</surname><given-names>H</given-names></name></person-group><article-title>Assessing students&#x02019; communication skills: Validation of a global rating</article-title><source>Advances in Health Sciences Education</source><year>2008</year><volume>13</volume><fpage>583</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1007/s10459-007-9074-2</pub-id><pub-id pub-id-type="pmid">17636371</pub-id></mixed-citation></ref><ref id="CR22"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spencer</surname><given-names>L</given-names></name><name><surname>Spencer</surname><given-names>S</given-names></name></person-group><source>Competence at work</source><year>1993</year><publisher-loc>(New York</publisher-loc><publisher-name>John Wiley&#x00026;Sons, Inc)</publisher-name></mixed-citation></ref><ref id="CR23"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Steinaker</surname><given-names>NW</given-names></name><name><surname>Bell</surname><given-names>MR</given-names></name></person-group><source>The experiential taxonomy: A new approach to teaching and learning</source><year>1979</year><publisher-loc>New York</publisher-loc><publisher-name>Academic Press</publisher-name></mixed-citation></ref><ref id="CR24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweller</surname><given-names>J</given-names></name><name><surname>Merri&#x000eb;nboer</surname><given-names>JJG</given-names></name><name><surname>Paas</surname><given-names>F</given-names></name></person-group><article-title>Cognitive architecture and instructional design</article-title><source>Educational Psychology Review</source><year>1998</year><volume>10</volume><fpage>251</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1023/A:1022193728205</pub-id></mixed-citation></ref><ref id="CR25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gog</surname><given-names>T</given-names></name><name><surname>Paas</surname><given-names>F</given-names></name></person-group><article-title>Instructional efficiency: Revisiting the original construct in educational research</article-title><source>Educational Psychologist</source><year>2008</year><volume>43</volume><fpage>16</fpage><lpage>26</lpage></mixed-citation></ref><ref id="CR26"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Merri&#x000eb;nboer</surname><given-names>JJG</given-names></name></person-group><source>Training complex cognitive skills</source><year>1997</year><publisher-loc>Englewood Cliffs</publisher-loc><publisher-name>Educational Technology Publications</publisher-name></mixed-citation></ref><ref id="CR27"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Merri&#x000eb;nboer</surname><given-names>JJG</given-names></name><name><surname>Kirschner</surname><given-names>PA</given-names></name></person-group><source>Ten steps to complex learning</source><year>2007</year><publisher-loc>Mahwah</publisher-loc><publisher-name>Erlbaum/Taylor and Francis</publisher-name></mixed-citation></ref><ref id="CR28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merri&#x000eb;nboer</surname><given-names>JJG</given-names></name><name><surname>Kirschner</surname><given-names>PA</given-names></name><name><surname>Paas</surname><given-names>F</given-names></name><name><surname>Sloep</surname><given-names>PB</given-names></name><name><surname>Cani&#x000eb;ls</surname><given-names>MCJ</given-names></name></person-group><article-title>Towards an integrated approach for research on lifelong learning</article-title><source>Educational Technology</source><year>2009</year><volume>49</volume><issue>3</issue><fpage>3</fpage><lpage>14</lpage></mixed-citation></ref><ref id="CR29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merri&#x000eb;nboer</surname><given-names>JJG</given-names></name><name><surname>Sweller</surname><given-names>J</given-names></name></person-group><article-title>Cognitive load theory and complex learning: Recent developments and future directions</article-title><source>Educational Psychology Review</source><year>2005</year><volume>17</volume><fpage>147</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1007/s10648-005-3951-0</pub-id></mixed-citation></ref><ref id="CR30"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Merri&#x000eb;nboer</surname><given-names>JJG</given-names></name><name><surname>Klink</surname><given-names>MR</given-names></name><name><surname>Hendriks</surname><given-names>M</given-names></name></person-group><source>Competenties: Van complicaties tot compromis [Competencies: From complications to compromise]</source><year>2002</year><publisher-loc>The Hague</publisher-loc><publisher-name>Educational council</publisher-name></mixed-citation></ref><ref id="CR31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wesselink</surname><given-names>R</given-names></name><name><surname>Biemans</surname><given-names>HJA</given-names></name><name><surname>Mulder</surname><given-names>M</given-names></name><name><surname>Elsen</surname><given-names>ER</given-names></name></person-group><article-title>Competence-based VET as seen by Dutch researchers</article-title><source>European Journal of Vocational Training</source><year>2007</year><volume>40</volume><fpage>38</fpage><lpage>50</lpage></mixed-citation></ref><ref id="CR32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wierstra</surname><given-names>RFA</given-names></name><name><surname>Kanselaar</surname><given-names>G</given-names></name><name><surname>Linden</surname><given-names>JL</given-names></name><name><surname>Lodewijks</surname><given-names>HGLC</given-names></name></person-group><article-title>Learning environment perceptions of European university students</article-title><source>Learning Environments Research</source><year>1999</year><volume>2</volume><fpage>79</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1023/A:1009962024666</pub-id></mixed-citation></ref></ref-list></back></article>
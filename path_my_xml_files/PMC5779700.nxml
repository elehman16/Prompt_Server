<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">29360843</article-id><article-id pub-id-type="pmc">5779700</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0191673</article-id><article-id pub-id-type="publisher-id">PONE-D-17-24820</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Bioassays and Physiological Analysis</subject><subj-group><subject>Electrophysiological Techniques</subject><subj-group><subject>Brain Electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain Electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain Electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain Electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain Mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Clinical Medicine</subject><subj-group><subject>Clinical Neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Management Engineering</subject><subj-group><subject>Decision Analysis</subject><subj-group><subject>Decision Trees</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Decision Analysis</subject><subj-group><subject>Decision Trees</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Human Factors Engineering</subject><subj-group><subject>Man-Computer Interface</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth Sciences</subject><subj-group><subject>Geology</subject><subj-group><subject>Sedimentary Geology</subject><subj-group><subject>Perturbation (Geology)</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane Potential</subject><subj-group><subject>Evoked Potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane Potential</subject><subj-group><subject>Evoked Potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Evoked Potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Evoked Potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Evoked Potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Statistical Methods</subject><subj-group><subject>Analysis of Variance</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (Mathematics)</subject><subj-group><subject>Statistical Methods</subject><subj-group><subject>Analysis of Variance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>White Noise</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Steady state visual evoked potential (SSVEP) based brain-computer interface (BCI) performance under different perturbations</article-title><alt-title alt-title-type="running-head">SSVEP-based BCI performance under different perturbations</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9832-6591</contrib-id><name><surname>&#x00130;&#x0015f;can</surname><given-names>Zafer</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Nikulin</surname><given-names>Vadim V.</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Investigation</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff003"><sup>3</sup></xref><xref ref-type="aff" rid="aff004"><sup>4</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Centre for Cognition and Decision Making, National Research University Higher School of Economics, Moscow, Russian Federation</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Cognitive Neuroimaging Unit, CEA DRF/Joliot Institute, INSERM, Universit&#x000e9; Paris-Sud, Universit&#x000e9; Paris-Saclay, NeuroSpin center, Gif-sur-Yvette, France</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Department of Neurology, Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany</addr-line></aff><aff id="aff004"><label>4</label>
<addr-line>Neurophysics Group, Department of Neurology, Charit&#x000e9;-University Medicine Berlin, Campus Benjamin Franklin, Berlin, Germany</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Lei</surname><given-names>Xu</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>School of Psychology, CHINA</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>zaferiscan@yahoo.com</email> (Z&#x00130;); <email>nikulin@cbs.mpg.de</email> (VVN)</corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>1</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>13</volume><issue>1</issue><elocation-id>e0191673</elocation-id><history><date date-type="received"><day>5</day><month>7</month><year>2017</year></date><date date-type="accepted"><day>9</day><month>1</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2018 &#x00130;&#x0015f;can, Nikulin</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>&#x00130;&#x0015f;can, Nikulin</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0191673.pdf"/><abstract><p>Brain-computer interface (BCI) paradigms are usually tested when environmental and biological artifacts are intentionally avoided. In this study, we deliberately introduced different perturbations in order to test the robustness of a steady state visual evoked potential (SSVEP) based BCI. Specifically we investigated to what extent a drop in performance is related to the degraded quality of EEG signals or rather due to increased cognitive load. In the online tasks, subjects focused on one of the four circles and gave feedback on the correctness of the classification under four conditions randomized across subjects: Control (no perturbation), Speaking (counting loudly and repeatedly from one to ten), Thinking (mentally counting repeatedly from one to ten), and Listening (listening to verbal counting from one to ten). Decision tree, Na&#x000ef;ve Bayes and K-Nearest Neighbor classifiers were used to evaluate the classification performance using features generated by canonical correlation analysis. During the online condition, Speaking and Thinking decreased moderately the mean classification accuracy compared to Control condition whereas there was no significant difference between Listening and Control conditions across subjects. The performances were sensitive to the classification method and to the perturbation conditions. We have not observed significant artifacts in EEG during perturbations in the frequency range of interest except in theta band. Therefore we concluded that the drop in the performance is likely to have a cognitive origin. During the Listening condition relative alpha power in a broad area including central and temporal regions primarily over the left hemisphere correlated negatively with the performance thus most likely indicating active suppression of the distracting presentation of the playback. This is the first study that systematically evaluates the effects of natural artifacts (i.e. mental, verbal and audio perturbations) on SSVEP-based BCIs. The results can be used to improve individual classification performance taking into account effects of perturbations.</p></abstract><funding-group><funding-statement>The study has been funded by the Russian Academic Excellence Project &#x02018;5-100&#x02019;. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript</funding-statement></funding-group><counts><fig-count count="9"/><table-count count="3"/><page-count count="17"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All data files are available from the Zenodo database (DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1146091">10.5281/zenodo.1146091</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All data files are available from the Zenodo database (DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1146091">10.5281/zenodo.1146091</ext-link>).</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Brain-computer interfaces (BCIs) have potential to help severely disabled people by translating the intentions of subjects into a number of different commands [<xref rid="pone.0191673.ref001" ref-type="bibr">1</xref>]. Due to its safety and high time resolution, electroencephalogram (EEG) based BCIs have become popular and various designs using different signals (e.g. P300 [<xref rid="pone.0191673.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0191673.ref003" ref-type="bibr">3</xref>], sensorimotor rhythms [<xref rid="pone.0191673.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0191673.ref005" ref-type="bibr">5</xref>]) have been proposed. Among them, steady state visual evoked potentials (SSVEPs) are particularly attractive due to high signal to noise ratio (SNR) [<xref rid="pone.0191673.ref006" ref-type="bibr">6</xref>] and robustness [<xref rid="pone.0191673.ref007" ref-type="bibr">7</xref>]. SSVEP is a resonance phenomenon which can be observed mainly in electrodes over the occipital and parietal lobes of brain when a subject looks at a light source flickering at a constant frequency [<xref rid="pone.0191673.ref007" ref-type="bibr">7</xref>]. In this case, there is an increase in the amplitude of the EEG at flickering frequencies and their harmonics and there are different methods to extract the frequency components of SSVEPs. Recently, canonical correlation analysis (CCA) has become a popular approach for analyzing these frequency components as its performance was higher compared to traditionally used Fourier transform [<xref rid="pone.0191673.ref008" ref-type="bibr">8</xref>] and minimum energy combination [<xref rid="pone.0191673.ref009" ref-type="bibr">9</xref>]. Several extensions to standard CCA method were proposed and their performances were evaluated [<xref rid="pone.0191673.ref010" ref-type="bibr">10</xref>].</p><p>Robustness of CCA [<xref rid="pone.0191673.ref011" ref-type="bibr">11</xref>] and SSVEPs [<xref rid="pone.0191673.ref012" ref-type="bibr">12</xref>&#x02013;<xref rid="pone.0191673.ref014" ref-type="bibr">14</xref>] to different experimental conditions were mentioned in several papers. In [<xref rid="pone.0191673.ref011" ref-type="bibr">11</xref>], authors showed that CCA was robust to walking (movement) conditions in SSVEP detection. In [<xref rid="pone.0191673.ref012" ref-type="bibr">12</xref>], authors systematically evaluated the effects of walking locomotion on the SSVEPs using a mobile EEG system and showed that the SSVEP offline detection accuracy decreased as the walking speed increased. In [<xref rid="pone.0191673.ref013" ref-type="bibr">13</xref>], the authors found lesser mental load and fatigue for motion-reversal visual attention task compared to the paradigm with the conventional flickering. In another study, authors showed that an addition of a visual noise can boost both offline and online performances of an evoked potential-based BCI [<xref rid="pone.0191673.ref014" ref-type="bibr">14</xref>]. Although the latter was a steady-state motion visual evoked potential-based BCI, the study is relevant in terms of introducing perturbations. However, in none of the studies mentioned above, there were mental, verbal or audio perturbations introduced to the BCI system. Yet it is exactly these types of perturbations and mental loads that are relevant for the everyday use of BCI outside of the sterile environment of the research laboratories.</p><p>In this study, we evaluated a performance of a four-class BCI based on SSVEPs under different perturbations where the subjects were speaking, thinking or listening depending on the given task. We hypothesized that, although the SSVEP is a robust phenomenon, different perturbations (i.e. verbal, mental, audio) should have varying effects within and across subjects due to concurrent performance of another task and thus due to the changes in the attention dedicated to the BCI performance. To the best of our knowledge, this is the first study that systematically analyses the effects of these perturbations on the online performance of SSVEP based BCI within and across subjects.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><sec id="sec003"><title>Participants</title><p>Participants were recruited in the summer-autumn 2016 using the database of the Centre for Cognition and Decision Making at Higher School of Economics (HSE). Most of them were students at HSE. There was no payment for the participants and this was made clear before the recruitment. Twenty-seven healthy subjects (eight males) between 18 and 41 years of age (mean: 26 &#x000b1; 1, SE) participated in the study after giving a written informed consent in accordance with the Declaration of Helsinki. Experiments were approved by the Local Ethics Committee of National Research University Higher School of Economics, Moscow. All subjects participated in both offline and online tasks. However, three subjects (two males) were excluded from the analyses due to their relatively low offline task performances. Two of them reported that they were not able to focus properly in the offline task. Therefore, twenty-four subjects (three left-handed) were included in the results.</p></sec><sec id="sec004"><title>Experiment setup</title><p>EEG were recorded in an electrically shielded dark cabin. Stimulus presentation and recording computer was outside of the recording room. Stimulus paradigms were prepared in Matlab software (The MathWorks, Inc., Natick, Massachusetts, United States) using Psychophysics Toolbox Version 3 (<ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link>). The main stimuli are composed of four circles placed in different locations with individual flickering frequencies (<italic>f</italic>): 5.45 (up), 8.57 (down), 12 (right), and 15 (left) Hz). Participants followed the stimuli presented on a 28 inch Ultra HD LED Monitor (Samsung LED LU28D590DS) with a resolution of 1920 x 1080 pixels and a refresh rate of 60 Hz. Duty cycle was determined as 1/(60/<italic>f</italic>). Hence during one cycle, the related circle was white only in one frame and it was black for the other frames. In this case, when the frame color is reversed once in every four frames, we obtain 15 (i.e. 60/4) Hz. Similarly 12 (60/5), 8.57 (60/7) and 5.45 (60/11) Hz were obtained. In order to avoid performance problems due to the frequency resolution, there was at least 3-Hz gap between the selected frequencies. We did not use frequencies that are multiples of each other in order to prevent the coincidence of the first harmonics of one flicker frequency corresponding to the second harmonics of another stimulus. During the experiment the distance between the participants and the monitor was 90 cm. <xref ref-type="fig" rid="pone.0191673.g001">Fig 1</xref> shows the positions of the flickering circles on the monitor.</p><fig id="pone.0191673.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g001</object-id><label>Fig 1</label><caption><title>Subject and the position of the flickering circles on LED monitor.</title><p>Vertical and horizontal field of view were 23.48 and 39.42 degrees respectively. Diameter of the circles corresponded to 3.94 degrees. Circle centers were positioned at 6.92 degrees from the central fixation point.</p></caption><graphic xlink:href="pone.0191673.g001"/></fig></sec><sec id="sec005"><title>Offline task</title><p>Offline experiment starts with a welcome message after a blank screen. For each of 25 trials, there is an instruction on the screen informing a subject to focus on the presented circle. Subjects then focus on each of the four randomly presented flickering circles indicated by a red oval frame for three seconds with an inter-stimulus interval (ISI) of one second. Trial ends with a blank screen. After all trials are finished, the experiment ends with a "thank you" message. Subjects were instructed to blink during ISI in order to avoid blink-related artifacts during the presentation of the flickering stimuli. This task lasts for about nine minutes. The timing of the offline task is presented in <xref ref-type="fig" rid="pone.0191673.g002">Fig 2A</xref>. A demonstration of one trial of the offline task is given in <xref ref-type="supplementary-material" rid="pone.0191673.s001">S1 Video</xref>.</p><fig id="pone.0191673.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g002</object-id><label>Fig 2</label><caption><title>Timing of the tasks.</title><p>A: Offline, B: Online.</p></caption><graphic xlink:href="pone.0191673.g002"/></fig></sec><sec id="sec006"><title>Online task</title><p>Online experiment starts with a welcome message after a blank screen. For each of 100 trials, there is an instruction on the screen informing subject to focus on the circle. Then, subject focuses on one of the four flickering white circles for three seconds. In this task, the subjects were free to select the circle to focus on. In the end of three seconds, the classification result (i.e. circle in a specific location) is presented on the screen with a red color. Subjects either confirm (pressed &#x02018;Y&#x02019; key) or reject (pressed &#x02018;N&#x02019; key and then specify the correct response with the arrow keys) the location using keyboard. There is no time limit for the subject feedback. After all trials are finished, the experiment ends with a "thank you" message. The timing of the online task is presented in <xref ref-type="fig" rid="pone.0191673.g002">Fig 2B</xref>. A demonstration of one trial of the online task is given in <xref ref-type="supplementary-material" rid="pone.0191673.s002">S2 Video</xref>.</p><p>The online task was performed under four conditions randomized across subjects:</p><list list-type="order"><list-item><p>Control: There is no counting (no perturbation).</p></list-item><list-item><p>Speaking: Subjects counted loudly and repeatedly from 1 to 10 (verbal perturbation).</p></list-item><list-item><p>Thinking: Subjects counted repeatedly from 1 to 10 mentally (mental perturbation).</p></list-item><list-item><p>Listening: Subjects listened to their pre-recorded voice in waveform audio file format (.wav) with a sampling rate at 22 kHz and sample size of 16 bits) when they were counting from 1 to 10 (audio perturbation).</p></list-item></list><p>The subjects counted in a constant speed with which they felt comfortable.</p></sec><sec id="sec007"><title>EEG recordings</title><p>EEG were recorded with ActiCHamp amplifier using PyCorder software (Brain Products) from 60 channels using the 64-Channel Standard Electrode Layout for actiCHamp except for FT9, FT10, TP9 and TP10. Reference electrode was on the left mastoid. Three electrooculographic (EOG) electrodes were placed above the nasion and below the outer canthi of the eyes as indicated in [<xref rid="pone.0191673.ref015" ref-type="bibr">15</xref>]. Electrode impedances were kept under 20 k&#x003a9;. Sampling rate was 1 kHz.</p></sec><sec id="sec008"><title>Preprocessing</title><p>EEG were segmented using the stimuli markers that specify the start and end of the flickering. Trend in the segmented data was removed and the data was filtered with a band-pass filter with cut-off frequencies of 0.53 and 40 Hz in order to remove DC component and high frequency artifacts including power line noise (50 Hz). No extra artifact removal method was used as SSVEPs are not sensitive to low frequency artifacts like eye or body movements [<xref rid="pone.0191673.ref016" ref-type="bibr">16</xref>]. All 60 EEG channels were included in the offline and online tasks to have consistency across subjects. Fieldtrip toolbox was used for both offline and online analysis [<xref rid="pone.0191673.ref017" ref-type="bibr">17</xref>].</p></sec><sec id="sec009"><title>Power spectrum</title><p>Fast Fourier Transform was used with Hanning window to calculate the power spectrum of the preprocessed EEG for a 3s stimuli length. The spectrum contained both the first and second harmonics (The n<sup>th</sup> harmonic = n &#x000d7; the stimulation frequency) of the flickering frequencies. Frequency spectrum included delta (1&#x02013;3 Hz), theta (4&#x02013;7 Hz), alpha (8&#x02013;13 Hz), and beta (14&#x02013;30 Hz) bands. Relative power was calculated as the ratio of the sum of the power at a specific band (e.g. delta) over the sum of the power in the broad spectrum (1&#x02013;30 Hz).</p></sec><sec id="sec010"><title>Feature extraction</title><p>Canonical correlation analysis (CCA) was used to generate features for classification. For the detailed implementation of the method see [<xref rid="pone.0191673.ref018" ref-type="bibr">18</xref>]. Briefly, we generated data composed of sine and cosine functions that have the same lengths as the segmented EEG data. Then the canonical correlations were calculated between the EEG and the sine and cosine segments. The frequencies of the sine and cosine functions corresponded to the first and second harmonics of the stimulation frequencies. Therefore, two canonical correlation values were calculated for each of the four stimulation frequencies and their second harmonics. These sixteen (8 &#x000d7; 2) correlation values were used as the features for the classification.</p></sec><sec id="sec011"><title>Classification</title><p>Decision tree, Na&#x000ef;ve Bayes and K-Nearest Neighbor classifiers were used to evaluate the BCI classification performance using features generated by CCA. These classifiers were used in previous SSVEP-based BCI designs in the literature: K-NN [<xref rid="pone.0191673.ref019" ref-type="bibr">19</xref>], Na&#x000ef;ve Bayes [<xref rid="pone.0191673.ref020" ref-type="bibr">20</xref>], and Decision Tree [<xref rid="pone.0191673.ref021" ref-type="bibr">21</xref>]. All three classifiers were implemented with the Statistics Toolbox of Matlab. Offline classification accuracy was calculated for different length (1s, 2s, and 3s) of the stimuli using leave-one-out approach. Before the online classification, the classifiers were trained using the whole data in the offline task. All three classifiers were used in the online classification. However, as each classifier generated different classification output, only one of them (Na&#x000ef;ve Bayes classifier prediction) was presented to the subject to prevent confusion. Here, the decision to use Na&#x000ef;ve Bayes classifier among others was based on its popularity in BCI research. Importantly, when selecting the classifier in advance, we did not know about the final results of classification accuracy among all classifiers, which was only possible to assess upon the completion of all online experiments. Online classification was performed for the stimuli length of 3s.</p></sec><sec id="sec012"><title>Statistical analysis</title><p>Two-way repeated measures analysis of variance (ANOVA) was used to investigate the differences in the mean performance of subjects depending on the classifier and stimulus duration factors for the offline task, and depending on the classifier and perturbation factors for the online task. Differences between the classifiers in the offline task and the differences between the conditions in the online task across subjects were evaluated by paired sample t-test with false discovery rate (FDR) correction [<xref rid="pone.0191673.ref022" ref-type="bibr">22</xref>]. Correlations between the relative power during stimuli presentation and the performances across subjects were calculated for delta, theta, alpha (low alpha: 8&#x02013;10 Hz, high alpha: 10&#x02013;13 Hz) and beta bands with Pearson correlation coefficient and the significance was estimated using cluster-based permutation statistics, which take into account spatial proximity of the electrodes with significant effects [<xref rid="pone.0191673.ref023" ref-type="bibr">23</xref>]. Differences in the duration of the experiments and power of oscillations in different frequency bands among online conditions across subjects were evaluated using one-way analysis of variance (ANOVA).</p></sec></sec><sec sec-type="results" id="sec013"><title>Results</title><sec id="sec014"><title>Offline classification</title><p>In <xref ref-type="fig" rid="pone.0191673.g003">Fig 3</xref>, mean accuracies (%) with standard errors are given for varying stimuli length from 1 to 3 seconds in the offline part of the experiment.</p><fig id="pone.0191673.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g003</object-id><label>Fig 3</label><caption><title>Mean accuracies (%) (N = 24) vs. stimuli length&#x02013;offline task.</title><p>*: <italic>p</italic>&#x0003c;0.05, **: <italic>p</italic>&#x0003c;0.01, ***: <italic>p</italic>&#x0003c;0.001 (FDR corrected). Comparisons (t-test) between the classifiers.</p></caption><graphic xlink:href="pone.0191673.g003"/></fig><p>In <xref ref-type="fig" rid="pone.0191673.g003">Fig 3</xref>, when the stimuli length was short (i.e. 1s), Na&#x000ef;ve Bayes classifier accuracy was higher than the accuracy of K-NN and Decision Tree classifiers (p&#x0003c;0.001 in each case). However, there was no significant difference between the performance of K-NN and Decision Tree classifiers. For the larger stimuli length (i.e. 2s), Na&#x000ef;ve Bayes classifier accuracy was still higher than the accuracy of K-NN (p&#x0003c;0.05), and Decision Tree (p&#x0003c;0.01) classifiers. For this length, K-NN accuracy was also superior to Decision Tree (p&#x0003c;0.01). For the longest stimuli length (i.e. 3s), K-NN accuracy was higher than the accuracies of Na&#x000ef;ve Bayes (p&#x0003c;0.05) and Decision Tree (p&#x0003c;0.01) classifiers. For this length, Na&#x000ef;ve Bayes performance was still superior to Decision Tree (p&#x0003c;0.05).</p><p>Repeated measures ANOVA (see <xref ref-type="table" rid="pone.0191673.t001">Table 1</xref>) showed significant difference in the mean accuracies in the offline task depending on the classifier, stimuli length and their interaction (<italic>p</italic>&#x0003c;0.001 in each case).</p><table-wrap id="pone.0191673.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.t001</object-id><label>Table 1</label><caption><title>Two-way repeated measures ANOVA table for mean accuracies in the offline task depending on classifier and stimuli length (i.e. duration).</title></caption><alternatives><graphic id="pone.0191673.t001g" xlink:href="pone.0191673.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1"><italic>Source</italic></th><th align="justify" rowspan="1" colspan="1"><italic>SS</italic></th><th align="justify" rowspan="1" colspan="1"><italic>df</italic></th><th align="justify" rowspan="1" colspan="1"><italic>MS</italic></th><th align="justify" rowspan="1" colspan="1"><italic>F</italic></th><th align="justify" rowspan="1" colspan="1"><italic>p</italic></th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1"><bold>Duration</bold></td><td align="justify" rowspan="1" colspan="1">3.391</td><td align="justify" rowspan="1" colspan="1">2</td><td align="justify" rowspan="1" colspan="1">1.695</td><td align="justify" rowspan="1" colspan="1">82.07</td><td align="justify" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Classifier</bold></td><td align="justify" rowspan="1" colspan="1">0.081</td><td align="justify" rowspan="1" colspan="1">2</td><td align="justify" rowspan="1" colspan="1">0.040</td><td align="justify" rowspan="1" colspan="1">25.62</td><td align="justify" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Duration x Classifier</bold></td><td align="justify" rowspan="1" colspan="1">0.049</td><td align="justify" rowspan="1" colspan="1">4</td><td align="justify" rowspan="1" colspan="1">0.012</td><td align="justify" rowspan="1" colspan="1">11.87</td><td align="justify" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Duration x Subject</bold></td><td align="justify" rowspan="1" colspan="1">0.950</td><td align="justify" rowspan="1" colspan="1">46</td><td align="justify" rowspan="1" colspan="1">0.021</td><td align="justify" rowspan="1" colspan="1">-</td><td align="justify" rowspan="1" colspan="1">-</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Classifier x Subject</bold></td><td align="justify" rowspan="1" colspan="1">0.073</td><td align="justify" rowspan="1" colspan="1">46</td><td align="justify" rowspan="1" colspan="1">0.002</td><td align="justify" rowspan="1" colspan="1">-</td><td align="justify" rowspan="1" colspan="1">-</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Duration x Classifier x Subject</bold></td><td align="justify" rowspan="1" colspan="1">0.094</td><td align="justify" rowspan="1" colspan="1">92</td><td align="justify" rowspan="1" colspan="1">0.001</td><td align="justify" rowspan="1" colspan="1">-</td><td align="justify" rowspan="1" colspan="1">-</td></tr></tbody></table></alternatives></table-wrap><p>Mean classification accuracies across subjects were &#x0003e; = 97% for all classifiers (mean &#x000b1; SD; K-NN: 99.54 &#x000b1; 0.72, Na&#x000ef;ve Bayes: 98.93 &#x000b1; 1.69, Decision Tree: 96.62 &#x000b1; 4.30) in offline condition when the stimuli length was 3 s.</p><p>In <xref ref-type="fig" rid="pone.0191673.g004">Fig 4</xref>, median power spectrum of all subjects and the power spectrum of a representative subject (Subject 18) are presented depending on the flickering frequency (F) in the offline task.</p><fig id="pone.0191673.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g004</object-id><label>Fig 4</label><caption><title>Median power spectrum of all subjects and the power spectrum of a representative subject (Subject 18) depending on the frequency (F) of the circle with attentional focus.</title><p>Left: f = 15 Hz, Right: f = 12 Hz, Down: f = 8.57 Hz, Up: f = 5.45 Hz. flickering frequency (f) in the offline task. Power was averaged across trials. Each single trace represents one of the 60 EEG channels.</p></caption><graphic xlink:href="pone.0191673.g004"/></fig><p>In <xref ref-type="fig" rid="pone.0191673.g004">Fig 4</xref>, peaks corresponding to the first and the second harmonics of the flickering frequencies can be seen. As the alpha band coincides with the first or second harmonics of the flickering frequencies (except F = 15 Hz), some of the stimuli peaks are overlapped with the endogenous alpha oscillations.</p><p>In <xref ref-type="fig" rid="pone.0191673.g005">Fig 5</xref>, median power topography of all subjects and the power topography of a representative subject (Subject 18) are presented depending on the flickering frequency (F) in the offline task.</p><fig id="pone.0191673.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g005</object-id><label>Fig 5</label><caption><p><bold>Median power topography of all subjects (A) and the power topography of a representative subject (Subject 18) (B) in different frequency bands depending on the flicker frequency (F) of the circle with the attention focus in the offline task.</bold> Left: f = 15 Hz, Right: f = 12 Hz, Down: f = 8.57 Hz, Up: f = 5.45 Hz. Power was averaged across trials.</p></caption><graphic xlink:href="pone.0191673.g005"/></fig><p>In <xref ref-type="fig" rid="pone.0191673.g005">Fig 5</xref>, the increase of the power in the corresponding flickering frequency over the visual cortex can be seen. Besides, the spatial shift of the peak power in the occipital channels depending on focusing to the left or right visual field is noticeable.</p></sec><sec id="sec015"><title>Online classification</title><p>In <xref ref-type="fig" rid="pone.0191673.g006">Fig 6</xref>, mean accuracies (%) with standard errors are provided for a stimuli length of 3 seconds under different perturbations.</p><fig id="pone.0191673.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g006</object-id><label>Fig 6</label><caption><title>Mean accuracies (%) under different perturbations (N = 24)&#x02013;online tasks (3 s).</title><p>*: <italic>p</italic>&#x0003c;0.05, **: <italic>p</italic>&#x0003c;0.01, ***: <italic>p</italic>&#x0003c;0.001 (FDR corrected). Comparisons (t-test) between the perturbations.</p></caption><graphic xlink:href="pone.0191673.g006"/></fig><p>In <xref ref-type="fig" rid="pone.0191673.g006">Fig 6</xref>, for the K-NN classifier, accuracy in Control condition was higher than Speaking (p&#x0003c;0.01) and Thinking (p&#x0003c;0.05) conditions. Besides, accuracy in Listening condition was higher than in Speaking (p&#x0003c;0.05) condition. For the Na&#x000ef;ve Bayes classifier, accuracies in Control and Listening conditions were higher than in Speaking (p&#x0003c;0.05 in each case) and Thinking (p&#x0003c;0.05 in each case) conditions. For the Decision Tree classifier, again, accuracy in Control condition was higher than in Speaking (p&#x0003c;0.001) and Thinking (p&#x0003c;0.05) conditions. Moreover, performance in Listening condition was also superior to Speaking (p&#x0003c;0.01) and Thinking (p&#x0003c;0.05) conditions. Furthermore, accuracy in Thinking condition was higher (p&#x0003c;0.05) than the accuracy in Speaking condition for this classifier.</p><p>Repeated measures ANOVA (see <xref ref-type="table" rid="pone.0191673.t002">Table 2</xref>) showed significant difference in the mean accuracies in the online tasks depending on the classifier, perturbation, and their interaction (<italic>p</italic>&#x0003c;0.001).</p><table-wrap id="pone.0191673.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.t002</object-id><label>Table 2</label><caption><title>Two-way repeated measures ANOVA table for mean accuracies in the online task depending on classifier and perturbations.</title></caption><alternatives><graphic id="pone.0191673.t002g" xlink:href="pone.0191673.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1"><italic>Source</italic></th><th align="justify" rowspan="1" colspan="1"><italic>SS</italic></th><th align="justify" rowspan="1" colspan="1"><italic>df</italic></th><th align="justify" rowspan="1" colspan="1"><italic>MS</italic></th><th align="justify" rowspan="1" colspan="1"><italic>F</italic></th><th align="justify" rowspan="1" colspan="1"><italic>p</italic></th></tr></thead><tbody><tr><td align="justify" rowspan="1" colspan="1"><bold>Perturbation</bold></td><td align="justify" rowspan="1" colspan="1">2347.8</td><td align="justify" rowspan="1" colspan="1">3</td><td align="justify" rowspan="1" colspan="1">782.6</td><td align="justify" rowspan="1" colspan="1">11.47</td><td align="justify" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Classifier</bold></td><td align="justify" rowspan="1" colspan="1">6445.0</td><td align="justify" rowspan="1" colspan="1">2</td><td align="justify" rowspan="1" colspan="1">3222.5</td><td align="justify" rowspan="1" colspan="1">24.65</td><td align="justify" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Perturbation x Classifier</bold></td><td align="justify" rowspan="1" colspan="1">201.7</td><td align="justify" rowspan="1" colspan="1">6</td><td align="justify" rowspan="1" colspan="1">33.6</td><td align="justify" rowspan="1" colspan="1">4.55</td><td align="justify" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Perturbation x Subject</bold></td><td align="justify" rowspan="1" colspan="1">4707.2</td><td align="justify" rowspan="1" colspan="1">69</td><td align="justify" rowspan="1" colspan="1">68.2</td><td align="justify" rowspan="1" colspan="1">-</td><td align="justify" rowspan="1" colspan="1">-</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Classifier x Subject</bold></td><td align="justify" rowspan="1" colspan="1">6012.5</td><td align="justify" rowspan="1" colspan="1">46</td><td align="justify" rowspan="1" colspan="1">130.7</td><td align="justify" rowspan="1" colspan="1">-</td><td align="justify" rowspan="1" colspan="1">-</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Perturbation x Classifier x Subject</bold></td><td align="justify" rowspan="1" colspan="1">1018.7</td><td align="justify" rowspan="1" colspan="1">138</td><td align="justify" rowspan="1" colspan="1">7.3</td><td align="justify" rowspan="1" colspan="1">-</td><td align="justify" rowspan="1" colspan="1">-</td></tr></tbody></table></alternatives></table-wrap><p>In Thinking and Speaking conditions during the online task the mean classification accuracy was decreased across subjects in all classifiers compared to the Control condition. However, this decrease was surprisingly small amounting to only about 5%. There was no significant difference between Listening and Control conditions. In <xref ref-type="fig" rid="pone.0191673.g007">Fig 7</xref>, median power spectrum of all subjects and a representative subject (Subject 18) were presented depending on the online task using the SSVEP responses from all focused circles.</p><fig id="pone.0191673.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g007</object-id><label>Fig 7</label><caption><title>Median power spectrum of all subjects and the power spectrum of a representative subject (Subject 18) depending on the online conditions: Control, Speaking, Thinking, and Listening.</title><p>Power was averaged across trials. The activity relates to all flickering frequencies. Each trace represents one of the 60 EEG channels.</p></caption><graphic xlink:href="pone.0191673.g007"/></fig><p>In <xref ref-type="fig" rid="pone.0191673.g007">Fig 7</xref>, median power of all subjects did not reveal pronounced artifacts among the online conditions due to perturbation conditions. Only in the Speaking condition, one could observe that the median delta band power was visibly higher than in the other conditions. To further verify this observation, we averaged power spectra across channels individually for each subject and condition and compared then the power in delta, theta, alpha and beta power bands using ANOVA (See <xref ref-type="table" rid="pone.0191673.t003">Table 3</xref>).</p><table-wrap id="pone.0191673.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.t003</object-id><label>Table 3</label><caption><title>One-way ANOVA table for power depending on perturbation.</title></caption><alternatives><graphic id="pone.0191673.t003g" xlink:href="pone.0191673.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1"><italic>Frequency band</italic></th><th align="justify" rowspan="1" colspan="1"><italic>Source</italic></th><th align="justify" rowspan="1" colspan="1"><italic>SS</italic></th><th align="justify" rowspan="1" colspan="1"><italic>df</italic></th><th align="justify" rowspan="1" colspan="1"><italic>MS</italic></th><th align="justify" rowspan="1" colspan="1"><italic>F</italic></th><th align="justify" rowspan="1" colspan="1"><italic>p</italic></th></tr></thead><tbody><tr><td align="justify" rowspan="3" colspan="1"><bold>Delta</bold></td><td align="justify" rowspan="1" colspan="1"><bold>Perturbation</bold></td><td align="justify" rowspan="1" colspan="1">1084</td><td align="justify" rowspan="1" colspan="1">3</td><td align="justify" rowspan="1" colspan="1">361.332</td><td align="justify" rowspan="1" colspan="1">25.2</td><td align="justify" rowspan="1" colspan="1">&#x0003c;0.001</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Error</bold></td><td align="justify" rowspan="1" colspan="1">1146.89</td><td align="justify" rowspan="1" colspan="1">80</td><td align="justify" rowspan="1" colspan="1">14.336</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Total</bold></td><td align="justify" rowspan="1" colspan="1">2230.89</td><td align="justify" rowspan="1" colspan="1">83</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr><tr><td align="justify" rowspan="3" colspan="1"><bold>Theta</bold></td><td align="justify" rowspan="1" colspan="1"><bold>Perturbation</bold></td><td align="justify" rowspan="1" colspan="1">3.25</td><td align="justify" rowspan="1" colspan="1">3</td><td align="justify" rowspan="1" colspan="1">1.082</td><td align="justify" rowspan="1" colspan="1">4.5</td><td align="justify" rowspan="1" colspan="1">0.0055</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Error</bold></td><td align="justify" rowspan="1" colspan="1">21.17</td><td align="justify" rowspan="1" colspan="1">88</td><td align="justify" rowspan="1" colspan="1">0.241</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Total</bold></td><td align="justify" rowspan="1" colspan="1">24.42</td><td align="justify" rowspan="1" colspan="1">91</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr><tr><td align="justify" rowspan="3" colspan="1"><bold>Alpha</bold></td><td align="justify" rowspan="1" colspan="1"><bold>Perturbation</bold></td><td align="justify" rowspan="1" colspan="1">0.23</td><td align="justify" rowspan="1" colspan="1">3</td><td align="justify" rowspan="1" colspan="1">0.078</td><td align="justify" rowspan="1" colspan="1">0.88</td><td align="justify" rowspan="1" colspan="1">0.4533</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Error</bold></td><td align="justify" rowspan="1" colspan="1">7.03</td><td align="justify" rowspan="1" colspan="1">80</td><td align="justify" rowspan="1" colspan="1">0.088</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Total</bold></td><td align="justify" rowspan="1" colspan="1">7.26</td><td align="justify" rowspan="1" colspan="1">83</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr><tr><td align="justify" rowspan="3" colspan="1"><bold>Beta</bold></td><td align="justify" rowspan="1" colspan="1"><bold>Perturbation</bold></td><td align="justify" rowspan="1" colspan="1">0.06</td><td align="justify" rowspan="1" colspan="1">3</td><td align="justify" rowspan="1" colspan="1">0.021</td><td align="justify" rowspan="1" colspan="1">1.88</td><td align="justify" rowspan="1" colspan="1">0.1405</td></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Error</bold></td><td align="justify" rowspan="1" colspan="1">0.85</td><td align="justify" rowspan="1" colspan="1">76</td><td align="justify" rowspan="1" colspan="1">0.011</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr><tr><td align="justify" rowspan="1" colspan="1"><bold>Total</bold></td><td align="justify" rowspan="1" colspan="1">0.92</td><td align="justify" rowspan="1" colspan="1">79</td><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/><td align="justify" rowspan="1" colspan="1"/></tr></tbody></table></alternatives></table-wrap><p>The results of this comparison showed that the power of oscillations was different only in delta (<italic>p</italic>&#x0003c;0.001) and theta (<italic>p</italic> = 0.005) frequency range. Post-hoc comparisons showed that the power of oscillations in Speaking condition was significantly higher compared to all other conditions (p&#x0003c;0.001 in each case) in delta range and it was significantly higher than in Control (p = 0.015) and Listening (p = 0.009) conditions in theta frequency range. No significant differences at other frequencies were observed. When we restricted the same analysis to Oz electrode, the power of oscillations was different only in delta frequency range (<italic>p</italic>&#x0003c;0.001, F = 16.49) and post-hoc analysis showed that the power of delta oscillations in Speaking condition was significantly higher (p&#x0003c;0.001 in each case) compared to all other conditions in this range.</p><p>In <xref ref-type="fig" rid="pone.0191673.g008">Fig 8</xref>, median power topography of all subjects and the power topography of a typical subject (Subject 18) are presented depending on the perturbation conditions in the online tasks. In agreement with the results presented above, the increase in the delta and theta power in frontal channels can be observed in Speaking condition for the median power of all subjects (A).</p><fig id="pone.0191673.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g008</object-id><label>Fig 8</label><caption><p><bold>Median power topography of all subjects (A) and the power topography of a representative subject (Subject 18) in different frequency bands (B) depending on the perturbation condition in the online tasks.</bold> Power was averaged across trials and all target frequencies.</p></caption><graphic xlink:href="pone.0191673.g008"/></fig><p>For the Na&#x000ef;ve Bayes classifier, in Speaking, Thinking, and Listening conditions, the number of subjects with higher performance with respect to the Control condition was 5, 4, and 9 respectively. However, the number of subjects having lower performance with respect to their Control condition was 16, 18 and 12 respectively. The number of subjects who did not show any difference in their performance was 3, 2, and 3 respectively. Among them one subject had 100% accuracy in all conditions.</p><sec id="sec016"><title>Correlation of alpha power with the performance</title><p>Alpha oscillations are known to relate to task performance [<xref rid="pone.0191673.ref024" ref-type="bibr">24</xref>,<xref rid="pone.0191673.ref025" ref-type="bibr">25</xref>]. In order to gain further insight into the role of alpha oscillations during the online performance we correlated classification accuracy with the power of alpha oscillations in low (8&#x02013;10 Hz) and high (10&#x02013;13 Hz) frequency bands. In <xref ref-type="fig" rid="pone.0191673.g009">Fig 9</xref>, a significant negative Pearson correlation (cluster <italic>p</italic> = 0.02) between the relative high alpha (10&#x02013;13 Hz) power and the Na&#x000ef;ve Bayes classifier performance in the Listening task is presented. In none of the other conditions we observed significant correlation between alpha power and the BCI performance.</p><fig id="pone.0191673.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191673.g009</object-id><label>Fig 9</label><caption><title>Pearson correlation between relative high alpha (10&#x02013;13 Hz) power and the Na&#x000ef;ve Bayes classifier performance in the Listening condition across subjects.</title><p>A&#x02013;Topography of the correlations. Black dots represent the channels that belong to the negative cluster (<italic>p</italic> = 0.02, 1000 permutations). B&#x02014;An exemplary scatter plot for one of the channels (C4) from the significant cluster.</p></caption><graphic xlink:href="pone.0191673.g009"/></fig><p>One-way ANOVA showed no difference in duration of the experiment among online conditions across subjects. The mean duration for each of the online conditions was ~14 min.</p></sec></sec></sec><sec sec-type="conclusions" id="sec017"><title>Discussion</title><p>To evaluate a dependency of the classifier accuracy, an instance based (K-NN), a probabilistic (Na&#x000ef;ve Bayes) and an entropy based (Decision Tree) classifier were used. Different classification approaches can help in capturing the individual differences in brain responses and provide a clue to the understanding of the appropriate classification scheme for each subject.</p><p>Interestingly, there was no difference in the duration of the experiments among conditions across subjects. One might expect that the Speaking and the Thinking conditions would last longer as the mean accuracies for these conditions were lower than the Control and the Listening conditions. As subjects should indicate their correct choice in the presence of a mismatch between the classifier prediction and their selection, this potentially might add some extra time to the overall length of the experiment. We assume that subjects were trying to compensate for this loss involuntarily in order to sustain their internal rhythm of counting.</p><p>Another interesting observation relates to the individual differences in the evaluation of the difficulty of the conditions. In general, there was no consistency among subjects indicating that one condition was harder than the other, for some of the subjects Speaking was the hardest condition, yet for others Thinking or Listening condition.</p><p>Spectral analysis revealed that even during the Speaking condition the spectra of the EEG signals were not strongly contaminated with muscle or motion artifacts in the frequency ranges other than the delta and the theta range. These ranges, however, are usually not used for SSVEP. Because of this we conclude that the decrease in the classification accuracy during the Speaking condition is not only due to decreased quality of the EEG signal in theta band but also due to the other considerations, most likely having a cognitive origin as explained below. This observation is further corroborated by the fact that we also observed a decrease in the classification in the Thinking condition where EEG signals have not demonstrated the presence of artifacts. Silently counting is equivalent to motor imagery which in general activates similar areas as during the performance of real movements [<xref rid="pone.0191673.ref026" ref-type="bibr">26</xref>]. In this sense these two conditions in addition to BCI performance might be considered as an example of a dual task paradigm, which indeed is known to split the attention [<xref rid="pone.0191673.ref027" ref-type="bibr">27</xref>]. This in turn might explain the decrease in BCI accuracy exactly in these two conditions due to the drop in attention relating to focusing on the flickering stimuli. Recently, Brandl et al. [<xref rid="pone.0191673.ref028" ref-type="bibr">28</xref>] investigated the effects of five different distractions in addition to the Control condition on the motor-imagery based BCI performance using Common Spatial Patterns and Regularized Linear Discriminant Analysis. They showed that the decreased performance in distraction conditions can be improved using an ensemble of classifiers and a two-step classification method that first separates the corresponding distraction condition from the other conditions using one-vs-all approach and then applies different classifiers for each group to classify the data into left or right hand motor imagery. The distractions were based on visual (eyes-closed, watching video with a flicker), tactile, muscular and auditory conditions. Although there are similarities between their and our study in terms of the introducing perturbations, there are also considerable differences. Compared to [<xref rid="pone.0191673.ref028" ref-type="bibr">28</xref>] we used SSVEP based BCI. Moreover, we consistently investigated effects of mostly encountered distractors in real world, namely speaking (both loud and internal) and listening. Both studies demonstrate a necessity for further investigation of other perturbations for the development of robust BCI systems.</p><p>We did not find a difference in the performance between Listening and Control conditions. If we consider Listening task as a steady-state speech sound condition, this result is consistent with another study, where authors did not find any difference between quiet (i.e. Control in our case) and steady-state speech noise (i.e. Listening in our case) performances in a serial recall task [<xref rid="pone.0191673.ref029" ref-type="bibr">29</xref>].</p><p>In addition we also provide a possible implication of our study for the effect of noise on learning processes. Increase in the accuracies under different perturbations for some subjects shows that perturbations should not always be considered as negative in terms of BCI performance. In fact, there is an ongoing debate about the impact of noise and music on the performance in different tasks [<xref rid="pone.0191673.ref030" ref-type="bibr">30</xref>]. It has been shown that white noise improved the working memory performance in a delayed response task in monkeys [<xref rid="pone.0191673.ref031" ref-type="bibr">31</xref>]. However, in a recent study, white noise was detrimental in the working memory task whereas it had beneficial effects in other tasks [<xref rid="pone.0191673.ref032" ref-type="bibr">32</xref>]. Therefore, authors concluded that white noise has differential effects on perception and cognition depending on various factors (e.g. timing of white noise presentation) [<xref rid="pone.0191673.ref032" ref-type="bibr">32</xref>].</p><p>Specific to the Listening condition, here we present a neural marker of the task performance based on relative high alpha power in a broad range of cerebral cortex including central, temporal and parietal regions. Subjects with higher relative high alpha power during the stimuli presentation in the Listening condition had lower performance. Interestingly the negative correlation was strongest over the left hemisphere (<xref ref-type="fig" rid="pone.0191673.g009">Fig 9</xref>) which is known to be primarily responsible for the language processing [<xref rid="pone.0191673.ref033" ref-type="bibr">33</xref>]. Stronger alpha power usually indicates active inhibition over the areas whose activity should be suppressed [<xref rid="pone.0191673.ref034" ref-type="bibr">34</xref>] for instance in order to avoid effects of task distractors. In our case playback of the counting can actually be considered as a distractor with respect to the performance of the BCI task. Negative correlation between the relative alpha power and performance might indicate that in subjects where the listening to the presented speech had stronger distracting effects (i.e. worse performance)&#x02013;the suppression was strongest leading to larger power of alpha oscillations over the centro-temporal areas of the left hemisphere. The lack of such correlation in other two distracting conditions (Speaking and Thinking) can also be due to abovementioned presence of two tasks where additional neuronal processing, involved with task switching, cause extra modulation of alpha oscillations thus leading to the masking of the correlation.</p><p>We believe that various classification techniques used in our study are sufficient for the demonstration of the effects of perturbations on SSVEP-based BCI performance for the following reasons:</p><list list-type="order"><list-item><p>As we argue in the discussion, the drop in the performance is likely to be due to the cognitive factors such as divided attention because of the presence of the second task and not due to the degraded signal quality where the use of other classifiers indeed might have an advantage.</p></list-item><list-item><p>Although absolute performance might potentially have a boost of a few percent when using other classifiers, the relative effect of perturbations is likely to be similar.</p></list-item><list-item><p>The fact that we detected a moderate effect of perturbations using all three classifiers indicates that the detection of perturbations is not related to a specific classifier.</p></list-item></list><p>In this study, we limited our feature extraction method to CCA and used CCA-based features with different classifiers. Using lately introduced methods (e.g. multivariate synchronization index [<xref rid="pone.0191673.ref035" ref-type="bibr">35</xref>], temporally local multivariate synchronization index [<xref rid="pone.0191673.ref036" ref-type="bibr">36</xref>]) to detect the frequency in SSVEPs can be a subject of future study.</p></sec><sec sec-type="conclusions" id="sec018"><title>Conclusions</title><p>We quantified the robustness of a SSVEP-based online BCI under different perturbations using CCA features. Conditions requiring active performance such as loud or silent counting resulted only in slight decrease in BCI performance which indicates that SSVEP-based BCI can be used in parallel during the conversations. The fact that in some subjects perturbations resulted even in better performance indicates that the different cognitive strategy can be used for improving the accuracy of BCI within individuals.</p></sec><sec sec-type="supplementary-material" id="sec019"><title>Supporting information</title><supplementary-material content-type="local-data" id="pone.0191673.s001"><label>S1 Video</label><caption><title>A demonstrative video showing one trial from the offline task.</title><p>Subject focuses on each of the four randomly presented flickering circles indicated by the red oval frame for three seconds with an inter-stimulus interval (ISI) of one second.</p><p>(MP4)</p></caption><media xlink:href="pone.0191673.s001.mp4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0191673.s002"><label>S2 Video</label><caption><title>A demonstrative video showing several trials from the online task.</title><p>Subject focuses on one of the four flickering circles for three seconds and confirms (presses &#x02018;Y&#x02019; key) or rejects (presses &#x02018;N&#x02019; key and then specifies the correct response with the arrow keys) the classification result using keyboard.</p><p>(MP4)</p></caption><media xlink:href="pone.0191673.s002.mp4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>The authors would like to thank Elena Sokolova for her contribution in the initial stage of the experiment.</p></ack><ref-list><title>References</title><ref id="pone.0191673.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Mak</surname><given-names>JN</given-names></name>, <name><surname>Wolpaw</surname><given-names>JR</given-names></name>. <article-title>Clinical Applications of Brain-Computer Interfaces: Current State and Future Prospects</article-title>. <source>IEEE Rev Biomed Eng</source>. <year>2009</year>;<volume>2</volume>: <fpage>187</fpage>&#x02013;<lpage>199</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/RBME.2009.2035356">10.1109/RBME.2009.2035356</ext-link></comment>
<pub-id pub-id-type="pmid">20442804</pub-id></mixed-citation></ref><ref id="pone.0191673.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Farwell</surname><given-names>LA</given-names></name>, <name><surname>Donchin</surname><given-names>E</given-names></name>. <article-title>Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials</article-title>. <source>Electroencephalogr Clin Neurophysiol</source>. Ireland; <year>1988</year>;<volume>70</volume>: <fpage>510</fpage>&#x02013;<lpage>523</lpage>. <pub-id pub-id-type="pmid">2461285</pub-id></mixed-citation></ref><ref id="pone.0191673.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Guger</surname><given-names>C</given-names></name>, <name><surname>Daban</surname><given-names>S</given-names></name>, <name><surname>Sellers</surname><given-names>E</given-names></name>, <name><surname>Holzner</surname><given-names>C</given-names></name>, <name><surname>Krausz</surname><given-names>G</given-names></name>, <name><surname>Carabalona</surname><given-names>R</given-names></name>, <etal>et al</etal>
<article-title>How many people are able to control a P300-based brain&#x02013;computer interface (BCI)?</article-title>
<source>Neurosci Lett</source>. <year>2009</year>;<volume>462</volume>: <fpage>94</fpage>&#x02013;<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neulet.2009.06.045">10.1016/j.neulet.2009.06.045</ext-link></comment>
<pub-id pub-id-type="pmid">19545601</pub-id></mixed-citation></ref><ref id="pone.0191673.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>McFarland</surname><given-names>DJ</given-names></name>, <name><surname>Neat</surname><given-names>GW</given-names></name>, <name><surname>Read</surname><given-names>RF</given-names></name>, <name><surname>Wolpaw</surname><given-names>JR</given-names></name>. <article-title>An EEG-based method for graded cursor control</article-title>. <source>Psychobiology</source>. <year>1993</year>;<volume>21</volume>: <fpage>77</fpage>&#x02013;<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03327130">10.3758/BF03327130</ext-link></comment></mixed-citation></ref><ref id="pone.0191673.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Yuan</surname><given-names>H</given-names></name>, <name><surname>He</surname><given-names>B</given-names></name>. <article-title>Brain-Computer Interfaces Using Sensorimotor Rhythms: Current State and Future Perspectives</article-title>. <source>IEEE Trans Biomed Eng</source>. <year>2014</year>;<volume>61</volume>: <fpage>1425</fpage>&#x02013;<lpage>1435</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TBME.2014.2312397">10.1109/TBME.2014.2312397</ext-link></comment>
<pub-id pub-id-type="pmid">24759276</pub-id></mixed-citation></ref><ref id="pone.0191673.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Srinivasan</surname><given-names>R</given-names></name>, <name><surname>Bibi</surname><given-names>FA</given-names></name>, <name><surname>Nunez</surname><given-names>PL</given-names></name>. <article-title>Steady-state visual evoked potentials: distributed local sources and wave-like dynamics are sensitive to flicker frequency</article-title>. <source>Brain Topogr</source>. <year>2006</year>;<volume>18</volume>: <fpage>167</fpage>&#x02013;<lpage>187</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10548-006-0267-4">10.1007/s10548-006-0267-4</ext-link></comment>
<pub-id pub-id-type="pmid">16544207</pub-id></mixed-citation></ref><ref id="pone.0191673.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Friman</surname><given-names>O</given-names></name>, <name><surname>Volosyak</surname><given-names>I</given-names></name>, <name><surname>Graser</surname><given-names>A</given-names></name>. <article-title>Multiple channel detection of steady-state visual evoked potentials for brain-computer interfaces</article-title>. <source>IEEE Trans Biomed Eng</source>. United States; <year>2007</year>;<volume>54</volume>: <fpage>742</fpage>&#x02013;<lpage>750</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TBME.2006.889160">10.1109/TBME.2006.889160</ext-link></comment>
<pub-id pub-id-type="pmid">17405382</pub-id></mixed-citation></ref><ref id="pone.0191673.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>C</given-names></name>, <name><surname>Wu</surname><given-names>W</given-names></name>, <name><surname>Gao</surname><given-names>X</given-names></name>. <article-title>Frequency recognition based on canonical correlation analysis for SSVEP-based BCIs</article-title>. <source>IEEE Trans Biomed Eng</source>. United States; <year>2007</year>;<volume>54</volume>: <fpage>1172</fpage>&#x02013;<lpage>1176</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TBME.2006.886577">10.1109/TBME.2006.886577</ext-link></comment>
<pub-id pub-id-type="pmid">17549911</pub-id></mixed-citation></ref><ref id="pone.0191673.ref009"><label>9</label><mixed-citation publication-type="other">Nan W, Wong CM, Wang B, Wan F, Mak PU, Mak PI, et al. A comparison of minimum energy combination and canonical correlation analysis for SSVEP detection. 2011 5th International IEEE/EMBS Conference on Neural Engineering. 2011. pp. 469&#x02013;472. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/NER.2011.5910588">10.1109/NER.2011.5910588</ext-link></comment></mixed-citation></ref><ref id="pone.0191673.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Nakanishi</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>Y-T</given-names></name>, <name><surname>Jung</surname><given-names>T-P</given-names></name>. <article-title>A Comparison Study of Canonical Correlation Analysis Based Methods for Detecting Steady-State Visual Evoked Potentials</article-title>. <name><surname>Yao</surname><given-names>D</given-names></name>, editor. <source>PLoS One</source>. San Francisco, CA USA: Public Library of Science; <year>2015</year>;<volume>10</volume>: <fpage>e0140703</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0140703">10.1371/journal.pone.0140703</ext-link></comment>
<pub-id pub-id-type="pmid">26479067</pub-id></mixed-citation></ref><ref id="pone.0191673.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>Y-P</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Jung</surname><given-names>T-P</given-names></name>. <article-title>A mobile SSVEP-based brain-computer interface for freely moving humans: the robustness of canonical correlation analysis to motion artifacts</article-title>. <source>Conf Proc</source>. Annu Int Conf IEEE Eng Med Biol Soc IEEE Eng Med Biol Soc Annu Conf. United States; <year>2013</year>;<volume>2013</volume>: <fpage>1350</fpage>&#x02013;<lpage>1353</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/EMBC.2013.6609759">10.1109/EMBC.2013.6609759</ext-link></comment>
<pub-id pub-id-type="pmid">24109946</pub-id></mixed-citation></ref><ref id="pone.0191673.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>Y-P</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Wei</surname><given-names>C-S</given-names></name>, <name><surname>Jung</surname><given-names>T-P</given-names></name>. <article-title>Assessing the quality of steady-state visual-evoked potentials for moving humans using a mobile electroencephalogram headset</article-title> [Internet]. <source>Frontiers in Human Neuroscience</source>. <year>2014</year> p. <fpage>182</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fnhum.2014.00182">http://journal.frontiersin.org/article/10.3389/fnhum.2014.00182</ext-link>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2014.00182">10.3389/fnhum.2014.00182</ext-link></comment>
<pub-id pub-id-type="pmid">24744718</pub-id></mixed-citation></ref><ref id="pone.0191673.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>J</given-names></name>, <name><surname>Xu</surname><given-names>G</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>M</given-names></name>, <name><surname>Han</surname><given-names>C</given-names></name>, <name><surname>Jia</surname><given-names>Y</given-names></name>. <article-title>Effects of Mental Load and Fatigue on Steady-State Evoked Potential Based Brain Computer Interface Tasks: A Comparison of Periodic Flickering and Motion-Reversal Based Visual Attention</article-title>. <source>PLoS One</source>. Public Library of Science; <year>2016</year>;<volume>11</volume>: <fpage>e0163426</fpage> Available: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0163426">10.1371/journal.pone.0163426</ext-link></comment>
<pub-id pub-id-type="pmid">27658216</pub-id></mixed-citation></ref><ref id="pone.0191673.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>J</given-names></name>, <name><surname>Xu</surname><given-names>G</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>F</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <etal>et al</etal>
<article-title>Addition of visual noise boosts evoked potential-based brain-computer interface</article-title>. <source>Sci Rep</source>. The Author(s); <year>2014</year>;<volume>4</volume>: <fpage>4953</fpage> Available: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep04953">10.1038/srep04953</ext-link></comment>
<pub-id pub-id-type="pmid">24828128</pub-id></mixed-citation></ref><ref id="pone.0191673.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Schl&#x000f6;gl</surname><given-names>A</given-names></name>, <name><surname>Keinrath</surname><given-names>C</given-names></name>, <name><surname>Zimmermann</surname><given-names>D</given-names></name>, <name><surname>Scherer</surname><given-names>R</given-names></name>, <name><surname>Leeb</surname><given-names>R</given-names></name>, <name><surname>Pfurtscheller</surname><given-names>G</given-names></name>. <article-title>A fully automated correction method of {EOG} artifacts in {EEG} recordings</article-title>. <source>Clin Neurophysiol</source>. <year>2007</year>;<volume>118</volume>: <fpage>98</fpage>&#x02013;<lpage>104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.clinph.2006.09.003">10.1016/j.clinph.2006.09.003</ext-link></comment>
<pub-id pub-id-type="pmid">17088100</pub-id></mixed-citation></ref><ref id="pone.0191673.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>Z</given-names></name>. <article-title>Physical connections between different SSVEP neural networks</article-title>. <source>Sci Rep</source>. The Author(s); <year>2016</year>;<volume>6</volume>: <fpage>22801</fpage> Available: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep22801">10.1038/srep22801</ext-link></comment>
<pub-id pub-id-type="pmid">26952961</pub-id></mixed-citation></ref><ref id="pone.0191673.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Oostenveld</surname><given-names>R</given-names></name>, <name><surname>Fries</surname><given-names>P</given-names></name>, <name><surname>Maris</surname><given-names>E</given-names></name>, <name><surname>Schoffelen</surname><given-names>J-M</given-names></name>. <article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>. <source>Intell Neurosci</source>. New York, NY, United States: Hindawi Publishing Corp.; <year>2011</year>;<volume>2011</volume>: <issue>1</issue>:1&#x02013;1:<fpage>9</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2011/156869">10.1155/2011/156869</ext-link></comment>
<pub-id pub-id-type="pmid">21253357</pub-id></mixed-citation></ref><ref id="pone.0191673.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>&#x00130;&#x0015f;can</surname><given-names>Z</given-names></name>, <name><surname>Dokur</surname><given-names>Z</given-names></name>. <article-title>A novel steady-state visually evoked potential-based brain&#x02013;computer interface design: Character Plotter</article-title>. <source>Biomed Signal Process Control</source>. <year>2014</year>;<volume>10</volume>: <fpage>145</fpage>&#x02013;<lpage>152</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bspc.2013.11.009">http://dx.doi.org/10.1016/j.bspc.2013.11.009</ext-link></mixed-citation></ref><ref id="pone.0191673.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Kwak</surname><given-names>N-S</given-names></name>, <name><surname>Muller</surname><given-names>K-R</given-names></name>, <name><surname>Lee</surname><given-names>S-W</given-names></name>. <article-title>A lower limb exoskeleton control system based on steady state visual evoked potentials</article-title>. <source>J Neural Eng</source>. England; <year>2015</year>;<volume>12</volume>: <fpage>56009</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/12/5/056009">10.1088/1741-2560/12/5/056009</ext-link></comment></mixed-citation></ref><ref id="pone.0191673.ref020"><label>20</label><mixed-citation publication-type="other">Bender T, Kjaer TW, Thomsen CE, Sorensen HBD, Puthusserypady S. Semi-supervised adaptation in ssvep-based brain-computer interface using tri-training. 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2013. pp. 4279&#x02013;4282. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/EMBC.2013.6610491">10.1109/EMBC.2013.6610491</ext-link></comment></mixed-citation></ref><ref id="pone.0191673.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Muller</surname><given-names>SMT</given-names></name>, <name><surname>Diez</surname><given-names>PF</given-names></name>, <name><surname>Bastos-Filho</surname><given-names>TF</given-names></name>, <name><surname>Sarcinelli-Filho</surname><given-names>M</given-names></name>, <name><surname>Mut</surname><given-names>V</given-names></name>, <name><surname>Laciar</surname><given-names>E</given-names></name>. <article-title>SSVEP-BCI implementation for 37&#x02013;40 Hz frequency range</article-title>. <source>Conf Proc</source>. Annu Int Conf IEEE Eng Med Biol Soc IEEE Eng Med Biol Soc Annu Conf. United States; <year>2011</year>;<volume>2011</volume>: <fpage>6352</fpage>&#x02013;<lpage>6355</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/IEMBS.2011.6091568">10.1109/IEMBS.2011.6091568</ext-link></comment>
<pub-id pub-id-type="pmid">22255791</pub-id></mixed-citation></ref><ref id="pone.0191673.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Benjamini</surname><given-names>Y</given-names></name>, <name><surname>Hochberg</surname><given-names>Y</given-names></name>. <article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title>. <source>J R Stat Soc Ser B</source>. Wiley for the Royal Statistical Society; <year>1995</year>;<volume>57</volume>: <fpage>289</fpage>&#x02013;<lpage>300</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2346101">10.2307/2346101</ext-link></comment></mixed-citation></ref><ref id="pone.0191673.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Maris</surname><given-names>E</given-names></name>, <name><surname>Oostenveld</surname><given-names>R</given-names></name>. <article-title>Nonparametric statistical testing of EEG-and MEG-data</article-title>. <source>J Neurosci Methods</source>. Elsevier; <year>2007</year>;<volume>164</volume>: <fpage>177</fpage>&#x02013;<lpage>190</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2007.03.024">10.1016/j.jneumeth.2007.03.024</ext-link></comment>
<pub-id pub-id-type="pmid">17517438</pub-id></mixed-citation></ref><ref id="pone.0191673.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Fink</surname><given-names>A</given-names></name>, <name><surname>Neubauer</surname><given-names>AC</given-names></name>. <article-title>EEG alpha oscillations during the performance of verbal creativity tasks: Differential effects of sex and verbal intelligence</article-title>. <source>Int J Psychophysiol</source>. <year>2006</year>;<volume>62</volume>: <fpage>46</fpage>&#x02013;<lpage>53</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.ijpsycho.2006.01.001">10.1016/j.ijpsycho.2006.01.001</ext-link></comment>
<pub-id pub-id-type="pmid">16503062</pub-id></mixed-citation></ref><ref id="pone.0191673.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>de Graaf</surname><given-names>TA</given-names></name>, <name><surname>Gross</surname><given-names>J</given-names></name>, <name><surname>Paterson</surname><given-names>G</given-names></name>, <name><surname>Rusch</surname><given-names>T</given-names></name>, <name><surname>Sack</surname><given-names>AT</given-names></name>, <name><surname>Thut</surname><given-names>G</given-names></name>. <article-title>Alpha-Band Rhythms in Visual Task Performance: Phase-Locking by Rhythmic Sensory Stimulation</article-title>. <source>PLoS One</source>. Public Library of Science; <year>2013</year>;<volume>8</volume>: <fpage>e60035</fpage> Available: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0060035">10.1371/journal.pone.0060035</ext-link></comment>
<pub-id pub-id-type="pmid">23555873</pub-id></mixed-citation></ref><ref id="pone.0191673.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>McFarland</surname><given-names>DJ</given-names></name>, <name><surname>Miner</surname><given-names>LA</given-names></name>, <name><surname>Vaughan</surname><given-names>TM</given-names></name>, <name><surname>Wolpaw</surname><given-names>JR</given-names></name>. <article-title>Mu and beta rhythm topographies during motor imagery and actual movements</article-title>. <source>Brain Topogr</source>. United States; <year>2000</year>;<volume>12</volume>: <fpage>177</fpage>&#x02013;<lpage>186</lpage>. <pub-id pub-id-type="pmid">10791681</pub-id></mixed-citation></ref><ref id="pone.0191673.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Ettwig</surname><given-names>JF</given-names></name>, <name><surname>Bronkhorst</surname><given-names>AW</given-names></name>. <article-title>Attentional Switches and Dual-Task Interference</article-title>. <name><surname>Aky&#x000fc;rek</surname><given-names>E</given-names></name>, editor. <source>PLoS One</source>. San Francisco, CA USA: Public Library of Science; <year>2015</year>;<volume>10</volume>: <fpage>e0118216</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0118216">10.1371/journal.pone.0118216</ext-link></comment>
<pub-id pub-id-type="pmid">25730112</pub-id></mixed-citation></ref><ref id="pone.0191673.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Brandl</surname><given-names>S</given-names></name>, <name><surname>Frolich</surname><given-names>L</given-names></name>, <name><surname>Hohne</surname><given-names>J</given-names></name>, <name><surname>Muller</surname><given-names>K-R</given-names></name>, <name><surname>Samek</surname><given-names>W</given-names></name>. <article-title>Brain-computer interfacing under distraction: an evaluation study</article-title>. <source>J Neural Eng</source>. England; <year>2016</year>;<volume>13</volume>: <fpage>56012</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/1741-2560/13/5/056012">10.1088/1741-2560/13/5/056012</ext-link></comment>
<pub-id pub-id-type="pmid">27578310</pub-id></mixed-citation></ref><ref id="pone.0191673.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Perham</surname><given-names>N</given-names></name>, <name><surname>Vizard</surname><given-names>J</given-names></name>. <article-title>Can preference for background music mediate the irrelevant sound effect?</article-title>
<source>Appl Cogn Psychol</source>. John Wiley &#x00026; Sons, Ltd; <year>2011</year>;<volume>25</volume>: <fpage>625</fpage>&#x02013;<lpage>631</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/acp.1731">10.1002/acp.1731</ext-link></comment></mixed-citation></ref><ref id="pone.0191673.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Dalton</surname><given-names>BH</given-names></name>, <name><surname>Behm</surname><given-names>DG</given-names></name>. <article-title>Effects of noise and music on human and task performance: A systematic review</article-title>. <source>Occup Ergon</source>. IOS Press; <year>2007</year>;<volume>7</volume>: <fpage>143</fpage>&#x02013;<lpage>152</lpage>.</mixed-citation></ref><ref id="pone.0191673.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Carlson</surname><given-names>S</given-names></name>, <name><surname>Rama</surname><given-names>P</given-names></name>, <name><surname>Artchakov</surname><given-names>D</given-names></name>, <name><surname>Linnankoski</surname><given-names>I</given-names></name>. <article-title>Effects of music and white noise on working memory performance in monkeys</article-title>. <source>Neuroreport</source>. England; <year>1997</year>;<volume>8</volume>: <fpage>2853</fpage>&#x02013;<lpage>2856</lpage>. <pub-id pub-id-type="pmid">9376518</pub-id></mixed-citation></ref><ref id="pone.0191673.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Herweg</surname><given-names>NA</given-names></name>, <name><surname>Bunzeck</surname><given-names>N</given-names></name>. <article-title>Differential effects of white noise in cognitive and perceptual tasks</article-title> [Internet]. <source>Frontiers in Psychology</source>. <year>2015</year> p. <fpage>1639</fpage> Available: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01639">http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01639</ext-link>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2015.01639">10.3389/fpsyg.2015.01639</ext-link></comment>
<pub-id pub-id-type="pmid">26579024</pub-id></mixed-citation></ref><ref id="pone.0191673.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Vigneau</surname><given-names>M</given-names></name>, <name><surname>Beaucousin</surname><given-names>V</given-names></name>, <name><surname>Herve</surname><given-names>PY</given-names></name>, <name><surname>Duffau</surname><given-names>H</given-names></name>, <name><surname>Crivello</surname><given-names>F</given-names></name>, <name><surname>Houde</surname><given-names>O</given-names></name>, <etal>et al</etal>
<article-title>Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing</article-title>. <source>Neuroimage</source>. United States; <year>2006</year>;<volume>30</volume>: <fpage>1414</fpage>&#x02013;<lpage>1432</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2005.11.002">10.1016/j.neuroimage.2005.11.002</ext-link></comment>
<pub-id pub-id-type="pmid">16413796</pub-id></mixed-citation></ref><ref id="pone.0191673.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Jensen</surname><given-names>O</given-names></name>, <name><surname>Mazaheri</surname><given-names>A</given-names></name>. <article-title>Shaping functional architecture by oscillatory alpha activity: gating by inhibition</article-title>. <source>Front Hum Neurosci</source>. Switzerland; <year>2010</year>;<volume>4</volume>: <fpage>186</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2010.00186">10.3389/fnhum.2010.00186</ext-link></comment>
<pub-id pub-id-type="pmid">21119777</pub-id></mixed-citation></ref><ref id="pone.0191673.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>P</given-names></name>, <name><surname>Cheng</surname><given-names>K</given-names></name>, <name><surname>Yao</surname><given-names>D</given-names></name>. <article-title>Multivariate synchronization index for frequency recognition of SSVEP-based brain-computer interface</article-title>. <source>J Neurosci Methods</source>. Netherlands; <year>2014</year>;<volume>221</volume>: <fpage>32</fpage>&#x02013;<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2013.07.018">10.1016/j.jneumeth.2013.07.018</ext-link></comment>
<pub-id pub-id-type="pmid">23928153</pub-id></mixed-citation></ref><ref id="pone.0191673.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Guo</surname><given-names>D</given-names></name>, <name><surname>Xu</surname><given-names>P</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Yao</surname><given-names>D</given-names></name>. <article-title>Robust frequency recognition for SSVEP-based BCI with temporally local multivariate synchronization index</article-title>. <source>Cogn Neurodyn</source>. <year>2016</year>;<volume>10</volume>: <fpage>505</fpage>&#x02013;<lpage>511</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11571-016-9398-9">10.1007/s11571-016-9398-9</ext-link></comment>
<pub-id pub-id-type="pmid">27891199</pub-id></mixed-citation></ref></ref-list></back></article>
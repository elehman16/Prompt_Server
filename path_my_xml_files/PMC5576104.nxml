<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Implement Sci</journal-id><journal-id journal-id-type="iso-abbrev">Implement Sci</journal-id><journal-title-group><journal-title>Implementation Science : IS</journal-title></journal-title-group><issn pub-type="epub">1748-5908</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28851459</article-id><article-id pub-id-type="pmc">5576104</article-id><article-id pub-id-type="publisher-id">635</article-id><article-id pub-id-type="doi">10.1186/s13012-017-0635-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Psychometric assessment of three newly developed implementation outcome measures</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6996-9480</contrib-id><name><surname>Weiner</surname><given-names>Bryan J.</given-names></name><address><email>bjweiner@uw.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lewis</surname><given-names>Cara C.</given-names></name><address><email>lewis.cc@ghc.org</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Stanick</surname><given-names>Cameo</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Powell</surname><given-names>Byron J.</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Dorsey</surname><given-names>Caitlin N.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Clary</surname><given-names>Alecia S.</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Boynton</surname><given-names>Marcella H.</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Halko</surname><given-names>Heather</given-names></name><xref ref-type="aff" rid="Aff8">8</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122986657</institution-id><institution-id institution-id-type="GRID">grid.34477.33</institution-id><institution>Department of Global Health, </institution><institution>University of Washington, </institution></institution-wrap>1510 San Juan Road, Box 357965, Seattle, WA 98195 USA </aff><aff id="Aff2"><label>2</label>Kaiser Permanente Washington Health Research Institute, MacColl Center for Health Care Innovation, 1730 Minor Avenue, Suite 1600, Seattle, WA 98101 USA </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0790 959X</institution-id><institution-id institution-id-type="GRID">grid.411377.7</institution-id><institution>Department of Psychological and Brain Sciences, </institution><institution>Indiana University, </institution></institution-wrap>1101 E 10th Street, Bloomington, IN 47405 USA </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122986657</institution-id><institution-id institution-id-type="GRID">grid.34477.33</institution-id><institution>Department of Psychiatry and Behavioral Sciences, </institution><institution>University of Washington, </institution></institution-wrap>325 Ninth Street, Seattle, WA 98104 USA </aff><aff id="Aff5"><label>5</label>Hathaway-Sycamores Child and Family Services, 210 S DeLacey Ave, Suite 110, Pasadena, CA 91105-2074 USA </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122483208</institution-id><institution-id institution-id-type="GRID">grid.10698.36</institution-id><institution>Department of Health Policy and Management, </institution><institution>University of North Carolina at Chapel Hill, </institution></institution-wrap>135 Dauer Drive, Chapel Hill, NC 27599 USA </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122483208</institution-id><institution-id institution-id-type="GRID">grid.10698.36</institution-id><institution>Department of Health Behavior, </institution><institution>University of North Carolina at Chapel Hill, </institution></institution-wrap>135 Dauer Drive, Chapel Hill, NC 27599 USA </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2192 5772</institution-id><institution-id institution-id-type="GRID">grid.253613.0</institution-id><institution>Department of Psychology, </institution><institution>University of Montana, </institution></institution-wrap>Missoula, USA </aff></contrib-group><pub-date pub-type="epub"><day>29</day><month>8</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>29</day><month>8</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>12</volume><elocation-id>108</elocation-id><history><date date-type="received"><day>17</day><month>4</month><year>2017</year></date><date date-type="accepted"><day>8</day><month>8</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s). 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">Implementation outcome measures are essential for monitoring and evaluating the success of implementation efforts. Yet, currently available measures lack conceptual clarity and have largely unknown reliability and validity. This study developed and psychometrically assessed three new measures: the Acceptability of Intervention Measure (AIM), Intervention Appropriateness Measure (IAM), and Feasibility of Intervention Measure (FIM).</p></sec><sec><title>Methods</title><p id="Par2">Thirty-six implementation scientists and 27 mental health professionals assigned 31 items to the constructs and rated their confidence in their assignments. The Wilcoxon one-sample signed rank test was used to assess substantive and discriminant content validity. Exploratory and confirmatory factor analysis (EFA and CFA) and Cronbach alphas were used to assess the validity of the conceptual model. Three hundred twenty-six mental health counselors read one of six randomly assigned vignettes depicting a therapist contemplating adopting an evidence-based practice (EBP). Participants used 15 items to rate the therapist&#x02019;s perceptions of the acceptability, appropriateness, and feasibility of adopting the EBP. CFA and Cronbach alphas were used to refine the scales, assess structural validity, and assess reliability. Analysis of variance (ANOVA) was used to assess known-groups validity. Finally, half of the counselors were randomly assigned to receive the same vignette and the other half the opposite vignette; and all were asked to re-rate acceptability, appropriateness, and feasibility. Pearson correlation coefficients were used to assess test-retest reliability and linear regression to assess sensitivity to change.</p></sec><sec><title>Results</title><p id="Par3">All but five items exhibited substantive and discriminant content validity. A trimmed CFA with five items per construct exhibited acceptable model fit (CFI&#x000a0;=&#x000a0;0.98, RMSEA&#x000a0;=&#x000a0;0.08) and high factor loadings (0.79 to 0.94). The alphas for 5-item scales were between 0.87 and 0.89. Scale refinement based on measure-specific CFAs and Cronbach alphas using vignette data produced 4-item scales (&#x003b1;&#x02019;s from 0.85 to 0.91). A three-factor CFA exhibited acceptable fit (CFI&#x000a0;=&#x000a0;0.96, RMSEA&#x000a0;=&#x000a0;0.08) and high factor loadings (0.75 to 0.89), indicating structural validity. ANOVA showed significant main effects, indicating known-groups validity. Test-retest reliability coefficients ranged from 0.73 to 0.88. Regression analysis indicated each measure was sensitive to change in both directions.</p></sec><sec><title>Conclusions</title><p id="Par4">The AIM, IAM, and FIM demonstrate promising psychometric properties. Predictive validity assessment is planned.</p></sec><sec><title>Electronic supplementary material</title><p>The online version of this article (doi:10.1186/s13012-017-0635-3) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Acceptability of Intervention Measure (AIM)</kwd><kwd>Intervention Appropriateness Measure (IAM)</kwd><kwd>Feasibility of Intervention Measure (FIM)</kwd><kwd>Implementation research</kwd><kwd>Acceptability</kwd><kwd>Feasibility</kwd><kwd>Appropriateness</kwd><kwd>Implementation outcomes</kwd><kwd>Measure</kwd><kwd>Structural validity</kwd><kwd>Test-retest</kwd><kwd>Known-groups</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH106510</award-id><principal-award-recipient><name><surname>Powell</surname><given-names>Byron J.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2017</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par17">Implementation outcomes play a critical role in implementation research and practice. Acceptability, appropriateness, feasibility, adoption, fidelity, cost, penetration, and sustainability serve not only as indicators of the effects of implementation processes (i.e., outcomes in their own right) but also as preconditions for attaining desired service delivery and clinical outcomes (i.e., intermediate outcomes) [<xref ref-type="bibr" rid="CR1">1</xref>]. Reliable, valid, and pragmatic measures of these outcomes are essential for monitoring and evaluating the success of implementation efforts and comparing the effectiveness of alternative implementation strategies.</p><p id="Par18">Despite their importance, existing measures of implementation outcomes have largely unknown psychometric and pragmatic qualities [<xref ref-type="bibr" rid="CR2">2</xref>]. This status quo raises questions about these measures&#x02019; utility for building cumulative knowledge in implementation research or guiding implementation efforts in clinical and community settings. For example, most available measures have not been empirically tested for substantive or discriminant content validity. They are rarely defined or distinguished from one another and often employ similar items to assess purportedly different constructs [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. As a result, it is not clear exactly what these measures are assessing. Moreover, although most implementation outcome measures have been assessed for scale reliability, typically in the form of inter-item consistency, few if any have been assessed for other measurement properties that are of key importance to implementation researchers and other implementation stakeholders (e.g., intermediaries, policymakers, practice leaders), such as stability, responsiveness, and predictive validity [<xref ref-type="bibr" rid="CR2">2</xref>&#x02013;<xref ref-type="bibr" rid="CR4">4</xref>]. Importantly, existing measures of implementation outcomes have not been administered together in research studies, leaving open the question of whether some of these outcomes, which are conceptually distinguishable, are empirically distinguishable [<xref ref-type="bibr" rid="CR1">1</xref>].</p><p id="Par19">In this article, we report the results of a psychometric assessment of newly developed measures of three implementation outcomes: acceptability (Acceptability of Intervention Measure (AIM)), appropriateness (Intervention Appropriateness Measure (IAM)), and feasibility (Feasibility of Intervention Measure (FIM)). We selected these implementation outcomes because (a) they are often used in formative research or pilot studies as &#x0201c;leading indicators&#x0201d; of implementation success [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR5">5</xref>] and (b) they are conceptually distinct but likely to be empirically inter-related in complex ways [<xref ref-type="bibr" rid="CR1">1</xref>]. Evidence that two or more of these outcomes are highly correlated with each other would suggest that they could serve as proxies for each other. We created new measures of these outcomes because our preliminary work suggested that addressing the definitional ambiguities and overlapping item content in existing measures would have entailed substantial adaptations such that new measures would essentially be created. Consistent with the overarching goal of our team&#x02019;s work to enhance the psychometric strength and pragmatic quality of measures in implementation science [<xref ref-type="bibr" rid="CR4">4</xref>], we involved researchers and other stakeholders in the measure development and testing process.</p></sec><sec id="Sec2"><title>Conceptual framework</title><p id="Par20">Proctor and her colleagues defined acceptability, appropriateness, and feasibility as follows [<xref ref-type="bibr" rid="CR1">1</xref>]:</p><p id="Par21">
<italic>Acceptability</italic> is the perception among implementation stakeholders that a given treatment, service, practice, or innovation is agreeable, palatable, or satisfactory&#x02026;. <italic>Appropriateness</italic> is the perceived fit, relevance, or compatibility of the innovation or evidence-based practice for a given practice setting, provider, or consumer; and/or perceived fit of the innovation to address a particular issue or problem.&#x02026;<italic>Feasibility</italic> is defined as the extent to which a new treatment, or an innovation, can be successfully used or carried out within a given agency or setting.</p><p id="Par22">All three constructs imply an evaluation of the fit or match of something (e.g., an evidence-based practice (EBP)) and some criterion. For acceptability, the criterion is personal. Two people can view the same EBP and form different judgments about its acceptability to them if their needs, preferences, or expectations differ. For appropriateness, the criterion is technical or social. An EBP can be judged appropriate if it is seen as efficacious for achieving some purpose given existing conditions, including patients&#x02019; presenting problems, or seen as consistent with norms or values governing people&#x02019;s conduct in particular situations, including organizational mission and treatment philosophy. For feasibility, the criterion is practical. An EBP can be judged feasible if a task or an action can be performed relatively easily or conveniently given existing resources (e.g., effort, time, and money) and circumstances (e.g., timing or sociopolitical will). These three constructs are semantically similar, as evidenced by some overlap in the synonyms for each in the dictionary; however, they can be distinguished conceptually and operationally based on the criterion that each references in the evaluation of fit or match.</p><p id="Par23">These three constructs could be difficult to distinguish empirically if an antecedent condition meets more than one criterion upon which fit or match is evaluated. For example, when mandated, as EBP could be judged both as unacceptable if a provider regards its required use as undercutting her clinical autonomy and as inappropriate if a provider sees it as contrary to organizational treatment philosophies. Similarly, when standardized, an EBP could be judged both as inappropriate, if a provider regards it as ineffective in meeting patients&#x02019; varied needs, and as infeasible if a provider sees it as difficult to implement given her organization&#x02019;s staffing and resources. Although perceptions of EBP acceptability, appropriateness, and feasibility are likely to covary frequently, it is possible to imagine, create, or observe conditions in which they do not. For example, given the criteria for evaluating fit or match described above and drawing on constructs in the Consolidated Framework for Implementation Research [<xref ref-type="bibr" rid="CR6">6</xref>], we expect that acceptability is likely to vary to a greater extent than either appropriateness or feasibility as a function of individuals&#x02019; willingness to try new things (openness) on a limited, revocable basis (trialability). Appropriateness is likely to vary to a greater extent than either acceptability or feasibility as a function of consistency with professional values (norms) or perceived efficacy in meeting patient needs (relevance). Feasibility is likely to vary to a greater extent than either acceptability or appropriateness as a function of cost or time (resource availability) or ease of implementation or use (complexity). Proctor and colleagues propose that these three implementation outcomes are most salient in the adoption phase of the innovation-decision process. Although predictive validity is not assessed below, acceptability, appropriateness, and feasibility are hypothesized to be associated with EBP adoption [<xref ref-type="bibr" rid="CR1">1</xref>].</p></sec><sec id="Sec3"><title>Methods</title><p id="Par24">We conducted three studies to assess the psychometric properties of the newly developed measures of acceptability (AIM), appropriateness (IAM), and feasibility (FIM). In study 1, we assessed the measures&#x02019; substantive and discriminant content validity by administering a web-based survey to a sample of implementation researchers and implementation-experienced mental health professionals. In study 2, we examined the structural validity, reliability, and known-groups validity of the three constructs by conducting an experimental vignette study with mental health counselors. In study 3, we examined the measures&#x02019; test-retest reliability and sensitivity to change by re-administering the experiment to the same participants several weeks after study 2.</p></sec><sec id="Sec4"><title>Study 1</title><sec id="Sec5"><title>Method</title><p id="Par25">Substantive validity refers to the extent to which a measure is judged to be reflective of, or theoretically linked to, some construct of interest [<xref ref-type="bibr" rid="CR7">7</xref>]. When substantive validity is demonstrated for several measures simultaneously, discriminant content validity is also demonstrated [<xref ref-type="bibr" rid="CR8">8</xref>]. Typically, substantive validity is assessed informally and qualitatively by asking a few researchers (usually colleagues of the measure developer) to consider whether a measure&#x02019;s items seem representative of the construct&#x02019;s theoretical content [<xref ref-type="bibr" rid="CR9">9</xref>]. In this study, we took a formal, quantitative approach by asking a large group of researchers and practitioners to rate the extent to which items reflect the constructs they were intended to measure [<xref ref-type="bibr" rid="CR8">8</xref>].</p><sec id="Sec6"><title>Design, participants, and procedures</title><p id="Par26">Fifty-one Ph.D.-trained implementation scientists and 52 implementation-experienced mental health professionals were recruited by email through the Society for Implementation Research Collaboration (SIRC) [<xref ref-type="bibr" rid="CR10">10</xref>] and the authors professional networks. Thirty-six implementation scientists and 27 mental health professionals agreed to participate in the study. By involving both researchers and practitioners, we sought to increase the relevance of the measures to both stakeholder groups [<xref ref-type="bibr" rid="CR11">11</xref>]. Sixty-eight percent of the study participants identified as male, 94% identified as Caucasian, 3% identified as African American, and 3% identified as Asian. Eleven percent identified as Hispanic. Seventy-eight percent of the implementation scientists reported receiving funding as a principal investigator to study or evaluate implementation. Seventeen percent of the mental health professionals were administrators; 13% were clinicians. One professional identified as both. On average, these professionals had 8&#x000a0;years of experience implementing EBPs or leading EBP implementation projects in mental health settings.</p><p id="Par27">The study participants completed a web-based survey in which the three constructs were defined at the top of the survey and 31 items reflecting the three constructs were listed below in random order. The participants assigned each item to the construct that they perceived the item measured. The participants could assign an item to more than one construct. They rated their confidence in each assignment from 0% for &#x0201c;not at all confident&#x0201d; to 100% for &#x0201c;extremely confident.&#x0201d; Correct assignments (i.e., items assigned to intended constructs) were coded 1 (i.e., a &#x0201c;match&#x0201d;). Incorrect assignments were coded &#x02212;&#x02009;1 (i.e., &#x0201c;no match&#x0201d;). The survey design ensured that no missing data occurred for assignments. Each assignment was multiplied by its accompanying confidence rating. Thus, weighted assignments ranged from &#x02212;&#x02009;1 to 1.</p></sec><sec id="Sec7"><title>Measures</title><p id="Par28">We employed a deductive approach to item generation [<xref ref-type="bibr" rid="CR9">9</xref>], whereby we used the definitions and conceptual framework described above to ascertain whether items from existing measures, obtained through the Society for Implementation Research Collaboration [<xref ref-type="bibr" rid="CR12">12</xref>] and literature searches, adequately capture the theoretical content of the construct. Based on our findings, 12 items were generated to measure acceptability (e.g., &#x0201c;This EBP seems fine&#x0201d;), 11 items were written to measure appropriateness (e.g., &#x0201c;This EBP seems suitable.&#x0201d;), and 8 items were produced to measure feasibility (e.g., &#x0201c;This EBP seems doable.&#x0201d;). See Table <xref rid="Tab1" ref-type="table">1</xref>. The appropriateness items did not specify a purpose (e.g., &#x0201c;for treating depression&#x0201d;), person (e.g., &#x0201c;for my patients&#x0201d;), or situation (e.g., &#x0201c;for my organization&#x0201d;) in order to keep the substantive validity assessment of these items general rather than restricted.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Discriminant content validity analysis, <italic>N</italic>&#x000a0;=&#x000a0;63</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Construct</th><th>Item</th><th>Median</th><th>Wilx</th><th/></tr></thead><tbody><tr><td>Acceptability</td><td>This EBP seems fine.</td><td>.1</td><td>.012</td><td/></tr><tr><td/><td>This EBP seems good enough.</td><td>0</td><td>.163</td><td>ns</td></tr><tr><td/><td>This EBP will do.</td><td>.1</td><td>.062</td><td>ns</td></tr><tr><td/><td>This EBP meets my approval.</td><td>.8</td><td>.000</td><td/></tr><tr><td/><td>This EBP meets my needs.</td><td>.2</td><td>.006</td><td/></tr><tr><td/><td>This EBP is okay.</td><td>.2</td><td>.005</td><td/></tr><tr><td/><td>This EBP is satisfactory.</td><td>.6</td><td>.000</td><td/></tr><tr><td/><td>I have no objection to this EBP.</td><td>.7</td><td>.000</td><td/></tr><tr><td/><td>This EBP is pretty good.</td><td>0</td><td>.027</td><td>ns</td></tr><tr><td/><td>This EBP is appealing.</td><td>.7</td><td>.000</td><td/></tr><tr><td/><td>I like this EBP.</td><td>.8</td><td>.000</td><td/></tr><tr><td/><td>I welcome use of this EBP</td><td>.8</td><td>.000</td><td/></tr><tr><td>Appropriateness</td><td>This EBP seems right.</td><td>0</td><td>.174</td><td>ns</td></tr><tr><td/><td>This EBP seems fitting.</td><td>.75</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems suitable.</td><td>.7</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems reasonable.</td><td>&#x02212;&#x02009;.5</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems applicable.</td><td>.75</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems right on the button.</td><td>.2</td><td>.050</td><td>ns</td></tr><tr><td/><td>This EBP seems proper.</td><td>.6</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems apt.</td><td>.7</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems like a good match.</td><td>.8</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems well aligned.</td><td>.6</td><td>.000</td><td/></tr><tr><td>Feasibility</td><td>This EBP seems practical.</td><td>.25</td><td>.002</td><td/></tr><tr><td/><td>This EBP seems realistic.</td><td>.6</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems workable.</td><td>.7</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems implementable.</td><td>.9</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems possible.</td><td>.9</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems viable.</td><td>.35</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems doable.</td><td>1</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems challenging.</td><td>.7</td><td>.000</td><td/></tr><tr><td/><td>This EBP seems easy to use.</td><td>.9</td><td>.000</td><td/></tr></tbody></table><table-wrap-foot><p>
<italic>ns</italic> not significant at .05 level after false discovery rate controlling procedure for multiple tests</p></table-wrap-foot></table-wrap>
</p></sec><sec id="Sec8"><title>Data analysis</title><p id="Par29">To assess substantive and discriminant content validity, we used the Wilcoxon one-sample signed rank test to determine whether items represented the intended construct more so than the other constructs (i.e., demonstrated discriminant content validity) [<xref ref-type="bibr" rid="CR8">8</xref>]. Specifically, an item was said to measure a construct if its median weighted assignment to that construct was significantly greater than zero. The Wilcoxon test was used instead of the <italic>t</italic> test since the weighted assignments were not normally distributed. Hochberg&#x02019;s correction was used to correct for multiple tests [<xref ref-type="bibr" rid="CR13">13</xref>]. We calculated intraclass correlation coefficients (ICCs) using two-way mixed effects model to assess the level of agreement in item assignments among all participants, and also within stakeholder groups, across all 31 items and for each construct [<xref ref-type="bibr" rid="CR14">14</xref>]. Weighted assignments were used to calculate ICCs.</p><p id="Par30">To assess the validity of the conceptual model, we performed both exploratory and confirmatory factor analysis (EFA and CFA) on the same data using M<italic>plus</italic> 7 [<xref ref-type="bibr" rid="CR15">15</xref>]. A separate five-factor EFA with promax oblique rotation for each construct was estimated to allow for the possibility of additional factors beyond the three expected factors, resulting from possible item cross-loadings, and to trim the number of items. A three-factor CFA model using robust weighted least-squares estimation was then tested using M<italic>plus</italic> 7 by loading the five best-performing items on their respective constructs. We used the following guidelines for determining good model fit: comparative fit index (CFI) equal to or greater than 0.95 and root mean square error of approximation (RMSEA) equal to or less than 0.08 [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. Power analysis based on the RMSEA test of close fit estimated power at 84% with a sample size of 60 participants [<xref ref-type="bibr" rid="CR18">18</xref>]. Since the weighted assignments were not normally distributed, prior to running the EFA and CFA, we collapsed the weighted assignments into intervals spanning 20 points each (0&#x000a0;=&#x000a0;0, 1&#x02013;19%&#x000a0;=&#x000a0;1, 20&#x02013;39%&#x000a0;=&#x000a0;2, 40&#x02013;59%&#x000a0;=&#x000a0;3, 60&#x02013;79%&#x000a0;=&#x000a0;4, 80&#x02013;99%&#x000a0;=&#x000a0;5, and 100%&#x000a0;=&#x000a0;6) and used the re-scaled items as categorical variables. This approach was consistent with the expressed distributions and variability of the responses.</p></sec></sec><sec id="Sec9"><title>Results</title><p id="Par31">Judges exhibited a high degree of consistency in their item assignments. The ICC for all participants across all 31 items was 0.89 (95% CI 0.84&#x02013;0.92). The ICC for all participants for each construct was as follows: 0.82 (95% CI 0.63&#x02013;0.94) for acceptability, 0.94 (95% CI 0.86&#x02013;0.98) for appropriateness, and 0.87 (95% CI 0.72&#x02013;0.96) for feasibility. Weighted assignments for both stakeholder groups&#x02014;Ph.D.-trained implementation scientists and implementation-experienced mental health professionals&#x02014;were pooled in subsequent analysis since no significant group differences were observed.</p><p id="Par32">Table <xref rid="Tab1" ref-type="table">1</xref> displays the results of the discriminant content validity analysis. Median weighted assignments for all but five items were significantly greater than zero after applying the Hochberg correction for multiple tests. This indicated that the participants judged the items to reflect to a significantly greater degree the constructs they were intended to measure than they did the other constructs. The five items that did not have sufficient discriminant content validity were &#x0201c;This EBP is good enough,&#x0201d; &#x0201c;This EBP will do,&#x0201d; and &#x0201c;This EBP is pretty good&#x0201d; for acceptability and &#x0201c;This EBP seems right&#x0201d; and &#x0201c;This EBP seems right on the button&#x0201d; for appropriateness. The negative median weighted assignment for &#x0201c;This EBP seems reasonable&#x0201d; indicates that the respondents had greater confidence in assigning this item to acceptability or feasibility than to appropriateness. Hence, this item also lacks discriminant content validity.</p><p id="Par33">The five-factor EFA model indicated that all but two items loaded on a single factor (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>). The item &#x0201c;This EBP is fine&#x0201d; cross-loaded on the acceptability factor and a fourth, undefined factor. The item &#x0201c;This EBP seems reasonable&#x0201d; did not load on the appropriateness factor. For each construct, the five items with the strongest factor scores and inter-item correlations were selected for testing in a CFA model. All but one of these items (&#x0201c;This EBP seems well aligned&#x0201d;) was among the top-five mean weighted assignments for its intended construct. Internal consistency of the five-item scales was good: acceptability (&#x003b1;&#x000a0;=&#x000a0;.89), appropriateness (&#x003b1;&#x000a0;=&#x000a0;.87), and feasibility (&#x003b1;&#x000a0;=&#x000a0;.89).</p><p id="Par34">The CFA model exhibited factor loadings that ranged between .79 and .94 (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>), and model fit was adequate, as evidenced by CFI&#x000a0;=&#x000a0;0.98 and RMSEA&#x000a0;=&#x000a0;0.08 (CI, 0.04&#x02013;0.11). The correlations among the three factors were high, ranging from 0.77 to 0.91. Although these correlations are high, a CFA model loading all 15 items on a single factor exhibited poor model fit, as evidenced by CFI&#x000a0;=&#x000a0;0.94 and RMSEA&#x000a0;=&#x000a0;0.15 (CI, 0.13&#x02013;0.18) and an increase in the BIC from 2493.5 for the three-factor model to 2565.4 for the one-factor model. These results suggested that the three-factor CFA model is a better representation of the factor structure of the scale items than a one-factor model.<fig id="Fig1"><label>Fig. 1</label><caption><p>Confirmatory factor analysis of substantive validity data</p><p>
<italic>N</italic>&#x000a0;=&#x000a0;63. &#x003c7;<sup>2</sup>(87) = 120.6, <italic>p</italic> =.01; CFI = 0.98 ; RMSEA = 0.08, CI [0.04-0.11]. All path loadings are significant at <italic>p</italic> &#x0003c; .05</p></caption><graphic xlink:href="13012_2017_635_Fig1_HTML" id="MO1"/></fig>
</p></sec></sec><sec id="Sec10"><title>Study 2</title><sec id="Sec11"><title>Method</title><p id="Par35">In study 2, we ascertained the structural validity, reliability, and known-groups validity of our new measures in an online experimental study. Structural validity refers to the extent to which the inter-relationships among items measuring a construct accord with the expected internal structure of the construct [<xref ref-type="bibr" rid="CR19">19</xref>]. Reliability here refers to the extent to which the items measuring a construct exhibit internal consistency [<xref ref-type="bibr" rid="CR15">15</xref>]. Known-groups validity refers to the extent to which a measure is sensitive to known differences between groups [<xref ref-type="bibr" rid="CR20">20</xref>].</p><sec id="Sec12"><title>Design, participants, and procedures</title><p id="Par36">Study participants were a convenience sample of 326 counselors belonging to the American Mental Health Counselors Association (AMHCA). These participants were obtained through an email invitation sent to 2000 non-retired clinical members of AMHCA, with non-respondents receiving up to three reminder emails spaced a week apart. Twenty-eight percent of the 326 study participants identified as male, 71% as female, and one person as non-binary gender. Ninety-one percent identified as Caucasian, 5% as African American, 2% as Asian, and 2% as &#x0201c;Other.&#x0201d; Four percent identified as Hispanic, Latino, or Spanish origin. Seventy-seven percent indicated that they work in a private practice setting. Fifty-three percent reported that they had implemented an EBP or led the implementation of an EBP in the past 5&#x000a0;years.</p><p id="Par37">The study participants read one of six randomly assigned vignettes in which a therapist discusses with a colleague her thoughts about adopting measurement-based care (MBC). MBC is an evidence-based practice that involves collecting client progress and outcome data on a routine basis using brief, validated measures and using those data to inform treatment decisions [<xref ref-type="bibr" rid="CR21">21</xref>]. In each of the vignettes, the descriptive content of the therapist&#x02019;s discussion was designed to systematically vary with respect to her assessment of MBC&#x02019;s acceptability, appropriateness, and feasibility. Guided by the conceptual framework, we manipulated two levels (high versus low) of the three constructs by varying the following information in the vignettes: openness, trialability, relevance for patient needs, implementation complexity, and resource availability. The participants rated the therapist&#x02019;s perceptions of the acceptability, appropriateness, and feasibility of adopting MBC based on the views that the therapist expressed in the vignette, not their own personal opinion of MBC. Due to a study design error, two of the eight possible vignettes were not fielded: one in which acceptability was high, appropriateness low, and feasibility high; and the other where acceptability was low, appropriateness high, and feasibility low. The six fielded vignettes manipulated all other combinations of the levels of the constructs. While unfortunate, this design error did not preclude us from assessing reliability, structural validity, or known-groups validity. However, it did limit our ability to explore discriminant validity, especially between acceptability and appropriateness, as two of the four possible vignettes in which these constructs were designed to move in opposite directions were not included. The vignettes were pilot-tested with a convenience sample of five Ph.D. implementation scientists and implementation-experienced mental health professionals and revised prior to use.</p></sec><sec id="Sec13"><title>Measures</title><p id="Par38">Using a 5-point ordinal scale that ranged from &#x0201c;completely disagree&#x0201d; to &#x0201c;completely agree,&#x0201d; participants rated the three constructs using the 15 items comprising the three 5-item scales developed in study 1. Item wording was changed slightly from study 1 to accommodate the hypothetical nature of the assessments. Specifically, the subject of each item was changed from first person to third person and from &#x0201c;EBP&#x0201d; to &#x0201c;MBC&#x0201d; (e.g., &#x0201c;I like this EBP&#x0201d; to &#x0201c;She likes MBC&#x0201d;). All items were randomly ordered within and across vignettes to avoid ordering effects.</p></sec><sec id="Sec14"><title>Analysis</title><p id="Par39">To refine the scales, we estimated separate CFAs for each scale, examined factor loadings for individual items, and computed Cronbach alphas for inter-item consistency. To assess structural validity, we estimated the same 3-factor CFA model as in study 1 using M<italic>plus</italic> 7 [<xref ref-type="bibr" rid="CR15">15</xref>] using maximum likelihood estimation. For comparison purposes, we computed two other models: a 2-factor model with acceptability and appropriateness items loading on a single factor, and an omnibus 1-factor model -. We used the same CFA model fit guidelines as those employed in study 1. To assess known-groups validity, we conducted a 2<sup>3</sup> analysis of variance (ANOVA) with the Tukey test for multiple comparisons, to determine whether the mean levels of the implementation outcomes (i.e., high versus low levels of acceptability, appropriateness, and feasibility) varied as expected based on information manipulation in the vignettes.</p></sec></sec><sec id="Sec15"><title>Results</title><p id="Par40">Factor loadings in the one-factor CFAs for each scale ranged from 0.84 to 0.92, with one exception (see Additional file <xref rid="MOESM2" ref-type="media">2</xref>). The item, &#x0201c;She has no objection to MBC,&#x0201d; exhibited a factor loading of 0.50. In order to trim the scales further, the poorest performing item in each of the scale was dropped. In the case of acceptability, the &#x0201c;no objection&#x0201d; item was dropped due to its low factor loading. The &#x0201c;MBC seems well aligned&#x0201d; and &#x0201c;MBC seems workable&#x0201d; items were dropped from the appropriateness and feasibility scales, respectively, based on their lower correlations with the other items in their respective scales. The Cronbach alphas for the trimmed 4-item scales were 0.85 for acceptability, 0.91 for appropriateness, and 0.89 for feasibility.</p><p id="Par41">The factor loadings for the 3-factor CFA ranged from 0.75 to 0.89 and fit for the three-factor CFA model was acceptable. This was evidenced by CFI&#x000a0;=&#x000a0;0.96 and RMSEA&#x000a0;=&#x000a0;0.08 [CI, 0.06&#x02013;0.09] [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR22">22</xref>] after allowing the error terms between &#x0201c;MBC meets her approval&#x0201d; and &#x0201c;MBC is appealing to her&#x0201d; as well as between &#x0201c;She welcomes MBC&#x0201d; and &#x0201c;She likes MBC&#x0201d; to correlate to account for differences in sentence structure (i.e., passive versus active voice)(see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). By comparison, the factor loadings for the omnibus 1-factor CFA model ranged from 0.55 to 0.84 and the model fit was quite poor (CFA&#x000a0;=&#x000a0;0.73, RMSEA&#x000a0;=&#x000a0;.20 [CI, 0.19&#x02013;0.21]). The 2-factor CFA model fit the data better than the one-factor model, with factor loading ranging from 0.63 to 0.89 and model fit improving (CFI&#x000a0;=&#x000a0;.89, RMSEA&#x000a0;=&#x000a0;.13 [CI, 0.12&#x02013;0.14]). However, the 3-factor model fit the data better than either the 1-factor or 2-factor model. This was evidenced by the absolute model fit statistics referenced above as well as an increase in the BIC from 8499.4 for the 3-factor model to 8672.5 for the 2-factor model, with a further increase to 9085.5 for the 1-factor model. For the 2-factor model the correlations among the factors ranged from 0.36 to 0.77. The relatively high correlation between the acceptability and appropriateness scales (0.77) might have been inflated by the design error mentioned earlier. In four of the six vignettes fielded, these two constructs were designed to move in the same direction, either both high or both low. The correlation in these vignettes was 0.81. In the remaining two vignettes fielded, these constructs were designed to move in opposite directions, one high and the other low. The correlation in these vignettes was 0.36. Had the other two vignettes been fielded, in which these two constructs were also designed to move in opposite directions, the overall correlation of these two construct would likely have been lower than 0.77. In sum, these results indicate that the measures exhibit structural validity, although the discriminant validity of the acceptability and appropriateness measures is uncertain.<fig id="Fig2"><label>Fig. 2</label><caption><p>Confirmatory factor analysis of structural validity data</p><p>
<italic>N</italic>&#x000a0;=&#x000a0;326. &#x003c7;<sup>2</sup>(49) = 147.9, <italic>p</italic> = &#x0003c;.001; CFI = 0.96; RMSEA = 0.079, CI [0.06-0.09]. All path loadings are significant at <italic>p</italic> &#x0003c; .05</p></caption><graphic xlink:href="13012_2017_635_Fig2_HTML" id="MO2"/></fig>
</p><p id="Par42">The 2<sup>3</sup> ANOVAs revealed medium- to large-size main effects of each manipulation on the relevant scale score [<xref ref-type="bibr" rid="CR23">23</xref>] (see Table <xref rid="Tab2" ref-type="table">2</xref>). The manipulation of information designed to convey acceptability, appropriateness, and feasibility accounted for 42%, 37%, and 42% of the variance in the corresponding scale scores. The variance accounted for was reasonable given that study participants had to infer the level of acceptability, appropriateness, or feasibility in the vignettes. The medium-size main effects for acceptability in the appropriateness model and appropriateness in the acceptability model signals that respondents had some difficulty discriminating these two constructs. However, these constructs might have been overly conflated as a result of the loss of two vignettes in which these constructs were designed to move in opposite directions. The incomplete factorial design also precluded a full exploration of the two-way and three-way interactions. Nonetheless, the significant main effects provide preliminary evidence of known-groups validity: specifically, the three measures can differentiate groups with known (designed) differences in the levels of these constructs.<table-wrap id="Tab2"><label>Table 2</label><caption><p>ANOVA predicting acceptability, appropriateness, and feasibility of MBC, <italic>N</italic>&#x000a0;=&#x000a0;326</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th colspan="6">Dependent variables</th></tr><tr><th/><th/><th colspan="2">Acceptability</th><th colspan="2">Appropriateness</th><th colspan="2">Feasibility</th></tr><tr><th>Source</th><th>
<italic>df</italic>
</th><th>
<italic>F</italic>
</th><th>
<italic>&#x003b7;</italic>
<sup>2</sup>
</th><th>
<italic>F</italic>
</th><th>
<italic>&#x003b7;</italic>
<sup>2</sup>
</th><th>
<italic>F</italic>
</th><th>
<italic>&#x003b7;</italic>
<sup>2</sup>
</th></tr></thead><tbody><tr><td>AC</td><td>1</td><td>114.1*</td><td>.26</td><td>20.3*</td><td>.06</td><td>4.1</td><td>.01</td></tr><tr><td>AP</td><td>1</td><td>15.4*</td><td>.05</td><td>66.0*</td><td>.17</td><td>4.9</td><td>.02</td></tr><tr><td>FE</td><td>1</td><td>0.58</td><td>.002</td><td>6.0*</td><td>.02</td><td>143.5*</td><td>.31</td></tr><tr><td>AC &#x000d7; AP</td><td>1</td><td>0.59</td><td>.002</td><td>0.18</td><td>.001</td><td>0.66</td><td>.00</td></tr><tr><td>AC &#x000d7; FE</td><td>1</td><td>0.48</td><td>.001</td><td>0.24</td><td>.001</td><td>1.3</td><td>.00</td></tr><tr><td>AP &#x000d7; FE</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>AC &#x000d7; AP &#x000d7; FE</td><td/><td/><td/><td/><td/><td/><td/></tr><tr><td>Residual</td><td>320</td><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td colspan="2">
<italic>R</italic>
<sup>2</sup>&#x000a0;=&#x000a0;.43 (Adj <italic>R</italic>
<sup>2</sup>&#x000a0;=&#x000a0;.42)</td><td colspan="2">
<italic>R</italic>
<sup>2</sup>&#x000a0;=&#x000a0;.37 (Adj <italic>R</italic>
<sup>2</sup>&#x000a0;=&#x000a0;.36)</td><td colspan="2">
<italic>R</italic>
<sup>2</sup>&#x000a0;=&#x000a0;.42 (Adj <italic>R</italic>
<sup>2</sup>&#x000a0;=&#x000a0;.41)</td></tr></tbody></table><table-wrap-foot><p>The dependent variables are the acceptability, appropriateness, and feasibility 4-item scales. A design error precluded full exploration of two-way and three-way interactions</p><p>
<italic>AC</italic> acceptability factor (high versus low, manipulated in vignette), <italic>AP</italic> appropriateness factor (high versus low, manipulated in vignette), <italic>FE</italic> feasibility factor (high versus low, manipulated in vignette)</p><p>*<italic>p</italic> &#x0003c; .05</p></table-wrap-foot></table-wrap>
</p></sec></sec><sec id="Sec16"><title>Study 3</title><sec id="Sec17"><title>Method</title><p id="Par43">In study 3, we ascertained the test-rest reliability (stability) and sensitivity to change (responsiveness) of the new measures in an online experimental study with a subsample of the respondents from study 2. Test-retest reliability refers to the extent to which test scores are consistent over time [<xref ref-type="bibr" rid="CR24">24</xref>]. Sensitivity to change refers to the extent to which test scores detect a change in circumstances [<xref ref-type="bibr" rid="CR24">24</xref>]. Sensitivity to change, or responsiveness, is one of Glasgow and Riley&#x02019;s required criterion for pragmatic measures [<xref ref-type="bibr" rid="CR11">11</xref>].</p><sec id="Sec18"><title>Design, participants, and procedures</title><p id="Par44">Half of participants in study 2 were randomly assigned to receive the same vignette they received earlier (test-retest reliability); the other half were randomly assigned to receive the exact opposite vignette they received earlier (sensitivity to change). For example, a participant who earlier received a vignette in which the factors were high-high-low would receive a vignette in which the factors are low-low-high. We stratified the random assignment across vignettes to ensure balance in the conditions for the test-retest and sensitivity analysis.</p><p id="Par45">Of the 326 counselors who participated in study 2, 296 provided email addresses for a follow-up survey. These individuals received an email invitation to participate in study 3, with non-respondents receiving up to three reminder emails spaced a week apart. One-hundred ninety-two participants (65%) responded to the follow-up survey. Twenty-six percent identified as male, 73% as female. Two participants identified as non-binary gender. Ninety-four percent identified as Caucasian, 2% as African American, 2% as Asian, and 2% as (&#x02018;Other&#x02019;). Five percent identified as Hispanic, Latino, or Spanish origin. Seventy-four percent indicated they worked in a private practice setting. Fifty-five percent reported that they had implemented an EBP or led the implementation of an EBP in the past 5&#x000a0;years. Of the 192 study participants responding to the follow-up survey, 95 generated data for the test-retest reliability assessment and 97 generated data for the sensitivity to change assessment.</p><p id="Par46">We used the same data collection procedures we used in study 2. Study 3 began 3&#x000a0;weeks after study 2 concluded. Median response time between surveys was 7&#x000a0;weeks.</p></sec><sec id="Sec19"><title>Measures</title><p id="Par47">We used the same measures as in study 2. That is, we used the short (4-item) version of the AIM, IAM, and FIM with 5-point ordinal response options (Additional file <xref rid="MOESM3" ref-type="media">3</xref>).</p></sec><sec id="Sec20"><title>Data analysis</title><p id="Par48">We assessed inter-item consistency by computing Cronbach alphas for the four-item scales. We assessed test-retest reliability by calculating Pearson correlation coefficients corrected for attenuation due to measurement error. Disattenuation involved dividing the Pearson correlation coefficient by the square root of the product of the Cronbach alphas for the scales [<xref ref-type="bibr" rid="CR25">25</xref>]. Disattenuated scales demonstrating correlations greater than 0.70 were considered test-retest reliable. We assessed sensitivity to change using linear regression models to predict the difference score (or change in the implementation outcome measure) based on whether the vignette assignment order was low-high, high-low, or high-high; the assignment order low-low served as the reference group.</p></sec></sec><sec id="Sec21"><title>Results</title><p id="Par49">As previously noted, Cronbach alphas for the 4-item scales from the structural validity survey (study 2) were 0.85 for acceptability, 0.91 for appropriateness, and 0.89 for feasibility. The Cronbach alphas for the scales from the test-retest reliability survey were 0.83 for acceptability, 0.87 for appropriateness, and 0.88 for feasibility. Two observations were removed as bivariate outliers for the appropriateness scales, dropping the <italic>N</italic> to 93 for the test-retest reliability assessment. The Pearson correlation coefficients corrected for attenuation due to measurement error were 0.80 for acceptability, 0.73 for appropriateness, and 0.88 for feasibility. All three correlations exceeded 0.70; hence, the three measures demonstrated acceptable test-retest reliability.</p><p id="Par50">Regression analysis indicated that vignette assignment order explained 41, 42, and 46% of the variance in change in the acceptability, appropriateness, and feasibility measures, respectively (see Table <xref rid="Tab3" ref-type="table">3</xref>). The regression coefficients for the assignment order low-high and high-low were statistically significant and signed in the expected direction for each implementation outcome. These results indicate that each measure was sensitive to change in both directions, from low to high and high to low, as study participants responded to the information manipulated in the vignettes.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Regression analysis of sensitivity to change, <italic>N</italic>&#x000a0;=&#x000a0;97</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="3">Acceptability</th><th colspan="3">Appropriateness</th><th colspan="3">Feasibility</th></tr><tr><th/><th>beta</th><th>SE</th><th>
<italic>p</italic> value</th><th>beta</th><th>SE</th><th>
<italic>p</italic> value</th><th>beta</th><th>SE</th><th>
<italic>p</italic> value</th></tr></thead><tbody><tr><td>Intercept</td><td>&#x02212;&#x02009;0.06</td><td>0.10</td><td>0.5732</td><td>0.10</td><td>0.12</td><td>0.4192</td><td>0.09</td><td>0.13</td><td>0.4816</td></tr><tr><td>Vignette 1 low on construct<break/>Vignette 2 high on construct</td><td>0.76</td><td>0.15</td><td>&#x0003c;&#x02009;.0001</td><td>0.68</td><td>0.17</td><td>&#x0003c;&#x02009;.0001</td><td>0.92</td><td>0.18</td><td>&#x0003c;&#x02009;.0001</td></tr><tr><td>Vignette 1 high on construct<break/>Vignette 2 low on construct</td><td>&#x02212;&#x02009;0.90</td><td>0.14</td><td>&#x0003c;&#x02009;.0001</td><td>&#x02212;&#x02009;1.18</td><td>0.17</td><td>&#x0003c;&#x02009;.0001</td><td>&#x02212;&#x02009;1.26</td><td>0.18</td><td>&#x0003c;&#x02009;.0001</td></tr><tr><td>Vignette 1 high on construct<break/>Vignette 2 high on construct</td><td>0.02</td><td>0.15</td><td>0.9082</td><td>&#x02212;&#x02009;0.20</td><td>0.16</td><td>0.2093</td><td>&#x02212;&#x02009;0.10</td><td>0.17</td><td>0.5744</td></tr><tr><td>Adjusted <italic>R</italic>
<sup>2</sup>
</td><td colspan="3">0.41</td><td colspan="3">0.42</td><td colspan="3">0.46</td></tr></tbody></table></table-wrap>
</p></sec></sec><sec id="Sec22"><title>Discussion</title><p id="Par51">This study sought to advance implementation science by systematically developing valid, reliable, and pragmatic measures of three key implementation outcomes: acceptability (Acceptability of Intervention Measure (AIM)), appropriateness (Intervention Appropriateness Measure (IAM)), and feasibility (Feasibility of Intervention Measure (FIM)). Substantive and discriminant content validity assessment involving implementation researchers and implementation-experienced mental health professionals indicated that most of the items that we generated reflected the conceptual content of these three implementation outcomes. Exploratory and confirmatory factor analyses produced brief 5-item scales with acceptable model fit and high reliability. Although the scales were highly correlated, nested confirmatory factor analysis models provided evidence that the three implementation outcomes are best represented from an empirical perspective as distinguishable constructs, just as Proctor and colleagues suggest [<xref ref-type="bibr" rid="CR1">1</xref>].</p><p id="Par52">Scale refinement through construct-specific confirmatory factor analysis of data obtained from a vignette study involving practicing mental health counselors resulted in trimmed 4-item scales. Nested confirmatory factor analysis models provided evidence of structural validity, with the three-factor model demonstrating acceptable model fit and high-scale reliability. Analysis of variance provided evidence of known-groups validity, with medium- to large-size main effects of each manipulation on the relevant scale score. Although the design error precluded a full exploration of discriminant validity, the analysis of variance indicated that the newly developed measures of acceptability, appropriateness, and feasibility can differentiate groups with known differences in the levels of these implementation outcomes.</p><p id="Par53">Finally, test-retest reliability and sensitivity to change were demonstrated when a subsample of mental health counselors participating in the vignette study randomly received either the same vignette or the opposite vignettes and re-rated the implementation outcomes. These measurement properties are important to researchers and other stakeholders (e.g., intermediaries, policymakers, practice leaders) yet are rarely assessed. Importantly, regression analysis indicated that the implementation outcome measures were sensitive to change in both directions: high to low and low to high. This makes the measures useful for assessing the impact of planned strategies or unexpected events on practitioners&#x02019; perceptions of acceptability, appropriateness, and feasibility.</p><sec id="Sec23"><title>Contributions to implementation science and practice</title><p id="Par54">This study contributes to the literature by developing valid and reliable measures of important implementation outcomes [<xref ref-type="bibr" rid="CR1">1</xref>]. The field of implementation science has been deemed a Tower of Babel given the lack of conceptual clarity and consistency in its terminology [<xref ref-type="bibr" rid="CR26">26</xref>], and concerns about the state of measurement in the field have been well documented [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. These concerns extend to the measurement of implementation outcomes, as, despite their centrality to understanding the extent to which implementation is successful, valid and reliable measures are lacking [<xref ref-type="bibr" rid="CR2">2</xref>]. This study fills that gap by developing valid and reliable measures of three implementation outcomes that are salient to a wide range of implementation studies as well as a wide range of pilot, efficacy, and effectiveness studies. Indeed, with an increasing focus on designing for dissemination and implementation [<xref ref-type="bibr" rid="CR29">29</xref>], and some going as far as to say that all effectiveness studies be Hybrid I effectiveness-implementation studies [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR31">31</xref>], measuring constructs such as acceptability, appropriateness, and feasibility will be a ubiquitous need. These outcomes are relevant to assessing stakeholders&#x02019; perceptions of clinical and public health interventions, as well as assessing perceptions of implementation strategies, which are often complex interventions in and of themselves [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR33">33</xref>]. Assessing these outcomes early in the research process may ensure that interventions and implementation strategies are optimized and fit with end-users&#x02019; preferences.</p><p id="Par55">In addition to the need for valid and reliable measures of implementation-related constructs, there is a need for pragmatic measures [<xref ref-type="bibr" rid="CR11">11</xref>]. Implementation stakeholders are unlikely to use measures unless they possess these qualities, which may include broad domains such as being (1) useful in informing decision-making, (2) compatible with the settings in which they are employed, (3) easy to use, and (4) acceptable [<xref ref-type="bibr" rid="CR34">34</xref>]. Pragmatic measures are particularly important for low-resource settings [<xref ref-type="bibr" rid="CR35">35</xref>]. There are several examples of recent efforts to develop pragmatic measures for implementation constructs such as organizational readiness for change [<xref ref-type="bibr" rid="CR36">36</xref>], implementation leadership [<xref ref-type="bibr" rid="CR37">37</xref>], and implementation climate [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. We sought to ensure that our measures were pragmatic, and we believe that we have accomplished that in three ways. First, we sought to develop measures that were brief. We began by developing 12 or fewer items for each construct, and our psychometric testing resulted in final measures with only four items per construct. Second, we made each item as general as possible by not specifying a specific context or clinical problem within the items. For example, the appropriateness items did not specify a purpose (e.g., &#x0201c;for treating depression&#x0201d;), person (e.g., &#x0201c;for my patients&#x0201d;), or situation (e.g., &#x0201c;for my organization&#x0201d;); those wishing to use these measures could add such referents to explore specific aspects of appropriateness (i.e., social or technical fit). Third, we purposefully made the measures &#x0201c;open access&#x0201d; to ensure that the scales are freely available to all who might wish to use them. Our hope is that developing a measure that is free, brief, easy to use and not context- or treatment-specific will increase the chances of its use broadly in implementation research and practice. One of the unfortunate realities in implementation science is that, to date, the majority of current measures are developed for the purpose of a single study (usually with minimal conceptual clarity and psychometric testing) and then never used again. This state of affairs precludes our ability to develop generalizable knowledge in implementation science and, more specifically, knowledge about how current measures perform across a wide range of contexts. Of course, whether these measures ultimately demonstrate predictive validity within the field of mental health and other clinical settings is yet to be determined.</p><p id="Par56">Finally, we have laid out a systematic process for measure development and testing that we believe is both replicable and feasible. In doing so, we stress the importance of clearly defining constructs and engaging in domain delineation processes that ensure that constructs are sufficiently differentiated from similar constructs. We also stress careful psychometric testing, and lay out a process for establishing substantive validity, discriminate validity, structural validity, discriminant validity, known-groups validity, test-retest reliability, and sensitivity to change. The measurement development and testing process took 15&#x000a0;months, suggesting that the process could be completed within the period of a grant-funded implementation study. We encourage other teams to replicate this methodology and suggest further refinements that may enhance the efficiency and effectiveness of this process.</p></sec><sec id="Sec24"><title>Limitations</title><p id="Par57">Though there were a number of strengths in the current study, there were also limitations. First, correlations among the three factors of acceptability, appropriateness, and feasibility were at times fairly high and discriminant validity was not fully tested due to a survey design error. Future research would benefit from further explorations of the discriminant validity of these constructs.</p><p id="Par58">Replication is also needed. Testing the measures with samples of providers that have different backgrounds and characteristics than the samples included here would yield important information about generalizability (e.g., structural invariance). Likewise, testing the measures with different methods or materials would also be useful.</p></sec><sec id="Sec25"><title>Future directions</title><p id="Par59">We plan to administer prospectively our newly developed measures to a large sample of providers faced with the decision to adopt, or not adopt, an EBP. We will then evaluate whether their perceptions of the acceptability, appropriateness, and feasibility of the EBP predict their adoption of the EBP. If the predictive validity of our measures is established, researchers and practitioners would have a brief tool for assessing early on if staff are likely to adopt an EBP or if more work needs to be done to increase the EBP&#x02019;s acceptability, appropriateness, or feasibility.</p><p id="Par60">Glasgow and Riley [<xref ref-type="bibr" rid="CR11">11</xref>] argue that measures need to be pragmatic if they are to be useful (and used) outside the context of research. Pragmatic features of measures include sensitivity to change, brevity, psychometric strength, actionability, and relevance to stakeholders. While their list of pragmatic features is helpful, it was not developed with stakeholder input and therefore might not reflect what stakeholders view as important. To address this limitation, we are working with stakeholders to define and operationalize pragmatic features of measures, with the goal of developing rating criteria that can be used to assess the pragmatic properties of measures, much like the psychometric properties of measures are assessed [<xref ref-type="bibr" rid="CR4">4</xref>]. In future work, we will apply these pragmatic rating criteria to the three new measures of implementation outcomes developed here.</p></sec></sec><sec id="Sec26"><title>Conclusions</title><p id="Par61">In sum, we developed three new measures (the Acceptability of Implementation Measure, Implementation Appropriateness Measure, and Feasibility of Intervention Measure) that are considered to be important implementation outcomes in their own right as well as leading indicators of other implementation outcomes, such as adoption. Our development procedures resulted in 12 items (four for each construct) that are both valid and reliable measures of these implementation outcomes. Predictive validity will be assessed in a forthcoming prospective follow-up study. We will also subject these measures to a formal evaluation of pragmatic properties, testing features beyond their brief nature and sensitivity to change. These measures have great potential for widespread use across implementation studies regardless of intervention focus, target disease/problem, and setting because of their general wording, boosting their ability to generate cumulative knowledge. Moreover, the measure development process employed in this set of studies presents a replicable and relatively efficient method.</p></sec><sec sec-type="supplementary-material"><title>Additional files</title><sec id="Sec27"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="13012_2017_635_MOESM1_ESM.docx"><label>Additional file 1:</label><caption><p>Exploratory factor analysis (EFA) using promax rotation in <italic>Mplus</italic> 7. This file includes the EFA data from the web-based survey results in which 31 items were assigned to three constructs (acceptability, appropriateness, and feasibility). (DOCX 28 kb)</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="13012_2017_635_MOESM2_ESM.docx"><label>Additional file 2:</label><caption><p>Scale reliabilities and factor loadings from single-factor confirmatory factor analysis of structural validity vignette data, <italic>N</italic>&#x000a0;=&#x000a0;326. This file includes factor loadings in the one-factor CFAs. (DOCX 16 kb)</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="13012_2017_635_MOESM3_ESM.docx"><label>Additional file 3:</label><caption><p>Final version of the Acceptability of Intervention Measure (AIM), Intervention Appropriateness Measure (IAM), and Feasibility of Intervention Measure (FIM). (DOCX 16 kb)</p></caption></media></supplementary-material>
</p></sec></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>AIM</term><def><p id="Par5">Acceptability of Intervention Measure</p></def></def-item><def-item><term>AMHCA</term><def><p id="Par6">American Mental Health Counselors Association</p></def></def-item><def-item><term>ANOVA</term><def><p id="Par7">Analysis of variance</p></def></def-item><def-item><term>CFA</term><def><p id="Par8">Confirmatory factor analysis</p></def></def-item><def-item><term>CFI</term><def><p id="Par9">Comparative fit index</p></def></def-item><def-item><term>EBP</term><def><p id="Par10">Evidence-based practice</p></def></def-item><def-item><term>EFA</term><def><p id="Par11">Exploratory factor analysis</p></def></def-item><def-item><term>FIM</term><def><p id="Par12">Feasibility of Intervention Measure</p></def></def-item><def-item><term>IAM</term><def><p id="Par13">Intervention Appropriateness Measure</p></def></def-item><def-item><term>ICC</term><def><p id="Par14">Intraclass correlation coefficients</p></def></def-item><def-item><term>MBC</term><def><p id="Par15">Measurement-based care</p></def></def-item><def-item><term>RMSEA</term><def><p id="Par16">Root mean square error of approximation</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Electronic supplementary material</bold></p><p>The online version of this article (doi:10.1186/s13012-017-0635-3) contains supplementary material, which is available to authorized users.</p></fn></fn-group><ack><title>Acknowledgements</title><p>Not applicable.</p><sec id="FPar1"><title>Funding</title><p id="Par62">This study was funded by the US National Institute of Mental Health (NIMH, 1R01MH106510). BP received additional support from grants and contracts from the National Institutes of Health (L30MH108060, R25MH080916, P30AI050410, and UL1TR001111). The views expressed are those of the authors and not necessarily those of the National Institutes of Health.</p></sec><sec id="FPar2"><title>Availability of data and materials</title><p id="Par63">The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></sec></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>BW, CL, CS conceived the study. BW, CL, CS, MB, and BP contributed to the research design and protocol development. MB and AC made substantial contributions to the analysis of the data. BW, CL, CS, BP, CD, AC, and HH drafted the manuscript and revised it critically for important intellectual content. All authors read and approved the final manuscript.</p></notes><notes notes-type="COI-statement"><sec id="FPar3"><title>Ethics approval and consent to participate</title><p id="Par64">The Institutional Review Board of the University of North Carolina at Chapel Hill reviewed this study and deemed it not human subjects research (IRB #16-0237).</p></sec><sec id="FPar4"><title>Consent for publication</title><p id="Par65">Not applicable.</p></sec><sec id="FPar5"><title>Competing interests</title><p id="Par66">The authors declare that they have no competing interests.</p></sec><sec id="FPar6"><title>Publisher&#x02019;s Note</title><p id="Par67">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proctor</surname><given-names>E</given-names></name><name><surname>Silmere</surname><given-names>H</given-names></name><name><surname>Raghavan</surname><given-names>R</given-names></name><name><surname>Hovmand</surname><given-names>P</given-names></name><name><surname>Aarons</surname><given-names>G</given-names></name><name><surname>Bunger</surname><given-names>A</given-names></name><etal/></person-group><article-title>Outcomes for implementation research: conceptual distinctions, measurement challenges, and research agenda</article-title><source>Admin Pol Ment Health</source><year>2011</year><volume>38</volume><issue>2</issue><fpage>65</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1007/s10488-010-0319-7</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>CC</given-names></name><name><surname>Fischer</surname><given-names>S</given-names></name><name><surname>Weiner</surname><given-names>BJ</given-names></name><name><surname>Stanick</surname><given-names>C</given-names></name><name><surname>Kim</surname><given-names>M</given-names></name><name><surname>Martinez</surname><given-names>RG</given-names></name></person-group><article-title>Outcomes for implementation science: an enhanced systematic review of instruments using evidence-based rating criteria</article-title><source>Implement Sci</source><year>2015</year><volume>10</volume><fpage>155</fpage><pub-id pub-id-type="doi">10.1186/s13012-015-0342-x</pub-id><pub-id pub-id-type="pmid">26537706</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez</surname><given-names>RG</given-names></name><name><surname>Lewis</surname><given-names>CC</given-names></name><name><surname>Weiner</surname><given-names>BJ</given-names></name></person-group><article-title>Instrumentation issues in implementation science</article-title><source>Implement Sci</source><year>2014</year><volume>9</volume><fpage>118</fpage><pub-id pub-id-type="doi">10.1186/s13012-014-0118-8</pub-id><pub-id pub-id-type="pmid">25185799</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>CC</given-names></name><name><surname>Weiner</surname><given-names>BJ</given-names></name><name><surname>Stanick</surname><given-names>C</given-names></name><name><surname>Fischer</surname><given-names>SM</given-names></name></person-group><article-title>Advancing implementation science through measure development and evaluation: a study protocol</article-title><source>Implement Sci</source><year>2015</year><volume>10</volume><fpage>102</fpage><pub-id pub-id-type="doi">10.1186/s13012-015-0287-0</pub-id><pub-id pub-id-type="pmid">26197880</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowen</surname><given-names>DJ</given-names></name><name><surname>Kreuter</surname><given-names>M</given-names></name><name><surname>Spring</surname><given-names>B</given-names></name><name><surname>Cofta-Woerpel</surname><given-names>L</given-names></name><name><surname>Linnan</surname><given-names>L</given-names></name><name><surname>Weiner</surname><given-names>D</given-names></name><etal/></person-group><article-title>How we design feasibility studies</article-title><source>Am J Prev Med</source><year>2009</year><volume>36</volume><issue>5</issue><fpage>452</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1016/j.amepre.2009.02.002</pub-id><pub-id pub-id-type="pmid">19362699</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damschroder</surname><given-names>LJ</given-names></name><name><surname>Aron</surname><given-names>DC</given-names></name><name><surname>Keith</surname><given-names>RE</given-names></name><name><surname>Kirsh</surname><given-names>SR</given-names></name><name><surname>Alexander</surname><given-names>JA</given-names></name><name><surname>Lowery</surname><given-names>JC</given-names></name></person-group><article-title>Fostering implementation of health services research findings into practice: a consolidated framework for advancing implementation science</article-title><source>Implement Sci</source><year>2009</year><volume>4</volume><fpage>50</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-4-50</pub-id><pub-id pub-id-type="pmid">19664226</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>JC</given-names></name><name><surname>Gerbing</surname><given-names>DW</given-names></name></person-group><article-title>Predicting the performance of measures in a confirmatory factor analysis with a pretest assessment of their substantive validities</article-title><source>J Appl Psychol</source><year>1991</year><volume>76</volume><issue>5</issue><fpage>732</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.1037/0021-9010.76.5.732</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huijg</surname><given-names>JM</given-names></name><name><surname>Gebhardt</surname><given-names>WA</given-names></name><name><surname>Crone</surname><given-names>MR</given-names></name><name><surname>Dusseldorp</surname><given-names>E</given-names></name><name><surname>Presseau</surname><given-names>J</given-names></name></person-group><article-title>Discriminant content validity of a theoretical domains framework questionnaire for use in implementation research</article-title><source>Implement Sci</source><year>2014</year><volume>9</volume><fpage>11</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-9-11</pub-id><pub-id pub-id-type="pmid">24423394</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinkin</surname><given-names>TR</given-names></name></person-group><article-title>A brief tutorial on the development of measures for use in survey questionnaires</article-title><source>Organ Res Methods</source><year>1998</year><volume>1</volume><issue>1</issue><fpage>104</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1177/109442819800100106</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Society for Implementation Research Collaboration. <ext-link ext-link-type="uri" xlink:href="https://societyforimplementationresearchcollaboration.org">https://societyforimplementationresearchcollaboration.org</ext-link> (2017). Accessed 11 Jan 2017.</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasgow</surname><given-names>RE</given-names></name><name><surname>Riley</surname><given-names>WT</given-names></name></person-group><article-title>Pragmatic measures: what they are and why we need them</article-title><source>Am J Prev Med</source><year>2013</year><volume>45</volume><issue>2</issue><fpage>237</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.amepre.2013.03.010</pub-id><pub-id pub-id-type="pmid">23867032</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Lewis CC, Stanick CF, Martinez RG, Weiner BJ, Kim M, Barwick M, et al. The Society for Implementation Research Collaboration Instrument Review Project: a methodology to promote rigorous evaluation. Implement Sci. 2015;10(2). doi:10.1186/s13012-014-0193-x.</mixed-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>A sharper Bonferroni procedure for multiple tests of significance</article-title><source>Biometrika</source><year>1988</year><volume>75</volume><issue>4</issue><fpage>800</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1093/biomet/75.4.800</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dixon</surname><given-names>D</given-names></name><name><surname>Pollard</surname><given-names>B</given-names></name><name><surname>Johnston</surname><given-names>M</given-names></name></person-group><article-title>What does the chronic pain grade questionnaire measure?</article-title><source>Pain</source><year>2007</year><volume>130</volume><issue>3</issue><fpage>249</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1016/j.pain.2006.12.004</pub-id><pub-id pub-id-type="pmid">17257751</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Muth&#x000e9;n</surname><given-names>LK</given-names></name><name><surname>Muth&#x000e9;n</surname><given-names>BO</given-names></name></person-group><source>Mplus user&#x02019;s guide. seventh edition ed</source><year>2012</year><publisher-loc>Muth&#x000e9;n &#x00026; Muth&#x000e9;n</publisher-loc><publisher-name>Los Angeles</publisher-name></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiber</surname><given-names>JB</given-names></name><name><surname>Nora</surname><given-names>A</given-names></name><name><surname>Stage</surname><given-names>FK</given-names></name><name><surname>Barlow</surname><given-names>EA</given-names></name><name><surname>King</surname><given-names>J</given-names></name></person-group><article-title>Reporting structural equation modeling and confirmatory factor analysis results: a review</article-title><source>J Educ Res</source><year>2006</year><volume>99</volume><issue>6</issue><fpage>323</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.3200/JOER.99.6.323-338</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hox</surname><given-names>JJ</given-names></name><name><surname>Bechger</surname><given-names>TM</given-names></name></person-group><article-title>An introduction to structural equation modelling</article-title><source>Fam Sci Rev</source><year>1998</year><volume>11</volume><fpage>354</fpage><lpage>373</lpage></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>RC</surname><given-names>MC</given-names></name><name><surname>Browne</surname><given-names>MW</given-names></name><name><surname>Sugawara</surname><given-names>HM</given-names></name></person-group><article-title>Power analysis and determination of sample size for covariance structure modeling</article-title><source>Psychol Methods</source><year>1996</year><volume>1</volume><issue>2</issue><fpage>130</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1037/1082-989X.1.2.130</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Messick</surname><given-names>S</given-names></name></person-group><article-title>Validity of psychological assessment: validation of inferences from persons&#x02019; responses and performances as scientific inquiry into score meaning</article-title><source>Am Psychol</source><year>1995</year><volume>50</volume><issue>9</issue><fpage>741</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1037/0003-066X.50.9.741</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>M</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Michalos</surname><given-names>AC</given-names></name></person-group><article-title>Known-groups validity</article-title><source>Encyclopedia of quality of life and well-being research</source><year>2014</year><publisher-loc>Dodrecht</publisher-loc><publisher-name>Springer Netherlands</publisher-name><fpage>3481</fpage><lpage>3482</lpage></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>K</given-names></name><name><surname>Lewis</surname><given-names>CC</given-names></name></person-group><article-title>Using measurement-based care to enhance any treatment</article-title><source>Cogn Behav Pract</source><year>2015</year><volume>22</volume><issue>1</issue><fpage>49</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.cbpra.2014.01.010</pub-id><pub-id pub-id-type="pmid">27330267</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>L</given-names></name><name><surname>Bentler</surname><given-names>PM</given-names></name></person-group><article-title>Cutoff criteria for fit indexes in covariance structure analysis: conventional criteria versus new alternatives</article-title><source>Struct Equ Modeling</source><year>1999</year><volume>6</volume><issue>1</issue><fpage>1</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1080/10705519909540118</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><source>Statistical power analysis for the behavioral sciences. Second edition ed</source><year>1988</year><publisher-loc>Lawrence Earlbaum Associates</publisher-loc><publisher-name>Hillsdale</publisher-name></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>WMK</surname><given-names>T</given-names></name><name><surname>JP D.</surname></name></person-group><source>The research methods knowledge base</source><year>2006</year><edition>3</edition><publisher-loc>Cincinnati</publisher-loc><publisher-name>Atomic Dog Publishing</publisher-name></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spearman</surname><given-names>C</given-names></name></person-group><article-title>The proof and measurement of association between two things</article-title><source>Am J Psychol</source><year>1904</year><volume>15</volume><issue>1</issue><fpage>72</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.2307/1412159</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKibbon</surname><given-names>KA</given-names></name><name><surname>Lokker</surname><given-names>C</given-names></name><name><surname>Wilczynski</surname><given-names>NL</given-names></name><name><surname>Ciliska</surname><given-names>D</given-names></name><name><surname>Dobbins</surname><given-names>M</given-names></name><name><surname>Davis</surname><given-names>DA</given-names></name><etal/></person-group><article-title>A cross-sectional study of the number and frequency of terms used to refer to knowledge translation in a body of health literature in 2006: a Tower of Babel?</article-title><source>Implement Sci</source><year>2010</year><volume>5</volume><fpage>16</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-5-16</pub-id><pub-id pub-id-type="pmid">21080976</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasgow</surname><given-names>RE</given-names></name></person-group><article-title>Critical measurement issues in translational research</article-title><source>Res Soc Work Pract</source><year>2009</year><volume>19</volume><issue>5</issue><fpage>560</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1177/1049731509335497</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Proctor</surname><given-names>EK</given-names></name><name><surname>Powell</surname><given-names>BJ</given-names></name><name><surname>Feely</surname><given-names>M</given-names></name></person-group><source>Measurement in dissemination and implementation science. In: Kendall RSBPC, editor. Dissemination and implementation of evidence-based practices in child and adolescent mental health</source><year>2014</year><publisher-loc>New York</publisher-loc><publisher-name>Oxford University Press</publisher-name><fpage>22</fpage><lpage>43</lpage></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brownson</surname><given-names>RC</given-names></name><name><surname>Jacobs</surname><given-names>JA</given-names></name><name><surname>Tabak</surname><given-names>RG</given-names></name><name><surname>Hoehner</surname><given-names>CM</given-names></name><name><surname>Stamatakis</surname><given-names>KA</given-names></name></person-group><article-title>Designing for dissemination among public health researchers: findings from a national survey in the United States</article-title><source>Am J Public Health</source><year>2013</year><volume>103</volume><issue>9</issue><fpage>1693</fpage><lpage>1699</lpage><pub-id pub-id-type="doi">10.2105/AJPH.2012.301165</pub-id><pub-id pub-id-type="pmid">23865659</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname><given-names>MS</given-names></name><name><surname>Damschroder</surname><given-names>L</given-names></name><name><surname>Hagedorn</surname><given-names>H</given-names></name><name><surname>Smith</surname><given-names>J</given-names></name><name><surname>Kilbourne</surname><given-names>AM</given-names></name></person-group><article-title>An introduction to implementation science for the non-specialist</article-title><source>BMC Psychol</source><year>2015</year><volume>3</volume><fpage>32</fpage><pub-id pub-id-type="doi">10.1186/s40359-015-0089-9</pub-id><pub-id pub-id-type="pmid">26376626</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curran</surname><given-names>GM</given-names></name><name><surname>Bauer</surname><given-names>M</given-names></name><name><surname>Mittman</surname><given-names>B</given-names></name><name><surname>Pyne</surname><given-names>JM</given-names></name><name><surname>Stetler</surname><given-names>C</given-names></name></person-group><article-title>Effectiveness-implementation hybrid designs: combining elements of clinical effectiveness and implementation research to enhance public health impact</article-title><source>Med Care</source><year>2012</year><volume>50</volume><issue>3</issue><fpage>217</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1097/MLR.0b013e3182408812</pub-id><pub-id pub-id-type="pmid">22310560</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Powell</surname><given-names>BJ</given-names></name><name><surname>Proctor</surname><given-names>EK</given-names></name><name><surname>Glisson</surname><given-names>CA</given-names></name><name><surname>Kohl</surname><given-names>PL</given-names></name><name><surname>Raghavan</surname><given-names>R</given-names></name><name><surname>Brownson</surname><given-names>RC</given-names></name><etal/></person-group><article-title>A mixed methods multiple case study of implementation as usual in children&#x02019;s social service organizations: study protocol</article-title><source>Implement Sci</source><year>2013</year><volume>8</volume><fpage>92</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-8-92</pub-id><pub-id pub-id-type="pmid">23961701</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Powell</surname><given-names>BJ</given-names></name><name><surname>Waltz</surname><given-names>TJ</given-names></name><name><surname>Chinman</surname><given-names>MJ</given-names></name><name><surname>Damschroder</surname><given-names>LJ</given-names></name><name><surname>Smith</surname><given-names>JL</given-names></name><name><surname>Matthieu</surname><given-names>MM</given-names></name><etal/></person-group><article-title>A refined compilation of implementation strategies: results from the Expert Recommendations for Implementing Change (ERIC) project</article-title><source>Implement Sci</source><year>2015</year><volume>10</volume><fpage>21</fpage><pub-id pub-id-type="doi">10.1186/s13012-015-0209-1</pub-id><pub-id pub-id-type="pmid">25889199</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Powell</surname><given-names>BJ</given-names></name><name><surname>Weiner</surname><given-names>BJ</given-names></name><name><surname>Stanick</surname><given-names>CF</given-names></name><name><surname>Halko</surname><given-names>H</given-names></name><name><surname>Dorsey</surname><given-names>C</given-names></name><name><surname>Lewis</surname><given-names>CC</given-names></name></person-group><source>Stakeholders&#x02019; perceptions of criteria for pragmatic measurement in implementation: a concept mapping approach (oral presentation). 9th Annual Conference on the Science of Dissemination &#x00026; Implementation</source><year>2016</year><publisher-loc>Washington, D. C</publisher-loc><publisher-name>Academy Health and the National Institutes of Health</publisher-name></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beidas</surname><given-names>RS</given-names></name><name><surname>Stewart</surname><given-names>RE</given-names></name><name><surname>Walsh</surname><given-names>L</given-names></name><name><surname>Lucas</surname><given-names>S</given-names></name><name><surname>Downey</surname><given-names>MM</given-names></name><name><surname>Jackson</surname><given-names>K</given-names></name><etal/></person-group><article-title>Free, brief, and validated: standardized instruments for low-resource mental health settings</article-title><source>Cogn Behav Pract</source><year>2015</year><volume>22</volume><issue>1</issue><fpage>5</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.cbpra.2014.02.002</pub-id><pub-id pub-id-type="pmid">25642130</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shea</surname><given-names>CM</given-names></name><name><surname>Jacobs</surname><given-names>SR</given-names></name><name><surname>Esserman</surname><given-names>DA</given-names></name><name><surname>Bruce</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>BJ</given-names></name></person-group><article-title>Organizational readiness for implementing change: a psychometric assessment of a new measure</article-title><source>Implement Sci</source><year>2014</year><volume>9</volume><fpage>7</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-9-7</pub-id><pub-id pub-id-type="pmid">24410955</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aarons</surname><given-names>GA</given-names></name><name><surname>Ehrhart</surname><given-names>MG</given-names></name><name><surname>Farahnak</surname><given-names>LR</given-names></name></person-group><article-title>The Implementation Leadership Scale (ILS): development of a brief measure of unit level implementation leadership</article-title><source>Implement Sci</source><year>2014</year><volume>9</volume><issue>1</issue><fpage>45</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-9-45</pub-id><pub-id pub-id-type="pmid">24731295</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehrhart</surname><given-names>MG</given-names></name><name><surname>Aarons</surname><given-names>GA</given-names></name><name><surname>Farahnak</surname><given-names>LR</given-names></name></person-group><article-title>Assessing the organizational context for EBP implementation: the development and validity testing of the Implementation Climate Scale (ICS)</article-title><source>Implement Sci</source><year>2014</year><volume>9</volume><fpage>157</fpage><pub-id pub-id-type="doi">10.1186/s13012-014-0157-1</pub-id><pub-id pub-id-type="pmid">25338781</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>SR</given-names></name><name><surname>Weiner</surname><given-names>BJ</given-names></name><name><surname>Bunger</surname><given-names>AC</given-names></name></person-group><article-title>Context matters: measuring implementation climate among individuals and groups</article-title><source>Implement Sci</source><year>2014</year><volume>9</volume><fpage>46</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-9-46</pub-id><pub-id pub-id-type="pmid">24742308</pub-id></element-citation></ref></ref-list></back></article>
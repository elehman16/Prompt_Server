<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Comput Math Methods Med</journal-id><journal-id journal-id-type="iso-abbrev">Comput Math Methods Med</journal-id><journal-id journal-id-type="publisher-id">CMMM</journal-id><journal-title-group><journal-title>Computational and Mathematical Methods in Medicine</journal-title></journal-title-group><issn pub-type="ppub">1748-670X</issn><issn pub-type="epub">1748-6718</issn><publisher><publisher-name>Hindawi Publishing Corporation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">24382980</article-id><article-id pub-id-type="pmc">3872151</article-id><article-id pub-id-type="doi">10.1155/2013/380245</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>The Study of Randomized Visual Saliency Detection Algorithm</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2277-1765</contrib-id><name><surname>Chen</surname><given-names>Yuantao</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Weihong</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref><xref ref-type="corresp" rid="cor1">*</xref></contrib><contrib contrib-type="author"><name><surname>Kuang</surname><given-names>Fangjun</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Gao</surname><given-names>Shangbing</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="I1"><sup>1</sup>School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China</aff><aff id="I2"><sup>2</sup>School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha 410114, China</aff><aff id="I3"><sup>3</sup>Department of Computer Science and Technology, Hunan Vocational Institute of Safety &#x00026; Technology, Changsha 410151, China</aff><author-notes><corresp id="cor1">*Weihong Xu: <email>xwhxd@163.com</email></corresp><fn fn-type="other"><p>Academic Editor: Jianlong Qiu</p></fn></author-notes><pub-date pub-type="ppub"><year>2013</year></pub-date><pub-date pub-type="epub"><day>9</day><month>12</month><year>2013</year></pub-date><volume>2013</volume><elocation-id>380245</elocation-id><history><date date-type="received"><day>15</day><month>9</month><year>2013</year></date><date date-type="accepted"><day>20</day><month>11</month><year>2013</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2013 Yuantao Chen et al.</copyright-statement><copyright-year>2013</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/3.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>Image segmentation process for high quality visual saliency map is very dependent on the existing visual saliency metrics. It is mostly only get sketchy effect of saliency map, and roughly based visual saliency map will affect the image segmentation results. The paper had presented the randomized visual saliency detection algorithm. The randomized visual saliency detection method can quickly generate the same size as the original input image and detailed results of the saliency map. The randomized saliency detection method can be applied to real-time requirements for image content-based scaling saliency results map. The randomization method for fast randomized video saliency area detection, the algorithm only requires a small amount of memory space can be detected detailed oriented visual saliency map, the presented results are shown that the method of visual saliency map used in image after the segmentation process can be an ideal segmentation results.</p></abstract></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>The researchers in recent years have made a lot of content-based image and video image scaling method [<xref rid="B1" ref-type="bibr">1</xref>&#x02013;<xref rid="B7" ref-type="bibr">7</xref>]. These images and video scaling methods are intended by changing the ratio of the image or video and the resolution so that the image or video to the terminal equipment suitable for the target display, and try to save your images and video in the critical content. In these images based on image content and video scaling process, how to quickly detect visual saliency areas is needed to solve the problem.</p><p>Now image pixels based point in visual saliency region detection method [<xref rid="B8" ref-type="bibr">8</xref>&#x02013;<xref rid="B11" ref-type="bibr">11</xref>] are mostly single pixel of each calculation system significantly, because the large number of pixels will result in an overall computation huge. Some methods are even also building high-dimensional vector to perform the search tree structure [<xref rid="B8" ref-type="bibr">8</xref>], and the method's time complexity and space complexity are much higher than other method. Therefore, many existing regional visual saliency detection method [<xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B9" ref-type="bibr">9</xref>] can only detect relatively rough area significant results. The literature [<xref rid="B10" ref-type="bibr">10</xref>, <xref rid="B11" ref-type="bibr">11</xref>] of the proposed method is made from the image analysis spectral angle to calculate the input original image on the saliency region. The literature [<xref rid="B12" ref-type="bibr">12</xref>] is based on machine learning methods to obtain the input of the original image visual saliency area. These methods can accurately detect the original image smaller target, mainly used in target identification and target tracking. They are proposed for the randomization of visual saliency detection methods with the literature's [<xref rid="B8" ref-type="bibr">8</xref>] method is mainly used in image processing of the visual detection and image saliency map area.</p><p>According to the literature [<xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B13" ref-type="bibr">13</xref>], they are related to outcomes inspiration, and the paper have proposed a novel randomized visual saliency detection algorithm, which is based on high-quality content visual saliency regional approach. In the method, the first application of randomized looking algorithm for rapid detection, the input image belongs pyramid each corresponding roughly saliency region. Secondly, rough visual saliency region meticulous process, and remove as randomized seeking algorithm to generate noise signal, once again the image pyramid of different levels of consolidation for saliency region careful treatment. Finally, for each pixel, adaptive update saliency value, resulting in detailed oriented saliency final result.</p><p>The paper has described the randomized visual saliency detection algorithm and it is a randomized algorithm. It requires no additional data structure to construct auxiliary systems saliency region detection related work, and only need to store the original input image and the system output saliency results figure required memory to be able to perform. The randomized algorithms have quickly produced from the original input image size and exactly the same meticulous system visual saliency area graph. The randomized algorithms can be easily performed on the graphics processing unit to achieve even parallel computing. These advantages make the proposed efficient randomized visual saliency region detection method become used in video sequences in real time. The system has generated from the corresponding visual saliency map, thereby improving image content-based video scaling to generate the overall quality of the results.</p></sec><sec id="sec2"><title>2. Image Visual Saliency Detection</title><p>Image processing saliency region detection has been the fields of computer vision problems remain unresolved. At present, significant dependence on specific areas related applications will generate many significant regional differences customized versions with a variety of regions of interest ROI detection algorithm coexist. Existing significant region detection algorithm is mainly focused on looking for the human visual attention first fixed pixel or object. The visual saliency for understanding human has visual attention pixel and related application, such as auto-focus applications. While the rest of the visual saliency detection algorithms are more emphasis than the specific detection of the single object in the image.</p><p>According to the literature [<xref rid="B8" ref-type="bibr">8</xref>], the earliest of the early original behavior of the human visual system and neural network architecture combining visual attention mechanism put forward a visual saliency system. The literature [<xref rid="B8" ref-type="bibr">8</xref>] had proposed the algorithm for multiscale image features that are combined to obtain unitary visual saliency map. Image multiscale features include six brightness feature maps, twenty four orientation maps, twelve colorful characteristics of the map. To be able to quickly detect multiscale features of the original image, the literature's [<xref rid="B8" ref-type="bibr">8</xref>] method only cursory level calculated to be roughly characterized map. In fact, the method in literature [<xref rid="B8" ref-type="bibr">8</xref>] had generated a rough visual salience effect corresponding region map algorithm and different methods, the proposed method is to generate the degree of detail of the input image size exactly matches the original visual saliency map.</p><p>The literature [<xref rid="B9" ref-type="bibr">9</xref>] in the context of the proposed content-based visual salience, this algorithm aims to detect the input image can represent a specific area of the scene. Other people had think pixel visual saliency is determined by the pixel area of the center of the image related to block said, because the region reflects the pixel block location of its context. If the center pixel of the relevant image region block <italic>i</italic> is shown as <italic>p</italic>
<sub><italic>i</italic></sub> and the other image region in the original image block differences is large. It can be seen image pixel <italic>i</italic> as highly visual saliency pixels.</p><p>Definition of <italic>d</italic>
<sub>color</sub>(<italic>p</italic>
<sub><italic>i</italic></sub>, <italic>p</italic>
<sub><italic>j</italic></sub>) is the result of the image area to the quantized block <italic>p</italic>
<sub><italic>i</italic></sub> and <italic>p</italic>
<sub><italic>j</italic></sub> on the CIE Lab color space separated by the Euclidean distance, and normalized to correspond to the [0,1]. When <italic>d</italic>
<sub>color</sub>(<italic>p</italic>
<sub><italic>i</italic></sub>, <italic>p</italic>
<sub><italic>j</italic></sub>) is relatively large in any image area block <italic>p</italic>
<sub><italic>j</italic></sub>, then the image pixel <italic>i</italic> considered statistically visual saliency.</p><p>
<italic>d</italic>
<sub>position</sub>(<italic>p</italic>
<sub><italic>i</italic></sub>, <italic>p</italic>
<sub><italic>j</italic></sub>) is the image area defined the Euclidean distance of block <italic>p</italic>
<sub><italic>i</italic></sub> and block <italic>p</italic>
<sub><italic>j</italic></sub> between the locations, this corresponds to the normalized distance [0,1]. Based on these two definitions in the literature [<xref rid="B8" ref-type="bibr">8</xref>], the measure of two corresponding blocks of an image area no similarity measure between the methods are shown in formula (<xref ref-type="disp-formula" rid="EEq1">1</xref>). Consider
<disp-formula id="EEq1"><label>(1)</label><mml:math id="M1"><mml:mtable><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>color</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext>position</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>The literature [<xref rid="B9" ref-type="bibr">9</xref>] had proposed the method considers only the <italic>K</italic> blocks of high similarity image area (if the area of the image block high similarity with the image block <italic>p</italic>
<sub><italic>i</italic></sub> are distinct from, then the entire image area in the original image are obviously different block in the image area block <italic>p</italic>
<sub><italic>i</italic></sub>). Therefore, for each image block <italic>p</italic>
<sub><italic>i</italic></sub>, the original input image in accordance with formula (<xref ref-type="disp-formula" rid="EEq1">1</xref>) to find the highest similarity of <italic>K</italic> blocks of the image area, and according to formula (<xref ref-type="disp-formula" rid="EEq2">2</xref>) to calculate the pixel position <italic>i</italic> visual saliency. Consider
<disp-formula id="EEq2"><label>(2)</label><mml:math id="M2"><mml:mtable><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>exp</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>The literature [<xref rid="B9" ref-type="bibr">9</xref>] had implemented methods can only detect single width or height is 256 pixels rough saliency map image area. Because each pixel need to be independent saliency calculations, so a very large amount of computation, and in order to Find the fastest most similar block of the image area, you need to construct a sparse grid lookup based on high-dimensional vector tree. Therefore, if the detailed oriented images individually calculated for each pixel of the saliency value, the whole process will be very slow. This idea is based on the Goferman's improvement of the proposed method and others, but not all the pixels in the entire image to find the area of the image most similar block to calculate the pixel saliency value, instead of using a random method all pixels from the image dot image regions extracted 2<italic>K</italic> blocks, and to ensure image regions where <italic>p</italic>
<sub><italic>i</italic></sub> of the highest similarity blocks and blocks of the image area, the image area of the other blocks of the <italic>K</italic> discarded. This improved method of extraction as long as 2<italic>K</italic>randomized blocks image regions, but do not create an auxiliary high-dimensional vector lookup tree structure to improve search efficiency. Compared with the conventional variety, this improved method of the time is short, less space.</p><p>In the literature [<xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B9" ref-type="bibr">9</xref>], the method can effectively detect the input of the original image with the visual saliency image related areas. Because depending on the image on the original image all the pixels directly calculate the corresponding visual saliency map, computation is very large and take up more memory area. So the literature [<xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B9" ref-type="bibr">9</xref>] just got a rough diagram of saliency. The proposed method can be constructed directly on the original image detail oriented visual saliency map, the detailed oriented visual saliency map will be in many image processing or video scaling processing and other applications on a variety of application areas.</p><p>
<xref ref-type="fig" rid="fig1">Figure 1</xref> can show the detailed use of visual saliency map application Scale-and-Stretch method to perform image scaling relevant scaling results obtained. The experimental results show detailed oriented visual saliency map scaled to produce better results. <xref ref-type="fig" rid="fig1"> Figure 1(a)</xref> of the input image size is 476 &#x000d7; 704. According to the literature [<xref rid="B8" ref-type="bibr">8</xref>]'s method, <xref ref-type="fig" rid="fig1">Figure 1(b)</xref> is shown as visual saliency map. <xref ref-type="fig" rid="fig1"> Figure 1(c)</xref> under the literature [<xref rid="B9" ref-type="bibr">9</xref>]'s method of visual saliency map. <xref ref-type="fig" rid="fig1"> Figure 1(d)</xref> is using a random method for detecting the visual saliency, and <xref ref-type="fig" rid="fig1">Figure 1(e)</xref> of the figure has produced by the method according to the visual saliency recovery results of the literature [<xref rid="B9" ref-type="bibr">9</xref>]. <xref ref-type="fig" rid="fig1"> Figure 1(f)</xref> of the picture is shown as the general result of the input image. <xref ref-type="fig" rid="fig1"> Figure 1(f)</xref> the following figure is shown as the method according to the random visual saliency map recovery results.</p><p>
<xref ref-type="fig" rid="fig2">Figure 2</xref> is shown as the comparison of the paper's method and Itti method, Goferman method, the method can get a high degree of detail of the visual saliency map. <xref ref-type="fig" rid="fig2"> Figure 2(a)</xref> is the size of 742 &#x000d7; 495 for the input image. According to Itti method, <xref ref-type="fig" rid="fig2">Figure 2(b)</xref> had produced the visual saliency. According to Goferman method, <xref ref-type="fig" rid="fig2">Figure 2(c)</xref> had produced the visual saliency. <xref ref-type="fig" rid="fig2"> Figure 2(d)</xref> is shown as the randomization method according to this layer for detecting a rough visual saliency. <xref ref-type="fig" rid="fig2"> Figure 2(e)</xref> had produced according to the Gaussian image pyramid of multilayer detailed visual saliency map.</p></sec><sec id="sec3"><title>3. Randomized Visual Saliency Detection Method</title><p>The detection method have proposed in the paper is divided into four stages. (1) The first stage is carried out according to the original input image processing, application information at various levels of Gaussian image pyramid to get rough on all levels of the visual saliency map. (2) The second stage is to use the a coarse level of visual saliency detailed diagram of step, the purpose is to remove coarse of the visual saliency map as various image generated randomization noise signal. (3) The third stage through the multilayer careful visual saliency map of the combined Resultant combined with multiscale feature visual saliency map. (4) The fourth stage selectively noise signal for those regions of high integration significantly after the merger to obtain the final results of the visual saliency map. The results for the saliency map modest degree of detail, but if required to achieve a high detection speed, you can use this randomized visual saliency detection algorithm in the first two stages to generate roughly the speed of the visual saliency map. When the degree of detail required and the original input image exactly match the same visual saliency map when fully implemented in four phases may be applied to multiscale feature for image processing, high-quality produce detailed-oriented visual saliency map.</p><p>The original image at the two dimensional coordinates of the mapping function <italic>f</italic> : <italic>P</italic> &#x02192; <italic>S</italic> is used in the process of random saliency map (RSM). It is to be in the original input image coordinates of all pixels in the two-dimensional coordinates defined on the mapping function <italic>f</italic>; <italic>S</italic> represents the normalized execution obtained after the process of visual salience values. Located in the original input image pixel <italic>p</italic> on <italic>P</italic>. <italic>P</italic> corresponds to the original input image in [0, 1] on the visual saliency value of <italic>s</italic>, the mapping function of <italic>f</italic>(<italic>p</italic>) = <italic>s</italic>. The function's values are normalized to [0, 1] within the visual saliency values are stored in the same size and the <italic>P</italic>-dimensional array.</p><sec id="sec3.1"><title>3.1. Randomization Visual Saliency Detection</title><p>Let <italic>s</italic> = <italic>f</italic>(<italic>p</italic>) be through with multiple pixel <italic>p</italic> in close proximity with the relevant image region blocks for visual saliency value calculation, the specific methods such as formula (<xref ref-type="disp-formula" rid="EEq3">3</xref>) below. Consider
<disp-formula id="EEq3"><label>(3)</label><mml:math id="M3"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>In formula (<xref ref-type="disp-formula" rid="EEq3">3</xref>), <italic>R</italic>
<sub><italic>i</italic></sub> is a random distribution of the variable. Its value is limited to the range of [&#x02212;1,1]&#x000d7;[&#x02212;1,1]; <italic>w</italic> is the original input image size 1/2; <italic>&#x003b1;</italic> selection window is the attenuation factor, which is used to the <italic>i</italic> = 1 to <italic>i</italic> = 2<italic>K</italic> images regional blocks until search radius reduced to a single pixel. If <italic>i</italic> &#x0003c; 2<italic>K</italic>, then <italic>i</italic> = 1, the candidate region has been the test until the number of blocks is 2<italic>K</italic>. In this follow-up implementation section, <italic>&#x003b1;</italic> = 0.75.</p><p>According to the formula (<xref ref-type="disp-formula" rid="EEq1">1</xref>) in the candidate region 2<italic>K</italic> blocks of the candidate area is calculated in the block is not the similarity between the <italic>P</italic> value. This algorithm preserves only 2<italic>K</italic> candidate in an image block in 1/2 dissimilarity values smaller blocks of the image area, discard the remaining 1/2 block of the image area. According to the formula (<xref ref-type="disp-formula" rid="EEq2">2</xref>) image area reserved for the <italic>K</italic> block coordinates the visual saliency <italic>p</italic> value calculation process. In this paper of the experimental, part <italic>K</italic> is 32. <italic>P</italic> in accordance with the pixel selection anywhere approximate 2<italic>K</italic> blocks of candidate image region, the block is similar to the original input image area image <italic>P</italic> in all possible partial region of the candidate region, as this is not exactly of the sample collection process and therefore not entirely sampling will give up to get a sample of the visual saliency map introduced a degree of sampling error.</p><p>However, with the number of samples 2<italic>K</italic> gradually becoming large, there certainly will be significantly reduced sampling error. <xref ref-type="fig" rid="fig3"> Figure 3</xref> is the application of the fourth line image. The randomized significant value is calculated after the calculation of the image in the Gaussian pyramid rough layer obtained after the detection of the visual saliency map.</p></sec><sec id="sec3.2"><title>3.2. Detailed Visual Saliency Map</title><p>As mentioned above, the use of randomization means of testing to get a rough visual saliency map, as well as an insufficient number of samples collected random sample error and other reasons will lead to a lot of noise random exist.</p><p>In <xref ref-type="fig" rid="fig3">Figure 3(a)</xref> on the fourth line in the resulting image, the expression of the direct access of a rough visual saliency map, this line can be found in the visual saliency map contains a large randomized noise. The random noise is the cause of randomly selected image region 2<italic>K</italic> block execution formula (<xref ref-type="disp-formula" rid="EEq3">3</xref>) to calculate the noise generated. This after the relevant comparison, using eight neighborhood visual salience values, can be rough for the visual saliency map of meticulous execution process. Eight neighbor method from the pixel coordinates of the point <italic>p</italic> corresponding to the eight directions for selection of neighboring pixels by eight neighbor candidate coordinate method for image region obtained randomly selected candidate block and the domain block image is very different ways. Because <italic>p</italic> coordinates neighborhood image similarity is high, so the eight neighbor coordinate method may make <italic>p</italic> coordinates saliency value be higher than the actual image saliency value smaller; if according to this method to obtain the saliency values normalization process, will lead to the aforementioned resultant rough visual saliency map of the corresponding noise generated. But neighboring coordinates saliency and adjacent to such high similarity, so need for the neighborhood saliency values quite different pixel coordinate positions detailed oriented. We chose detailed oriented neighborhood with eight large differences between visual salience values, because they are not high enough credibility. <xref ref-type="fig" rid="fig3"> Figure 3</xref> on the third line of the image is done by this randomization method for visual saliency in <xref ref-type="sec" rid="sec2">Section 2</xref> of the proceeds of roughly a careful visual saliency map of the corresponding results obtained. The third line in <xref ref-type="fig" rid="fig3">Figure 3</xref> highlights the saliency map detailed map of the visual saliency than a rough map of the visual effect significantly smoother and clearer.</p></sec><sec id="sec3.3"><title>3.3. Multilevel Visual Saliency Consolidation Area Map</title><p>Delicate area of visual saliency map also cannot achieve complete removal of the noise signal. To multiscale visual saliency feature further into the visual saliency map the final results of which will be adjacent to a rough of visual saliency maps and detailed oriented visual saliency map fusion and aggregated to the visual saliency map of the specific expression in formula (<xref ref-type="disp-formula" rid="EEq4">4</xref>) shows
<disp-formula id="EEq4"><label>(4)</label><mml:math id="M4"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mtext>RSM</mml:mtext></mml:mrow><mml:mrow><mml:mtext>merged</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>merge</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>RSM</mml:mtext></mml:mrow><mml:mrow><mml:mtext>refined</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mtext>RSM</mml:mtext></mml:mrow><mml:mrow><mml:mtext>refined</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>In formula (<xref ref-type="disp-formula" rid="EEq4">4</xref>), RSM<sub>merged</sub>
<sup><italic>i</italic></sup> is <italic>i</italic>th layer through the combined saliency map randomized, RSM<sub>refined</sub>
<sup><italic>i</italic></sup> is <italic>i</italic>th layer of the randomized after careful visual saliency map. <xref ref-type="fig" rid="fig3"> Figure 3</xref> on the second line of the visual saliency map is detailed through the different levels of visual saliency map obtained after merging relevant results. RSM<sub>refined</sub>
<sup><italic>i</italic>&#x02212;1</sup> is a layer closest to the <italic>i</italic> first to <italic>i</italic> &#x02212; 1 layer of fine visual saliency map. If the size of RSM<sub>refined</sub>
<sup><italic>i</italic>&#x02212;1</sup> and RSM<sub>refined</sub>
<sup><italic>i</italic></sup> are different, then the merger has made that levels would need to be the size of RSM<sub>refined</sub>
<sup><italic>i</italic>&#x02212;1</sup> to RSM<sub>refined</sub>
<sup><italic>i</italic></sup> sizes closer.</p><p>Try using the weighted algorithm processes, the visual saliency map of the combined method, and the average combined visual saliency map meticulous approach to the visual saliency map of the combined operation. And after the relevant experimental results show that the combined weighted saliency, map method can be significantly higher than the average visual diagram consolidation method better. In the <italic>i</italic>-layer <italic>p</italic> coordinate position, detailed oriented saliency value compared to <italic>S</italic>
<sub>(<italic>r</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic></sup>, <italic>S</italic>
<sub>(<italic>r</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic></sup> which is normalized to [0,1] range. To calculate <italic>i</italic>th layer combined saliency values, the first <italic>i</italic> &#x02212; 1 layer on a rough visual saliency map will be adjusted to <italic>i</italic>th layer and visual saliency map the same size. Use of meticulous visual salience values expressed in scaled <italic>S</italic>
<sub>(<italic>r</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic>&#x02212;1</sup> after the operation of the visual saliency map coordinate position on RSM<sub>refined</sub>
<sup><italic>i</italic>&#x02212;1</sup> visual salience values <italic>p</italic>. The adjacent layer of detailed visual saliency value is the result of the merger shown in the following formula. Consider
<disp-formula id="EEq5"><label>(5)</label><mml:math id="M5"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="EEq6"><label>(6)</label><mml:math id="EEq6EBAAACACCA"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1.0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="EEq7"><label>(7)</label><mml:math id="EEq7EAAAACACCA"><mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>According to formula (<xref ref-type="disp-formula" rid="EEq6">6</xref>) and formula (<xref ref-type="disp-formula" rid="EEq7">7</xref>), in the first layer detailed oriented saliency value <italic>S</italic>
<sub>(<italic>r</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic></sup> and <italic>i</italic> &#x02212; 1 layer detailed oriented visual saliency value <italic>S</italic>
<sub>(<italic>r</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic>&#x02212;1</sup> have been identified in the case, you can calculate <italic>w</italic>
<sub><italic>p</italic></sub>
<sup><italic>i</italic></sup>, <italic>w</italic>
<sub><italic>p</italic></sub>
<sup><italic>i</italic>&#x02212;1</sup>, when <italic>S</italic>
<sub>(<italic>r</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic>&#x02212;1</sup> = 0.0, then <italic>S</italic>
<sub>(<italic>m</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic></sup> = <italic>S</italic>
<sub>(<italic>r</italic>,<italic>p</italic>)</sub>
<sup><italic>i</italic></sup>. According to formula (<xref ref-type="disp-formula" rid="EEq5">5</xref>) calculation of the combined fine of saliency values, <xref ref-type="fig" rid="fig3">Figure 3</xref> of the second line saliency map is the result of combined effects of saliency map, which shows through the combined operation of the visual saliency map can be two levels of each saliency multiscale feature aggregated, and the resulting saliency results figure smoother and clearer.</p><p>In most cases, a multilevel implementation of the merger proceeds saliency map has been able to achieve the expected results. The visual saliency map algorithm can be suitable for basic image editing application. And <xref ref-type="sec" rid="sec3">Section 3</xref> of the paper some of the same, set at <italic>p</italic> coordinates appear higher saliency value indicates an image with a higher reliability. Therefore, in the most cursory of neighboring hierarchy for visual saliency map updates replace the original merger levels above the saliency map. In <xref ref-type="fig" rid="fig3">Figure 3</xref>, the second to the fifth are been updated saliency value of saliency map. In summary, it can be seen from <xref ref-type="fig" rid="fig3">Figure 3</xref>, the first line of the fifth saliency map is this randomized saliency detection results obtained optimal saliency map.</p></sec></sec><sec id="sec4"><title>4. The Experimental Results and Analysis</title><p>Using Matlab 2012b simulation development environment for the proposed algorithm of the paper is implemented and simulated in the Microsoft Windows 7 operating system environment to achieve this randomization visual saliency detection algorithms, and uses this randomized saliency detection algorithm based on a lot of the original input image generation results of a large number of saliency map. All this to get results saliency map are in the Pentium R Dual-Core CPU E8400 generated on individual PC. This got a lot of visual saliency results chart shows randomized visual saliency detection methods available in different levels of complexity of the original input image to obtain good results for image detection effect and visual saliency map, but also by the paper randomized visual saliency detection methods produced results visual saliency map and Itti methods, Geforman methods were compared, and the resulting visual saliency map detected more clear and detailed. Figures <xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig6">6</xref> shows this method to generate a plurality of randomized visual saliency results map.</p><p>It can be seen from <xref ref-type="fig" rid="fig1">Figure 1</xref> that this randomized saliency detection method can produce with the size of the original input image <italic>P</italic> of the same meticulous visual saliency map. It can be detailed oriented visual saliency map of the image content scaling process conducted applications, than Itti method, Goferman obtained by the method saliency map has better image zoom effect. It also describes using detailed oriented visual saliency map to the original input image significant areas of the visual content of the image scaling results more realistic figure.</p><p>
<xref ref-type="fig" rid="fig2">Figure 2</xref> shows that the use of randomized saliency detection method produces results saliency map method than the application Goferman detection based on image content obtained Itti visual saliency maps and methods of visual saliency map clearer and more detailed.</p><p>
<xref ref-type="fig" rid="fig4">Figure 4</xref> randomization methods and Itti methods, Goferman methods were compared. The Experimental results show that the experimental treatment with a size of 800 &#x000d7; 600 of the original input image, the method of randomization is very relevant stages of processing time significantly lower than Itti method, Goferman methods; and the randomization method if applied to the video card's GPU, execution time is exponentially decreasing number. For the same original input image correlation processing, the system memory usage method comparison results show that the application Goferman highest share memory space requirements, and the application of the randomized saliency detection method memory usage is less than Itti methods and Goferman method, the method of randomization if applied to the video card's GPU, the memory space it occupies will definitely decrease.</p><p>
<xref ref-type="fig" rid="fig5">Figure 5</xref> shows the randomized saliency detection methods resulting saliency map and Goferman methods of visual saliency map performs the comparison relevant results, the comparison result indicates that the randomization method produces visual saliency map results Figure more meticulous. <xref ref-type="fig" rid="fig6"> Figure 6</xref> shows the randomization method produces significant results other visual diagram instance.</p><p>Colorful image segmentation algorithm is currently assessed are generally subjective judgment by the human eye, the author proposed algorithm is applied to image segmentation results with standard libraries Berkeley image segmentation results were compared with the human eye to the right algorithm for qualitative assessment.</p><p>
<xref ref-type="fig" rid="fig7">Figure 7</xref> for the application of this randomized saliency detection algorithm for single image for image segmentation of the specific process; (a) is the original color image, (b) is the first stages of the coarse visual saliency map, (c) is for rough flower visual saliency map stepwise detailed oriented resulting visual saliency map, (d) is a combination of multiscale features of the visual saliency map, (e) is obtained after the final integration of the visual saliency map, and (f) is based on the final visual saliency map image segmentation procedure performed after the final segmentation results obtained.</p><p>
<xref ref-type="fig" rid="fig8">Figure 8</xref> are listed separately using the standard algorithm for image library part Berkeley image segmentation results while the application lists the mean shift algorithm as a result of the comparison as well as the human eye horizontal segmentation results, Where <xref ref-type="fig" rid="fig8">Figure 8(a)</xref> is the original color image, <xref ref-type="fig" rid="fig8">Figure 8(b)</xref> for the application of mean shift algorithm for image segmentation results, <xref ref-type="fig" rid="fig8">Figure 8(c)</xref> for this article randomized saliency detection algorithm for image segmentation, and <xref ref-type="fig" rid="fig8">Figure 8(d)</xref> the human eye segmentation results. From the segmentation results in <xref ref-type="fig" rid="fig8">Figure 8</xref>, it can be seen the use of classical mean shift algorithm in color image detail areas cause over-segmentation, the proposed randomized saliency detection method is according to the saliency map for significant regional location, access to and the human eye split almost consistent image segmentation.</p></sec><sec id="sec5"><title>5. Conclusions</title><p>The paper has presented the randomized visual saliency detection algorithm. The randomized visual saliency detection method can quickly generate the same size as the original input image and detailed results of the saliency map. The randomized visual saliency detection method can be applied to real-time requirements for image content-based scaling saliency results map. The randomization method for fast video randomized significant area detection, this algorithm consists of four phases: the first use of randomization visual saliency detection method first generates roughly oriented visual saliency map. The second stage for generating rough video significant results figure careful treatment, removal of correlated noise signal. The third stage for a rough video saliency map to merge in order to extract multiscale features to obtain a multilevel video significant results figure. The fourth stage for video significant results has enhanced graph merging of the final high-quality video and detailed results were visual saliency figure.</p></sec></body><back><ack><title>Acknowledgment</title><p>This work is supported by the project supported by Scientific Research Fund of Hunan Provincial Education Department (no. 12B005), a project supported by Hunan Province Science and Technology Planning (no. 2012FJ3005, no. 2012SK4046), a project supported by the Research Foundation from Ministry of Education of China (no. 208098), and a project supported by the Hunan Province Undergraduates Innovating Experimentation Project (no. (2013) 191-501).</p></ack><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avidan</surname><given-names>S</given-names></name><name><surname>Shamir</surname><given-names>A</given-names></name></person-group><article-title>Seam carving for content-aware image resizing</article-title><source><italic>ACM Transactions on Graphics</italic></source><year>2007</year><volume>26</volume><issue>3, article 10</issue><pub-id pub-id-type="other">2-s2.0-34547653853</pub-id></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>B</given-names></name><name><surname>Sen</surname><given-names>P</given-names></name></person-group><article-title>Video carving</article-title><conf-name>Proceedings of the Eurographics</conf-name><conf-date>2008</conf-date><conf-loc>Hersonissos, Greece</conf-loc></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Ma</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><article-title>Automatic browsing of large pictures on mobile devices</article-title><conf-name>Proceedings of the 11th ACM International Conference on Multimedia (MM &#x02019;03)</conf-name><conf-date>November 2003</conf-date><conf-loc>Berkeley, Calif, USA</conf-loc><fpage>148</fpage><lpage>155</lpage><pub-id pub-id-type="other">2-s2.0-2342587364</pub-id></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pritch</surname><given-names>Y</given-names></name><name><surname>Kav-Venaki</surname><given-names>E</given-names></name><name><surname>Peleg</surname><given-names>S</given-names></name></person-group><article-title>Shift-map image editing</article-title><conf-name>Proceedings of the 12th International Conference on Computer Vision (ICCV &#x02019;09)</conf-name><conf-date>October 2009</conf-date><conf-loc>Kyoto, Japan</conf-loc><fpage>151</fpage><lpage>158</lpage><pub-id pub-id-type="other">2-s2.0-77953227270</pub-id></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinstein</surname><given-names>M</given-names></name><name><surname>Shamir</surname><given-names>A</given-names></name><name><surname>Avidan</surname><given-names>S</given-names></name></person-group><article-title>Improved seam carving for video retargeting</article-title><source><italic>ACM Transactions on Graphics</italic></source><year>2008</year><volume>27</volume><issue>3, article 16</issue><pub-id pub-id-type="other">2-s2.0-49249116348</pub-id></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Santella</surname><given-names>A</given-names></name><name><surname>Agrawala</surname><given-names>M</given-names></name><name><surname>DeCarlo</surname><given-names>D</given-names></name><name><surname>Salesin</surname><given-names>D</given-names></name><name><surname>Cohen</surname><given-names>M</given-names></name></person-group><article-title>Gaze-based interaction for semi-automatic photo cropping</article-title><conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name><conf-date>April 2006</conf-date><conf-loc>Montreal, Canada</conf-loc><fpage>771</fpage><lpage>780</lpage><pub-id pub-id-type="other">2-s2.0-33745864043</pub-id></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>L</given-names></name><name><surname>Guttmann</surname><given-names>M</given-names></name><name><surname>Cohen-Or</surname><given-names>D</given-names></name></person-group><article-title>Non-homogeneous content-driven video-retargeting</article-title><conf-name>Proceedings of the 11th IEEE International Conference on Computer Vision (ICCV &#x02019;07)</conf-name><conf-date>October 2007</conf-date><conf-loc>Rio de Janeiro, Brazil</conf-loc><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="other">2-s2.0-50649119505</pub-id></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Niebur</surname><given-names>E</given-names></name></person-group><article-title>A model of saliency-based visual attention for rapid scene analysis</article-title><source><italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic></source><year>1998</year><volume>20</volume><issue>11</issue><fpage>1254</fpage><lpage>1259</lpage><pub-id pub-id-type="other">2-s2.0-0032204063</pub-id></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goferman</surname><given-names>S</given-names></name><name><surname>Zelnik-Manor</surname><given-names>L</given-names></name><name><surname>Tal</surname><given-names>A</given-names></name></person-group><article-title>Context-aware saliency detection</article-title><conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR &#x02019;10)</conf-name><conf-date>June 2010</conf-date><conf-loc>San Francisco, Calif, USA</conf-loc><fpage>2376</fpage><lpage>2383</lpage><pub-id pub-id-type="other">2-s2.0-77956008283</pub-id></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><article-title>Saliency detection: a spectral residual approach</article-title><conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR &#x02019;07)</conf-name><conf-date>June 2007</conf-date><conf-loc>Minneapolis, Minn, USA</conf-loc><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="other">2-s2.0-35148814949</pub-id></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>CL</given-names></name><name><surname>Ma</surname><given-names>Q</given-names></name><name><surname>Zhang</surname><given-names>LM</given-names></name></person-group><article-title>Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &#x02019;08)</conf-name><conf-date>June 2008</conf-date><conf-loc>Anchorage, Alaska, USA</conf-loc><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="other">2-s2.0-51949107445</pub-id></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tilke</surname><given-names>J</given-names></name><name><surname>Ehinger</surname><given-names>K</given-names></name><name><surname>Durand</surname><given-names>F</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><article-title>Learning to predict where humans look</article-title><conf-name>Proceedings of the 12th International Conference on Computer Vision (ICCV &#x02019;09)</conf-name><conf-date>October 2009</conf-date><conf-loc>Kyoto, Japan</conf-loc><fpage>2106</fpage><lpage>2113</lpage><pub-id pub-id-type="other">2-s2.0-77953205576</pub-id></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname><given-names>C</given-names></name><name><surname>Shechtman</surname><given-names>E</given-names></name><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Goldman</surname><given-names>DB</given-names></name></person-group><article-title>PatchMatch: a randomized correspondence algorithm for structural image editing</article-title><source><italic>ACM Transactions on Graphics</italic></source><year>2009</year><volume>28</volume><issue>3, article 24</issue><pub-id pub-id-type="other">2-s2.0-70349655522</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Saliency regional results and graphics scaling results.</p></caption><graphic xlink:href="CMMM2013-380245.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Visual saliency maps of the results.</p></caption><graphic xlink:href="CMMM2013-380245.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Various stages of saliency results figure.</p></caption><graphic xlink:href="CMMM2013-380245.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The comparison of randomize methods, Itti, Goferman.</p></caption><graphic xlink:href="CMMM2013-380245.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Randomize method and Goferman produces visual saliency results maps: (a) the input image; (b) Goferman produces visual saliency results; (c) randomize method produces visual saliency results.</p></caption><graphic xlink:href="CMMM2013-380245.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Visual saliency results example: no. 1 and no. 3 are the input image; no. 2 and no. 4 are the application of the randomize method produces visual saliency maps.</p></caption><graphic xlink:href="CMMM2013-380245.006"/></fig><fig id="fig7" orientation="portrait" position="float"><label>Figure 7</label><caption><p>The image segmentation algorithm in the paper.</p></caption><graphic xlink:href="CMMM2013-380245.007"/></fig><fig id="fig8" orientation="portrait" position="float"><label>Figure 8</label><caption><p>Image segmentation results comparison maps.</p></caption><graphic xlink:href="CMMM2013-380245.008"/></fig></floats-group></article>
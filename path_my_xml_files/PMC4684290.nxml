<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26683053</article-id><article-id pub-id-type="pmc">4684290</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0143177</article-id><article-id pub-id-type="publisher-id">PONE-D-15-04556</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Peer Assessment Enhances Student Learning: The Results of a Matched Randomized Crossover Experiment in a College Statistics Class</article-title><alt-title alt-title-type="running-head">Peer Assessment Enhances Student Learning</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Dennis L.</given-names></name><xref ref-type="aff" rid="aff001">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Harris</surname><given-names>Naftali</given-names></name><xref ref-type="aff" rid="aff002">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Walther</surname><given-names>Guenther</given-names></name><xref ref-type="aff" rid="aff002">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Baiocchi</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff002">
<sup>2</sup>
</xref><xref ref-type="aff" rid="aff003">
<sup>3</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001">
<label>1</label>
<addr-line>Department of Statistics, California Polytechnic State University, San Luis Obispo, CA, 93407, United States of America</addr-line>
</aff><aff id="aff002">
<label>2</label>
<addr-line>Department of Statistics, Stanford University, Stanford, CA, 94305, United States of America</addr-line>
</aff><aff id="aff003">
<label>3</label>
<addr-line>Prevention Research Center, Stanford School of Medicine, Stanford, CA, 94305, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Dalby</surname><given-names>Andrew R.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Westminster, UNITED KINGDOM</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con" id="contrib001"><p>Conceived and designed the experiments: DLS MB GW. Performed the experiments: MB DLS GW NH. Analyzed the data: DLS MB. Contributed reagents/materials/analysis tools: DLS GW. Wrote the paper: DLS MB GW.</p></fn><corresp id="cor001">* E-mail: <email>dsun09@calpoly.edu</email> (DLS); <email>baiocchi@stanford.edu</email> (MB)</corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>12</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>10</volume><issue>12</issue><elocation-id>e0143177</elocation-id><history><date date-type="received"><day>25</day><month>2</month><year>2015</year></date><date date-type="accepted"><day>1</day><month>11</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; 2015 Sun et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Sun et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:type="simple" xlink:href="pone.0143177.pdf"/><abstract><p>Feedback has a powerful influence on learning, but it is also expensive to provide. In large classes it may even be impossible for instructors to provide individualized feedback. Peer assessment is one way to provide personalized feedback that scales to large classes. Besides these obvious logistical benefits, it has been conjectured that students also learn from the practice of peer assessment. However, this has never been conclusively demonstrated. Using an online educational platform that we developed, we conducted an in-class matched-set, randomized crossover experiment with high power to detect small effects. We establish that peer assessment causes a small but significant gain in student achievement. Our study also demonstrates the potential of web-based platforms to facilitate the design of high-quality experiments to identify small effects that were previously not detectable.</p></abstract><funding-group><funding-statement>Stanford University supplied a grant to fund the development of our online homework platform (&#x0003c;$10,000USD). This platform is being used on an ongoing basis by the department, separately from the study. A grant from the Agency for Health Research &#x00026; Quality provided support for some of Professor Baiocchi's time (KHS022192A).</funding-statement></funding-group><counts><fig-count count="2"/><table-count count="2"/><page-count count="7"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data are available, with restrictions, from the authors who may be contacted at <email>baiocchi@stanford.edu</email> or <email>dsun09@calpoly.edu</email>. Restrictions are in place to protect against person-level identification, as required by the IRB and the informed consent process. Before data can be accessed, privacy agreements will need to be in place.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data are available, with restrictions, from the authors who may be contacted at <email>baiocchi@stanford.edu</email> or <email>dsun09@calpoly.edu</email>. Restrictions are in place to protect against person-level identification, as required by the IRB and the informed consent process. Before data can be accessed, privacy agreements will need to be in place.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Feedback is one of the single most important factors influencing student learning [<xref rid="pone.0143177.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0143177.ref002" ref-type="bibr">2</xref>]. However, it is often not possible to provide feedback that is both detailed and prompt, thus limiting its effectiveness in practice [<xref rid="pone.0143177.ref003" ref-type="bibr">3</xref>]. In large college classes and massively open online courses (MOOCs), providing personalized feedback to students is especially challenging.</p><p>While automated feedback can be an adequate substitute in some cases [<xref rid="pone.0143177.ref004" ref-type="bibr">4</xref>], many concepts and skills are still challenging for a machine to evaluate. Consider the following question about the interpretation of a p-value, an important concept in introductory statistics.</p><disp-quote><p>Josh flips a coin 100 times. The coin comes up heads 60 times. He calculates the p-value to be about 2% for testing the null hypothesis that the coin is fair. Explain what this 2% means in the context of this problem.</p></disp-quote><p>The correct answer is that the 2% represents the probability of observing a result at least this large if the coin were fair. However, a common misconception among students is that it represents the probability the coin is fair. Even with state-of-the-art semantic parsing, machines cannot accurately discriminate incorrect answers from correct ones [<xref rid="pone.0143177.ref005" ref-type="bibr">5</xref>]. On the other hand, a human who understands the concept would have little difficulty distinguishing the two.</p><p>Thus, the problem of providing feedback falls into the large class of tasks that are relatively easy for a human but challenging for a machine. Such tasks are fertile ground for crowdsourcing, which has been applied to otherwise intractable problems with surprising success [<xref rid="pone.0143177.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0143177.ref007" ref-type="bibr">7</xref>]. Feedback can be &#x0201c;crowdsourced&#x0201d; by having students grade one another, a practice known as <italic>peer assessment</italic>. Peer assessment provides as many graders as students, enabling more timely and thorough feedback [<xref rid="pone.0143177.ref008" ref-type="bibr">8</xref>]. It has already made personalized feedback feasible in a number of settings&#x02014;most notably MOOCs&#x02014;where it otherwise would be impossible [<xref rid="pone.0143177.ref009" ref-type="bibr">9</xref>].</p><p>Instructors often have two main concerns about peer assessment. The first is whether students can be trusted to grade accurately. This question has been extensively studied in the literature, and the consensus is that peer grades are at least comparable to instructor grades [<xref rid="pone.0143177.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0143177.ref011" ref-type="bibr">11</xref>]. The second is logistics; peer assessment is logistically complicated if students have to exchange papers in person. However, web-based tools have largely solved this problem. Most modern learning management systems (LMS) come with a built-in peer assessment tool that automatically distributes student responses to peer graders.</p><p>Therefore, peer assessment is a workable solution to the problem of feedback; it reduces the burden to the instructors with minimal sacrifice to quality. On top of this, it has been conjectured that students also learn in the process of providing feedback. If true, then peer assessment may be more than just a useful tool to manage large classes; it can be a pedagogical tool that is both effective and inexpensive [<xref rid="pone.0143177.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0143177.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0143177.ref013" ref-type="bibr">13</xref>].</p><p>This claim was perhaps most visibly advanced in the U.S. Supreme Court case <italic>Owasso v</italic>. <italic>Falvo</italic> (2002) [<xref rid="pone.0143177.ref014" ref-type="bibr">14</xref>]. Although the case was primarily concerned with whether peer assessment violated students&#x02019; privacy, Justice Anthony Kennedy praised peer assessment in his majority opinion, saying</p><disp-quote><p>Correcting a classmate's work can be as much a part of the assignment as taking the test itself. It is a way to teach material again in a new context, and it helps show students how to assist and respect fellow pupils [<xref rid="pone.0143177.ref011" ref-type="bibr">11</xref>].</p></disp-quote><p>However, to date, there has been scant empirical evidence for this claim. The evidence is based mostly on surveys of student and teacher perceptions [<xref rid="pone.0143177.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0143177.ref015" ref-type="bibr">15</xref>]. Only a few studies have attempted to quantify the effect on an objective criterion such as achievement, but most have been correlational studies. A representative study in this latter category is [<xref rid="pone.0143177.ref016" ref-type="bibr">16</xref>], which examined whether peer assessment improved students&#x02019; writing abilities. However, the study lacked a control group, so it is not possible to know whether students improved any more with peer assessment than they would have otherwise. Furthermore, the study measured achievement using the students&#x02019; own peer grades, rather than an objective measure (e.g., scores given by a third-party observer who was blinded to the treatment). To our knowledge, only one randomized experiment has ever been conducted to measure the effect of peer assessment on achievement, but the study lacked statistical power to reach a conclusion either way [<xref rid="pone.0143177.ref017" ref-type="bibr">17</xref>]. This gap in the literature has been noted by several researchers, who have suggested this as an important direction of future research [<xref rid="pone.0143177.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0143177.ref018" ref-type="bibr">18</xref>].</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and Methods</title><p>We were interested in whether peer assessment could aid conceptual understanding and problem solving, two skills that are especially relevant in science, technology, engineering, and mathematics (STEM) classes. We conducted a randomized controlled trial (RCT) in a large introductory statistics class, using a crossover design to enhance precision. Written, informed consent was obtained from all participants, and the study was approved by the Stanford University Institutional Review Board. The ten-week course was divided into an introductory unit and four main units. The introductory unit was excluded from the study so that students had time to become acclimated to peer assessment. Each student was then assigned to participate in peer assessment for exactly two of the main units and to the control group for the other two. In all, there were four treatment arms, each one receiving the peer assessment treatment (T) and control (C) in a different order over the four main units (TCTC, CTCT, TCCT, and CTTC).</p><p>This crossover design controls for all differences between students, since each student participates both as a treatment and a control subject at different times in the course. This eliminates what is arguably the largest source of variability in educational studies: variation between students. To further ensure against baseline differences between the treatment and control groups, we augmented this design with matched-pairs randomization. Students were matched based on covariate information (e.g., class year, previous statistics experience), and the students within each pair were assigned to complementary treatments in all four of the main units (i.e., TCTC vs. CTCT). <xref rid="pone.0143177.g001" ref-type="fig">Fig 1</xref> shows the result of the matched pairs design, alongside a random pairing for comparison. The resulting covariate balance for the actual randomization is provided in (<xref rid="pone.0143177.s001" ref-type="supplementary-material">S1 File</xref>), showing that the matched-pairs design produced indistinguishable groups.</p><fig id="pone.0143177.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0143177.g001</object-id><label>Fig 1</label><caption><title>Two plots showing the effect of the matched pairs randomization design (1A) as compared with complete randomization (1B).</title><p>Each point represents a student&#x02019;s covariate information, and each connecting edge indicates that those students have been assigned to opposite treatment groups. The edges in the matched pairs design are much shorter than under complete randomization, confirming that matching produces more similar randomizations.</p></caption><graphic xlink:href="pone.0143177.g001"/></fig><p>The treatment was defined as follows: in weeks that students were required to participate in peer assessment, they provided scores and comments on the homework responses of three peers. In turn, they received feedback on their homework from three peers. All homework responses and peer assessments were submitted through an online platform, and all responses and reviews were anonymized before distribution to ensure privacy. Students in the control group also submitted their homework responses online but did not participate in peer assessment and had their homework graded by instructors. In order to control for the possible effect of feedback timing, feedback was delivered to the two groups simultaneously. Also, the students in the control group were provided the same solution key as the one provided to the peer graders. For more details about the specific implementation of peer assessment, please refer to (<xref rid="pone.0143177.s001" ref-type="supplementary-material">S1 File</xref>).</p><p>To measure achievement, students completed a quiz after each unit that measured the short-term effect of peer assessment. The students also took a comprehensive final exam that measured longer-term learning. These assessments consisted entirely of free-response questions that required explanations or calculations. The instructors, who were blinded to the treatment groups of the students, graded all assessments to ensure consistency for the purposes of the study.</p><p>Finally, the study was fully replicated in a different academic term with a different instructor. The same homework questions were used in the two terms, but different exam questions were used. In all, 148 students participated in the study during the first term (autumn) and 239 students during the second (winter). Because the crossover design should eliminate any student or instructor effects, we pooled the data from the two terms to obtain a single sample of 387 students. However, in the analyses, we excluded any students who failed to comply with the peer assessment treatment or to complete the assessments. This left us with 299 students in the analysis of the unit quiz scores and 320 students in the analysis of the final exam scores. Although excluding non-compliers can sometimes bias the treatment effect, our crossover design ensures that non-compliers are excluded from both the treatment and control groups. As a result, we obtain an unbiased estimate of the treatment effect on the subpopulation of students who would be affected by the peer assessment intervention. A further discussion of this and the definition of compliance can be found in (<xref rid="pone.0143177.s001" ref-type="supplementary-material">S1 File</xref>).</p></sec><sec sec-type="results" id="sec003"><title>Results</title><p>The students who participated in peer assessment during a given unit performed significantly better on the unit quizzes (Cohen's d = .115, t(298) = 2.92, p = .002), as compared with students who did not. Students participating in peer assessment also did better on the corresponding questions on the final exam (d = .122, t(319) = 3.03, p = .001), which suggests that the benefits of peer assessment persist over time. These results are summarized in <xref rid="pone.0143177.t001" ref-type="table">Table 1</xref>. In the context of our course, where the standard deviations of exam scores ranged from 15 to 25 percentage points, an effect size of .122 would translate to a 2 to 3 percentage point increase in the average exam score. <xref rid="pone.0143177.g002" ref-type="fig">Fig 2</xref>, which depicts the actual distribution of scores for one of the unit quizzes, shows that a modest increase in average score can be practically important.</p><table-wrap id="pone.0143177.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0143177.t001</object-id><label>Table 1</label><caption><title>The effect sizes of peer assessment in the short term and long term.</title><p>(Standard errors are shown in parentheses.)</p></caption><alternatives><graphic id="pone.0143177.t001g" xlink:href="pone.0143177.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Type of Effect</th><th align="center" rowspan="1" colspan="1">Effect Size</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Short Term (as measured by unit quizzes)</td><td align="center" rowspan="1" colspan="1">0.115 (0.04)</td></tr><tr><td align="left" rowspan="1" colspan="1">Long Term (as measured by final exam)</td><td align="center" rowspan="1" colspan="1">0.122 (0.04)</td></tr></tbody></table></alternatives></table-wrap><fig id="pone.0143177.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0143177.g002</object-id><label>Fig 2</label><caption><title>Distribution of scores for the control (blue) and treatment (red) groups on quiz 5 in the winter quarter.</title><p>The dashed vertical lines designate the means. (The difference in means on this quiz was 5.9.) Similar plots for all of the quizzes and final exam may be found in (<xref rid="pone.0143177.s001" ref-type="supplementary-material">S1 File</xref>).</p></caption><graphic xlink:href="pone.0143177.g002"/></fig><p>To understand the magnitude of this effect size, we also calculated the &#x0201c;effect sizes&#x0201d; of well-known achievement gaps: between males and females, between underrepresented minorities and others, between students with more math background and students with less, etc. These achievement gaps are reported in <xref rid="pone.0143177.t002" ref-type="table">Table 2</xref>. We estimated the gap twice, once at the beginning of the course (using quiz 1 scores, which was administered prior to randomization) and again at the end of the course (using final exam scores). We see that the effect size of peer assessment, which is .122, represents about 40% of the gender achievement gap and about 20% of the racial achievement gap, which are persistent challenges in college science classes [<xref rid="pone.0143177.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0143177.ref020" ref-type="bibr">20</xref>]. By comparing the gaps before and after the course, we see also that the course tended to reduce achievement gaps, although we do not have enough evidence to attribute this to the peer assessment intervention.</p><table-wrap id="pone.0143177.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0143177.t002</object-id><label>Table 2</label><caption><title>Achievement gaps in our population of students, reported as an effect size.</title><p>We show the gap before and after the course. (Standard errors are shown in parentheses.) The &#x0201c;before&#x0201d; numbers were calculated using scores on a pre-quiz administered prior to the randomization. The &#x0201c;after&#x0201d; numbers were calculated using scores on the final exam.</p></caption><alternatives><graphic id="pone.0143177.t002g" xlink:href="pone.0143177.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Achievement Gap</th><th align="center" rowspan="1" colspan="1">Difference before Course</th><th align="center" rowspan="1" colspan="1">Difference after Course</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Gender achievement gap (1 = male)</td><td align="center" rowspan="1" colspan="1">0.32 (0.12)</td><td align="center" rowspan="1" colspan="1">0.13 (0.12)</td></tr><tr><td align="left" rowspan="1" colspan="1">Racial achievement gap (1 = underrepresented minority)</td><td align="center" rowspan="1" colspan="1">&#x02212;.61 (0.14)</td><td align="center" rowspan="1" colspan="1">&#x02212;.42 (0.13)</td></tr><tr><td align="left" rowspan="1" colspan="1">Statistics background (1 = passed AP stats)</td><td align="center" rowspan="1" colspan="1">0.54 (0.11)</td><td align="center" rowspan="1" colspan="1">0.59 (0.12)</td></tr><tr><td align="left" rowspan="1" colspan="1">Math background (1 = course beyond calculus)</td><td align="center" rowspan="1" colspan="1">0.68 (0.10)</td><td align="center" rowspan="1" colspan="1">0.54 (0.11)</td></tr><tr><td align="left" rowspan="1" colspan="1">Class year (1 = upperclassman)</td><td align="center" rowspan="1" colspan="1">0.22 (0.12)</td><td align="center" rowspan="1" colspan="1">0.07 (0.11)</td></tr></tbody></table></alternatives></table-wrap><p>We also surveyed students on their perception of the benefit of peer assessment on a scale from 1 to 5, with 1 indicating &#x0201c;not helpful at all" and 5 indicating &#x0201c;extremely helpful". Although the median student reported finding peer assessment only &#x0201c;somewhat helpful," there was virtually zero correlation (r = .01, p = .94) between a student's perception of the benefit and the estimated benefit. This lends credence to our concern that surveys may not be the best measure of student learning. A further analysis of student reaction to peer assessment can be found in (<xref rid="pone.0143177.s001" ref-type="supplementary-material">S1 File</xref>).</p></sec><sec sec-type="conclusions" id="sec004"><title>Discussion</title><p>This study has established that peer assessment produces concrete gains in student achievement, above and beyond the effect of receiving feedback. Thus, peer assessment is unique among educational interventions in that the usual cost-benefit tradeoff seems not to apply: it saves instructors time, while also benefiting students. This suggests that peer assessment should not be limited to MOOCs and large classes where it is the only option, but that it has a place even in smaller settings where it is not strictly needed.</p><p>This study is also a demonstration of the role that web-based platforms, such as learning management systems and MOOCs, can play in education research. The ability to personalize an individual&#x02019;s experience using such platforms is easier than in traditional formats. This makes individual-level randomizations, the core of high quality RCTs, much easier. While we have focused on peer assessment specifically, a similar study design could be used to investigate other questions as well. The transformational impact of web-based educational tools may be their ability to facilitate experiments in the classroom, enabling us to obtain unprecedented insight into the learning process.</p></sec><sec sec-type="supplementary-material" id="sec005"><title>Supporting Information</title><supplementary-material content-type="local-data" id="pone.0143177.s001"><label>S1 File</label><caption><title>Supplementary Information for <italic>Peer assessment enhances student learning</italic>.</title><p>(PDF)</p></caption><media xlink:href="pone.0143177.s001.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>We are grateful to Sergio Bacallado and Renjie You for discussions, as well as to Zhou Fan, Scott Freeman, Max Grazier-G&#x02019;sell, and Carl Wieman for their careful reading of our paper and helpful suggestions.</p></ack><ref-list><title>References</title><ref id="pone.0143177.ref001"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Bangert-Drowns</surname><given-names>RL</given-names></name>, <name><surname>Kulik</surname><given-names>C-LC</given-names></name>, <name><surname>Kulik</surname><given-names>JA</given-names></name>, <name><surname>Morgan</surname><given-names>M</given-names></name> (<year>1991</year>). <article-title>The instructional effect of feedback in test-like events</article-title>. <source>Review of Educational Research</source>, <volume>61</volume>, <fpage>213</fpage>&#x02013;<lpage>238</lpage>
</mixed-citation></ref><ref id="pone.0143177.ref002"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Hattie</surname><given-names>J</given-names></name>, <name><surname>Timperley</surname><given-names>H</given-names></name>. <source>The Power of Feedback Review of Educational Research</source>,<volume>77</volume>, <fpage>1</fpage> (<year>2007</year>).</mixed-citation></ref><ref id="pone.0143177.ref003"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Kulik</surname><given-names>JA</given-names></name>, <name><surname>Kulik</surname><given-names>CC</given-names></name> (<year>1988</year>). <article-title>Timing of feedback and verbal learning</article-title>. <source>Review of Educational Research</source>, <volume>58</volume>(<issue>1</issue>), <fpage>79</fpage>&#x02013;<lpage>97</lpage>.).</mixed-citation></ref><ref id="pone.0143177.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Linn</surname><given-names>MC</given-names></name>, <name><surname>Gerard</surname><given-names>LF</given-names></name>, <name><surname>Ryoo</surname><given-names>K</given-names></name>, <name><surname>Liu</surname><given-names>L</given-names></name>, <name><surname>Rafferty</surname><given-names>AN</given-names></name> (<year>2014</year>). &#x0201c;<article-title>Computer-guided inquiry to improve science learning</article-title>.&#x0201d; <source>Science</source>, <volume>344</volume>, <fpage>155</fpage>&#x02013;<lpage>156</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1245980">10.1126/science.1245980</ext-link></comment>
<pub-id pub-id-type="pmid">24723599</pub-id></mixed-citation></ref><ref id="pone.0143177.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Leacock</surname><given-names>C</given-names></name>, <name><surname>Chodorow</surname><given-names>M</given-names></name>. <source>Computers and the Humanities</source>. <volume>37</volume>, <fpage>389</fpage> (<year>2003</year>).</mixed-citation></ref><ref id="pone.0143177.ref006"><label>6</label><mixed-citation publication-type="journal">
<name><surname>von Ahn</surname><given-names>L</given-names></name>, <name><surname>Maurer</surname><given-names>B</given-names></name>, <name><surname>McMillen</surname><given-names>C</given-names></name>, <name><surname>Abraham</surname><given-names>D</given-names></name>, <name><surname>Blum</surname><given-names>M</given-names></name>. <article-title>reCAPTCHA: human-based character recognition via Web security measures</article-title>. <source>Science</source>
<volume>321</volume>, <fpage>1465</fpage> (<year>2008</year>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1160379">10.1126/science.1160379</ext-link></comment>
<pub-id pub-id-type="pmid">18703711</pub-id></mixed-citation></ref><ref id="pone.0143177.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Cooper</surname><given-names>S</given-names></name>, <name><surname>Khatib</surname><given-names>F</given-names></name>, <name><surname>Treuille</surname><given-names>A</given-names></name>, <name><surname>Barbero</surname><given-names>J</given-names></name>, <name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Beenen</surname><given-names>M</given-names></name>, <etal>et al</etal>
<article-title>Predicting protein structures with a multiplayer online game</article-title>. <source>Nature</source>
<volume>466</volume>, <fpage>756</fpage> (<year>2010</year>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature09304">10.1038/nature09304</ext-link></comment>
<pub-id pub-id-type="pmid">20686574</pub-id></mixed-citation></ref><ref id="pone.0143177.ref008"><label>8</label><mixed-citation publication-type="book">
<name><surname>Falchikov</surname><given-names>N</given-names></name>. <source>Improving assessment through student involvement</source> (<publisher-name>Routledge</publisher-name>, <year>2005</year>).</mixed-citation></ref><ref id="pone.0143177.ref009"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Kulkarni</surname><given-names>C</given-names></name>, <name><surname>Wei</surname><given-names>KP</given-names></name>, <name><surname>Le</surname><given-names>H</given-names></name>, <name><surname>Cia</surname><given-names>D</given-names></name>, <name><surname>Papadopoulos</surname><given-names>K</given-names></name>, <name><surname>Cheng</surname><given-names>J</given-names></name>, <etal>et al</etal>
<article-title>Peer and self assessment in massive online classes. ACM Trans. Comput.-Hum</article-title>. <source>Interact</source>. <volume>20</volume> (<year>2013</year>).</mixed-citation></ref><ref id="pone.0143177.ref010"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Topping</surname><given-names>K</given-names></name>. <article-title>Peer assessment between students in colleges and universities</article-title>. <source>Rev. Ed. Res</source>. <volume>68</volume>, <fpage>249</fpage> (<year>1998</year>).</mixed-citation></ref><ref id="pone.0143177.ref011"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Falchikov</surname><given-names>N</given-names></name>, <name><surname>Goldfinch</surname><given-names>J</given-names></name>. <article-title>Student peer assessment in higher education: a meta-analysis comparing peer and teacher marks</article-title>. <source>Rev. Ed. Res</source>. <volume>70</volume>, <fpage>287</fpage> (<year>2000</year>).</mixed-citation></ref><ref id="pone.0143177.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Bloom</surname><given-names>BS</given-names></name>. <article-title>The 2 Sigma Problem: The Search for Methods of Group Instruction as Effective as One-to-One Tutoring</article-title>. <source>Educ. Res</source>. <volume>13</volume>, <fpage>4</fpage> (<year>1984</year>).</mixed-citation></ref><ref id="pone.0143177.ref013"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Topping</surname><given-names>KJ</given-names></name>. <article-title>Peer Assessment</article-title>. <source>Theory into Practice</source>
<volume>48</volume>, <fpage>20</fpage> (<year>2009</year>).</mixed-citation></ref><ref id="pone.0143177.ref014"><label>14</label><mixed-citation publication-type="book">
<source>Supreme Court of the United States</source>, <name><surname>Owasso</surname><given-names>v</given-names></name>. <publisher-name>Falvo</publisher-name> (<year>2002</year>).</mixed-citation></ref><ref id="pone.0143177.ref015"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Tinapple</surname><given-names>D</given-names></name>, <name><surname>Olson</surname><given-names>L</given-names></name>, <name><surname>Sadauska</surname><given-names>J</given-names></name>. <article-title>CritViz: Web-based software supporting peer critique in large creative classrooms</article-title>. <source>Bull. IEEE Tech. Com. on Learn. Tech.</source>, <volume>15</volume>, <fpage>1</fpage> (<year>2013</year>).</mixed-citation></ref><ref id="pone.0143177.ref016"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Gerdeman</surname><given-names>RD</given-names></name>, <name><surname>Russelll</surname><given-names>AA</given-names></name>, <name><surname>Worden</surname><given-names>KJ</given-names></name>. <article-title>Web-based student writing and reviewing in a large biology lecture course</article-title>. <source>J. Coll. Sci. Teaching</source>, <volume>36</volume>, <fpage>5</fpage> (<year>2007</year>).</mixed-citation></ref><ref id="pone.0143177.ref017"><label>17</label><mixed-citation publication-type="journal">
<name><surname>Sadler</surname><given-names>PM</given-names></name>, <name><surname>Good</surname><given-names>E</given-names></name>. <article-title>The Impact of Self- and Peer-Grading on Student Learning</article-title>. <source>Ed. Assess</source>. <volume>11</volume>, <fpage>1</fpage> (<year>2006</year>).</mixed-citation></ref><ref id="pone.0143177.ref018"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Freeman</surname><given-names>S</given-names></name>, <name><surname>Parks</surname><given-names>JW</given-names></name>. <article-title>How Accurate is Peer Grading?</article-title>
<source>CBE Life Sci. Ed</source>. <volume>9</volume>, <fpage>482</fpage> (<year>2010</year>).</mixed-citation></ref><ref id="pone.0143177.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Miyake</surname><given-names>A</given-names></name>, <name><surname>Kost-Smith</surname><given-names>LE</given-names></name>, <name><surname>Finkelstein</surname><given-names>ND</given-names></name>, <name><surname>Pollock</surname><given-names>SJ</given-names></name>, <name><surname>Cohen</surname><given-names>GL</given-names></name>, <name><surname>Ito</surname><given-names>TA</given-names></name>. <article-title>Reducing the gender achievement gap in college science: a classroom study of values affirmation</article-title>. <source>Science</source>
<volume>330</volume>, <fpage>1234</fpage> (<year>2010</year>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1195996">10.1126/science.1195996</ext-link></comment>
<pub-id pub-id-type="pmid">21109670</pub-id></mixed-citation></ref><ref id="pone.0143177.ref020"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Haak</surname><given-names>DC</given-names></name>, <name><surname>HilleRisLambers</surname><given-names>J</given-names></name>, <name><surname>Pitre</surname><given-names>E</given-names></name>, <name><surname>Freeman</surname><given-names>S</given-names></name>. <article-title>Increased structure and active learning reduce the achievement gap in introductory biology</article-title>. <source>Science</source>
<volume>332</volume>, <fpage>1213</fpage> (<year>2011</year>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1204820">10.1126/science.1204820</ext-link></comment>
<pub-id pub-id-type="pmid">21636776</pub-id></mixed-citation></ref></ref-list></back></article>
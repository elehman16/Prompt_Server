<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26539138</article-id><article-id pub-id-type="pmc">4611966</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2015.01578</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Comprehension and engagement in survey interviews with virtual agents</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Conrad</surname><given-names>Frederick G.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/185458/overview"/></contrib><contrib contrib-type="author"><name><surname>Schober</surname><given-names>Michael F.</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/138399/overview"/></contrib><contrib contrib-type="author"><name><surname>Jans</surname><given-names>Matt</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/279680/overview"/></contrib><contrib contrib-type="author"><name><surname>Orlowski</surname><given-names>Rachel A.</given-names></name><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib><contrib contrib-type="author"><name><surname>Nielsen</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff6"><sup>6</sup></xref></contrib><contrib contrib-type="author"><name><surname>Levenstein</surname><given-names>Rachel</given-names></name><xref ref-type="aff" rid="aff7"><sup>7</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Michigan Program in Survey Methodology, Institute for Social Research, University of Michigan</institution><country>Ann Arbor, MI, USA</country></aff><aff id="aff2"><sup>2</sup><institution>Joint Program in Survey Methodology, University of Maryland</institution><country>College Park, MD, USA</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Psychology, New School for Social Research</institution><country>New York, NY, USA</country></aff><aff id="aff4"><sup>4</sup><institution>Center for Health Policy Research, University of California at Los Angeles</institution><country>Los Angeles, CA, USA</country></aff><aff id="aff5"><sup>5</sup><institution>Department of Epidemiology, School of Public Health, University of Michigan</institution><country>Ann Arbor, MI, USA</country></aff><aff id="aff6"><sup>6</sup><institution>Department of Biostatistics, Center for Cancer Biostatistics, University of Michigan Medical School</institution><country>Ann Arbor, MI, USA</country></aff><aff id="aff7"><sup>7</sup><institution>University of Chicago Consortium on Chicago School Research, Urban Education Institute, University of Chicago</institution><country>Chicago, IL, USA</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Sebastian Loth, Universit&#x000e4;t Bielefeld, Germany</p></fn><fn fn-type="edited-by"><p>Reviewed by: Jonathan Gratch, University of Southern California, USA; Verena Nitsch, Universit&#x000e4;t der Bundeswehr M&#x000fc;nchen, Germany</p></fn><corresp id="fn001">*Correspondence: Frederick G. Conrad <email xlink:type="simple">fconrad@umich.edu</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Cognitive Science, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>20</day><month>10</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>6</volume><elocation-id>1578</elocation-id><history><date date-type="received"><day>08</day><month>3</month><year>2015</year></date><date date-type="accepted"><day>29</day><month>9</month><year>2015</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2015 Conrad, Schober, Jans, Orlowski, Nielsen and Levenstein.</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Conrad, Schober, Jans, Orlowski, Nielsen and Levenstein</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>This study investigates how an onscreen virtual agent's dialog capability and facial animation affect survey respondents' comprehension and engagement in &#x0201c;face-to-face&#x0201d; interviews, using questions from US government surveys whose results have far-reaching impact on national policies. In the study, 73 laboratory participants were randomly assigned to respond in one of four interviewing conditions, in which the virtual agent had either high or low dialog capability (implemented through Wizard of Oz) and high or low facial animation, based on motion capture from a human interviewer. Respondents, whose faces were visible to the Wizard (and videorecorded) during the interviews, answered 12 questions about housing, employment, and purchases on the basis of fictional scenarios designed to allow measurement of comprehension accuracy, defined as the fit between responses and US government definitions. Respondents answered more accurately with the high-dialog-capability agents, requesting clarification more often particularly for ambiguous scenarios; and they generally treated the high-dialog-capability interviewers more socially, looking at the interviewer more and judging high-dialog-capability agents as more personal and less distant. Greater interviewer facial animation did not affect response accuracy, but it led to more displays of engagement&#x02014;acknowledgments (verbal and visual) and smiles&#x02014;and to the virtual interviewer's being rated as less natural. The pattern of results suggests that a virtual agent's dialog capability and facial animation differently affect survey respondents' experience of interviews, behavioral displays, and comprehension, and thus the accuracy of their responses. The pattern of results also suggests design considerations for building survey interviewing agents, which may differ depending on the kinds of survey questions (sensitive or not) that are asked.</p></abstract><kwd-group><kwd>virtual agent</kwd><kwd>survey interviewing</kwd><kwd>social signals</kwd><kwd>comprehension</kwd><kwd>dialog capability</kwd><kwd>facial animation</kwd></kwd-group><counts><fig-count count="4"/><table-count count="5"/><equation-count count="0"/><ref-count count="79"/><page-count count="20"/><word-count count="17227"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>An important source of knowledge about society is what people report in survey interviews that produce the data for official (government) statistics, e.g., population estimates on employment, health and crime. Data from such survey interviews, which provide essential input for policy decisions, are administered on a very large scale; for example, more than 60,000 US households per month are recruited to participate in the Current Population Survey, from which the US unemployment rate is calculated, and for the European Social Survey (ESS) in 2012, 54,600 standardized face-to-face interviews were carried out in 29 countries (Ferrin and Kriesi, <xref rid="B27" ref-type="bibr">2014</xref>). Results from these interviews can have far-reaching consequences: even small changes in reported US unemployment rates, for example, can affect world financial markets, and results from the ESS make &#x0201c;a major contribution to the creation of effective social and economic policies in Europe&#x0201d; (Geoghegan-Quinn, <xref rid="B31" ref-type="bibr">2012</xref>). So understanding what leads to accurate responses, and to participants' willingness to engage in such surveys, is societally important (Schober and Conrad, <xref rid="B64" ref-type="bibr">2015</xref>).</p><p>Although survey interviews have traditionally been administered by humans either face-to-face or on the telephone, the landscape is changing: surveys are increasingly &#x0201c;self-administered&#x0201d; (that is, administered by automated systems, as in online surveys in a web browser possibly on a mobile device; Mavletova and Couper, <xref rid="B53" ref-type="bibr">2014</xref>), and new human and automated modes are being explored (Conrad and Schober, <xref rid="B17" ref-type="bibr">2008</xref>), e.g., videomediated interviewing (Anderson, <xref rid="B3" ref-type="bibr">2008</xref>), text message surveys (Schober et al., <xref rid="B65" ref-type="bibr">2015</xref>), and speech dialog system surveys (Bloom, <xref rid="B8" ref-type="bibr">2008</xref>; Johnston et al., <xref rid="B43" ref-type="bibr">2013</xref>). Exploring new ways of administering surveys is sensible given declining survey response rates and the growing expenses of carrying out human-administered interviews (see, e.g., Groves, <xref rid="B35" ref-type="bibr">2011</xref>; Keeter, <xref rid="B44" ref-type="bibr">2012</xref>; Massey and Tourangeau, <xref rid="B52" ref-type="bibr">2013</xref>), but the task is complex: new interviewing methods will only be adopted if they lead to high quality data (accurate responses, and response and completion rates comparable to or better than those in other modes) and to respondents satisfied with their experience.</p><p>One new interviewing technology that has been proposed to promote high quality data&#x02014;as measured by disclosure of sensitive information and (presumably more) honest responding&#x02014;uses animated virtual humans to ask questions and capture responses (Lucas et al., <xref rid="B49" ref-type="bibr">2014</xref>; see also DeVault et al., <xref rid="B21" ref-type="bibr">2014</xref>; Gratch et al., <xref rid="B33" ref-type="bibr">2014</xref>). The promise is that virtual interviewers can promote rapport and engagement with participants while simultaneously providing a feeling of safety and anonymity that is much more difficult to achieve with a human interviewer, and at the same time allowing users to display (and even learn to improve) the social cues they display in interaction with humans (Baur et al., <xref rid="B6" ref-type="bibr">2013</xref>). And some of the findings are promising: Lucas et al. (<xref rid="B49" ref-type="bibr">2014</xref>) found that people in a semi-structured clinical health screening interview disclosed more sensitive information in open-ended responses to a virtual interviewer they believed was automated than to one that was clearly operated by a human. von der P&#x000fc;tten et al. (<xref rid="B76" ref-type="bibr">2011</xref>) found that a more talkative interviewing agent led students to reveal more personal information and to produce more words in answering some open-ended questions on love and relationships.</p><p>The evidence on how virtual interviewers might affect responses in surveys that produce social science and government data, on the other hand, is less promising with respect to disclosure. The one study thus far (Lind et al., <xref rid="B48" ref-type="bibr">2013</xref>) focused on responses to questions about sensitive and potentially embarrassing topics (alcohol and drug use, sexual behavior) and questions about personal behaviors (exercise, religious attendance); such questions can lead at least some respondents to answer in ways that present themselves in a more positive light in survey interviews where human interviewers ask the questions compared to when a computer presents textual or spoken questions (Tourangeau and Smith, <xref rid="B72" ref-type="bibr">1996</xref>; Turner et al., <xref rid="B73" ref-type="bibr">1998</xref>; Kreuter et al., <xref rid="B47" ref-type="bibr">2008</xref>). The finding was that automation did increase disclosure relative to a human interviewer, but only with the audio-only (no facial representation) interface; there were few if any differences in responses to the virtual interviewers relative to a human interviewer (Lind et al., <xref rid="B48" ref-type="bibr">2013</xref>).</p><p>Here we explore how virtual interviewers affect answers to the kinds of questions about facts and behaviors (e.g., &#x0201c;How many bedrooms are there in your house?&#x0201d; &#x0201c;Last week did you do any work for pay?&#x0201d;) that are especially common in survey interviews that produce official statistics and that, in most cases, are not particularly threatening or embarrassing to answer. Because these questions generally concern non-sensitive, mundane topics, we are not focused on how virtual human interviewers might affect disclosure. Instead, we explore how and whether virtual human interviewers promote conscientious task performance&#x02014;accurate survey responding, which depends on comprehending the questions in the way the survey designers intended&#x02014;and respondent engagement in these particular kinds of interviews. In our experiment we varied two features (among the many other potentially manipulable features of a virtual survey interviewer, see Lind et al., <xref rid="B48" ref-type="bibr">2013</xref>)&#x02014;the interviewer's <italic>dialog capability</italic> and <italic>facial animation&#x02014;</italic>and explored whether they have independent or compound effects.</p><sec><title>Background</title><p>The kinds of survey interviews we examine here have particular features that distinguish them from other kinds of interaction (Schaeffer, <xref rid="B60" ref-type="bibr">1991</xref>; Houtkoop-Steenstra, <xref rid="B37" ref-type="bibr">2000</xref>; Schober and Conrad, <xref rid="B62" ref-type="bibr">2002</xref>), as well as from other kinds of interviews. The survey interview is an interactive situation in which (usually) the interviewer, as a representative of the survey designers (researchers), initiates the dialog and &#x0201c;drives&#x0201d; the interaction according to a script (Suchman and Jordan, <xref rid="B70" ref-type="bibr">1990</xref>), asking the respondent questions (that usually specify the answer categories) about her opinions and behaviors.</p><p>This kind of standardized wording and administration procedure is intended to make responses comparable across interviews. In the most strictly standardized interviews, interviewers are required to ask questions exactly as scripted and use only &#x0201c;neutral probes&#x0201d; like &#x0201c;Let me repeat the question&#x0201d; or &#x0201c;Whatever it means to you&#x0201d; if respondents say anything that isn't an acceptable answer (e.g., something other than a response option included in the question), so as to ensure that all respondents receive the same stimulus and to avoid the possibility that interviewers will bias responses (Fowler and Mangione, <xref rid="B30" ref-type="bibr">1990</xref>). This can lead to perverse interactions in which interviewers thwart respondents' efforts to understand what they are being asked by refusing to provide the clarification that respondents seek (Suchman and Jordan, <xref rid="B70" ref-type="bibr">1990</xref>), and in which interviewers violate ordinary norms of conversation by failing to &#x0201c;ground&#x0201d; the meaning of utterances they themselves have produced (Schober and Conrad, <xref rid="B62" ref-type="bibr">2002</xref>).</p><p>Analyses of these kinds of survey interviews demonstrate that respondents can misinterpret ordinary expressions in questions (like &#x0201c;bedroom&#x0201d; and &#x0201c;work for pay&#x0201d;)&#x02014;that is, interpret them differently than the survey designers intend&#x02014;much more often than one might think (Conrad and Schober, <xref rid="B16" ref-type="bibr">2000</xref>), because the mapping or &#x0201c;fit&#x0201d; between their circumstances and the question concepts may not be straightforward (consider someone whose room originally designed as a den is being used as a bedroom, or whose freelance work included pay-in-kind). This is particularly a problem when interviews are strictly standardized; in more collaborative or &#x0201c;conversational&#x0201d; interviews, where interviewers and respondents work together to make sure respondents understand questions as intended (e.g., Schober and Conrad, <xref rid="B61" ref-type="bibr">1997</xref>; Conrad and Schober, <xref rid="B16" ref-type="bibr">2000</xref>), respondents generally interpret questions much more accurately. The best response accuracy, overall, seems to result not only when respondents request clarification if they believe they need it (&#x0201c;What do you mean by work for pay exactly?&#x0201d;), but when interviewers can also volunteer clarification when they believe respondents need it (Schober et al., <xref rid="B68" ref-type="bibr">2004</xref>).</p><p>When designing a virtual interviewer for these kinds of surveys, a key consideration is, therefore, which features will best help respondents understand the questions as they are intended. Based on what is known about respondent comprehension in human-administered interviews, a virtual interviewer that can clarify question meaning when explicitly asked to do so and when it determines the respondent would better understand the question if its meaning were clarified&#x02014;what we will call here a virtual interviewer with greater <italic>dialog capability</italic>&#x02014;should, in principle, lead to more accurate comprehension. Whether this is actually the case with a virtual interviewer has not been demonstrated. Evidence from other automated implementations of survey interviews suggests that it could be the case, but it is not a foregone conclusion that it will be. For example, respondents' accuracy in a text-based web survey (Conrad et al., <xref rid="B18" ref-type="bibr">2007</xref>) and in a (wizarded) spoken dialog survey system (Ehlen et al., <xref rid="B22" ref-type="bibr">2007</xref>) improves when the system can provide clarification after a long period of inactivity or silence, but it does not improve in conditions where the only way to obtain clarification is to explicitly request it.</p><p>Whether high dialog capability interviewing systems with a facial representation will similarly promote comprehension is unclear. The addition of a face to the interface could make respondents even more reluctant to request clarification about ordinary words like &#x0201c;bedroom&#x0201d; and &#x0201c;job,&#x0201d; as they sometimes seem to be with human interviewers (Schober et al., <xref rid="B68" ref-type="bibr">2004</xref>). Or, on the other hand, it could make them think the automated interviewer has greater agency and capabilities, and is thus better positioned to engage in clarification dialog. Because users' attributions about animated agents are likely to vary depending on the characteristics of the face&#x02014;both static and dynamic (e.g., McDonnell et al., <xref rid="B55" ref-type="bibr">2012</xref>; Piwek et al., <xref rid="B59" ref-type="bibr">2014</xref>)&#x02014;one might expect that survey response accuracy could be affected by how an animated virtual interviewer is visually implemented: survey respondents may evaluate the agent's competence and its likelihood of being able to provide useful clarification as greater when it behaves in a more human-like way. That is, they might assume that a more human-like face on a virtual interviewer means that the interviewer will comprehend requests for clarification better, and that the interviewer may better perceive the <italic>respondent's</italic> paralinguistic and facial displays of need for clarification (Schober et al., <xref rid="B66" ref-type="bibr">2012</xref>).</p></sec><sec><title>Hypotheses</title><p>In the study reported here, we test the following hypotheses about how a virtual survey interviewer's dialog capability and facial characteristics affect respondents' comprehension (as measured by the accuracy of their answers&#x02014;our primary measure of task success). We also test how these factors affect respondents' social engagement with the interviewer, as measured by their behavioral displays as well as their subjective assessments of the interviewer. The facial characteristic that our hypotheses focus on is motion or <italic>facial animation</italic>: whether the face moves in a more or less human-like way, that is, with more or fewer channels of motion. We examine facial animation because this strikes us an attribute that is particularly likely to affect respondents' interpretation of a virtual interviewer's humanness; this is consistent with evidence in other task contexts that users interpret an embodied agent's intentions based more on audio and animation than on the render style of the character (McDonnell et al., <xref rid="B55" ref-type="bibr">2012</xref>).</p><sec><title>Hypotheses about comprehension</title><disp-quote><p><italic>Hypothesis 1</italic>: <italic>Dialog capability and comprehension</italic>. A virtual interviewer with greater dialog capability will improve respondents' comprehension of survey questions, particularly when the fit between terms in the survey questions and the circumstances respondents are answering about is not straightforward.</p></disp-quote><p>This hypothesis will be supported to the extent that respondents treat a virtual interviewer with high dialog capability as better able than a low-dialog-capability virtual interviewer to interpret (1) their explicit requests for clarification and (2) indirect evidence of comprehension difficulty, both spoken and visual. If dialog capability affects comprehension in this way, its effect should be measurable both by response accuracy and by the number of requests for clarification. The basic mechanism is that more clarification should correct more misconceptions and resolve more ambiguities; the effect of dialog capability should be most evident when comprehension problems of this sort are most frequent, i.e., when the virtual interviewer asks questions about concepts that correspond in an ambiguous way to respondents' circumstances or whose definitions run counter to respondents' intuitions. We manipulate this experimentally in the study reported here.</p><p>The evidence to date that evaluates the effect of virtual survey interviewers on the quality of responses does not provide evidence about whether dialog capability works the same way or to the same extent with human and virtual interviewers. For example, while the Lind et al. (<xref rid="B48" ref-type="bibr">2013</xref>) study concerned survey interviews, the authors did not design the virtual interviewers to provide clarification; moreover the interaction was not entirely spoken: the interviewing agents asked questions orally but respondents answered by clicking or typing. If clarification does not work the same way&#x02014;if respondents don't solicit or interpret clarification in the same way&#x02014;with virtual interviewers in a spoken dialog interview as they do with human interviewers, the hypothesis will not be supported. This could occur if, for example, respondents do not treat the virtual interviewer as conversationally competent&#x02014;which might be affected by the interviewer's facial animation.</p><disp-quote><p><italic>Hypothesis 2</italic>: <italic>Facial animation and comprehension</italic>. A virtual interviewer with more facial animation will improve respondents' comprehension of survey questions.</p></disp-quote><p>This hypothesis will be supported if survey respondents attend better or try harder at the survey response task when an interviewer seems more human-like, which can result from a virtual agent's increased motion (Hyde et al., <xref rid="B39" ref-type="bibr">2013</xref>; Piwek et al., <xref rid="B59" ref-type="bibr">2014</xref>). The evidence is that perceiving another person's facial motion <italic>can</italic> improve at least some kinds of task success. For perceptual tasks, for example, people tend to be better at detecting a speaker's identity when presented with a moving than a static face (see Xiao et al., <xref rid="B78" ref-type="bibr">2014</xref>, for a review), and they can comprehend speech even in noisy conditions better with facial (especially mouth) motion cues than without (Alexanderson and Beskow, <xref rid="B1" ref-type="bibr">2014</xref>). In avatar-mediated communication, participants are better able to detect truth and deception when an avatar has realistic eye motion (Steptoe et al., <xref rid="B69" ref-type="bibr">2010</xref>).</p><p>On the other hand, in a survey interview setting where the measure was disclosure of sensitive information rather than comprehension accuracy, Lind et al. (<xref rid="B48" ref-type="bibr">2013</xref>) found less disclosure to a high-motion virtual interviewer than to a low-motion interviewer for some survey questions, and no difference in disclosure for others. To the extent that these disclosure findings are relevant to comprehension and response accuracy for non-sensitive survey questions, increased facial motion in a virtual interviewer may not improve survey task performance, and this hypothesis will not be supported.</p><disp-quote><p><italic>Hypothesis 3: Interactive effects of facial animation and dialog capability on comprehension</italic>. A virtual interviewer with more facial animation may improve respondents' comprehension of survey questions particularly when the interviewer has greater dialog capability. To put it another way, a virtual interviewer's dialog capability may improve comprehension particularly when the interviewer's facial animation is consistent with greater dialog competence.</p></disp-quote><p>If a virtual interviewer's greater facial animation suggests that it has greater dialog competence, respondents may be particularly more likely to seek clarification (explicitly request it) or to provide indirect evidence of their need for clarification (paralinguistic or facial), and thereby comprehend and answer more accurately, than if an interviewer has less facial animation. If so, this would predict an interaction: greater clarification-seeking or evidence of need for clarification, and thus improved response accuracy, with a high-animation agent in a high-dialog-capability condition.</p><p>On the other hand, greater facial animation could lead to unrealistic expectations that the agent's dialog competence is fully human, which could subsequently conflict with the agent's actual abilities; in this case, greater facial animation could, paradoxically, lead to poorer comprehension if the respondent relies solely on the interviewer to diagnose need for clarification. One could also imagine other interactive effects: an interviewer with low facial animation might lead users to <italic>under</italic>estimate the dialog capability of high-dialog-capability agents, and thus request clarification or produce indirect evidence of need for clarification less often than would be optimal.</p><p>Although hypotheses about interactive effects of a virtual interviewer's dialog capability and facial animation have not been tested before, the plausibility of such effects is strengthened by the finding that survey respondents in face-to-face interviews produce more paralinguistic displays of need for clarification (speech disfluencies) and avert their gaze more often for unreliable answers in high-dialog-capability (conversational) than low-dialog-capability (strictly standardized) interviews (Schober et al., <xref rid="B66" ref-type="bibr">2012</xref>). Of course, human interviewers have high facial animation in the sense we are exploring here, unless their facial mobility is impaired from neurological illness or cosmetic interventions, and yet when they conduct standardized interviews they are required to restrict their ordinary dialog capability; so a mismatch between facial animation and dialog capability is not unusual in human-administered survey interviews. On the other hand, if comprehension in surveys depends mostly on the conceptual content conveyed by dialog, the interviewer's facial animation will not interact with dialog capability in affecting respondents' comprehension.</p></sec><sec><title>Hypotheses about engagement</title><p>Independent of comprehension or clarification-seeking behavior, a virtual interviewer's dialog capability and facial animation could have independent or interactive effects on survey respondents' engagement with the interview, as evidenced by their social behaviors during the interaction (e.g., time spent looking at the virtual interviewer, nods and verbal acknowledgments, and smiles) and by how they experience the interview subjectively.</p><p>Respondents' engagement in survey interviews&#x02014;their involvement, attentiveness, and conscientiousness&#x02014;is critical for obtaining accurate data. But respondents can be less engaged in the interview task than would be desirable, perhaps because most do not ask to be interviewed (the researchers invite them via an interviewer). In conventional survey modes, evidence of respondents' lack of engagement can be seen in their terminating an interview before it is completed (see Peytchev, <xref rid="B58" ref-type="bibr">2009</xref> for a discussion of breakoffs in online questionnaires) and in their least-effort &#x0201c;satisficing&#x0201d; as they answer questions, for example selecting the same response option again and again in a battery of questions (e.g., Chang and Krosnick, <xref rid="B11" ref-type="bibr">2010</xref>). Our focus here is on respondents' behavioral displays of engagement during the course of a virtual interview&#x02014;their gaze, their spoken and visual acknowledgments, and their smiles&#x02014;and their reported post-interview assessments of their interview experience.</p><p>With this focus, we test the following hypotheses:</p><disp-quote><p><italic>Hypothesis 4: Dialog capability and engagement</italic>. A virtual interviewer whose interaction is more like everyday conversation&#x02014;who can clarify the questions&#x02014;will engage respondents more than a virtual interviewer with low dialog capability.</p></disp-quote><p>One might expect that when survey respondents interact with a virtual interviewer with more human-like capabilities they will behave more as they do in ordinary conversation: they will look at their interlocutor more, acknowledge their understanding more (nod, produce backchannels like &#x0201c;okay&#x0201d;), display social cues (smile), and rate the interaction as more positive. To our knowledge this has not been examined directly, but accounts of frustration experienced by respondents whose standardized interviewers are prevented from providing clarification (e.g., Suchman and Jordan, <xref rid="B70" ref-type="bibr">1990</xref>) are consistent with this hypothesis.</p><disp-quote><p><italic>Hypothesis 5: Facial animation and engagement</italic>. A virtual interviewer whose facial movement is more human-like will engage respondents more than a virtual interviewer with low facial animation.</p></disp-quote><p>From other domains of interaction with virtual agents, the evidence is that people judge agents with more (bodily) motion as more acceptable and human (Piwek et al., <xref rid="B59" ref-type="bibr">2014</xref>), and that realistic characters that move more are judged more positively (Hyde et al., <xref rid="B39" ref-type="bibr">2013</xref>). The benefits of more human-like behavior may well extend to the survey context: Conrad et al. (<xref rid="B15" ref-type="bibr">2013</xref>) demonstrated that people invited to participate in (human-administered) telephone survey interviews were more likely to agree to participate when the interviewers spoke less robotically (with more disfluencies) during the invitation interaction. And Foucault Welles and Miller (<xref rid="B29" ref-type="bibr">2013</xref>) demonstrated that respondents in face-to-face (human-administered) survey interviews reported feeling greater rapport (which is presumably related to their feelings of engagement) when <italic>interviewers</italic> nodded and smiled more, and when they gazed at respondents' faces less.</p><disp-quote><p><italic>Hypothesis 6: Interactive effects of facial animation and dialog capability on engagement</italic>. A virtual interviewer with more facial animation may increase respondents' engagement particularly when the interviewer has greater dialog capability.</p></disp-quote><p>Any effects of dialog capability and facial animation on respondents' display of social cues or assessment of the interviewer could be independent, or they could interact. The same range of possible interaction effects exists for measures of engagement as for comprehension. The combination of low dialog capability and low facial animation could lead to particularly unengaging or alienating interaction. High facial animation could lead to unrealistic expectations about an interviewer's dialog capability, which when thwarted could lead respondents to be <italic>less</italic> engaged with the interviewer. Low facial animation could lead to underestimation of a high dialog capability interviewer's competence, which could lead respondents to attend less fully to or disengage with the interviewer.</p></sec></sec></sec><sec sec-type="materials|methods" id="s2"><title>Materials and methods</title><p>Our strategy in this study was to bring participants to our laboratory to respond to 12 questions about housing, work and purchases taken from US government surveys, which they answered on the basis of scenarios describing fictional circumstances. This allowed us to directly assess the accuracy of their responses&#x02014;which also measures the extent to which their comprehension of the terms in the survey questions fits what the official definitions of those terms would require. Participants (respondents) were randomly assigned to be interviewed by a (Wizard-of-Oz) interviewing agent with either high or low facial animation (many channels/points of motion vs. few) and high or low dialog capability (conducting interviews in either a collaborative or strictly standardized style). For each respondent, half the fictional scenarios were designed to map onto the survey questions in a straightforward way and half in a complicated way. Thus, the experimental design was 2 &#x000d7; 2 &#x000d7; 2.</p><p>Although having respondents answer about fictional scenarios as opposed to about their own lives reduces ecological validity, it has the advantage of allowing direct assessment of accuracy of comprehension during the interviews. In other studies with human interviewers we have used post-interview self-administered questionnaires (Suessbrick et al., <xref rid="B71" ref-type="bibr">2000</xref>; Schober et al., <xref rid="B66" ref-type="bibr">2012</xref>) and human-administered re-interviews (Conrad and Schober, <xref rid="B16" ref-type="bibr">2000</xref>; Suessbrick et al., <xref rid="B71" ref-type="bibr">2000</xref>) as alternate (less direct) methods for assessing comprehension and survey response accuracy, under the logic that response change when respondents are provided with a standard definition of a survey term is likely to reflect the correction of a misinterpretation in the original interview; the findings in those studies are highly consistent with the findings produced when responses are based on fictional scenarios, and so in the current study we use fictional scenarios. The questions and scenarios in the current study are the same as those used in previous laboratory studies of telephone interviews (Schober and Conrad, <xref rid="B61" ref-type="bibr">1997</xref>; Schober et al., <xref rid="B68" ref-type="bibr">2004</xref>) and of online text- and speech-based interviewing systems (Conrad et al., <xref rid="B18" ref-type="bibr">2007</xref>; Ehlen et al., <xref rid="B22" ref-type="bibr">2007</xref>). Although the participant sample and time frame for this experiment make a comparison with those studies not entirely parallel, they provide relevant context for evaluating respondents' performance with a virtual interviewer in the current study.</p><sec><title>Experiment materials</title><sec><title>Survey questions</title><p>The 12 survey questions were adapted to apply to the fictional scenarios that respondents would be answering about, rather than about the respondent's own circumstances: four questions about employment from the US Current Population Survey (e.g., &#x0201c;Last week, did Chris do any work for pay?&#x0201d; filling in the name of the fictional character Chris in the question &#x0201c;Last week, did you do any work for pay?&#x0201d;), four questions about purchases from the Current Point of Purchase Survey (e.g., &#x0201c;Has Kelly purchased or had expenses for household furniture?&#x0201d;), and four questions about housing from the Consumer Price Index Housing Survey (e.g., &#x0201c;How many bedrooms are there in this house?&#x0201d;). Each question had a corresponding official definition for its key concepts developed by the sponsoring agency. For example, for the question &#x0201c;Has Kelly purchased or had expenses for household furniture,&#x0201d; the official definition of household furniture is this:</p><disp-quote><p>Tables, chairs, footstools, sofas, china cabinets, utility carts, bars, room dividers, bookcases, desks, beds, mattresses, box springs, chests of drawers, night tables, wardrobes, and unfinished furniture. Do not include TV, radio, and other sound equipment, lamps and lighting fixtures, outdoor furniture, infants' furniture, or appliances (US Bureau of the Census and US Bureau of Labor Statistics, <xref rid="B74" ref-type="bibr">1993</xref>).</p></disp-quote><p>(Supplementary Table <xref ref-type="supplementary-material" rid="SM1">1</xref> includes all questions and the official definitions relevant to each question).</p><p>The questions were ordered for the experiment to correspond with the order in which they appeared in the survey from which they were drawn, and counterbalanced across domains for different respondents to make sure that any effects observed in the experiment could not be attributed to the order in which the virtual interviewer asked about the different domains. So one respondent would answer purchase questions followed by housing questions followed by employment questions, another would answer housing questions followed by employment questions followed by purchase questions, etc.</p></sec><sec><title>Respondent scenarios</title><p>Fictional scenarios on the basis of which respondents were to answer the questions were assembled into paper packets, with one page per scenario. In actual surveys respondents most often answer based on their recall and self-assessment; using scenarios is more similar to situations when respondents answer by consulting their personal records, and, more importantly, allows us to isolate and focus on comprehension&#x02014;there is no autobiographical recall involved when respondents answer based on scenarios. For factual questions about respondents' behaviors or circumstances, the outcome of each exchange&#x02014;an answer to a survey question&#x02014;is either accurate or not (e.g., the respondent either has or has not done any work for pay in the last week). In principle this could be independently assessed if researchers were to have independent evidence about the respondent's circumstances (e.g., trustworthy records from the respondent's place of employment), but of course, in many cases (e.g., for many personal behaviors and for respondents' opinions) there is no independently verifiable evidence about the accuracy of responses.</p><p>The scenarios, which were not seen by the interviewing Wizard during the interview, consisted of work descriptions, purchase receipts, and floor plans. Two alternate scenarios were created for each question, one describing situations that mapped onto questions and the corresponding official definitions in a straightforward way (&#x0201c;straightforward mappings&#x0201d;) and one describing situations that mapped onto questions and official definitions in a complicated way (&#x0201c;complicated mappings&#x0201d;)&#x02014;for which respondents might well need clarification in order to answer the question in a way that fit the definition. For example, for the question about household furniture, the straightforward scenario was a receipt for the purchase of an end table. The complicated scenario was a receipt for the purchase of a floor lamp. The official definition&#x02014;which was not part of the materials given to the respondents, but could only be presented orally by a high-dialog-capability virtual interviewer&#x02014;clarified that for the purposes of this survey, a floor lamp is not to be counted as a household furniture purchase, and thus the answer to this question should be &#x0201c;no.&#x0201d; (The answer for the straightforward scenario should be &#x0201c;yes,&#x0201d; as an end table counts as a furniture purchase).</p><p>The selection of these scenarios thus allowed direct evaluation of whether the respondent had comprehended the question in a way that fit the official definitions. A respondent who answers &#x0201c;yes&#x0201d; to the household furniture question with a floor lamp receipt, or &#x0201c;no&#x0201d; with an end table receipt, is not interpreting the question as the survey designers intended; these responses can be classified as incorrect.</p><p>Scenario packets were assembled for each respondent that included half (6) straightforward and half (6) complicated scenarios, with two straightforward and two complicated scenarios per domain (employment, purchases, housing). The orderings of mappings were counterbalanced across respondents, such that the particular combination of straightforward and complicated mappings for one respondent was the complement of the combination for another. Across all respondents, both straightforward and complicated scenarios were presented equally often and in different orders, both so that the interviewing Wizard could not anticipate which scenario a particular respondent was encountering and so that any effects observed in the experiment could not be attributed to a particular sequence of mappings.</p></sec><sec><title>Additional interviewer utterances</title><p>In addition to the survey questions and the full definitions of relevant terms in the questions, all other allowable interviewer utterances in low and high dialog capability interviews were scripted. These included several introductions of the interview (e.g., &#x0201c;Hello, my name is Derek and today I will be asking you a few questions about housing, jobs and purchases.&#x0201d;), pre-interview practice material, neutral probes (e.g., &#x0201c;Is that a yes or a no?&#x0201d;), partial definitions (just the text that resolves the ambiguity in the corresponding complicated scenario), clarification offers (&#x0201c;It sounds like you're having some trouble. Can I give you a definition that might help?&#x0201d;), utterances to manage the dialog (e.g., &#x0201c;Yes,&#x0201d; &#x0201c;No,&#x0201d; &#x0201c;Please wait one moment&#x0201d;), and utterances to run the experimental session (&#x0201c;Please turn to the next page of your packet&#x0201d;; &#x0201c;I am going to ask the research assistant to help you. Just a minute please&#x0201d;). Supplementary Table <xref ref-type="supplementary-material" rid="SM2">2</xref> lists the full set of additional scripted utterances.</p></sec></sec><sec><title>Developing the virtual interviewers</title><p>The virtual interviewers for the four experimental conditions were created using famous3D's ProFACE video software (version 2.5) to make variants of a single 3D model of a head. We first video- and audio-recorded a human interviewer (a male graduate student in survey methodology who spoke American English) administering all survey questions, prompts, clarifications, and additional interviewer utterances, with 21 green and blue dots affixed to his face to capture 21 different motion channels (forehead, outer and inner brows, furrow, upper eyelids, region below the eyes, cheeks, right and left sides of nose, right and left lower lips, chin, etc.). With the ProFace software we captured his facial motion and mapped it to a face template, which could then be projected onto one of ProFace's existing models (Derek, in our case; see Figure <xref ref-type="fig" rid="F1">1</xref>) either using all motion channels (for the high facial animation conditions) or a subset (for the low facial animation conditions). All audio files used in the low dialog capability conditions were also used in the high dialog capability conditions; there were, of course, extra speech files (and accompanying video) for high dialog capability conditions (e.g., offers of clarification).</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>The Derek model that formed the basis of the four virtual interviewers</bold>.</p></caption><graphic xlink:href="fpsyg-06-01578-g0001"/></fig><p>Note that because all four virtual interviewers were based on the same head model, the interviewer's base level of visual realism or naturalism, which can affect how users judge and respond to virtual agents in other task contexts (e.g., Baylor and Kim, <xref rid="B7" ref-type="bibr">2004</xref>; MacDorman and Ishiguro, <xref rid="B51" ref-type="bibr">2006</xref>; Gong, <xref rid="B32" ref-type="bibr">2008</xref>; MacDorman et al., <xref rid="B50" ref-type="bibr">2009</xref>), was the same across all four conditions. In a job interview training task, Baur et al. (<xref rid="B6" ref-type="bibr">2013</xref>) found that interviewees criticized their interviewer as not looking realistic enough; our interviewer has a level of realism that reflects the photographic origins of the model, and is more realistic than the more cartoon-like survey interviewer in Lind et al. (<xref rid="B48" ref-type="bibr">2013</xref>), but there is simply not enough evidence in survey tasks about the optimal levels of realism for a virtual survey interviewer.</p><p>Also note that because the interviewers differed behaviorally on more than one feature, any effects on respondents must be attributed to bundles of behavioral features rather than individual features.</p><sec><title>Facial animation</title><p>Table <xref ref-type="table" rid="T1">1</xref> summarizes the major features of motion in the high and low facial animation interviewers. For the low facial animation conditions, seven motion channels were projected onto the Derek model: chin, left and right lower lips, left and right corners of mouth, and left and right peaks of lip. The low animation interviewer head and face do not move, the eyes do not blink, and the mouth does not change shape as the interviewer speaks&#x02014;it just opens and closes.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Facial animation features of virtual interviewers</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="left" rowspan="1" colspan="1"><bold>Low facial animation</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>High facial animation</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Head moves</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes, even when &#x0201c;listening&#x0201d;</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Face moves</td><td valign="top" align="left" rowspan="1" colspan="1">Only mouth</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Eyes move</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Eyes blink</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Mouth movement</td><td valign="top" align="left" rowspan="1" colspan="1">Only opens and closes during speech, but does not change shape</td><td valign="top" align="left" rowspan="1" colspan="1">Mouth forms appropriate shapes for sounds being produced</td></tr></tbody></table></table-wrap><p>For high facial animation conditions, in addition to the 21 channels of captured motion the interviewer's head and face move (applying ProFace's jitter function) at all times (even while waiting for responses, to give the appearance of listening), and his eyes blink. The interviewer's mouth forms appropriate shapes for the sounds he is producing; to improve the correspondence between the interviewer's mouth movements and speech, additional keyframes were added by hand beyond the captured motion at a fine level of granularity, with particular combinations of motions for different consonants and vowel sounds in the recordings, based on the judgments of an animator. Finally, stationary shoulders were added to make the head movements look more realistic.</p><p>See Supplemental Data for video examples of low and high facial animation introductions to the interview (Videos <xref ref-type="supplementary-material" rid="SM5">1</xref>, <xref ref-type="supplementary-material" rid="SM6">2</xref>) and for low and high facial animation variants of Purchases Question 3 (Videos <xref ref-type="supplementary-material" rid="SM7">3</xref>, <xref ref-type="supplementary-material" rid="SM8">4</xref>).</p></sec><sec><title>Dialog capability</title><p>Table <xref ref-type="table" rid="T2">2</xref> summarizes the major features of dialog capability in the high and low dialog capability interviewers. These were implemented by an experimenter behind the scenes (the Wizard) following protocols for which interviewer files (questions, neutral probes, definitions, etc.) were to be played to respondents in which sequence and in response to which respondent behaviors (see Wizard protocols below). In all cases the virtual interviewers presented the same questions, and (from the respondents' perspective) they could comprehend and register spoken answers.</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p><bold>Dialog capability features of virtual interviewers</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="left" rowspan="1" colspan="1"><bold>Low dialog capability</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>High dialog capability</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Reads question as worded</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Understands spoken answers</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Repeats question if asked</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Understands explicit requests for clarification</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Provides clarification when explicitly requested</td><td valign="top" align="left" rowspan="1" colspan="1">No: presents neutral probe (e.g., &#x0201c;Whatever it means to you&#x0201d;; &#x0201c;Let me repeat the question&#x0201d;)</td><td valign="top" align="left" rowspan="1" colspan="1">Yes: reads definition</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Offers clarification when it seems needed (based on respondent's verbal and visual behavior)</td><td valign="top" align="left" rowspan="1" colspan="1">No</td><td valign="top" align="left" rowspan="1" colspan="1">Yes</td></tr></tbody></table></table-wrap><p>The low dialog capability protocol was to administer a strictly standardized interview, as implemented in previous studies in this line of research (e.g., Schober and Conrad, <xref rid="B61" ref-type="bibr">1997</xref>; Schober et al., <xref rid="B68" ref-type="bibr">2004</xref>). The virtual interviewer presented the questions exactly as worded and could repeat questions if asked, but if a respondent explicitly requested clarification the interviewer would only provide a neutral probe (of the Wizard's choosing, just as in human-administered standardized interviews; see Video <xref ref-type="supplementary-material" rid="SM9">5</xref> in Supplementary Materials for an example).</p><p>The high dialog capability protocol was to administer &#x0201c;conversational&#x0201d; interviews, again as in Schober and Conrad (<xref rid="B61" ref-type="bibr">1997</xref>). After reading the question exactly as worded, the (wizarded) interviewer (a male graduate student) did whatever he thought was needed to make sure that the respondent had interpreted the question as intended&#x02014;to &#x0201c;ground&#x0201d; the meaning of terms in survey questions, to use Clark and colleagues' term (e.g., Clark and Wilkes-Gibbs, <xref rid="B14" ref-type="bibr">1986</xref>; Clark and Schaefer, <xref rid="B13" ref-type="bibr">1987</xref>; Clark, <xref rid="B12" ref-type="bibr">1996</xref>). In other words, the interviewer's task was to make sure that the respondent's interpretation fit the official definition. This included not only providing the full official definition if the respondent explicitly requested it but also offering clarification if the interviewer (Wizard) got the sense that the respondent might need it (see Video <xref ref-type="supplementary-material" rid="SM10">6</xref> in Supplementary Materials for an example). Given the nature of the video files and wizarding protocols, this implementation of conversational interviewing is not as fully flexible as human interviewers can provide, because our virtual interviewers could not provide fully tailored partial definitions or improvise unscripted dialog, but it is on the most flexible end of the continuum (see Schober et al., <xref rid="B68" ref-type="bibr">2004</xref>).</p></sec><sec><title>Pre-study: verifying distinctiveness of virtual interviewers</title><p>In order to increase our confidence that we had successfully manipulated what we hoped to in creating the virtual interviewer videos, we collected ratings of all 130 video clips in the experiment, both low and high facial animation versions. The clips included all questions, probes, definitions, and introductions to be used by both the low and high dialog capability virtual interviewers. Thirteen raters (11 female, two male; mean age 28.8, ranging from 24 to 34; all with bachelors' degrees, six graduate students in survey methodology) each rated 65 high- and low-animation video clips in one of two group viewing sessions. For each clip, each rater judged the virtual interviewer on a ten point scale for warmth (&#x0201c;How warm was Derek, with 0 being Not At All Warm and 10 being Very Warm?&#x0201d;), naturalness (&#x0201c;How natural was Derek, with 0 being Not At All Natural and 10 being Very Natural?&#x0201d;), and similarity to an actual interviewer (&#x0201c;To what degree did Derek seem like an actual interviewer, with 0 being Not At All Like An Interviewer and 10 being Very Much Like An Interviewer?&#x0201d;).</p><p>The ratings confirmed that the high facial animation virtual interviewers were, in the aggregate, perceived to be reliably warmer [4.58 vs. 2.78 on the 10-point scale, <italic>F</italic><sub>(1, 12)</sub> = 28.56, <italic>p</italic> &#x0003c; 0.001, &#x003b7;<sup>2</sup> = 0.704], more natural [5.23 vs. 2.95 on the 10-point scale, <italic>F</italic><sub>(1, 12)</sub> = 36.24, <italic>p</italic> &#x0003c; 0.001, &#x003b7;<sup>2</sup> = 0.751], and more like a human interviewer [6.24 vs. 4.36 on the 10-point scale, <italic>F</italic><sub>(1, 12)</sub> = 21.35, <italic>p</italic> = 0.001, &#x003b7;<sup>2</sup> = 0.640] than the low realism versions. The same pattern was observed for most individual clips, though not all. Although none of the ratings reached the top of the 10 point scale, these strongly reliable differences suggested to us that these implementations of virtual interviewers would be suitable for the experiment.</p></sec></sec><sec><title>Wizarding protocols</title><p>The virtual interviewers were controlled by mapping each video file to a key on the computer keyboard using ArKaos VJ software. This allowed the Wizard to present the next relevant file to the respondent by pressing a key, according to the relevant protocol for high or low dialog capability interviewing (see Table <xref ref-type="table" rid="T3">3</xref> for the Wizard's decision rules). Using the VJ software allowed seamless presentation of the video clips, so that the virtual interviewer appeared to the respondent to be acting on its own. The Wizard sat in a control room with a one-way mirror and live video feed view of the respondent. The control computers were set up so that the Wizard could view the respondent from a frontal overhead position and could also see the video file of the virtual interviewer as it was playing for the respondent.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p><bold>Wizard's decision rules</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Low dialog capability</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>High dialog capability</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Give respondent 3 min to familiarize him/herself with packet, and ignore respondent if he/she says he/she is ready</td><td valign="top" align="left" rowspan="1" colspan="1">Give respondent 3 min to familiarize him/herself with packet, but begin interview if respondent says he/she is ready</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Wait 10 s between transition and question clip, despite respondent behavior</td><td valign="top" align="left" rowspan="1" colspan="1">Wait for respondent to look at virtual interviewer before presenting next question clip</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Do not modify presentation of clips based on respondent's gaze or attention</td><td valign="top" align="left" rowspan="1" colspan="1">Stop presenting a clip if respondent stops looking at virtual interviewer</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Send research assistant to help respondent if in trouble</td><td valign="top" align="left" rowspan="1" colspan="1">Use virtual interviewer to assist respondent if in trouble. If not successful send research assistant</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">If respondent seems hesitant or confused, do nothing</td><td valign="top" align="left" rowspan="1" colspan="1">If respondent seems hesitant or confused, then offer help</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">If respondent asks for help, then present neutral probe</td><td valign="top" align="left" rowspan="1" colspan="1">If respondent asks for help not related to scenario, then present neutral probe</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">If respondent asks for help pertaining to scenario, then present entire definition</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">If respondent asks for help with specific mention of key concept, then present partial definition</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">If respondent interrupts virtual interviewer, then finish presenting clip. Wait for respondent to repeat him/herself</td><td valign="top" align="left" rowspan="1" colspan="1">If respondent interrupts virtual interviewer, then present waiting clip and address respondent's concern immediately</td></tr></tbody></table></table-wrap><p>The use of a Wizard allowed us to implement the high and low dialog capability virtual interviewers without programming a full survey dialog system with speech recognition and dialog management, which was beyond the scope of the current study [In other projects we have implemented a standardized survey spoken dialog system for mobile devices (Johnston et al., <xref rid="B43" ref-type="bibr">2013</xref>) and experimented with an automated telephone system that implements conversational interviewing, including modeling respondents' paralinguistic displays of need for clarification (Ehlen et al., <xref rid="B22" ref-type="bibr">2007</xref>)]. Because the same Wizard manipulated the virtual interviewers in this study across all conditions, his detection of and judgments of the meaning of respondents' facial and bodily displays and verbal behavior were likely to be consistent in the different conditions. This means that across the high and low facial animation conditions, the timing of turn transitions (the point at which speakers and listeners trade roles in conversation), which has been shown to affect perceptions of (in particular rapport with) virtual humans (Huang et al., <xref rid="B38" ref-type="bibr">2011</xref>), were deployed based on the same human Wizard judgments, appropriately for either the high or low dialog conditions. Thus, although by necessity the Wizard needed to be informed about respondents' experimental conditions (so that he could deploy the appropriate video files), the particular linguistic and interactive intuitions that the Wizard brought to the experiment did not differ across the conditions.</p></sec><sec><title>Post-interview measures</title><p>After completing the interview, respondents filled out an online questionnaire in which they reported their subjective experience interacting with the virtual interviewer on a number of dimensions (e.g., &#x0201c;How much did you enjoy interacting with Derek?&#x0201d;, &#x0201c;Would you say that Derek acted more like a computer or a person?&#x0201d;, &#x0201c;How often did Derek seem to act on his own?&#x0201d;). They also provided information about their technological experience (&#x0201c;How often, on average, do you use a computer?&#x0201d;) and their demographic and linguistic background (e.g., &#x0201c;Is English your native language?&#x0201d;). The full questionnaire is presented in Supplementary Table <xref ref-type="supplementary-material" rid="SM3">3</xref>.</p></sec><sec><title>Participants</title><p>Seventy-five participants (respondents) were recruited from the local site of the Craig's List online forum (<ext-link ext-link-type="uri" xlink:href="https://annarbor.craigslist.org/">https://annarbor.craigslist.org/</ext-link>) (<italic>n</italic> = 51) and through word of mouth (<italic>n</italic> = 21); for three respondents we do not have records about how they heard about the study.</p><p>Respondents, who were paid $35 for participating, were each randomly assigned to an experimental condition, except for two who were recruited specifically to replace two respondents (one in each high-dialog-capability condition) who expressed suspicion that the virtual interviewer was wizarded (the replaced and replacement respondents were all recruited through Craig's List). This led to a final data set with 18 respondents in three of the four conditions and 19 in the high-dialog-capability-high-facial animation condition.</p><p>In the final data set, the composition of the four groups did not differ reliably in age (<italic>F</italic> &#x0003c; 1), nor in recruitment source (<italic>p</italic>-values for all <bold>X</bold><sup>2</sup>&#x0003e;0.15.) The respondents ranged in age from 18 to 67 years (mean = 36.8); 38 were female and 35 were male. 56.2% of respondents reported being White, 20.5% Black, 16.4% Asian or Pacific Islander, and 5.5% reported being members of other groups. 37.4% of respondents reported their highest level of education as less than a bachelor's degree, 42.5% as a bachelor's degree, and 19.2% as a graduate or professional degree. As a group they were highly computer literate, with 84.9% reporting using a computer 5&#x02013;7 days per week. 89% reported that English was their native language.</p><p>All procedures that respondents followed, and all materials that were presented to them, were reviewed and approved by the University of Michigan IRB-HSBS (Institutional Review Board&#x02014;Health Sciences and Behavioral Sciences).</p></sec><sec><title>Procedure</title><p>Each respondent was escorted to a first room where he or she signed consent forms and was handed the packet of experimental scenarios on the basis of which he or she would be answering survey questions. A research assistant instructed respondents using the following script:</p><disp-quote><p>In this study, you will be asked 12 questions about fictional purchases, housing, and jobs. This interview is not like typical interviews. We will not be asking you about your own experiences but about the information contained in scenarios in this packet, so we can assess the accuracy of your responses. On each page there is one scenario, which corresponds to one question. You should answer each question based only on information in the corresponding scenario. Each scenario is independent of each other, so you should <italic>not</italic> use information from the previous page to answer a subsequent question. Some of the scenarios are dated; consider the date in the packet to be current, rather than responding based on today's date. You will receive additional information about this procedure once the interview begins. Let's enter the room now to start the interview.</p></disp-quote><p>Respondents were then led to a second room, which contained two mounted cameras, a chair, a table, a computer, a monitor displaying the virtual interviewer, a microphone on the table, and (in the high-dialog-capability conditions) a non-functioning web camera trained on the respondent to increase the plausibility that the virtual interviewer could sense the respondent. The room was free of other distractions. If a respondent asked about any of the equipment, the research assistant answered by saying, &#x0201c;I will be happy to answer your questions after the interview.&#x0201d; The research assistant then pointed at the monitor with the virtual interviewer and gave the following instructions:</p><disp-quote><p>You are going to be interviewed by Derek. Derek will speak to you, and you should respond aloud. Please look at Derek when he's speaking to you. Okay?</p><p>When I leave the room, Derek will introduce himself and give you the opportunity to familiarize yourself with the scenario. Please use all the available time to fully acquaint yourself with the entire packet. You may also want to review each scenario before answering its respective question.</p><p>This is a new way to conduct interviews and, therefore, might be a little rough around the edges. Please bear with us if there are any problems. Let me know if you experience any difficulty with the equipment. I am leaving now, but please feel free to knock on the door if you need my help. The interview will begin as soon as I leave the room. Any questions?</p></disp-quote><p>In the high-dialog-capability conditions, the research assistant presented the following additional instructions:</p><disp-quote><p>Please look at Derek when you are ready for the next question. Derek can hear and see you.</p><p>Sometimes, survey questions use ordinary words in unexpected ways. To be sure you understand the question, you may need to ask Derek to clarify particular words so please ask for clarification if you are <italic>at all</italic> unsure about what they mean. In fact, you may need to get clarification from Derek in order to answer accurately. Unlike what happens in some survey interviews, Derek <italic>is</italic> able to help you when you indicate you need help. So you should be sure to ask Derek for clarification if you are at all unsure about what a word means.</p></disp-quote><p>This description of the respondent's role in conversational interviews parallels the additional instructions in Schober and Conrad (<xref rid="B61" ref-type="bibr">1997</xref>).</p><p>The research assistant then left the room and the interview proceeded, starting with a first training question and scenario to familiarize the respondent with the task. The research assistant, who monitored the video and audio of the interview along with the Wizard, was available to enter the room if there were technical difficulties or if the respondent gave evidence of not having understood the instructions (e.g., about turning the page in their scenario packet for each next survey question).</p><p>After the interview, the research assistant escorted respondents to another lab room, where they filled out the on-line post-experiment questionnaire. Finally, they were asked whether they felt they were indeed interacting with a computer (to give them the opportunity to voice any suspicions that the virtual interviewer was wizarded), debriefed about the actual Wizard-of-Oz experiment setup, and paid for their participation.</p><p>The reported analyses are based on the 73 respondents who gave no evidence in the experiment debriefing of suspecting that the virtual interviewer was wizarded. From transcripts of the interviews, we know that no participant ever expressed any suspicion or asked any questions about how the virtual interviewer worked during the interview.</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><sec><title>Comprehension</title><p>To test our Hypotheses 1&#x02013;3 about comprehension, we first focus on response accuracy and then on respondents' and virtual interviewers' clarification behaviors. We adopt conventional thresholds for alpha, with levels of <italic>p</italic> &#x0003c; 0.05 as statistically significant (reliable) and 0.05 &#x0003c; <italic>p</italic> &#x0003c; 0.10 as marginal.</p><sec><title>Response accuracy</title><p>Respondents' comprehension was measured by observing, for each response, whether it matched what the official definition of the survey term would require.</p><p>As Figure <xref ref-type="fig" rid="F2">2</xref> shows, Hypothesis 1 was supported: virtual interviewers with high dialog capability led to significantly greater response accuracy (74.3%) than virtual interviewers with low dialog capability (60.2%), <italic>F</italic><sub>(1, 69)</sub> = 21.69, <italic>p</italic> &#x0003c; 0.001, &#x003b7;<sup>2</sup> = 239. This was entirely driven by the effect of dialog capability on response accuracy for complicated mapping scenarios (50.9% for high dialog capability and 25.9% for low dialog capability interviewers); in contrast, for straightforward mappings there was no effect of interviewer dialog capability on response accuracy (response accuracy was uniformly high in all conditions), as demonstrated by the interaction of mapping by dialog capability <italic>F</italic><sub>(1, 69)</sub> = 15.38, <italic>p</italic> &#x0003c; 0.001, &#x003b7;<sup>2</sup> = 0.182.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Response accuracy (percentage of survey responses that matched what the official definition would require) for straightforward and complicated scenarios (error bars represent SE's)</bold>.</p></caption><graphic xlink:href="fpsyg-06-01578-g0002"/></fig><p>Figure <xref ref-type="fig" rid="F2">2</xref> also shows that, contrary to Hypothesis 2, there was no evidence that the virtual interviewer's facial animation affected response accuracy, <italic>F</italic><sub>(1, 69)</sub> = 0.15, <italic>p</italic> = 0.70, &#x003b7;<sup>2</sup> = 0.002. To further investigate whether there really was no effect of facial animation on response accuracy, we computed a Bayes<sub>10</sub> factor (using the JASP, <xref rid="B42" ref-type="bibr">2015</xref> package) comparing the fit of the data under the null hypothesis (no effect of facial animation) and the alternative (see Jarosz and Wiley, <xref rid="B41" ref-type="bibr">2014</xref> for an account of the underlying logic). An estimated Bayes<sub>10</sub> factor (alternative/null) of 0.193 suggested that the data were 5.18:1 in favor of the null hypothesis, that is, 5.18 times more likely to occur under a model without including an effect of facial animation, rather than a model with it (in comparison, an estimated Bayes factor [alternative/null] for dialog capability is 2.092 in favor of the alternative hypothesis).</p><p>Contrary to Hypothesis 3, and further supporting the interpretation that the virtual interviewer's dialog capability was entirely responsible for response accuracy, is the finding that the interaction between response accuracy and facial animation was not significant, <italic>F</italic><sub>(1, 69)</sub> = 0.006, <italic>p</italic> = 0.94, &#x003b7;<sup>2</sup> = 0.000; the Bayes<sub>10</sub> factor for the interaction between dialog capability and facial animation is 0.386, suggesting that the data are 2.59:1 in favor of the null hypothesis.</p></sec><sec><title>Clarification behaviors</title><p>So that we could examine direct and indirect requests for clarification and their relationship with respondents' comprehension, complete transcripts of the survey question-answer sequences in each interview were created and coded. A coding scheme for all interviewer and respondent moves (see Supplementary Table <xref ref-type="supplementary-material" rid="SM4">4</xref>) was adapted from our previous studies with human interviewers (Schober et al., <xref rid="B68" ref-type="bibr">2004</xref>) that included codes for the behaviors we expected to differ between high- and low-dialog-capability interviews (e.g., offering clarification, providing definitions, providing neutral probes). In order to verify reliability of the coding, the majority of the question-answer sequences (86.6%) were coded again by a different coder; agreement between these two sets of codes was measured with Cohen's kappa, which at 0.988 was &#x0201c;almost perfect&#x0201d; by Everitt and Haye's (<xref rid="B24" ref-type="bibr">1992</xref>, p. 50) characterization.</p><p>Consistent with Hypothesis 1, respondents only ever requested or received clarification in the high dialog capability conditions, and not at all in the low dialog capability conditions. This makes sense because of course any requests with a low-dialog-capability virtual interviewer would be met with a neutral probe (e.g., &#x0201c;Let me repeat the question&#x0201d; or &#x0201c;whatever it means to you&#x0201d;) rather than substantive clarification (e.g., &#x0201c;In this survey we do not include floor lamps as furniture&#x0201d;).</p><p>Also consistent with Hypothesis 1 (see Table <xref ref-type="table" rid="T4">4</xref>), respondents with the high-dialog-capability virtual interviewers explicitly requested clarification more often&#x02014;nearly twice as often&#x02014;for complicated scenarios than for straightforward scenarios, and they correspondingly received clarification more than twice as often for complicated scenarios. The virtual interviewer also was more likely to comment on the respondent's need for clarification for complicated scenarios. Compared to explicitly requesting clarification, respondents indirectly indicated that they were having comprehension trouble (e.g., &#x0201c;I don't know whether to count that or not&#x0201d;) far less frequently, and they did not do this at different rates for different scenario types.</p><table-wrap id="T4" position="float"><label>Table 4</label><caption><p><bold>Percentage of question-answer sequences in which clarification and related speech occurred (SE's in parentheses)</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1"><bold>Scenario mapping</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Effect</bold></th><th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1"><bold>Facial animation</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Effect</bold></th></tr><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>Straight forward</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Complicated</bold></th><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>Low</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>High</bold></th><th rowspan="1" colspan="1"/></tr></thead><tbody><tr><td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Respondent explicit requests for clarification (&#x0201c;What do you mean by &#x02018;furniture&#x02019;?&#x0201d;)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>18.1 (3.7)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>35.2 (5.2)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 35)</sub> = 20.74,</bold><break/>
<bold><italic>p</italic> &#x0003c; 0.001,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.372</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">29.2 (5.9)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">24.1 (5.8)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 35)</sub> = 0.37,<break/>
<italic>p</italic> = 0.55,<break/> &#x003b7;<sup>2</sup> = 0.011</td></tr><tr><td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Respondent implicit requests for clarification (&#x0201c;I don't know whether to count that or not&#x0201d;)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">6.3 (1.8)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">4.4 (1.8)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 35)</sub> = 0.88,<break/>
<italic>p</italic> = 0.354,<break/> &#x003b7;<sup>2</sup> = 0.025</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">4.6 (2.1)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">6.2 (2.1)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 35)</sub> = 0.27,<break/>
<italic>p</italic> = 0.605,<break/> &#x003b7;<sup>2</sup> = 0.008</td></tr><tr><td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Virtual interviewer comments on respondent's confusion (&#x0201c;It sounds like you're having some trouble.&#x0201d;)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>3.6 (1.2)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>8.9 (1.7)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 35)</sub> = 8.08,</bold><break/>
<bold><italic>p</italic> = 0.007,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.188,</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>4.2 (1.6)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>8.4 (1.6)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 35)</sub> = 3.42,</bold><break/>
<bold><italic>p</italic> = 0.073</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.089</bold></td></tr><tr><td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Virtual interviewer offers clarification (&#x0201c;Can I help you?&#x0201d;)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">25.8 (3.4)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">25.2 (3.2)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 35)</sub> = 0.022,<break/>
<italic>p</italic> = 0.882,<break/> &#x003b7;<sup>2</sup> = 0.001</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>31.5 (3.8)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>19.6 (3.7)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 35)</sub> = 4.98,</bold><break/>
<bold><italic>p</italic> = 0.032,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.124</bold></td></tr><tr><td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Respondent rejects offer</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">5.3 (1.6)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">3.2 (1.1)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 35)</sub> = 1.36,<break/>
<italic>p</italic> = 0.251,<break/> &#x003b7;<sup>2</sup> = 0.037,</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">5.1 (3.4)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">3.4 (1.4)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 35)</sub> = 0.67,<break/>
<italic>p</italic> = 0.42,<break/> &#x003b7;<sup>2</sup> = 0.019</td></tr><tr><td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Virtual interviewer presents definition</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>16.3 (3.5)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold>36.6 (4.5)</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 35)</sub> = 26.55,</bold><break/>
<bold><italic>p</italic> &#x0003c; 0.001,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.431</bold></td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">29.6 (5.1)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">23.3 (5.0)</td><td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 35)</sub> = 0.80,<break/>
<italic>p</italic> = 0.38,<break/> &#x003b7;<sup>2</sup> = 0.022</td></tr></tbody></table><table-wrap-foot><p><italic>Statistically reliable and marginal differences are in bold face</italic>.</p></table-wrap-foot></table-wrap><p>Contrary to Hypothesis 2 (see Table <xref ref-type="table" rid="T4">4</xref>), there was no evidence that respondents in the high dialog capability conditions explicitly requested clarification any more often when the virtual interviewer had high than low facial animation, nor did they reject clarification or receive definitions any more often.</p><p>Even though there was no evidence that the virtual interviewer's facial animation affected respondents' requests for clarification, respondents with high animation virtual interviewers did have different clarification dialog experiences in a few other ways. Respondents with the high animation virtual interviewer were marginally more likely to be presented with a comment about their confusion (&#x0201c;It sounds like you're having some trouble&#x0201d;) than respondents with the low animation virtual interviewer. This is potentially consistent with Hypothesis 2, to the extent that respondents' non-verbal or paralinguistic evidence of confusion (beyond explicit or implicit verbal requests for clarification) differed enough between high and low animation virtual interviewers so as to affect the Wizard's presentation of such comments. On the other hand, Hypothesis 2 seems clearly contradicted by the less interpretable finding that respondents with a high facial animation virtual interviewer were reliably <italic>less</italic> likely to be offered unsolicited clarification. This would make sense if we saw other evidence that respondents requested clarification or provided evidence of confusion more with the low facial animation interviewer, but that is not what we observe. In any case, although we see little evidence for Hypothesis 2, the fact that clarification dialog can proceed differently when the interviewer has high or low facial animation suggests that the impact of facial animation on clarification dialog deserves further exploration.</p><p>Analyses of potential interactive effects of the interviewer's dialog capability and facial animation on respondents' requests for clarification and receiving clarification are not significant. Consistent with the response accuracy evidence, Hypothesis 3 is not supported by evidence from clarification behavior.</p></sec></sec><sec><title>Respondents' engagement</title><p>To test our Hypotheses 4&#x02013;6 about respondents' engagement, we first focus on respondents' gaze at the virtual interviewers, and then on their acknowledgment behaviors, smiles, and subjective assessments of the virtual interviewer.</p><sec><title>Gaze at the virtual interviewer</title><p>From the video recordings of respondents' faces, we used Sequence Viewer (<ext-link ext-link-type="uri" xlink:href="http://www.sequenceviewer.nl/">http://www.sequenceviewer.nl/</ext-link>) to code whether respondents were looking at the screen (i.e., at the virtual interviewer), at their paper packet, or elsewhere at every moment in each interview (from the research assistants' observations of video monitors during the pre-interview training sessions, we knew that respondents had all looked at the virtual interviewer for several minutes before the survey interview, as instructed). Respondents looked almost exclusively at their scenario packet and the virtual interviewer; they looked elsewhere in the room so rarely (less than 1% of the time) as to be negligible (see Figure <xref ref-type="fig" rid="F3">3</xref>).</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Percentage of time that respondents looked at the virtual interviewer and the scenario packet, on average, across the four conditions, broken down by whether they were answering questions that mapped onto the scenario in a straightforward (lighter shades) or complicated (darker shades) way</bold>. Gaze elsewhere (not at the virtual interviewer or scenario packet) was so rare (less than 1% of the time in all conditions) that it is not plotted.</p></caption><graphic xlink:href="fpsyg-06-01578-g0003"/></fig><p>Consistent with Hypothesis 4, respondents spent a greater proportion of the interview time looking at the high-dialog-capability virtual interviewers (29.8% of the time) than the low-dialog-capability virtual interviewers (21.1%), <italic>F</italic><sub>(1, 69)</sub> = 6.73, <italic>p</italic> = 0.012, &#x003b7;<sup>2</sup> = 0.089. In order to further explore this phenomenon (that is, to further understand how respondents' engagement as measured by gaze connected with clarification dialog), we examined respondents' gaze at the virtual interviewers for complicated and straightforward scenarios, because it was only in complicated scenarios that clarification dialog ever occurred. As Figure <xref ref-type="fig" rid="F3">3</xref> shows, respondents looked slightly but reliably <italic>less</italic> at the virtual interviewer (and more at their scenario packets) when the mappings between questions and scenarios were complicated (24.7% of the time) rather than straightforward (26.3% of the time), <italic>F</italic><sub>(1, 69)</sub> = 4.20, <italic>p</italic> &#x0003c; 0.05, &#x003b7;<sup>2</sup> = 0.057. This overall difference resulted particularly from the low-dialog-capability conditions (19.2% of the time for complicated scenarios and 23.1% for straightforward) rather than the high-dialog-capability conditions, where there was no difference in the proportions of time spent looking at the virtual interviewer based on scenario mappings (30.1 vs. 29.6%), interaction <italic>F</italic><sub>(1, 69)</sub> = 7.16, <italic>p</italic> &#x0003c; 0.01, &#x003b7;<sup>2</sup> = 0.094. Our interpretation is that in the low-dialog-capability conditions respondents were left to their own devices to figure out the right answer to the survey question, and so the only available useful information, if the virtual interviewer would not provide clarification, could come from examining the scenarios more closely. In the high dialog capability conditions, engagement with the virtual interviewer through gaze was greater and not related to the content of the scenarios<xref ref-type="fn" rid="fn0003"><sup>1</sup></xref>.</p><p>Contrary to Hypothesis 5, there is not sufficient evidence that respondents looked more at the virtual interviewers with high facial animation than those with low facial animation, <italic>F</italic><sub>(1, 69)</sub> = 1.22, <italic>p</italic> = 0.27, &#x003b7;<sup>2</sup> = 0.017. An estimated Bayes<sub>10</sub> factor (alternative/null) of 0.669 suggested that the data were 1.49:1 in favor of the null hypothesis, that is, 1.49 times more likely to occur under a model that does not include an effect of facial animation, rather than a model that does include it.</p><p>Contrary to Hypothesis 6, the virtual interviewer's facial animation did not interact with its dialog capability in affecting respondents' gaze behavior, <italic>F</italic><sub>(1, 69)</sub> = 0.50, <italic>p</italic> = 0.48, &#x003b7;<sup>2</sup> = 0.017. An estimated Bayes<sub>10</sub> factor (alternative/null) of 1.771 does not rule out the possibility that the data may favor Hypothesis 6, but it seems unlikely.</p><p>There are at least two possible explanations for this pattern of results&#x02014;that gaze increased with high-dialog-capability but not high-facial-animation interviewers&#x02014;given that our experimental conditions varied on more than one feature. One is that respondents with a high-dialog-capability virtual interviewer found the content of the interviewer's contributions (e.g., clarification dialog) compelling and human-like enough to spend a greater proportion of their time looking at the interviewer. Another is that respondents with a high-dialog-capability virtual interviewer fully trusted what they were told about the interviewer's perceptual capacity in the experiment instructions: that the high-dialog-capability interviewer could perceive their facial expression and gaze. The fact that respondents in the high-dialog-capability conditions were explicitly instructed to look at the interviewer when ready for the next question makes disentangling this more difficult, but we note that the increase in looking time at the high-dialog-capability interviewer is <italic>proportional</italic>, and occurs along with a substantial increase in interview duration; high-dialog-capability interviews took 7.26 min on average (SE 0.36 min) compared with low-dialog-capability interviews (5.53 min, SE 0.37 min), <italic>F</italic><sub>(1, 69)</sub> = 11.23, <italic>p</italic> = 0.001, &#x003b7;<sup>2</sup> = 0.140. So the increase in looking time seems to us unlikely to result only from looking at the interviewer during transitions between survey questions, which would need to be quite long (a full minute of the interview, or a full 5 s at each question transition) to account for the effect.</p><p>Although respondents in this experiment did look at their paper packets a substantial proportion of the time during the interview (which means that at those moments they could only have been listening to&#x02014;not watching&#x02014;the virtual interviewer), we consider the proportions of time looking at the virtual interviewer observed here to be sufficient to allow us to detect potential effects of the virtual interviewer's facial animation even in the conditions with less looking time. The fact that we did observe significant differences in multiple measures based on facial animation corroborates this judgment.</p></sec><sec><title>Acknowledgment behaviors</title><p>In face-to-face interactions interlocutors can acknowledge each other's utterances verbally and visually: they can use back channel utterances (e.g., &#x0201c;okay,&#x0201d; &#x0201c;all right,&#x0201d; &#x0201c;got it,&#x0201d; &#x0201c;thank you&#x0201d;; Yngve, <xref rid="B79" ref-type="bibr">1970</xref>) and they can nod, shake their heads, shrug their shoulders, raise their eyebrows, etc., in order to communicate continued attention and possible understanding (Allwood et al., <xref rid="B2" ref-type="bibr">1992</xref>; McClave, <xref rid="B54" ref-type="bibr">2000</xref>). Verbal and visual acknowledgments can be seen as part of an integrated multimodal system (Carter and Adolphs, <xref rid="B9" ref-type="bibr">2008</xref>) that displays engagement in an interaction.</p><p>To examine acknowledgments in our virtual interviews, we counted respondents' verbal back channel utterances from the interactional moves we had coded (see Supplementary Table <xref ref-type="supplementary-material" rid="SM4">4</xref>). We also coded head movements (nods, head shakes, other head movements like tilts), and other body or facial movements (like shoulder shrugs and eyebrow raising), using Sequence Viewer, based on the video recordings of the interviews. Just as reliability was measured for the interactional move coding (86.6% of question-answer sequences double-coded, see Section Comprehension), it was measured for these behaviors as well. Each of the individual behaviors was relatively rare in our sample, but coders' level of agreement was high: for head movements the coders' judgments agreed 92.5% of the time, and for other body movements they agreed 94.1% of the time [Cohen's kappas for these reliabilities were low, at 0.32 and 0.27, but as Viera and Garrett (<xref rid="B75" ref-type="bibr">2005</xref>) demonstrate, kappa can easily be a misleading index of agreement when the occurrence of what is coded is rare].</p><p>In our tests of Hypotheses 4&#x02013;6, we first looked at verbal backchannels alone, head movements alone, and particular body and facial movements alone. Because backchannels and particular head movements and particular body and facial movements occurred rarely enough that there was a risk that we would miss patterns relevant to our hypotheses given our sample size, we also aggregated across nods, head shakes, other head movements, and other body and facial movements.</p><p>For Hypothesis 4 (effects of interviewer's dialog capability on respondent engagement), we see only suggestive evidence in support of it. Respondents did not produce many backchannels (and many produced none), but they produced marginally more of them with the high dialog capability agents (0.32 per interview) than with the low-dialog-capability agents (0.18 per interview), <italic>F</italic><sub>(1, 69)</sub> = 2.82, <italic>p</italic> = 0.098, &#x003b7;<sup>2</sup> = 0.039. Analyses of all facial and bodily movements do not show any significant effects.</p><p>The evidence for Hypothesis 5 (effects of interviewer facial animation on respondent engagement) is also suggestive. Respondents were marginally more likely to produce one of these movements when the virtual interviewer had high facial animation (averaging 0.13 occurrences per speaking turn) than when virtual interviewer had low facial animation (0.08 occurrences per speaking turn, <italic>F</italic><sub>(1, 69)</sub> = 3.21, <italic>p</italic> = 0.078, &#x003b7;<sup>2</sup> = 0.039. But support for Hypothesis 5 becomes stronger if we also include verbal back channel utterances, taking Carter and Adolphs' (<xref rid="B9" ref-type="bibr">2008</xref>) multimodal view of acknowledgment behavior. As Figure <xref ref-type="fig" rid="F4">4</xref> shows, respondents were nearly twice as likely to display our aggregated acknowledgment behaviors (visual and verbal) when the virtual interviewer had high facial animation (at a rate of 0.18 occurrences per speaking turn) than when the virtual interview had low facial animation (0.11 occurrences per speaking turn), <italic>F</italic><sub>(1, 69)</sub> = 4.29, <italic>p</italic> &#x0003c; 0.05, &#x003b7;<sup>2</sup> = 0.059.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Respondents' rates of aggregated acknowledgment behaviors (verbal back channels, nods, head shakes, other head movements, and other body and facial movements) per speaking turn (error bars represent SE's)</bold>.</p></caption><graphic xlink:href="fpsyg-06-01578-g0004"/></fig><p>Hypothesis 6 predicted an interaction of the form that respondents would produce disproportionately more engagement behaviors with high-dialog-capability high-facial-animation virtual interviewers, and proportionately fewer with low-dialog-capability low-facial-animation interviewers. We see partial evidence in support of this hypothesis in one significant interaction of interviewer dialog capability and facial animation with respect to nods, <italic>F</italic><sub>(1, 69)</sub> = 5.81, <italic>p</italic> = 0.019, &#x003b7;<sup>2</sup> = 0.078. Partially consistent with Hypothesis 6, respondents nodded least with the low-dialog capability low-facial-animation interviewer (0.05 times per interview), but (unexpectedly) most with the high-dialog-capability <italic>low</italic>-facial animation-interviewer (0.26 times per interview). There were no other significant interaction effects.</p></sec><sec><title>Smiles</title><p>Another measure of respondents' engagement with the virtual interviewers is their frequency of smiling.</p><p>We thus coded respondents' smiles in order to compute smile frequency and duration. The coder (one of the authors) had been certified in the Facial Action Coding System (FACS; Ekman and Friesen, <xref rid="B23" ref-type="bibr">1978</xref>). We determined coding reliability for all the question-answer sequences for a subsample of 20% of the respondents, equally distributed in the four experimental conditions, as independently coded by a second coder (four respondents had to be excluded from this analysis because the resolution of the video was not sufficient for this level of facial coding). Coders' level of agreement on smile frequency was high (92.1%), with a Cohen's kappa of 0.66. Coders' judgments on smile duration were also highly correlated, <italic>r</italic> = 0.835, <italic>p</italic> &#x0003c; 0.0001 (considering all sequences) and <italic>r</italic> = 0.762, <italic>p</italic> &#x0003c; 0.0001 (considering only those sequences in which at least one smile was found by at least one coder).</p><p>Regarding Hypothesis 4, there were no reliable effects of the interviewer's dialog capability on smiles.</p><p>Regarding Hypothesis 5, respondents interacting with a high facial animation virtual interviewer smiled marginally more often (2.25 times over the course of their interview, SE 0.55) than respondents interacting with a low facial animation virtual interviewer (0.78 times, SE 0.55), <italic>F</italic><sub>(1, 68)</sub> = 3.62, <italic>p</italic> = 0.061, &#x003b7;<sup>2</sup> = 0.050. Respondents interacting with a high facial animation virtual interviewer also smiled marginally longer (11.5 s over the course of the interview, SE 3.1) than respondents interacting with a low facial animation virtual interviewer (3.0 s, SE 3.1), <italic>F</italic><sub>(1, 68)</sub> = 3.67, <italic>p</italic> = 0.060, &#x003b7;<sup>2</sup> = 0.051.</p><p>Regarding Hypothesis 6, there were no significant interactive effects of virtual interviewers' dialog capability and facial animation on respondents' smiles.</p></sec><sec><title>Respondents' self-reported subjective experience</title><p>A final set of measures of respondents' engagements was their responses to the post-experiment questionnaire in which they reported how they felt about and evaluated the virtual interviewers.</p><p>Table <xref ref-type="table" rid="T5">5</xref> presents the average ratings as well as ANOVA statistics for tests of Hypotheses 4&#x02013;6<xref ref-type="fn" rid="fn0004"><sup>2</sup></xref>. Note that all of these ratings are lower than one would expect if the respondents evaluated the virtual interviewer as being very human-like. But given the constraints of a standardized interviewing situation, it is also plausible that human interviewers who implemented these interviews would not be rated as particularly autonomous, personal, close, or sensitive, and they might also be rated as more robotic than human (the term &#x0201c;robotic&#x0201d; is sometimes used to caricature the behavior of rigidly standardized interviewers, for example in survey invitations, see Conrad et al., <xref rid="B15" ref-type="bibr">2013</xref>).</p><table-wrap id="T5" position="float"><label>Table 5</label><caption><p><bold>Respondents' subjective ratings of the virtual interviewers, presented in the order the ratings were elicited (SE's in parentheses)</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="left" rowspan="1" colspan="1"><bold>Response options</bold></th><th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1"><bold>Low dialog capability</bold></th><th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1"><bold>High dialog capability</bold></th><th valign="top" align="center" rowspan="2" colspan="1"><bold>Test of hypothesis 4: effect of dialog capability</bold></th><th valign="top" align="center" rowspan="2" colspan="1"><bold>Test of hypothesis 5: effect of facial animation</bold></th><th valign="top" align="center" rowspan="2" colspan="1"><bold>Test of hypothesis 6: interaction of dialog capability and facial animation</bold></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>Low facial animation</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>High facial animation</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Low facial animation</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>High facial animation</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">How comfortable were you with Derek at the start of the session?</td><td valign="top" align="left" rowspan="1" colspan="1">1: Not at all comfortable<break/> 5: Very comfortable</td><td valign="top" align="center" rowspan="1" colspan="1">3.83 (0.27)</td><td valign="top" align="center" rowspan="1" colspan="1">3.22 (0.27)</td><td valign="top" align="center" rowspan="1" colspan="1">3.33 (0.27)</td><td valign="top" align="center" rowspan="1" colspan="1">3.00 (0.26)</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 1.83,<break/>
<italic>p</italic> = 0.180,<break/> &#x003b7;<sup>2</sup> = 0.026</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 3.13</bold>,<break/>
<bold><italic>p</italic> = 0.081</bold>,<break/>
<bold>&#x003b7;<sup>2</sup> = 0.043</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 0.27,<break/>
<italic>p</italic> = 0.604,<break/> &#x003b7;<sup>2</sup> = 0.004</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">As the interview progressed, did your comfort with Derek increase, decrease, or stay the same?</td><td valign="top" align="left" rowspan="1" colspan="1">1: Decrease<break/> 2: Stay the same<break/> 3: Increase</td><td valign="top" align="center" rowspan="1" colspan="1">1.78 (0.15)</td><td valign="top" align="center" rowspan="1" colspan="1">1.78 (0.15)</td><td valign="top" align="center" rowspan="1" colspan="1">1.56 (0.15)</td><td valign="top" align="center" rowspan="1" colspan="1">1.21 (0.15)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 7.05,</bold><break/>
<bold><italic>p</italic> = 0.010,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.093</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 1.35,<break/>
<italic>p</italic> = 0.250,<break/> &#x003b7;<sup>2</sup> = 0.019</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 1.35,<break/>
<italic>p</italic> = 0.250,<break/> &#x003b7;<sup>2</sup> = 0.019</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">How natural was the interaction with Derek?</td><td valign="top" align="left" rowspan="1" colspan="1">1: Not at all natural<break/> 5: Very natural</td><td valign="top" align="center" rowspan="1" colspan="1">3.22 (0.27)</td><td valign="top" align="center" rowspan="1" colspan="1">2.61 (0.27)</td><td valign="top" align="center" rowspan="1" colspan="1">3.17 (0.27)</td><td valign="top" align="center" rowspan="1" colspan="1">2.74 (0.27)</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 0.02,<break/>
<italic>p</italic> = 0.898,<break/> &#x003b7;<sup>2</sup> = 0.000</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 3.65,</bold><break/>
<bold><italic>p</italic> = 0.06,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.050</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 0.11,<break/>
<italic>p</italic> = 0.741,<break/> &#x003b7;<sup>2</sup> = 0.002</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">How often did Derek seem to act on his own?</td><td valign="top" align="left" rowspan="1" colspan="1">1: Never<break/> 5: All the time</td><td valign="top" align="center" rowspan="1" colspan="1">2.56 (0.25)</td><td valign="top" align="center" rowspan="1" colspan="1">2.44 (0.25)</td><td valign="top" align="center" rowspan="1" colspan="1">3.83 (0.25)</td><td valign="top" align="center" rowspan="1" colspan="1">2.74 (0.25)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 9.82,</bold><break/>
<bold><italic>p</italic> = 0.003,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.124</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 5.80,</bold><break/>
<bold><italic>p</italic> = 0.019,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.078</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 3.86,</bold><break/>
<bold><italic>p</italic> = 0.053,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.053</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Would you say that Derek acted more like a computer or a person?</td><td valign="top" align="left" rowspan="1" colspan="1">1: Just like a computer<break/> 2: As much like a computer as a person<break/> 3: Just like a person</td><td valign="top" align="center" rowspan="1" colspan="1">1.44 (0.152)</td><td valign="top" align="center" rowspan="1" colspan="1">1.61 (0.15)</td><td valign="top" align="center" rowspan="1" colspan="1">2.11 (0.15)</td><td valign="top" align="center" rowspan="1" colspan="1">1.68 (0.15)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 6.02,</bold><break/>
<bold><italic>p</italic> = 0.017,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.080</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 0.75,<break/>
<italic>p</italic> = 0.391,<break/> &#x003b7;<sup>2</sup> = 0.011</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 3.88,</bold><break/>
<bold><italic>p</italic> = 0.053,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.053</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">How much did you enjoy interacting with Derek?</td><td valign="top" align="left" rowspan="1" colspan="1">1: Did not enjoy at all<break/> 5: Thoroughly enjoyed</td><td valign="top" align="center" rowspan="1" colspan="1">3.06 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1">3.22 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1">3.83 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1">3.37 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 4.41,</bold><break/>
<bold><italic>p</italic> = 0.039,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.060</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 0.46,<break/>
<italic>p</italic> = 0.500,<break/> &#x003b7;<sup>2</sup> = 0.007</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 2.06,<break/>
<italic>p</italic> = 0.156,<break/> &#x003b7;<sup>2</sup> = 0.029</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">How frustrating was it to be interviewed by Derek?</td><td valign="top" align="left" rowspan="1" colspan="1">1: Not at all frustrating<break/> 5: Very frustrating</td><td valign="top" align="center" rowspan="1" colspan="1">1.88 (0.24)</td><td valign="top" align="center" rowspan="1" colspan="1">2.11 (0.23)</td><td valign="top" align="center" rowspan="1" colspan="1">1.50 (0.23)</td><td valign="top" align="center" rowspan="1" colspan="1">1.84 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 68)</sub> = 2.01,<break/>
<italic>p</italic> = 0.161,<break/> &#x003b7;<sup>2</sup> = 0.029</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 68)</sub> = 1.54,<break/>
<italic>p</italic> = 0.219,<break/> &#x003b7;<sup>2</sup> = 0.022</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 68)</sub> = 0.06,<break/>
<italic>p</italic> = 0.806,<break/> &#x003b7;<sup>2</sup> = 0.001</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">I felt that Derek was&#x02026;</td><td valign="top" align="left" rowspan="1" colspan="1">1: Impersonal<break/> 5: Personal</td><td valign="top" align="center" rowspan="1" colspan="1">1.94 (0.23)</td><td valign="top" align="center" rowspan="1" colspan="1">2.35 (0.24)</td><td valign="top" align="center" rowspan="1" colspan="1">3.28 (0.23)</td><td valign="top" align="center" rowspan="1" colspan="1">2.68 (0.23)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 68)</sub> = 12.72,</bold><break/>
<bold><italic>p</italic> = 0.001,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.158</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 68)</sub> = 0.16,<break/>
<italic>p</italic> = 0.693,<break/> &#x003b7;<sup>2</sup> = 0.002</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 68)</sub> = 4.61,</bold><break/>
<bold><italic>p</italic> = 0.035,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.063</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">I felt that Derek was&#x02026;</td><td valign="top" align="left" rowspan="1" colspan="1">1: Distant<break/> 5: Close</td><td valign="top" align="center" rowspan="1" colspan="1">2.17 (0.24)</td><td valign="top" align="center" rowspan="1" colspan="1">2.77 (0.25)</td><td valign="top" align="center" rowspan="1" colspan="1">3.19 (0.26)</td><td valign="top" align="center" rowspan="1" colspan="1">2.79 (0.24)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 66)</sub> = 4.55,</bold><break/>
<bold><italic>p</italic> = 0.037,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.064</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 66)</sub> = 0.17,<break/>
<italic>p</italic> = 0.685,<break/> &#x003b7;<sup>2</sup> = 0.003</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 66)</sub> = 4.13,</bold><break/>
<bold><italic>p</italic> = 0.046,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.059</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">I felt that Derek was&#x02026;</td><td valign="top" align="left" rowspan="1" colspan="1">1: Inexpressive<break/> 5: Expressive</td><td valign="top" align="center" rowspan="1" colspan="1">2.56 (0.29)</td><td valign="top" align="center" rowspan="1" colspan="1">2.78 (0.29)</td><td valign="top" align="center" rowspan="1" colspan="1">3.00 (0.29)</td><td valign="top" align="center" rowspan="1" colspan="1">2.84 (0.28)</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 68)</sub> = 0.79,<break/>
<italic>p</italic> = 0.377,<break/> &#x003b7;<sup>2</sup> = 0.011</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 68)</sub> = 0.01,<break/>
<italic>p</italic> = 0.911,<break/> &#x003b7;<sup>2</sup> = 0.000</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 68)</sub> = 0.44,<break/>
<italic>p</italic> = 0.509,<break/> &#x003b7;<sup>2</sup> = 0.006</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">I felt that Derek was&#x02026;</td><td valign="top" align="left" rowspan="1" colspan="1">1: Insensitive<break/> 5: Sensitive</td><td valign="top" align="center" rowspan="1" colspan="1">2.56 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1">2.94 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1">3.22 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1">3.32 (0.22)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic><sub>(1, 69)</sub> = 5.59,</bold><break/>
<bold><italic>p</italic> = 0.021,</bold><break/>
<bold>&#x003b7;<sup>2</sup> = 0.075</bold></td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 1.21,<break/>
<italic>p</italic> = 0.275,<break/> &#x003b7;<sup>2</sup> = 0.017</td><td valign="top" align="center" rowspan="1" colspan="1"><italic>F</italic><sub>(1, 69)</sub> = 0.45,<break/>
<italic>p</italic> = 0.503,<break/> &#x003b7;<sup>2</sup> = 0.007</td></tr></tbody></table><table-wrap-foot><p><italic>Statistically significant and marginal effects are in bold</italic>.</p></table-wrap-foot></table-wrap><p>As detailed in Table <xref ref-type="table" rid="T5">5</xref>, Hypothesis 4 is supported on several fronts. Respondents with an interviewer high in dialog capability reported enjoying the interview more, and they rated the interviewer as more autonomous, more personal, less distant, and more sensitive than respondents with an interviewer low in dialog capability<xref ref-type="fn" rid="fn0005"><sup>3</sup></xref>. They also rated the interviewer as less like a computer. Unexpectedly, respondents with high dialog capability interviewers reported a greater decrease in comfort across the interview than respondents with the low dialog capability interviewers.</p><p>In contrast to the predictions of Hypothesis 5, there were significant effects of facial animation suggesting that interviewers with <italic>low</italic> facial animation were in some ways preferred. Respondents with low facial animation interviewers reported marginally greater comfort with the interviewer at the start of the session, and they rated the interviewer as marginally more natural and as reliably more autonomous (acting on his own), than did respondents with high facial animation interviewers.</p><p>The pattern uncovered in tests of Hypothesis 6 is consistent with that found for acknowledgments. Respondents with low facial animation interviewers were more likely (albeit marginally) to rate the interviewer as autonomous when the interviewer had high dialog capability (see Table <xref ref-type="table" rid="T5">5</xref>). These same respondents were also particularly more likely to rate the low facial animation interviewer as more personal, as less distant (closer), and as marginally more like a person than a computer. In other words, respondents found the interviewer to be particularly autonomous and personal when he looked more robotic (displayed less facial movement) but could converse like a human. The fact that the mean ratings in this condition (low facial animation/high dialog capability) stand out from the others, along with the (marginal) interaction effects, suggests that part of what is driving the main effects of dialog capability and facial animation on these items are the perceptions of this subgroup.</p></sec></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><sec><title>Summary</title><p>The findings reported here document that two important elements of human face-to-face interaction&#x02014;dialog capability and facial movement&#x02014;implemented in virtual survey interviewers differently affect respondents' comprehension and the nature of their engagement with the virtual interviewer. As tested in Hypotheses 1 and 4, respondents who interacted with a virtual interviewer with greater dialog capability (that is, which could help respondents interpret the questions as intended) provided more accurate answers and took more responsibility for their comprehension, requesting clarification more often. They looked at high-dialog-capability interviewers more, they produced marginally more backchannel responses, and they reported enjoying the interview more and finding the interviewer to be more personal and less distant. As tested in Hypotheses 2 and 5, respondents who interacted with a virtual interviewer with more facial animation displayed more evidence of engagement&#x02014;more verbal back channels and visual acknowledgments of the interviewer's utterances, and marginally more smiles. They also reported <italic>less</italic> comfort with the high facial animation interviewers and rated these interviewers as less natural. In testing Hypotheses 3 and 6, we observed that respondents (unexpectedly) nodded more and rated the virtual interviewer as more personal and less distant if it had high dialog capability and <italic>low</italic> facial animation.</p><p>The current findings extend work on people's reactions and behaviors when they talk with interviewing agents, for example telling stories to an agent that exhibits listening behavior (e.g., Gratch et al., <xref rid="B34" ref-type="bibr">2006</xref>; von der P&#x000fc;tten et al., <xref rid="B77" ref-type="bibr">2010</xref>), answering open-ended questions asked by a peer (e.g., Bailenson et al., <xref rid="B5" ref-type="bibr">2006</xref>) or answering open-ended questions asked by a deception-detecting kiosk agent (Nunamaker et al., <xref rid="B57" ref-type="bibr">2011</xref>), to the task of a survey interview for social measurement that uses closed categories as response options and that is designed to make statistical estimates of a population. The findings also extend work on disclosure of sensitive information in a survey interview with a virtual interviewer (Lind et al., <xref rid="B48" ref-type="bibr">2013</xref>) to an interview with non-sensitive questions that have verifiably correct and incorrect answers, and in which accurate comprehension of the terms in the questions is critical. Because of the nature of this survey task, our measures focus on aspects of the interaction and of respondents' behavior (e.g., response accuracy, smiles, acknowledgments) that have not been the focus in previous studies, where users' nuanced interpretation of what the virtual interviewer is asking is less essential.</p><p>While it is unclear where exactly our survey task fits into a taxonomy of tasks for which virtual humans have been designed, what is clear is that for this task the two features we experimentally manipulated have quite distinct effects. We assume this is because they engage different channels of communication (the exchange of spoken vs. visual information) and manifest themselves over different time scales&#x02014;a virtual agent's facial animation is visible to users as soon as any talking starts, while evidence of the agent's dialog capability unfolds more incrementally over time as the interviewer does or does not respond to the user's need for clarification. We hypothesize that our findings should generalize to other interactive tasks with virtual agents that share the central features of the current task: a need for grounding interpretation of terms in an agent's utterances and a need for the user to be sufficiently engaged to complete a task that someone else has initiated (Schober et al., <xref rid="B67" ref-type="bibr">2003</xref>).</p><p>While our experimental design allows us to see effects of what we manipulated, it does not allow us to disentangle the relative contributions of the bundled features that comprise the different agents. Of course, our agents' particular features could have been implemented differently (e.g., the agents could have had different vocal or visual attributes, or been unnamed or have had different names), and it is unknown how our findings would generalize to different implementations. Our experimental design also does not allow inference about potential (and intriguing) causal connections between our different measures. For example, we do not know whether respondents' attributions about the high-dialog-capability interviewer <italic>result from</italic> or <italic>cause</italic> or are <italic>independent of</italic> their improved comprehension: did respondents answer more accurately with a high dialog capability virtual interviewer because they enjoyed the interview more and found the interviewer more perceptive and responsive? Or did they enjoy the interview more because they were confident that they had comprehended the questions as intended? Did respondents smile more often and longer with a high facial animation virtual interviewer because they felt more engaged, as one might expect given Kr&#x000e4;mer et al.'s (<xref rid="B46" ref-type="bibr">2013</xref>) finding that users who were engaged in small talk with a virtual agent smiled more when the virtual agent smiled more? Or, alternatively (and consistent with our respondents' reports of less comfort), did they smile more because their smiles reflected distress or discomfort (e.g., Ansfield, <xref rid="B4" ref-type="bibr">2007</xref>)? The fact that respondents' subjective experience of a virtual survey interviewer&#x02014;their level of comfort, their enjoyment, how natural they feel the interaction to be&#x02014;can be correlated with their disclosure of sensitive information (Lind et al., <xref rid="B48" ref-type="bibr">2013</xref>) makes it plausible that users' affective reactions could be causally connected with their comprehension and behavioral displays even with non-sensitive survey questions of the sort asked here, but the current data only allow speculation.</p></sec><sec><title>Designing virtual survey interviewers</title><p>Animating virtual interviewing agents that could be used in, for example, a web survey with textual response is becoming increasingly straightforward with off-the-shelf tools. Instantiating dialog capability and speech recognition is a greater challenge, but the constrained nature of the survey interview task (a finite set of possible turns that can occur, standardized wording of questions, closed response options with limited vocabulary that a speech recognition system can handle, definitions of key terms already existing) can make implementing clarification dialog in a textual or speech interviewing system more plausible than in more open-ended or free-form conversational domains (Johnston et al., <xref rid="B43" ref-type="bibr">2013</xref>; Schober et al., <xref rid="B65" ref-type="bibr">2015</xref>).</p><p>Given the many possible ways to instantiate a virtual interviewer&#x02014;a range of possible expressivity, sensing capabilities and responsiveness to respondents' signals, and a range of more and less human-like facial motion and detail&#x02014;we propose the following design considerations for building virtual interviewers for actual surveys that produce population estimates:
<list list-type="bullet"><list-item><p><italic>Designing to maximize participation:</italic> Potential respondents are likely to vary in whether they will consent to interact with a virtual interviewer, for example, in an online survey. Perhaps the greatest deterrent is uncanniness (e.g., MacDorman et al., <xref rid="B50" ref-type="bibr">2009</xref>). The fact that participants in the current study reported that the virtual interviewers with more facial animation made them less comfortable and were less natural than virtual interviewers with less facial movement could result from people's finding the increased realism of high facial animation to be eerie, and this might reduce participation in virtual interviews by some sample members. But for others, this might not affect participation; in the Lind et al. (<xref rid="B48" ref-type="bibr">2013</xref>) study with a more cartoon-like interviewer, different respondents had completely opposite affective reactions from each other to the very same interviewing agent, and this correlated with their willingness to disclose sensitive information.</p></list-item><list-item><p><italic>Designing to maximize completion:</italic> Although in this study we did not include an interviewing condition without a facial representation, the increased engagement (more acknowledgments and smiles) that we observed with the high facial animation interviewers could translate to increased completion of questionnaires compared to self-administered online questionnaires without any virtual interviewer. Engagement could promote completion if respondents apply social norms from face-to-face interaction in which it would be rude to break off a conversation midstream, or because a moving talking face simply makes the task more interesting. To investigate this, one would need to carry out a study outside the laboratory (e.g., online) with naturalistic incentives (rather than our laboratory method with payment).</p></list-item><list-item><p><italic>Designing to maximize comprehension:</italic> As we have proposed for human interviewers (Schober and Conrad, <xref rid="B61" ref-type="bibr">1997</xref>; Conrad and Schober, <xref rid="B16" ref-type="bibr">2000</xref>), enabling virtual survey interviewers to engage in clarification dialog is likely to improve respondents' understanding of questions and thus the quality of the data collected in the survey. There are a number of ways to instantiate clarification dialog in a virtual interviewer, from providing scripted (spoken or even textual) definitions only when respondents request them to diagnosing the potential need for clarification based on respondents' disfluencies and gaze aversion (e.g., Ehlen et al., <xref rid="B22" ref-type="bibr">2007</xref>; Schober et al., <xref rid="B66" ref-type="bibr">2012</xref>). The findings in the current study suggest that system-initiated clarification is likely to be important for maximizing comprehension.</p></list-item><list-item><p><italic>Designing the interviewer's appearance and voice:</italic> It is essentially impossible to design a virtual human interviewer without creating the perception of some demographic characteristics. If the virtual interviewer communicates by speaking, its speech will inevitably have attributes such as dialect, a pitch range, prosody, and vocal quality. How the current findings, which are based on one 3D head model with particular visual and linguistic attributes, will generalize to virtual interviewers with other visual and linguistic attributes, will be a key design question: how a virtual interviewer's visual attributes (skin shade, eye color, hair style, facial features, clothing, hair covering, etc.) or speech style (accent, vocabulary, pronunciation) will affect respondents' judgments about the interviewer's perceived &#x0201c;social identity&#x0201d; (gender, race, social class, education, religious affiliation) and potentially respondents' answers to questions on some interview topics. It is well known that demographic characteristics of human interviewers can (undesirably) affect the distribution of responses (e.g., Hatchett and Schuman, <xref rid="B36" ref-type="bibr">1975</xref>) even in telephone interviews where only voice attributes are available (e.g., Cotter et al., <xref rid="B20" ref-type="bibr">1982</xref>; Finkel et al., <xref rid="B28" ref-type="bibr">1991</xref>). There is preliminary evidence that this kind of interviewer effect may also appear with virtual interviewers (Conrad et al., <xref rid="B19" ref-type="bibr">2011</xref>), and that gender and nationality attributions can occur for embodied agents more generally (Eyssel and Hegel, <xref rid="B25" ref-type="bibr">2012</xref>; Eyssel and Kuchenbrandt, <xref rid="B26" ref-type="bibr">2012</xref>).</p></list-item><list-item><p><italic>Designing for different types of survey questions</italic>: The current research suggests that virtual interviewers implemented with high dialog capability may promote accurate answers to factual questions about mundane topics for which complicated mappings are possible. However, it has been shown (Lind et al., <xref rid="B48" ref-type="bibr">2013</xref>) that when virtual interviewers ask questions about sensitive topics, respondents seem to answer most questions less truthfully (disclose less sensitive information) than when the same questions are spoken by a disembodied (audio) interviewer. If a survey investigates both non-sensitive and sensitive topics, one could imagine implementing the virtual interviewer for only the non-sensitive questions. To our knowledge this has never been attempted; much is unknown about how the intermittent display of a virtual interviewer might affect respondents' affective responses and whether removing an interviewer&#x02014;after being present&#x02014;could convincingly create a sense of privacy.</p></list-item><list-item><p><italic>Giving respondents a choice of interviewer?</italic> One potential advantage of implementing virtual survey interviewers is that one could let <italic>respondents</italic> choose an interviewer with the attributes (appearance, speech style) that they prefer, which is not a possibility with human interviewers. It is entirely unknown which attributes respondents would most want to be able to choose, whether providing choices will increase respondents' engagement and data quality, or how choosing an interviewer that makes respondents most comfortable might affect their effort in producing accurate responses.</p></list-item></list></p><p>Considering factors such as these, as well as those raised by Cassell and Miller (<xref rid="B10" ref-type="bibr">2008</xref>), will be essential if virtual survey interviewing systems are to be effective. The need for accurate survey data will continue; the question will be what kinds of interviewers and interviewing systems will best promote accurate data and respondent engagement in new technological environments (Schober and Conrad, <xref rid="B63" ref-type="bibr">2008</xref>), and what role embodied interviewing agents might best play.</p></sec><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><ack><p>Many thanks to Andy Bailin, Xiao Chen, Brian Lewis, Laura Lind, Miyuki Nishimura, Catherine Militello, Christian Prinz, and David Vannette for their assistance. This research was supported by National Science Foundation grants SES-0551294 to FC and MS and SES-0454832 to FC, and by faculty research funds to MS from the New School for Social Research.</p></ack><fn-group><fn id="fn0003"><p><sup>1</sup>This pattern of findings rules out an interpretation that greater gaze at the high-dialog-capability virtual interviewer resulted simply from respondents looking at the interviewer more at turn transitions, which is a well-documented phenomenon in human dialog (e.g., Kendon, <xref rid="B45" ref-type="bibr">1967</xref>). If this were the case, then there should be more looking at the interviewer for complicated than straightforward scenarios with the high dialog capability interviewers, because complicated scenarios involved more transitions (because of more clarification).</p></fn><fn id="fn0004"><p><sup>2</sup>We report parametric statistical analyses on these rating scale data so as to straightforwardly test our hypotheses about our two experimental factors and potential interactions. This approach is supported by <xref rid="B56" ref-type="bibr">Norman's</xref> (<xref rid="B56" ref-type="bibr">2010</xref>) arguments and evidence about the robustness of parametric analyses for interval data. But this does not mean that we are claiming that our participants treated the distances between intervals on our scales as equal, which is, of course, unknowable (Jamieson, <xref rid="B40" ref-type="bibr">2004</xref>); we simply are claiming that higher ratings are higher.</p></fn><fn id="fn0005"><p><sup>3</sup>We interpret the finding that autonomy was rated as lower for low-dialog-capability agents as reflecting respondents' assessment of the virtual interviewer's ability to reason and think (&#x0201c;act on his own&#x0201d;), as opposed to reflecting a judgment that the virtual interviewer had a human operator rather than being stand-alone software. While we can't, of course, rule out this possibility, the fact that the patterns of ratings are consistent on more items beyond the question about autonomy supports this view.</p></fn></fn-group><sec sec-type="supplementary-material" id="s5"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01578">http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01578</ext-link></p><supplementary-material content-type="local-data" id="SM1"><media xlink:href="Table1.PDF"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM2"><media xlink:href="Table2.PDF"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM3"><media xlink:href="Table3.PDF"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM4"><media xlink:href="Table4.PDF"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM5"><media xlink:href="Video1.MP4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM6"><media xlink:href="Video2.MP4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM7"><media xlink:href="Video3.MP4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM8"><media xlink:href="Video4.MP4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM9"><media xlink:href="Video5.MP4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM10"><media xlink:href="Video6.MP4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexanderson</surname><given-names>S.</given-names></name><name><surname>Beskow</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Animated Lombard speech: motion capture, facial animation and visual intelligibility of speech produced in adverse conditions</article-title>. <source>Comput. Speech Lang.</source>
<volume>28</volume>, <fpage>607</fpage>&#x02013;<lpage>618</lpage>. <pub-id pub-id-type="doi">10.1016/j.csl.2013.02.005</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allwood</surname><given-names>J.</given-names></name><name><surname>Nivre</surname><given-names>J.</given-names></name><name><surname>Ahlsen</surname><given-names>E.</given-names></name></person-group> (<year>1992</year>). <article-title>On the semantics and pragmatics of linguistic feedback</article-title>. <source>J. Semantics</source>
<volume>9</volume>, <fpage>1</fpage>&#x02013;<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1093/jos/9.1.1</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>A. H.</given-names></name></person-group> (<year>2008</year>). <article-title>Video-mediated interactions and surveys</article-title>, in <source>Envisioning the Survey Interview of the Future</source>, eds <person-group person-group-type="editor"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name></person-group> (<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons, Inc</publisher-name>), <fpage>95</fpage>&#x02013;<lpage>118</lpage>. <pub-id pub-id-type="doi">10.1002/9780470183373.ch5</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ansfield</surname><given-names>M. E.</given-names></name></person-group> (<year>2007</year>). <article-title>Smiling when distressed: when a smile is a frown turned upside down</article-title>. <source>Pers. Soc. Psychol. Bull.</source>
<volume>33</volume>, <fpage>763</fpage>&#x02013;<lpage>775</lpage>. <pub-id pub-id-type="doi">10.1177/0146167206297398</pub-id><pub-id pub-id-type="pmid">17483396</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bailenson</surname><given-names>J. N.</given-names></name><name><surname>Yee</surname><given-names>N.</given-names></name><name><surname>Merget</surname><given-names>D.</given-names></name><name><surname>Schroeder</surname><given-names>R.</given-names></name></person-group> (<year>2006</year>). <article-title>The effect of behavioral realism and form realism of real-time avatar faces on verbal disclosure, nonverbal disclosure, emotion recognition, and copresence in dyadic interaction</article-title>. <source>Presence</source>
<volume>15</volume>, <fpage>359</fpage>&#x02013;<lpage>372</lpage>. <pub-id pub-id-type="doi">10.1162/pres.15.4.359</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baur</surname><given-names>T.</given-names></name><name><surname>Damian</surname><given-names>I.</given-names></name><name><surname>Gebhard</surname><given-names>P.</given-names></name><name><surname>Porayska-Pomsta</surname><given-names>K.</given-names></name><name><surname>Andr&#x000e9;</surname><given-names>E.</given-names></name></person-group> (<year>2013</year>). <article-title>A job interview simulation: social cue-based interaction with a virtual character</article-title>, in <source>2013 International Conference on Social Computing (SocialCom)</source> (<publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>220</fpage>&#x02013;<lpage>227</lpage>. <pub-id pub-id-type="doi">10.1109/SocialCom.2013.39</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baylor</surname><given-names>A. L.</given-names></name><name><surname>Kim</surname><given-names>Y.</given-names></name></person-group> (<year>2004</year>). <article-title>Pedagogical agent design: the impact of agent realism, gender, ethnicity, and instructional role</article-title>, in <source>Intelligent Tutoring Systems, Proceedings, Lecture Notes in Computer Science</source>, <volume>Vol. 3220</volume> (<publisher-loc>Berlin; Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>592</fpage>&#x02013;<lpage>603</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-540-30139-4_56</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bloom</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>The speech IVR as a survey interviewing methodology</article-title>, in <source>Envisioning the Survey Interview of the Future</source>, eds <person-group person-group-type="editor"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name></person-group> (<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons, Inc</publisher-name>), <fpage>119</fpage>&#x02013;<lpage>136</lpage>. <pub-id pub-id-type="doi">10.1002/9780470183373.ch6</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>R.</given-names></name><name><surname>Adolphs</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>Linking the verbal and visual: new directions for corpus linguistics</article-title>. <source>Lang. Comput.</source>
<volume>64</volume>, <fpage>275</fpage>&#x02013;<lpage>291</lpage>. <pub-id pub-id-type="doi">10.1163/9789401205474_019</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cassell</surname><given-names>J.</given-names></name><name><surname>Miller</surname><given-names>P.</given-names></name></person-group> (<year>2008</year>). <article-title>Is it self-administration if the computer gives you encouraging looks?</article-title> in <source>Envisioning the Survey Interview of the Future</source>, eds <person-group person-group-type="editor"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name></person-group> (<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>), <fpage>161</fpage>&#x02013;<lpage>178</lpage>.</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L.</given-names></name><name><surname>Krosnick</surname><given-names>J. A.</given-names></name></person-group> (<year>2010</year>). <article-title>Comparing oral interviewing with self-administered computerized questionnaires: an experiment</article-title>. <source>Public Opin. Q.</source>
<volume>74</volume>, <fpage>154</fpage>&#x02013;<lpage>167</lpage>. <pub-id pub-id-type="doi">10.1093/poq/nfp090</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>H. H.</given-names></name></person-group> (<year>1996</year>). <source>Using Language</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>
<pub-id pub-id-type="doi">10.1017/CBO9780511620539</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>H. H.</given-names></name><name><surname>Schaefer</surname><given-names>E. F.</given-names></name></person-group> (<year>1987</year>). <article-title>Collaborating on contributions to conversations</article-title>. <source>Lang. Cogn. Process.</source>
<volume>2</volume>, <fpage>19</fpage>&#x02013;<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1080/01690968708406350</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>H. H.</given-names></name><name><surname>Wilkes-Gibbs</surname><given-names>D.</given-names></name></person-group> (<year>1986</year>). <article-title>Referring as a collaborative process</article-title>. <source>Cognition</source>
<volume>22</volume>, <fpage>1</fpage>&#x02013;<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1016/0010-0277(86)90010-7</pub-id><pub-id pub-id-type="pmid">3709088</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Broome</surname><given-names>J. S.</given-names></name><name><surname>Benk&#x000ed;</surname><given-names>J. R.</given-names></name><name><surname>Kreuter</surname><given-names>F.</given-names></name><name><surname>Groves</surname><given-names>R. M.</given-names></name><name><surname>Vannette</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Interviewer speech and the success of survey invitations</article-title>. <source>J. R. Stat. Soc.</source>
<volume>176</volume>, <fpage>191</fpage>&#x02013;<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-985X.2012.01064.x</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name></person-group> (<year>2000</year>). <article-title>Clarifying question meaning in a household telephone survey</article-title>. <source>Public Opin. Q.</source>
<volume>64</volume>, <fpage>1</fpage>&#x02013;<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1086/316757</pub-id><pub-id pub-id-type="pmid">10810073</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name></person-group> (eds.). (<year>2008</year>). <source>Envisioning the Survey Interview of the Future</source>, <volume>Vol. 542</volume> (<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>). <pub-id pub-id-type="doi">10.1002/9780470183373</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Coiner</surname><given-names>T.</given-names></name></person-group> (<year>2007</year>). <article-title>Bringing features of dialogue to web surveys</article-title>. <source>Appl. Cogn. Psychol.</source>
<volume>21</volume>, <fpage>165</fpage>&#x02013;<lpage>187</lpage>. <pub-id pub-id-type="doi">10.1002/acp.1335</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Nielsen</surname><given-names>D.</given-names></name></person-group> (<year>2011</year>). <article-title>Race-of-virtual-interviewer effects</article-title>, in <source>The 66th Annual Conference of the American Association for Public Opinion Research</source> (<publisher-loc>Phoenix, AZ</publisher-loc>).</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cotter</surname><given-names>P. R.</given-names></name><name><surname>Cohen</surname><given-names>J.</given-names></name><name><surname>Coulter</surname><given-names>P. B.</given-names></name></person-group> (<year>1982</year>). <article-title>Race-of-interviewer effects in telephone interviews</article-title>. <source>Public Opin. Q.</source>
<volume>46</volume>, <fpage>278</fpage>&#x02013;<lpage>284</lpage>. <pub-id pub-id-type="doi">10.1086/268719</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>DeVault</surname><given-names>D.</given-names></name><name><surname>Artstein</surname><given-names>R.</given-names></name><name><surname>Benn</surname><given-names>G.</given-names></name><name><surname>Dey</surname><given-names>T.</given-names></name><name><surname>Fast</surname><given-names>E.</given-names></name><name><surname>Gainer</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>SimSensei Kiosk: a virtual human interviewer for healthcare decision support</article-title>, in <source>Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems</source> (<publisher-loc>Paris</publisher-loc>: <publisher-name>International Foundation for Autonomous Agents and Multiagent Systems</publisher-name>), <fpage>1061</fpage>&#x02013;<lpage>1068</lpage>.</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehlen</surname><given-names>P.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name></person-group> (<year>2007</year>). <article-title>Modeling speech disfluency to predict conceptual misalignment in speech survey interfaces</article-title>. <source>Discourse Process.</source>
<volume>44</volume>, <fpage>245</fpage>&#x02013;<lpage>265</lpage>. <pub-id pub-id-type="doi">10.1080/01638530701600839</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name><name><surname>Friesen</surname><given-names>W. V.</given-names></name></person-group> (<year>1978</year>). <source>Facial Action Coding System: A Technique for the Measurement of Facial Movement.</source>
<publisher-loc>Palo Alto, CA</publisher-loc>: <publisher-name>Consulting Psychologists Press</publisher-name>.</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Everitt</surname><given-names>B. S.</given-names></name><name><surname>Haye</surname><given-names>D. F.</given-names></name></person-group> (<year>1992</year>). <source>Talking About Statistics: A Psychologist's Guide to Data Analysis</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Halsted Press</publisher-name>.</mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eyssel</surname><given-names>F.</given-names></name><name><surname>Hegel</surname><given-names>F.</given-names></name></person-group> (<year>2012</year>). <article-title>(S)he's got the look: gender stereotyping of robots</article-title>. <source>J. Appl. Soc. Psychol.</source>
<volume>42</volume>, <fpage>2213</fpage>&#x02013;<lpage>2230</lpage>. <pub-id pub-id-type="doi">10.1111/j.1559-1816.2012.00937.x</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eyssel</surname><given-names>F.</given-names></name><name><surname>Kuchenbrandt</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Social categorization of social robots: anthropomorphism as a function of robot group membership</article-title>. <source>Br. J. Soc. Psychol.</source>
<volume>51</volume>, <fpage>724</fpage>&#x02013;<lpage>731</lpage>. <pub-id pub-id-type="doi">10.1111/j.2044-8309.2011.02082.x</pub-id><pub-id pub-id-type="pmid">22103234</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Ferrin</surname><given-names>M.</given-names></name><name><surname>Kriesi</surname><given-names>H.</given-names></name></person-group> (<year>2014</year>). <source>Europeans' Understandings and Evaluations of Democracy: Topline Results from Round 6 of the European Social Survey. ESS Topline Results Series, (4)</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://www.europeansocialsurvey.org/permalink/800ea36f-3a8d-11e4-95d4-005056b8065f.pdf">http://www.europeansocialsurvey.org/permalink/800ea36f-3a8d-11e4-95d4-005056b8065f.pdf</ext-link></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkel</surname><given-names>S. E.</given-names></name><name><surname>Guterbock</surname><given-names>T. M.</given-names></name><name><surname>Borg</surname><given-names>M. J.</given-names></name></person-group> (<year>1991</year>). <article-title>Race-of-interviewer effects in a preelection poll Virginia 1989</article-title>. <source>Public Opin. Q.</source>
<volume>55</volume>, <fpage>313</fpage>&#x02013;<lpage>330</lpage>. <pub-id pub-id-type="doi">10.1086/269264</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Foucault Welles</surname><given-names>B.</given-names></name><name><surname>Miller</surname><given-names>P. V.</given-names></name></person-group> (<year>2013</year>). <article-title>Nonverbal correlates of survey rapport: an analysis of interviewer behavior</article-title>, in <source>The Cannell Workshop at the 2013 Meeting of the American Association for Public Opinion Research (AAPOR)</source> (<publisher-loc>Boston, MA</publisher-loc>).</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fowler</surname><given-names>F. J.</given-names></name><name><surname>Mangione</surname><given-names>T. W.</given-names></name></person-group> (<year>1990</year>). <source>Standardized Survey Interviewing: Minimizing Interviewer-Related Error.</source>
<publisher-loc>Newbury Park, CA</publisher-loc>: <publisher-name>SAGE Publications, Inc</publisher-name>.</mixed-citation></ref><ref id="B31"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Geoghegan-Quinn</surname><given-names>M.</given-names></name></person-group> (<year>2012</year>). <source>Opening Address to the ESS International Conference &#x02013; &#x02018;CrossNational Evidence from European Social Survey: Exploring Public Attitudes, Informing Public Policy in Europe&#x02019;</source>, Nicosia. Available online at: <ext-link ext-link-type="uri" xlink:href="http://www.europeansocialsurvey.org/docs/findings/ESS1_5_select_findings.pdf">http://www.europeansocialsurvey.org/docs/findings/ESS1_5_select_findings.pdf</ext-link> [Accessed Nov 23, 2012].</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>L.</given-names></name></person-group> (<year>2008</year>). <article-title>How social is social responses to computers? The function of the degree of anthropomorphism in computer representations</article-title>. <source>Comput. Hum. Behav.</source>
<volume>24</volume>, <fpage>1494</fpage>&#x02013;<lpage>1509</lpage>. <pub-id pub-id-type="doi">10.1016/j.chb.2007.05.007</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gratch</surname><given-names>J.</given-names></name><name><surname>Lucas</surname><given-names>G. M.</given-names></name><name><surname>King</surname><given-names>A. A.</given-names></name><name><surname>Morency</surname><given-names>L. P.</given-names></name></person-group> (<year>2014</year>). <article-title>It's only a computer: the impact of human-agent interaction in clinical interviews</article-title>, in <source>Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>International Foundation for Autonomous Agents and Multiagent Systems</publisher-name>), <fpage>85</fpage>&#x02013;<lpage>92</lpage>.</mixed-citation></ref><ref id="B34"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gratch</surname><given-names>J.</given-names></name><name><surname>Okhmatovskaia</surname><given-names>A.</given-names></name><name><surname>Lamothe</surname><given-names>F.</given-names></name><name><surname>Marsella</surname><given-names>S.</given-names></name><name><surname>Morales</surname><given-names>M.</given-names></name><name><surname>van der Werf</surname><given-names>R. J.</given-names></name><etal/></person-group> (<year>2006</year>). <article-title>Virtual rapport</article-title>, in <source>Proceedings of the 6th International Conference Intelligent Virtual Agents (IVA 2006)</source>, eds <person-group person-group-type="editor"><name><surname>Gratch</surname><given-names>J.</given-names></name><name><surname>Young</surname><given-names>M.</given-names></name><name><surname>Aylett</surname><given-names>R.</given-names></name><name><surname>Ballin</surname><given-names>D.</given-names></name><name><surname>Olivier</surname><given-names>P.</given-names></name></person-group> (<publisher-loc>Marina del Rey, CA</publisher-loc>: <publisher-name>Springer Verlag Berlin Heidelberg</publisher-name>), <fpage>14</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1007/11821830_2</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groves</surname><given-names>R. M.</given-names></name></person-group> (<year>2011</year>). <article-title>Three eras of survey research</article-title>. <source>Public Opin. Q.</source>
<volume>75</volume>, <fpage>861</fpage>&#x02013;<lpage>871</lpage>. <pub-id pub-id-type="doi">10.1093/poq/nfr057</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hatchett</surname><given-names>S.</given-names></name><name><surname>Schuman</surname><given-names>H.</given-names></name></person-group> (<year>1975</year>). <article-title>White respondents and race-of-interviewer effects</article-title>. <source>Public Opin. Q.</source>
<volume>39</volume>, <fpage>523</fpage>&#x02013;<lpage>528</lpage>. <pub-id pub-id-type="doi">10.1086/268249</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Houtkoop-Steenstra</surname><given-names>H.</given-names></name></person-group> (<year>2000</year>). <source>Interaction and the Standardized Survey Interview: The Living Questionnaire</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>
<pub-id pub-id-type="doi">10.1017/CBO9780511489457</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>L.</given-names></name><name><surname>Morency</surname><given-names>L. P.</given-names></name><name><surname>Gratch</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>Virtual rapport 2.0</article-title>, in <source>Proceedings of the 10th International Conference Intelligent Virtual Agents (IVA 2011)</source>, eds <person-group person-group-type="editor"><name><surname>H&#x000f6;gni Vilhj&#x000e1;lmsson</surname><given-names>H.</given-names></name><name><surname>Kopp</surname><given-names>S.</given-names></name><name><surname>Marsella</surname><given-names>S.</given-names></name><name><surname>Th&#x000f3;risson</surname><given-names>K. R.</given-names></name></person-group> (<publisher-loc>Marina del Rey, CA</publisher-loc>: <publisher-name>Springer Verlag Berlin Heidelberg</publisher-name>), <fpage>68</fpage>&#x02013;<lpage>79</lpage>.</mixed-citation></ref><ref id="B39"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hyde</surname><given-names>J.</given-names></name><name><surname>Carter</surname><given-names>E. J.</given-names></name><name><surname>Kiesler</surname><given-names>S.</given-names></name><name><surname>Hodgins</surname><given-names>J. K.</given-names></name></person-group> (<year>2013</year>). <article-title>Perceptual effects of damped and exaggerated facial motion in animated characters</article-title>, in <source>10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG13)</source> (<publisher-loc>Shanghai</publisher-loc>: <publisher-name>IEEE</publisher-name>). <pub-id pub-id-type="doi">10.1109/FG.2013.6553775</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jamieson</surname><given-names>S.</given-names></name></person-group> (<year>2004</year>). <article-title>Likert scales: how to (ab)use them</article-title>. <source>Med. Educ. 38</source>, <fpage>1217</fpage>&#x02013;<lpage>1218</lpage>. <pub-id pub-id-type="doi">10.1111/j.1365-2929.2004.02012.x</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarosz</surname><given-names>A. F.</given-names></name><name><surname>Wiley</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>What are the odds? A practical guide to computing and reporting Bayes Factors</article-title>. <source>J. Probl. Solving</source>
<volume>7</volume>, <fpage>2</fpage>
<pub-id pub-id-type="doi">10.7771/1932-6246.1167</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><collab>JASP</collab></person-group> (<year>2015</year>). Available online at: <ext-link ext-link-type="uri" xlink:href="http://jasp-stats.org/">http://jasp-stats.org/</ext-link></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>M.</given-names></name><name><surname>Ehlen</surname><given-names>P.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Antoun</surname><given-names>C.</given-names></name><name><surname>Fail</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Spoken dialog systems for automated survey interviewing</article-title>, in <source>Proceedings of the 14th Annual SIGDIAL Meeting on Discourse and Dialogue (SIGDIAL 2013)</source> (<publisher-loc>Metz</publisher-loc>), <fpage>329</fpage>&#x02013;<lpage>333</lpage>.</mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keeter</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>Presidential address: survey research, its new frontiers, and democracy</article-title>. <source>Public Opin. Q.</source>
<volume>76</volume>, <fpage>600</fpage>&#x02013;<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1093/poq/nfs044</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kendon</surname><given-names>A.</given-names></name></person-group> (<year>1967</year>). <article-title>Some functions of gaze-direction in social interaction</article-title>. <source>Acta Psychol.</source>
<volume>26</volume>, <fpage>22</fpage>&#x02013;<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/0001-6918(67)90005-4</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kr&#x000e4;mer</surname><given-names>N.</given-names></name><name><surname>Kopp</surname><given-names>S.</given-names></name><name><surname>Becker-Asano</surname><given-names>C.</given-names></name><name><surname>Sommer</surname><given-names>N.</given-names></name></person-group> (<year>2013</year>). <article-title>Smile and the world will smile with you&#x02014;The effects of a virtual agent's smile on users' evaluation and behavior</article-title>. <source>Int. J. Hum. Comput. Stud.</source>
<volume>71</volume>, <fpage>335</fpage>&#x02013;<lpage>349</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijhcs.2012.09.006</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreuter</surname><given-names>F.</given-names></name><name><surname>Presser</surname><given-names>S.</given-names></name><name><surname>Tourangeau</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>Social desirability bias in CATI, IVR, and Web surveys: the effects of mode and question sensitivity</article-title>. <source>Public Opin. Q.</source>
<volume>72</volume>, <fpage>847</fpage>&#x02013;<lpage>865</lpage>. <pub-id pub-id-type="doi">10.1093/poq/nfn063</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lind</surname><given-names>L. H.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Reichert</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>). <article-title>Why do survey respondents disclose more when computers ask the questions?</article-title>
<source>Public Opin. Q.</source>
<volume>77</volume>, <fpage>888</fpage>&#x02013;<lpage>935</lpage>. <pub-id pub-id-type="doi">10.1093/poq/nft038</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>G. M.</given-names></name><name><surname>Gratch</surname><given-names>J.</given-names></name><name><surname>King</surname><given-names>A.</given-names></name><name><surname>Morency</surname><given-names>L. P.</given-names></name></person-group> (<year>2014</year>). <article-title>It's only a computer: virtual humans increase willingness to disclose</article-title>. <source>Comput. Hum. Behav.</source>
<volume>37</volume>, <fpage>94</fpage>&#x02013;<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/j.chb.2014.04.043</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDorman</surname><given-names>K. F.</given-names></name><name><surname>Green</surname><given-names>R. D.</given-names></name><name><surname>Ho</surname><given-names>C.-C.</given-names></name><name><surname>Koch</surname><given-names>C. T.</given-names></name></person-group> (<year>2009</year>). <article-title>Too real for comfort: uncanny responses to computer generated faces</article-title>. <source>Comput. Hum. Behav.</source>
<volume>25</volume>, <fpage>695</fpage>&#x02013;<lpage>710</lpage>. <pub-id pub-id-type="doi">10.1016/j.chb.2008.12.026</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDorman</surname><given-names>K. F.</given-names></name><name><surname>Ishiguro</surname><given-names>H.</given-names></name></person-group> (<year>2006</year>). <article-title>The uncanny advantage of using androids in social and cognitive science research</article-title>. <source>Interact. Stud.</source>
<volume>7</volume>, <fpage>297</fpage>&#x02013;<lpage>337</lpage>. <pub-id pub-id-type="doi">10.1075/is.7.3.03mac</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massey</surname><given-names>D. S.</given-names></name><name><surname>Tourangeau</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>Introduction: new challenges to social measurement</article-title>. <source>Ann. Am. Acad. Pol. Soc. Sci.</source>
<volume>645</volume>, <fpage>6</fpage>&#x02013;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1177/0002716212463314</pub-id><pub-id pub-id-type="pmid">25506081</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mavletova</surname><given-names>A.</given-names></name><name><surname>Couper</surname><given-names>M. P.</given-names></name></person-group> (<year>2014</year>). <article-title>Mobile web survey design: scrolling versus paging, SMS versus email invitations</article-title>. <source>J. Surv. Stat. Methodol.</source>
<volume>2</volume>, <fpage>498</fpage>&#x02013;<lpage>518</lpage>. <pub-id pub-id-type="doi">10.1093/jssam/smu015</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClave</surname><given-names>E. Z.</given-names></name></person-group> (<year>2000</year>). <article-title>Linguistic functions of head movements in the context of speech</article-title>. <source>J. Pragmat.</source>
<volume>37</volume>, <fpage>855</fpage>&#x02013;<lpage>878</lpage>. <pub-id pub-id-type="doi">10.1016/S0378-2166(99)00079-X</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDonnell</surname><given-names>R.</given-names></name><name><surname>Breidt</surname><given-names>M.</given-names></name><name><surname>B&#x000fc;lthoff</surname><given-names>H. H.</given-names></name></person-group> (<year>2012</year>). <article-title>Render me real? Investigating the effect of render style on the perception of animated virtual humans</article-title>. <source>ACM Trans. Graph.</source>
<volume>31</volume>, <fpage>91</fpage>
<pub-id pub-id-type="doi">10.1145/2185520.2185587</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>G.</given-names></name></person-group> (<year>2010</year>). <article-title>Likert scales, levels of measurement and the &#x0201c;laws&#x0201d; of statistics</article-title>. <source>Adv. Health Sci. Educ. Theory Pract.</source>
<volume>15</volume>, <fpage>625</fpage>&#x02013;<lpage>632</lpage>. <pub-id pub-id-type="doi">10.1007/s10459-010-9222-y</pub-id><pub-id pub-id-type="pmid">20146096</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nunamaker</surname><given-names>J. F.</given-names></name><name><surname>Derrick</surname><given-names>D. C.</given-names></name><name><surname>Elkins</surname><given-names>A. C.</given-names></name><name><surname>Burgoon</surname><given-names>J. K.</given-names></name><name><surname>Patton</surname><given-names>M. W.</given-names></name></person-group> (<year>2011</year>). <article-title>Embodied conversational agent-based kiosk for automated interviewing</article-title>. <source>J. Manage. Inf. Syst.</source>
<volume>28</volume>, <fpage>17</fpage>&#x02013;<lpage>48</lpage>. <pub-id pub-id-type="doi">10.2753/MIS0742-1222280102</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peytchev</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>). <article-title>Survey breakoff</article-title>. <source>Public Opin. Q.</source>
<volume>73</volume>, <fpage>74</fpage>&#x02013;<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1093/poq/nfp014</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piwek</surname><given-names>L.</given-names></name><name><surname>McKay</surname><given-names>L. S.</given-names></name><name><surname>Pollick</surname><given-names>F. E.</given-names></name></person-group> (<year>2014</year>). <article-title>Empirical evaluation of the uncanny valley hypothesis fails to confirm the predicted effect of motion</article-title>. <source>Cognition</source>
<volume>130</volume>, <fpage>271</fpage>&#x02013;<lpage>277</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2013.11.001</pub-id><pub-id pub-id-type="pmid">24374019</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schaeffer</surname><given-names>N. C.</given-names></name></person-group> (<year>1991</year>). <article-title>Conversation with a purpose &#x02013; or conversation? Interaction in the standardized interview</article-title>, in <source>Survey Measurement and Process Quality</source>, eds <person-group person-group-type="editor"><name><surname>Biemer</surname><given-names>P. P.</given-names></name><name><surname>Groves</surname><given-names>R. M.</given-names></name><name><surname>Lyberg</surname><given-names>L. E.</given-names></name><name><surname>Mathiowetz</surname><given-names>N. A.</given-names></name><name><surname>Sudman</surname><given-names>S.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley</publisher-name>), <fpage>367</fpage>&#x02013;<lpage>391</lpage>.</mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name></person-group> (<year>1997</year>). <article-title>Does conversational interviewing reduce survey measurement error?</article-title>
<source>Public Opin. Q.</source>
<volume>61</volume>, <fpage>576</fpage>&#x02013;<lpage>602</lpage>.</mixed-citation></ref><ref id="B62"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name></person-group> (<year>2002</year>). <article-title>A collaborative view of standardized survey interviews</article-title>, in <source>Standardization and Tacit Knowledge: Interaction and Practice in the Survey Interview</source>, eds <person-group person-group-type="editor"><name><surname>Maynard</surname><given-names>D.</given-names></name><name><surname>Houtkoop-Steenstra</surname><given-names>H.</given-names></name><name><surname>Schaeffer</surname><given-names>N. C.</given-names></name><name><surname>van der Zouwen</surname><given-names>J.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>), <fpage>67</fpage>&#x02013;<lpage>94</lpage>.</mixed-citation></ref><ref id="B63"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name></person-group> (<year>2008</year>). <article-title>Survey interviews and new communication technologies</article-title>, in <source>Envisioning the Survey Interview of the Future</source>, eds <person-group person-group-type="editor"><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name></person-group> (<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1002/9780470183373.ch1</pub-id></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name></person-group> (<year>2015</year>). <article-title>Improving social measurement by understanding interaction in survey interviews</article-title>. <source>Policy Insights Behav. Brain Sci.</source>
<volume>2</volume>, <fpage>211</fpage>&#x02013;<lpage>219</lpage>. <pub-id pub-id-type="doi">10.1177/2372732215601112</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Antoun</surname><given-names>C.</given-names></name><name><surname>Ehlen</surname><given-names>P.</given-names></name><name><surname>Fail</surname><given-names>S.</given-names></name><name><surname>Hupp</surname><given-names>A. L.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Precision and disclosure in text and voice interviews on smartphones</article-title>. <source>PLoS ONE</source>
<volume>10</volume>:<fpage>e0128337</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0128337</pub-id><pub-id pub-id-type="pmid">26060991</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Dijkstra</surname><given-names>W.</given-names></name><name><surname>Ongena</surname><given-names>Y. P.</given-names></name></person-group> (<year>2012</year>). <article-title>Disfluencies and gaze aversion in unreliable responses to survey questions</article-title>. <source>J. Off. Stat.</source>
<volume>28</volume>, <fpage>555</fpage>&#x02013;<lpage>582</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://www.jos.nu/Articles/abstract.asp?article=284555">http://www.jos.nu/Articles/abstract.asp?article=284555</ext-link></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Ehlen</surname><given-names>P.</given-names></name><name><surname>Fricker</surname><given-names>S. S.</given-names></name></person-group> (<year>2003</year>). <article-title>How web surveys differ from other kinds of user interfaces</article-title>, in <source>Proceedings of the American Statistical Association, Section on Survey Research Methods</source> (<publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>American Statistical Association</publisher-name>). Available online at: <ext-link ext-link-type="uri" xlink:href="http://www.amstat.org/sections/srms/proceedings/y2003/files/jsm2003-000336.pdf">http://www.amstat.org/sections/srms/proceedings/y2003/files/jsm2003-000336.pdf</ext-link></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name><name><surname>Fricker</surname><given-names>S. S.</given-names></name></person-group> (<year>2004</year>). <article-title>Misunderstanding standardized language in research interviews</article-title>. <source>Appl. Cogn. Psychol.</source>
<volume>18</volume>, <fpage>169</fpage>&#x02013;<lpage>188</lpage>. <pub-id pub-id-type="doi">10.1002/acp.955</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Steptoe</surname><given-names>W.</given-names></name><name><surname>Steed</surname><given-names>A.</given-names></name><name><surname>Rovira</surname><given-names>A.</given-names></name><name><surname>Rae</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>Lie tracking: social presence, truth and deception in avatar-mediated telecommunication</article-title>, in <source>CHI'10: Proceedings of the 28th International Conference on Human Factors in Computing Systems</source> (<publisher-loc>Vancouver, BC</publisher-loc>), <fpage>1039</fpage>&#x02013;<lpage>1048</lpage>.</mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suchman</surname><given-names>L.</given-names></name><name><surname>Jordan</surname><given-names>B.</given-names></name></person-group> (<year>1990</year>). <article-title>Interactional troubles in face-to-face survey interviews</article-title>. <source>J. Am. Stat. Assoc.</source>
<volume>85</volume>, <fpage>232</fpage>&#x02013;<lpage>253</lpage>. <pub-id pub-id-type="doi">10.1080/01621459.1990.10475331</pub-id></mixed-citation></ref><ref id="B71"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Suessbrick</surname><given-names>A. L.</given-names></name><name><surname>Schober</surname><given-names>M. F.</given-names></name><name><surname>Conrad</surname><given-names>F. G.</given-names></name></person-group> (<year>2000</year>). <article-title>Different respondents interpret ordinary questions quite differently</article-title>, in <source>Proceedings of the American Statistical Association, Section on Survey Research Methods</source> (<publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>American Statistical Association</publisher-name>). Available online at: <ext-link ext-link-type="uri" xlink:href="https://www.amstat.org/sections/srms/proceedings/papers/2000_155.pdf">https://www.amstat.org/sections/srms/proceedings/papers/2000_155.pdf</ext-link></mixed-citation></ref><ref id="B72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tourangeau</surname><given-names>R.</given-names></name><name><surname>Smith</surname><given-names>T. W.</given-names></name></person-group> (<year>1996</year>). <article-title>Asking sensitive questions: the impact of data collection mode, question format, and question context</article-title>. <source>Public Opin. Q.</source>
<volume>60</volume>, <fpage>275</fpage>&#x02013;<lpage>304</lpage>. <pub-id pub-id-type="doi">10.1086/297751</pub-id></mixed-citation></ref><ref id="B73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>C. F.</given-names></name><name><surname>Ku</surname><given-names>L.</given-names></name><name><surname>Rogers</surname><given-names>S. M.</given-names></name><name><surname>Lindberg</surname><given-names>L. D.</given-names></name><name><surname>Pleck</surname><given-names>J. H.</given-names></name><name><surname>Sonenstein</surname><given-names>F. L.</given-names></name></person-group> (<year>1998</year>). <article-title>Adolescent sexual behavior, drug use, and violence: increased reporting with computer survey technology</article-title>. <source>Science</source>
<volume>280</volume>, <fpage>867</fpage>&#x02013;<lpage>873</lpage>. <pub-id pub-id-type="doi">10.1126/science.280.5365.867</pub-id><pub-id pub-id-type="pmid">9572724</pub-id></mixed-citation></ref><ref id="B74"><mixed-citation publication-type="book"><person-group person-group-type="author"><collab>US Bureau of the Census and US Bureau of Labor Statistics</collab></person-group> (<year>1993</year>). <source>Point of Purchase Survey 1993: Checklist A: Five-Year to One-Week Recall, Vol. 3.</source>
<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>US Department of Commerce, Bureau of the Census</publisher-name>.</mixed-citation></ref><ref id="B75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viera</surname><given-names>A. J.</given-names></name><name><surname>Garrett</surname><given-names>J. M.</given-names></name></person-group> (<year>2005</year>). <article-title>Understanding interobserver agreement: the kappa statistic</article-title>. <source>Fam. Med.</source>
<volume>37</volume>, <fpage>360</fpage>&#x02013;<lpage>363</lpage>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://www.stfm.org/FamilyMedicine/Vol37Issue5/Viera360">http://www.stfm.org/FamilyMedicine/Vol37Issue5/Viera360</ext-link><pub-id pub-id-type="pmid">15883903</pub-id></mixed-citation></ref><ref id="B76"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>von der P&#x000fc;tten</surname><given-names>A.</given-names></name><name><surname>Hoffmann</surname><given-names>L.</given-names></name><name><surname>Klatt</surname><given-names>J.</given-names></name><name><surname>Kr&#x000e4;mer</surname><given-names>N. C.</given-names></name></person-group> (<year>2011</year>). <article-title>Quid pro quo? Reciprocal self-disclosure and communicative accommodation towards a virtual interviewer</article-title>, in <source>Proceedings of the 10th International Conference Intelligent Virtual Agents (IVA 2011),</source> eds <person-group person-group-type="editor"><name><surname>H&#x000f6;gni Vilhj&#x000e1;lmsson</surname><given-names>H.</given-names></name><name><surname>Kopp</surname><given-names>S.</given-names></name><name><surname>Marsella</surname><given-names>S.</given-names></name><name><surname>Th&#x000f3;risson</surname><given-names>K. R.</given-names></name></person-group> (<publisher-loc>Marina del Rey, CA</publisher-loc>: <publisher-name>Springer Verlag Berlin Heidelberg</publisher-name>), <fpage>183</fpage>&#x02013;<lpage>194</lpage>.</mixed-citation></ref><ref id="B77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von der P&#x000fc;tten</surname><given-names>A. M.</given-names></name><name><surname>Kr&#x000e4;mer</surname><given-names>N. C.</given-names></name><name><surname>Gratch</surname><given-names>J.</given-names></name><name><surname>Kang</surname><given-names>S. H.</given-names></name></person-group> (<year>2010</year>). <article-title>&#x0201c;It doesn't matter what you are!&#x0201d; Explaining social effects of agents and avatars</article-title>. <source>Comput. Hum. Behav.</source>
<volume>26</volume>, <fpage>1641</fpage>&#x02013;<lpage>1650</lpage>. <pub-id pub-id-type="doi">10.1016/j.chb.2010.06.012</pub-id></mixed-citation></ref><ref id="B78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>N. G.</given-names></name><name><surname>Perrotta</surname><given-names>S.</given-names></name><name><surname>Quinn</surname><given-names>P. C.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Sun</surname><given-names>Y. H. P.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>). <article-title>On the facilitative effects of face motion on face recognition and its development</article-title>. <source>Front. Psychol.</source>
<volume>5</volume>:<issue>633</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2014.00633</pub-id></mixed-citation></ref><ref id="B79"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yngve</surname><given-names>V.</given-names></name></person-group> (<year>1970</year>). <article-title>On getting a word in edgewise</article-title>, in <source>Papers from the 6th Regional Meeting, Chicago Linguistic Society</source> (<publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>Chicago Linguistic Society</publisher-name>).</mixed-citation></ref></ref-list></back></article>
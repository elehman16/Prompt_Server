<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychology</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Research Foundation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23055988</article-id><article-id pub-id-type="pmc">3458489</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2012.00237</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Spatial Frequency Tuning during the Conscious and Non-Conscious Perception of Emotional Facial Expressions &#x02013; An Intracranial ERP Study</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Willenbockel</surname><given-names>Verena</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Lepore</surname><given-names>Franco</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Nguyen</surname><given-names>Dang Khoa</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Bouthillier</surname><given-names>Alain</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Gosselin</surname><given-names>Fr&#x000e9;d&#x000e9;ric</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Centre de Recherche en Neuropsychologie et Cognition, D&#x000e9;partement de Psychologie, Universit&#x000e9; de Montr&#x000e9;al</institution><country>Montr&#x000e9;al, QC, Canada</country></aff><aff id="aff2"><sup>2</sup><institution>Centre Hospitalier de I&#x02019;Universit&#x000e9; de Montr&#x000e9;al, H&#x000f4;pital Notre-Dame</institution><country>Montr&#x000e9;al, QC, Canada</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Jeroen J. A. Van Boxtel, University of California Los Angeles, USA</p></fn><fn fn-type="edited-by"><p>Reviewed by: Steven Matthew Thurman, University of California Los Angeles, USA; Oana Tudusciuc, California Institute of Technology, USA</p></fn><corresp id="fn001">*Correspondence: Fr&#x000e9;d&#x000e9;ric Gosselin, D&#x000e9;partement de Psychologie, Universit&#x000e9; de Montr&#x000e9;al, C.P. 6128 succursale Centre-Ville, Montr&#x000e9;al, QC, H3C 3J7, Canada. e-mail: <email xlink:type="simple">frederic.gosselin@umontreal.ca</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Frontiers in Consciousness Research, a specialty of Frontiers in Psychology.</p></fn></author-notes><pub-date pub-type="epub"><day>19</day><month>7</month><year>2012</year></pub-date><pub-date pub-type="collection"><year>2012</year></pub-date><volume>3</volume><elocation-id>237</elocation-id><history><date date-type="received"><day>20</day><month>2</month><year>2012</year></date><date date-type="accepted"><day>22</day><month>6</month><year>2012</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2012 Willenbockel, Lepore, Nguyen, Bouthillier and Gosselin.</copyright-statement><copyright-year>2012</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><license-p>This is an open-access article distributed under the terms of the <uri xlink:type="simple" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</uri>, which permits use, distribution and reproduction in other forums, provided the original authors and source are credited and subject to any copyright notices concerning any third-party graphics etc.</license-p></license></permissions><abstract><p>Previous studies have shown that complex visual stimuli, such as emotional facial expressions, can influence brain activity independently of the observers&#x02019; awareness. Little is known yet, however, about the &#x0201c;informational correlates&#x0201d; of consciousness &#x02013; i.e., which low-level information correlates with brain activation during conscious vs. non-conscious perception. Here, we investigated this question in the spatial frequency (SF) domain. We examined which SFs in disgusted and fearful faces modulate activation in the insula and amygdala over time and as a function of awareness, using a combination of intracranial event-related potentials (ERPs), SF Bubbles (Willenbockel et al., <xref ref-type="bibr" rid="B60">2010a</xref>), and Continuous Flash Suppression (CFS; Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>). Patients implanted with electrodes for epilepsy monitoring viewed face photographs (13&#x000b0;&#x02009;&#x000d7;&#x02009;7&#x000b0;) that were randomly SF filtered on a trial-by-trial basis. In the conscious condition, the faces were visible; in the non-conscious condition, they were rendered invisible using CFS. The data were analyzed by performing multiple linear regressions on the SF filters from each trial and the transformed ERP amplitudes across time. The resulting classification images suggest that many SFs are involved in the conscious and non-conscious perception of emotional expressions, with SFs between 6 and 10 cycles per face width being particularly important early on. The results also revealed qualitative differences between the awareness conditions for both regions. Non-conscious processing relied on low SFs more and was faster than conscious processing. Overall, our findings are consistent with the idea that different pathways are employed for the processing of emotional stimuli under different degrees of awareness. The present study represents a first step to mapping how SF information &#x0201c;flows&#x0201d; through the emotion-processing network with a high temporal resolution and to shedding light on the informational correlates of consciousness in general.</p></abstract><kwd-group><kwd>consciousness</kwd><kwd>emotional facial expressions</kwd><kwd>spatial frequency</kwd></kwd-group><counts><fig-count count="5"/><table-count count="1"/><equation-count count="0"/><ref-count count="66"/><page-count count="12"/><word-count count="9406"/></counts></article-meta></front><body><sec><title>Introduction</title><p>The look on someone&#x02019;s face can speak volumes. Emotional facial expressions convey a wealth of information, such as cues about a person&#x02019;s state of mind or warning signs of potentially threatening situations (e.g., reflected by fear) or materials (e.g., reflected by disgust). Human faces and brains are thought to have co-evolved to be efficient transmitters and decoders of emotional signals, respectively (Smith et al., <xref ref-type="bibr" rid="B45">2005</xref>; Schyns et al., <xref ref-type="bibr" rid="B42">2007</xref>, <xref ref-type="bibr" rid="B43">2009</xref>). Moreover, it has been claimed that emotional information from a face can be extracted without the observer&#x02019;s awareness (see Tamietto and De Gelder, <xref ref-type="bibr" rid="B50">2010</xref>, for a review). Numerous studies have shown that face stimuli rendered &#x0201c;invisible&#x0201d; using techniques such as backward masking (e.g., Smith, <xref ref-type="bibr" rid="B44">in press</xref>), binocular rivalry (e.g., Williams et al., <xref ref-type="bibr" rid="B63">2004</xref>), or Continuous Flash Suppression (CFS; e.g., Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>; Jiang and He, <xref ref-type="bibr" rid="B19">2006</xref>; Jiang et al., <xref ref-type="bibr" rid="B20">2009</xref>) can be processed sufficiently for the healthy brain to distinguish neutral from emotional expressions, including fear, disgust, and happiness. Differential brain responses to both invisible and visible facial expressions have been measured, for instance, using functional magnetic resonance imaging (fMRI; e.g., Williams et al., <xref ref-type="bibr" rid="B63">2004</xref>; Jiang and He, <xref ref-type="bibr" rid="B19">2006</xref>) and surface event-related potentials (ERPs; e.g., Jiang et al., <xref ref-type="bibr" rid="B20">2009</xref>; Smith, <xref ref-type="bibr" rid="B44">in press</xref>). Thus, it is now widely thought that facial expressions can influence neural activity and behavior independently of awareness, and that they constitute a stimulus class well suited for investigating differences between conscious and non-conscious perception in the human brain.</p><p>One fundamental question, which is the focus of the present article, concerns which &#x0201c;low-level&#x0201d; aspects of facial-expression signals modulate brain responses as a function of awareness. Faces are complex stimuli that contain information at various spatial frequencies (SFs). Broadly speaking, low SFs represent the coarse information in an image (e.g., luminance blobs), whereas high SFs represent the fine-grained information (e.g., fine wrinkles in a face). It is well known that the visual system filters any retinal input with multiple quasi-linear band-pass filters, each tuned to a specific range of SFs (see De Valois and De Valois, <xref ref-type="bibr" rid="B12">1990</xref>, for a review). The contribution of different SFs to the perception of facial expressions has been investigated in a number of fMRI (Vuilleumier et al., <xref ref-type="bibr" rid="B57">2003</xref>; Winston et al., <xref ref-type="bibr" rid="B65">2003b</xref>; Morawetz et al., <xref ref-type="bibr" rid="B24">2011</xref>) and surface ERP (Holmes et al., <xref ref-type="bibr" rid="B18">2005</xref>; Pourtois et al., <xref ref-type="bibr" rid="B37">2005</xref>; Schyns et al., <xref ref-type="bibr" rid="B42">2007</xref>, <xref ref-type="bibr" rid="B43">2009</xref>; Vlamings et al., <xref ref-type="bibr" rid="B56">2009</xref>) studies. However, the studies led to mixed findings and were limited in several respects. For instance, the low temporal resolution of fMRI and the low spatial resolution of surface ERPs did not allow for conclusions to be drawn about the precise temporal dynamics of SF processing in specific brain regions. Moreover, the SF filtering methods that were employed (low-pass, high-pass, or band-pass filtering) provided only a crude estimate of SF tuning. Also, the studies were restricted to consciously perceived face stimuli. Therefore, not much is known yet about the &#x0201c;informational correlates&#x0201d; of consciousness in this context &#x02013; i.e., precisely which SFs are correlated with localized brain signals during the conscious vs. non-conscious perception of emotional expressions.</p><p>The aim of the present study was to examine which SFs are correlated with brain signals in specific regions of the emotion-processing network under different awareness conditions. We had the opportunity to record intracranial ERPs from the insula and, to a lesser extent, from the amygdala of patients undergoing monitoring for medically intractable epilepsy. The insula and amygdala have previously been associated with the processing of disgust and fear, respectively (e.g., Adolphs et al., <xref ref-type="bibr" rid="B3">1994</xref>, <xref ref-type="bibr" rid="B4">1995</xref>; Phillips et al., <xref ref-type="bibr" rid="B36">1997</xref>, <xref ref-type="bibr" rid="B35">1998</xref>, <xref ref-type="bibr" rid="B34">2004</xref>; Krolak-Salmon et al., <xref ref-type="bibr" rid="B22">2003</xref>, <xref ref-type="bibr" rid="B23">2004</xref>; but for evidence that the insula also responds to fear, see, e.g., Morris et al., <xref ref-type="bibr" rid="B25">1998</xref>, and for evidence that the amygdala also responds to disgust, see Winston et al., <xref ref-type="bibr" rid="B64">2003a</xref>; Fitzgerald et al., <xref ref-type="bibr" rid="B15">2006</xref>; Van der Gaag et al., <xref ref-type="bibr" rid="B55">2007</xref>). Here, we traced which SFs in disgusted and fearful faces modulate activation in these two interconnected brain structures over time. Our study employed a novel combination of CFS (Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>), intracranial recordings, and the SF Bubbles technique (Willenbockel et al., <xref ref-type="bibr" rid="B60">2010a</xref>). We will elaborate on the three methods in the following paragraphs and briefly review some of their applications in previous studies.</p><p>CFS is a powerful method to render visual stimuli invisible (Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>). One of its main strengths is that it allows for suppressing stimuli from awareness for a long duration (i.e., up to several seconds). A second strength of CFS is that the onset of the suppression can be precisely timed. CFS involves presenting a static image to one of the observer&#x02019;s eyes, while dynamic high-contrast noise (e.g., Mondrian patterns flashed at 10&#x02009;Hz; Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>) is presented to the other eye. As a result of this dichoptic stimulation, typically only the noise is consciously perceived; the static stimulus is suppressed from the observer&#x02019;s awareness but nevertheless processed in the brain (e.g., Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>; Jiang and He, <xref ref-type="bibr" rid="B19">2006</xref>; Jiang et al., <xref ref-type="bibr" rid="B20">2009</xref>). Using CFS and fMRI, Jiang and He (<xref ref-type="bibr" rid="B19">2006</xref>) found that suppressed fearful compared with scrambled faces elicited significant activation in the fusiform face area, superior temporal sulcus, and the bilateral amygdalae. The amygdalae were also more activated by fearful than by neutral faces, independently of awareness. Using CFS in combination with surface ERPs, Jiang et al. (<xref ref-type="bibr" rid="B20">2009</xref>) observed significant amplitude differences to suppressed fearful vs. scrambled faces beginning at 140&#x02009;ms and to suppressed fearful vs. neutral faces starting at 220&#x02009;ms after stimulus onset. Overall, combining CFS with fMRI, which has a high spatial resolution, or with surface ERPs, which have a high temporal resolution, has provided important insights into the &#x0201c;where&#x0201d; or &#x0201c;when&#x0201d; of non-conscious facial expression processing &#x02013; but not both aspects simultaneously. In the present study, we combined CFS with intracranial recordings, which combine some of the advantages of fMRI and surface ERPs.</p><p>It has been argued that intracranial recordings currently provide the best combination of high temporal <italic>and</italic> high spatial resolution, plus large anatomical field-of-view and wide frequency bandwidth (Tsuchiya et al., <xref ref-type="bibr" rid="B52">2008</xref>). A number of previous intracranial ERP studies with patients undergoing epilepsy monitoring investigated the temporal dynamics of conscious emotional facial expression processing in the insula and amygdala (Krolak-Salmon et al., <xref ref-type="bibr" rid="B22">2003</xref>, <xref ref-type="bibr" rid="B23">2004</xref>; Pourtois et al., <xref ref-type="bibr" rid="B38">2010</xref>). Krolak-Salmon et al. (<xref ref-type="bibr" rid="B22">2003</xref>) found amplitude differences to disgusted vs. neutral, fearful, and happy expressions in the ventral anterior insula. This &#x0201c;disgust effect&#x0201d; started at approximately 300&#x02009;ms post stimulus onset when observers were engaged in an expression task and approximately 100&#x02009;ms later when they performed a face-gender task. In a similar study, a &#x0201c;fear effect&#x0201d; was observed in the amygdala, starting at 200&#x02009;ms in an expression task and later (after 600&#x02009;ms) in a face-gender task (Krolak-Salmon et al., <xref ref-type="bibr" rid="B23">2004</xref>). Pourtois et al. (<xref ref-type="bibr" rid="B38">2010</xref>) observed earlier amplitude differences to fearful vs. neutral faces in the amygdala, starting at 140&#x02009;ms post stimulus onset. This early effect was not affected by attention but an attentional modulation of emotional responses occurred at longer latencies (after 700&#x02009;ms). Intracranial ERPs were also used to study amygdala activation to masked emotional words (Naccache et al., <xref ref-type="bibr" rid="B28">2005</xref>). Differences between invisible threatening and neutral words were found after 800&#x02009;ms post stimulus onset. In the current study, we combined intracranial recordings with CFS to investigate the temporal dynamics of non-conscious emotional expression processing in these brain regions. Furthermore, we went beyond previous studies by examining precisely which SFs in fearful and disgusted faces modulate brain signals over time by combining intracranial recordings with the SF Bubbles technique (Willenbockel et al., <xref ref-type="bibr" rid="B60">2010a</xref>).</p><p>The Bubbles method (Gosselin and Schyns, <xref ref-type="bibr" rid="B17">2001</xref>) is a classification image technique that can be used to reveal which stimulus information modulates observers&#x02019; behavioral (e.g., Adolphs et al., <xref ref-type="bibr" rid="B2">2005</xref>; Smith et al., <xref ref-type="bibr" rid="B45">2005</xref>) or brain (Schyns et al., <xref ref-type="bibr" rid="B41">2003</xref>, <xref ref-type="bibr" rid="B42">2007</xref>, <xref ref-type="bibr" rid="B43">2009</xref>; Smith et al., <xref ref-type="bibr" rid="B46">2004</xref>, <xref ref-type="bibr" rid="B47">2007</xref>) responses. SF Bubbles (Willenbockel et al., <xref ref-type="bibr" rid="B60">2010a</xref>) is a variant of the technique that can be employed to examine which information in the SF domain correlates with observers&#x02019; responses. SF Bubbles involves randomly sampling the energy of visual stimuli at different SFs on a trial-by-trial basis and then performing a multiple linear regression on the information samples and the response measure of interest to precisely reveal the SF tuning curves for a given task. For example, Willenbockel et al. (<xref ref-type="bibr" rid="B60">2010a</xref>) used the technique to compare the SF tuning of upright and inverted face identification, and Thurman and Grossman (<xref ref-type="bibr" rid="B51">2011</xref>) employed it to investigate SF tuning for discriminating videos of human actions. In the latter study, the results obtained with SF Bubbles were directly compared with those from a more traditional band-pass filtering approach. The results from both methods were consistent but the authors stressed that SF Bubbles offers several advantages. Specifically, SF Bubbles allows for deriving SF tuning curves &#x02013; spanning the whole SF spectrum &#x02013; at a much higher resolution and based on a smaller number of trials. A second strength of the method is that randomly sampling multiple SFs simultaneously on a trial-by-trial basis minimizes the risk that participants adapt to a predictable stimulus manipulation (e.g., band-, low-, or high-pass filtering or critical band masking; see Sowden and Schyns, <xref ref-type="bibr" rid="B48">2006</xref>, for evidence of &#x0201c;channel surfing&#x0201d;). Moreover, SF Bubbles is unbiased in that no cutoff frequencies have to be chosen &#x02013; a parameter that differs considerably between previous experiments using traditional filtering methods (for examples from the emotion-processing literature, see, e.g., Vuilleumier et al., <xref ref-type="bibr" rid="B57">2003</xref>; Vlamings et al., <xref ref-type="bibr" rid="B56">2009</xref>; Morawetz et al., <xref ref-type="bibr" rid="B24">2011</xref>).</p><p>The combination of SF Bubbles with intracranial recordings employed in the current study allowed us to map the SF tuning of the insula and amygdala over time. In one condition, we used CFS to render SF filtered disgusted and fearful faces invisible (i.e., dynamic Mondrian patterns were presented to one eye while an &#x0201c;SF bubblized&#x0201d; emotional face was presented to the other eye). In the other condition, the filtered faces were visible (i.e., an &#x0201c;SF bubblized&#x0201d; face was presented to both eyes). Overall, this study represents a unique opportunity to shed light on the neural processing dynamics for ecologically important visual information as a function of awareness.</p></sec><sec sec-type="materials|methods"><title>Materials and Methods</title><sec><title>Participants</title><p>Three patients with medically intractable epilepsy gave their written informed consent and participated in this experiment. The patients were undergoing epilepsy monitoring at the H&#x000f4;pital Notre-Dame, Montr&#x000e9;al, to guide neurosurgical treatment. For this purpose, they had electrodes implanted under a clinical protocol; the electrode locations were chosen solely based on medical considerations. Our study was approved by the CHUM (Centre Hospitalier de l&#x02019;Universit&#x000e9; de Montr&#x000e9;al) ethics committee and took place at the hospital approximately 6&#x02013;10&#x02009;days after the electrode implantation. The participants were na&#x000ef;ve to the awareness aspect of the study until the debriefing after the experiment. All of them had normal or corrected-to-normal vision; further participant information is summarized in Table <xref ref-type="table" rid="T1">1</xref>.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Participant information</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">ID</th><th align="left" rowspan="1" colspan="1">Gender</th><th align="left" rowspan="1" colspan="1">Age (years)</th><th align="left" rowspan="1" colspan="1">Handedness</th><th align="left" rowspan="1" colspan="1">Seizure focus</th><th align="left" rowspan="1" colspan="1">Hemisphere recorded from</th><th align="left" rowspan="1" colspan="1">Number of trials</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">Female</td><td align="left" rowspan="1" colspan="1">39</td><td align="left" rowspan="1" colspan="1">Ambidextrous</td><td align="left" rowspan="1" colspan="1">Frontal operculum<break/>Temporal operculum<break/>Insula<break/>Superior temporal gyrus</td><td align="left" rowspan="1" colspan="1">Left</td><td align="left" rowspan="1" colspan="1">1920</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">Male</td><td align="left" rowspan="1" colspan="1">34</td><td align="left" rowspan="1" colspan="1">Ambidextrous</td><td align="left" rowspan="1" colspan="1">Hippocampus</td><td align="left" rowspan="1" colspan="1">Left</td><td align="left" rowspan="1" colspan="1">1823</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">Male</td><td align="left" rowspan="1" colspan="1">35</td><td align="left" rowspan="1" colspan="1">Right</td><td align="left" rowspan="1" colspan="1">Inferior frontal gyrus</td><td align="left" rowspan="1" colspan="1">Right</td><td align="left" rowspan="1" colspan="1">2007</td></tr></tbody></table></table-wrap></sec><sec><title>Anatomical location of the electrodes of interest</title><p>All patients had depth electrodes (Ad-Tech Medical Instrument Corporation, Racine, WI, USA) implanted in the insula, and additional grid, strip, or depth electrodes in other regions. One of the patients had a depth electrode implanted in the amygdala. The implantation schemes are described in detail in a previous article (Surbeck et al., <xref ref-type="bibr" rid="B49">2011</xref>). Patient 1 underwent an open microdissection of the Sylvian fissure (Type I implantation). In the anterior, medial, and posterior insula each, she had a Spencer depth electrode with a diameter of 1.1&#x02009;mm, which featured four contacts along its length. The contacts were of 2.3&#x02009;mm in length and spaced 5&#x02009;mm apart from center to center. Two contacts per electrode ended up in the insular cortex. In the amygdala, she also had a depth electrode with four contacts (1.1&#x02009;mm diameter, 2.3&#x02009;mm length, 10&#x02009;mm spacing). Patient 2 underwent the combined Yale-Grenoble stereotactic implantation (Type II). In the anterior and posterior insula each, he was implanted with a 10-contact Spencer depth electrode (1.1&#x02009;mm diameter, 2.3&#x02009;mm length, 10&#x02009;mm spacing). Patient 3 underwent a Type I implantation with a new hybrid operculo-insular electrode (Ad-Tech Medical Instrument Corporation, WI, USA), among other regular subdural electrodes. The hybrid electrode combined the design of a depth and a subdural strip electrode. The depth component featuring two contacts was implanted into the insular cortex. The length of that segment was 10&#x02009;mm and the diameter 1.1&#x02009;mm. The length of each contact was 2.4&#x02009;mm. Further information can be found in an article by Bouthillier et al. (<xref ref-type="bibr" rid="B7">2012</xref>).</p><p>High-resolution MRIs with 1&#x02009;mm-thick slices were obtained after the implantation to determine the exact position of the electrodes (Figure <xref ref-type="fig" rid="F1">1</xref>). A 3D representation of the electrodes with respect to the patient&#x02019;s brain was generated using Grid View software (Stellate Systems Inc., Montreal, QC, Canada; see also Wang et al., <xref ref-type="bibr" rid="B58">2005</xref>). In the analyses presented here we included two contacts per electrode implanted either in the anterior insula (Participants 1&#x02013;3) or in the amygdala (Participant 1).</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Panels show the locations of the electrode contacts of interest for each participant based on post-implantation MRIs (A, anterior; P, posterior; L, left; R, right)</bold>.</p></caption><graphic xlink:href="fpsyg-03-00237-g001"/></fig></sec><sec><title>Electrophysiological recording and stimulus display</title><p>Intracranial EEG was recorded at 2&#x02009;kHz using a Stellate Harmonie system (Stellate Systems, Inc., Montreal, QC, Canada). Either a subdural parietal contact (Participant 1), a subdural temporal contact (Participant 2), or the mastoids (Participant 3) served as a reference. This heterogeneity was not a concern to us because we were interested in the correlations between random SF filters and trial-by-trial voltage variations, which are robust to reference changes. The timing of the stimulus onsets was determined based on the recording of digital trigger signals by the Stellate eAmp using the eAMP Trigger Interface. A dual core 2.19&#x02009;GHz PC (AMD Athlon 64 X2 4200+) and a 17&#x02032;&#x02032; LCD display (VE700, ViewSonic, CA, USA) were used for presenting the stimuli. The gamma parameter was set to 1, to linearize the relationship between the RGB values and corresponding luminance values. The refresh rate was 60&#x02009;Hz and the resolution 1024&#x02009;&#x000d7;&#x02009;768 pixels. The luminance range in the green channel was diminished to match the red channel, which typically has a lower maximum luminance (min&#x02009;=&#x02009;0.4&#x02009;cd/m<sup>2</sup>, max&#x02009;=&#x02009;33.3&#x02009;cd/m<sup>2</sup>). All stimuli were shown on a gray background (13.57&#x02009;cd/m<sup>2</sup>) using the Psychophysics toolbox (Brainard, <xref ref-type="bibr" rid="B8">1997</xref>; Pelli, <xref ref-type="bibr" rid="B31">1997</xref>) for MATLAB (Mathworks, Natick, MA, USA).</p></sec><sec><title>Stimuli</title><p>Twelve grayscale face photographs (256&#x02009;&#x000d7;&#x02009;256 pixels) from the STOIC database (Roy, Roy, &#x000c9;thier-Majcher, Fortin, Belin, and Gosselin, submitted) served as base stimuli. The photographs depicted three male and three female faces, each with a disgusted and a fearful expression (Figure <xref ref-type="fig" rid="F2">2</xref>). The faces were cropped to exclude non-facial cues, and they were equated in mean luminance and contrast [root mean square (RMS) contrast of 0.2] using the SHINE toolbox (Willenbockel et al., <xref ref-type="bibr" rid="B61">2010b</xref>).</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Base face images with disgusted and fearful expressions used in the experiment</bold>.</p></caption><graphic xlink:href="fpsyg-03-00237-g002"/></fig><p>The SFs of the base images (see Figure <xref ref-type="supplementary-material" rid="SM1">S1</xref> in Supplementary Material for a plot of the spectral content of the base faces) were randomly sampled trial-by-trial using the SF Bubbles technique (Willenbockel et al., <xref ref-type="bibr" rid="B60">2010a</xref>). In brief, the to-be-filtered base image was padded with a uniform gray background and then subjected to a fast Fourier transform. The amplitude spectrum of the padded image was multiplied element-wise with a filter constructed in the following way: A vector consisting of randomly distributed binary elements (45 ones among 10,195 zeros) was convolved with a Gaussian kernel, referred to as an &#x0201c;SF bubble&#x0201d; (&#x003c3;&#x02009;=&#x02009;1.8). This yielded a smoothed sampling vector. The sampling vector was subjected to a logarithmic transformation to take into account the fact that the human visual system is more sensitive to low than to high SFs (e.g., De Valois and De Valois, <xref ref-type="bibr" rid="B12">1990</xref>). To obtain a two-dimensional filter, the log-transformed, smoothed sampling vector was then &#x0201c;rotated&#x0201d; about its origin. After multiplying the two-dimensional filter element-wise with the amplitude spectrum of the base image, the result was back-transformed into the image domain via an inverse fast Fourier transform. The &#x0201c;SF bubblized&#x0201d; image contained a random subset of the base image&#x02019;s SF content (see Figure <xref ref-type="fig" rid="F3">3</xref> for sample stimuli; for an illustration of the filtering procedure, see Figure <xref ref-type="fig" rid="F1">1</xref> in Willenbockel et al., <xref ref-type="bibr" rid="B60">2010a</xref>).</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Example of a face image filtered with the SF Bubbles technique on six hypothetical trials</bold>.</p></caption><graphic xlink:href="fpsyg-03-00237-g003"/></fig><p>The contrast level of the SF sampled stimuli was kept constant across experimental conditions but was adjusted for each participant so he/she reported being able to recognize the facial expressions in the <italic>visible face</italic> condition (see <xref ref-type="sec" rid="s1">Procedure</xref>) but did not detect the faces in the <italic>invisible face</italic> condition. For Participants 1 and 2, this resulted in a mean RMS contrast of 0.019 and for Participant 3 of 0.024. To be able to display stimuli with low contrast, we used Floyd-Steinberg dithering (Floyd and Steinberg, <xref ref-type="bibr" rid="B16">1976</xref>), which enhances the luminance resolution (see also Allard and Faubert, <xref ref-type="bibr" rid="B5">2008</xref>). The face stimuli subtended visual angles of approximately 7.1&#x000b0; horizontally and 12.8&#x000b0; vertically.</p><p>The high-contrast noise used for CFS (Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>) consisted of random elliptical Mondrian patterns (Figure <xref ref-type="fig" rid="F4">4</xref>; see also, e.g., Tsuchiya et al., <xref ref-type="bibr" rid="B54">2009</xref>). The mean RMS contrast of the Mondrians was 0.80 (<italic>SD</italic>&#x02009;=&#x02009;0.11). The noise fields were of 256&#x02009;&#x000d7;&#x02009;256 pixels and subtended horizontal and vertical visual angles of approximately 10.6&#x000b0; and 13.7&#x000b0;, respectively (see Figure <xref ref-type="supplementary-material" rid="SM1">S1</xref> in Supplementary Material for a plot of the spectral content of the Mondrians).</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Illustration of the paradigm</bold>. In the visible face condition, a stationary SF filtered face image was shown to both eyes simultaneously (i.e., in the red and green layers). In the invisible face condition, an SF filtered face was shown to one eye (i.e., in the red layer) while dynamic noise patterns were presented at 10&#x02009;Hz to the other eye (i.e., in the green layer). Target stimuli consisted of a stationary SF filtered stimulus presented to one eye and a combination of a face and noise patterns to the other eye. As a result, participants typically perceived the face image on visible face trials, only the dynamic noise on invisible face trials, and both face and noise on target trials. (Note that the contrast and brightness of the images was slightly modified in the figure to improve readability.)</p></caption><graphic xlink:href="fpsyg-03-00237-g004"/></fig><p>Target stimuli consisted of face/Mondrian composites (Figure <xref ref-type="fig" rid="F4">4</xref>). A composite was constructed by multiplying the pixel values of an SF sampled face image (RMS contrast&#x02009;=&#x02009;0.04) element-wise with those of a Mondrian noise field and then adjusting the contrast so it matched the Mondrians. For each target trial, five Mondrian/face composites were constructed using the same face image but different Mondrian patterns.</p></sec><sec id="s1"><title>Procedure</title><p>The participants took part in the experiment while sitting comfortably in their dimly lit hospital room. All stimuli appeared at the center of the computer screen and were viewed from a distance of 56&#x02009;cm through red-green anaglyph glasses. The glasses allowed us to simultaneously present distinct information to each eye of the participant (i.e., one eye with information in red and the other with information in green). Each trial began with a fixation cross presented for 500&#x02013;900&#x02009;ms (to both eyes), followed by a blank screen for 500&#x02013;900&#x02009;ms. Then, a face stimulus was displayed for 500&#x02009;ms in one of three conditions: the <italic>invisible face</italic> condition, the <italic>visible face</italic> condition, or the <italic>target</italic> condition (Figure <xref ref-type="fig" rid="F4">4</xref>).</p><p>In the invisible face condition, we employed CFS to suppress the face stimulus from awareness. The static SF sampled face image was presented to one eye (by showing it in the red layer of the RGB image) while the other eye was presented with suppression noise (i.e., Mondrians were presented in the green layer). The Mondrians changed at a rate of 10&#x02009;Hz (see also Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>). As a result, only the dynamic Mondrians were consciously perceived. In the visible face condition, both eyes were presented with the same SF filtered face by displaying it in both the red and green layers. On target trials, a face was shown to one eye (i.e., in the red layer) while a Mondrian/face composite was shown to the other eye (i.e., in the green layer) at 10&#x02009;Hz.</p><p>The participants were instructed to look at all images carefully and to press the space bar on a regular computer keyboard if they perceived Mondrian patterns and a face together on a given trial. The detection task allowed us to see if participants were paying attention to the stimuli and to evaluate for each CFS trial whether the faces were successfully suppressed. The interstimulus interval was adjusted for each participant to ensure that he/she had enough time for the keypress (see also Jiang and He, <xref ref-type="bibr" rid="B19">2006</xref>).</p><p>One experimental session typically consisted of five 105-trial blocks (plus one practice block in the first session), with breaks in between. After each session, the red and green lenses were swapped. 45.7% of the trials were invisible face trials, 45.7% were visible face trials, and 8.6% were target trials. The different trial types were randomly intermixed within each block. We recorded four sessions per participant (with a maximum of two sessions per day, depending on the patient&#x02019;s willingness for research participation and on clinical constraints). In total, Participant 1 completed 20 blocks, Participant 2, 19 blocks, and Participant 3, 21 blocks.</p></sec><sec><title>Analysis</title><p>The intracranial EEG data from all contacts of interest for each participant were segmented from 200&#x02009;ms before stimulus onset until 1500&#x02009;ms after stimulus onset and baseline corrected using Brain Vision Analyzer 2.0.1 (Brain Products GmbH, Munich, Germany). The following analyses were carried out with custom MATLAB programs. Target trials and all other trials on which a keypress was made were excluded from the SF analysis. Table <xref ref-type="table" rid="T1">1</xref> provides the exact number of trials included in the analyses for each participant.</p><p>To trace which SFs modulate the EEG amplitudes recorded from the insula or amygdala over time, we ran multiple linear regressions on the SF filters from each trial and the transformed EEG amplitudes within time bins of 20&#x02009;ms (separately for each participant, brain region, condition, and session). EEG amplitudes within a given time bin were transformed as follows: First, we averaged the recorded EEG amplitudes within the time bin and across the two contacts of interest from each electrode. Then we performed a median split across trials: we set the amplitude from a given trial to 1 if it was greater than or equal to the median of all trials or to &#x02212;1 if it fell below the median. This way, the impact of any abnormal amplitudes (e.g., due to epileptic spikes) was minimized without having to rely on a subjective trial rejection criterion. We then summed the filters from all trials weighted by the transformed amplitudes, which, here, is equivalent to a multiple linear regression. This was done separately for each of the 85 bins between 200&#x02009;ms before stimulus onset and 1500&#x02009;ms after stimulus onset.</p><p>The vectors of regression coefficients obtained for each time bin were stored in a time segment&#x02009;&#x000d7;&#x02009;SF sampling points matrix and smoothed using Gaussian kernels with a standard deviation of 4.0 time bins and 300 sampling points. The result was transformed into <italic>Z</italic>-scores &#x02013; henceforth called classification images (CIs). We focus here on the overall CIs for each brain structure (insula and amygdala) and for each awareness condition to maximize the signal-to-noise ratio. Separately for the visible and invisible face conditions, we summed the CIs across sessions and divided the result by the square root of the number of sessions (i.e., &#x0221a;4). We then summed the resulting CIs across emotional expressions and divided by the square root of the number of expressions (i.e., &#x0221a;2). In addition, to compute the insula CIs, we summed the respective CIs across participants and divided by the square root of the number of participants (i.e., &#x0221a;3). Statistical significance was evaluated using the Pixel test from the Stat4Ci toolbox (<italic>p&#x02009;</italic>&#x0003c;&#x02009;0.05, <italic>S</italic><sub>r</sub>&#x02009;=&#x02009;870400, FWHM<italic>&#x02009;</italic>=&#x02009;99.91, <italic>Z</italic><sub>crit</sub>&#x02009;=&#x02009;&#x000b1;3.78; Chauvin et al., <xref ref-type="bibr" rid="B9">2005</xref>).</p></sec></sec><sec><title>Results</title><sec><title>Behavioral results</title><p>The detection task served two purposes: (a) to ensure that participants stayed alert during the experiment, and (b) to check on each CFS trial whether the face broke through the suppression noise. The percentage of correctly detected targets for the three participants was very high (<italic>M</italic>&#x02009;=&#x02009;97.07%, <italic>SD</italic>&#x02009;=&#x02009;1.13%), suggesting that the participants paid attention to the stimuli. The percentage of detected non-targets was small (<italic>M</italic>&#x02009;=&#x02009;0.30%, <italic>SD</italic>&#x02009;=&#x02009;0.43%), which confirmed that the faces were successfully suppressed from awareness in the invisible face condition.</p></sec><sec><title>Spatial frequency results</title><p>Figure <xref ref-type="fig" rid="F5">5</xref> depicts the significant pixels (regardless of polarity) for each SF and time bin, up to 1.5&#x02009;s after stimulus onset for the overall insula and amygdala CIs (see Figure <xref ref-type="supplementary-material" rid="SM2">S2</xref> in Supplementary Material for the raw, non-thresholded, CIs). The purple pixels correspond to the visible face condition, the green pixels to the invisible face condition, and the black pixels indicate overlaps between the conditions. We will focus on the SFs that reached significance during stimulus presentation (0&#x02013;500&#x02009;ms). Note, however, that for both regions and visibility conditions, we found multiple other low-, mid-, and high-SF clusters to be significant after the offset of the stimulus.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Classification images for (A) the insula and (B) the amygdala</bold>. The classification images show the significant pixels for the invisible face condition (green) and the visible face condition (purple) for each spatial frequency (in cpf, cycles per face) and time segment between 200&#x02009;ms before stimulus onset and 1500&#x02009;ms after stimulus onset. Black regions indicate the overlap between the awareness conditions. The line graphs (summation plots) show the number of significant pixels across time for each spatial frequency (top) or across spatial frequencies for each time segment (right).</p></caption><graphic xlink:href="fpsyg-03-00237-g005"/></fig><p>Figure <xref ref-type="fig" rid="F5">5</xref>A shows the results for the insula (Participants 1&#x02013;3). In the visible face condition, SFs around 8.75 cycles per face width (cpf) reached significance at approximately 340&#x02009;ms after stimulus onset. In the invisible face condition, SFs around 9.40&#x02009;cpf became significant at approximately 140&#x02009;ms, followed by very low SFs around 2.27&#x02009;cpf. The latter attained significance at approximately 200&#x02009;ms and again at 420&#x02009;ms. The significant pixels of the two visibility conditions overlap for SFs around 9.04&#x02009;cpf between 340 and 400&#x02009;ms.</p><p>Figure <xref ref-type="fig" rid="F5">5</xref>B displays the results for the amygdala (Participant 1). In the visible face condition, SFs around 6.48&#x02009;cpf attained significance at approximately 240&#x02009;ms. In the invisible face condition, SFs around 5.51&#x02009;cpf became significant at about 140&#x02009;ms. Then, at approximately 260&#x02009;ms, very low SFs (1.95&#x02009;cpf) reached significance.</p><p>The line graphs (summation plots) on top of the CIs depict the number of significant pixels for each SF, collapsed across time. For both the insula and amygdala, they show quite clearly that processing in the invisible face condition relied on low SFs more than processing in the visible face condition. Likewise, the graphs on the right of the CIs show the number of significant pixels for each time bin, collapsed across SFs. For both regions, they indicate that significant correlations between SFs and brain signals occurred earlier for the invisible than for the visible face condition.</p></sec></sec><sec sec-type="discussion"><title>Discussion</title><p>The aim of the present study was to shed light on the informational correlates of consciousness in the context of emotional facial expression perception. Specifically, we examined which SFs in consciously and non-consciously perceived stimuli are correlated with brain signals in two key structures of the emotion-processing network &#x02013; the insula and the amygdala. We employed a novel combination of three techniques: intracranial recordings in awake human participants, SF Bubbles (Willenbockel et al., <xref ref-type="bibr" rid="B60">2010a</xref>), and CFS (Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>). To our knowledge, this is the first study that mapped the time course of SF tuning for specific regions of the emotion-processing network and as a function of awareness. In the following, we will put our findings into context by focusing, in turn, on (a) emotional expression perception and awareness, (b) awareness and SF processing (during face perception in general), and (c) SF processing and emotional expression perception. We will then briefly discuss our findings in light of theories on the neural pathways involved in emotion perception.</p><p>Disgusted and fearful faces were used as stimuli because previous work has shown that the insula and amygdala are implicated in the processing of these facial expressions. In particular, numerous studies led to the conclusion that the anterior insula is important for the processing of disgust (e.g., Phillips et al., <xref ref-type="bibr" rid="B36">1997</xref>, <xref ref-type="bibr" rid="B35">1998</xref>; Krolak-Salmon et al., <xref ref-type="bibr" rid="B22">2003</xref>), and the amygdala for the processing of fear (e.g., Adolphs et al., <xref ref-type="bibr" rid="B3">1994</xref>, <xref ref-type="bibr" rid="B4">1995</xref>; Morris et al., <xref ref-type="bibr" rid="B26">1996</xref>; Phillips et al., <xref ref-type="bibr" rid="B35">1998</xref>; Krolak-Salmon et al., <xref ref-type="bibr" rid="B23">2004</xref>); taken together, other studies indicated that these brain regions respond to both disgusted and fearful faces (e.g., Morris et al., <xref ref-type="bibr" rid="B25">1998</xref>; Winston et al., <xref ref-type="bibr" rid="B64">2003a</xref>; Fitzgerald et al., <xref ref-type="bibr" rid="B15">2006</xref>; Van der Gaag et al., <xref ref-type="bibr" rid="B55">2007</xref>; see also Anderson et al., <xref ref-type="bibr" rid="B6">2003</xref>). Our results replicate these findings. However, the emotion-specificity of the responses in these regions goes beyond the scope of this article.</p><p>Previous work has also shown that both disgusted and fearful expressions can be perceived independently of awareness (e.g., Smith, <xref ref-type="bibr" rid="B44">in press</xref>). Using various methods to render stimuli invisible, neuroimaging studies demonstrated that the amygdala is involved in the non-conscious processing of emotional faces (e.g., Whalen et al., <xref ref-type="bibr" rid="B59">1998</xref>; Morris et al., <xref ref-type="bibr" rid="B27">1999</xref>; Pasley et al., <xref ref-type="bibr" rid="B30">2004</xref>; Williams et al., <xref ref-type="bibr" rid="B63">2004</xref>, <xref ref-type="bibr" rid="B62">2006</xref>; Jiang and He, <xref ref-type="bibr" rid="B19">2006</xref>; but see Phillips et al., <xref ref-type="bibr" rid="B34">2004</xref>). Scarce studies found support for an involvement of the insula in the non-conscious processing of emotional stimuli (e.g., Sabatini et al., <xref ref-type="bibr" rid="B40">2009</xref>; but see, Anderson et al., <xref ref-type="bibr" rid="B6">2003</xref>, and Phillips et al., <xref ref-type="bibr" rid="B34">2004</xref>, for results that speak against automatic facial expression processing in the insula). The present results indicate that both structures play a role in perceiving emotional expressions, consciously and non-consciously.</p><p>In our visible face condition, the first significant correlations between stimulus information and brain signals occurred at approximately 340 and 240&#x02009;ms after stimulus onset in the insula and amygdala, respectively. In our invisible face condition, they were present as early as 140&#x02009;ms in both regions. Moreover, in both visibility conditions, we found significant correlations at long latencies, up to 1500&#x02009;ms after stimulus onset. These temporal dynamics appear largely consistent with the results from previous intracranial ERP studies on conscious emotional facial expression perception, although a direct comparison is difficult due to important methodological differences. In line with our finding that the response of the insula occurred later than that of the amygdala, previous results revealed emotional effects as early as 300&#x02009;ms post stimulus onset in the insula (Krolak-Salmon et al., <xref ref-type="bibr" rid="B22">2003</xref>), and as early as 140 (Pourtois et al., <xref ref-type="bibr" rid="B38">2010</xref>) or 200&#x02009;ms (Krolak-Salmon et al., <xref ref-type="bibr" rid="B23">2004</xref>) in the amygdala.</p><p>Furthermore, long-latency effects were present in previous intracranial ERP data as well. For instance, Krolak-Salmon et al. (<xref ref-type="bibr" rid="B23">2004</xref>) observed differential responses to fear vs. neutral or happy faces until 1100&#x02009;ms after stimulus onset in the amygdala of one patient. Pourtois et al. (<xref ref-type="bibr" rid="B38">2010</xref>) found late emotional effects in the amygdala that were modulated by attention, starting at approximately 700&#x02009;ms after stimulus onset and lasting more than 300&#x02009;ms. Finally, such late effects were seen in response to invisible emotional words in the amygdala (after 800&#x02009;ms after stimulus onset; Naccache et al., <xref ref-type="bibr" rid="B28">2005</xref>), suggesting that considerable time is needed for extracting emotional meaning. Naccache et al. (<xref ref-type="bibr" rid="B28">2005</xref>) speculated that top-down influences might amplify non-conscious amygdala activation in this context, without making information accessible to conscious report. Possibly, the late significant correlations with low-level information that we found also reflect feedback or top-down influences that amplify certain aspects of the stimuli later on.</p><p>Our CIs show complex patterns of SF tuning over time, for both the insula and the amygdala. A comparison of the CIs between awareness conditions revealed that invisible face processing relied on very low SFs (&#x0003c;3&#x02009;cpf) more than visible face processing, especially within the first 600&#x02009;ms after stimulus onset. The idea that SF processing and awareness interact during face perception has come up repeatedly in the literature but has, as far as we know, only been investigated in one published study (De Gardelle and Kouider, <xref ref-type="bibr" rid="B10">2010</xref>). The authors employed a masked priming paradigm with hybrid prime stimuli &#x02013; composed of the low SFs of one face and the high SFs of another face &#x02013; and a fame judgment task. Using behavioral measures, they discovered that both low SFs (&#x0003c;12&#x02009;cpf) and high SFs (&#x0003e;12&#x02009;cpf) could be processed without awareness. The influence of high SFs correlated with prime visibility (i.e., prime duration), whereas the influence of low SFs did not. De Gardelle and Kouider&#x02019;s results are consistent with ours inasmuch as we also found a broad range of SFs to be processed non-consciously. The qualitative differences that we observed between our awareness conditions, however, were not seen in their data. This discrepancy could be due to several methodological differences between the studies.</p><p>Whereas not much work has been done on SF processing and awareness, several studies have looked at SF processing during the conscious perception of fearful faces. The majority of studies imply that low SFs are particularly important for the perception of fear (Vuilleumier et al., <xref ref-type="bibr" rid="B57">2003</xref>; Winston et al., <xref ref-type="bibr" rid="B65">2003b</xref>; Pourtois et al., <xref ref-type="bibr" rid="B37">2005</xref>; Vlamings et al., <xref ref-type="bibr" rid="B56">2009</xref>; but see Holmes et al., <xref ref-type="bibr" rid="B18">2005</xref>; Morawetz et al., <xref ref-type="bibr" rid="B24">2011</xref>). For instance, in an fMRI study, Vuilleumier et al. (<xref ref-type="bibr" rid="B57">2003</xref>) observed larger amygdala responses to fearful than to neutral faces when stimuli were unfiltered or low-pass filtered (&#x0003c;6&#x02009;cpf), but not when they were high-pass filtered (&#x0003e;24&#x02009;cpf). In a recent surface ERP study (Vlamings et al., <xref ref-type="bibr" rid="B56">2009</xref>), it was found that fearful relative to neutral faces elicited a larger P1 component (i.e., a positive deflection around 100&#x02009;ms post stimulus onset) and a larger N170 (i.e., a negative deflection around 170&#x02009;ms), also only for low-pass (&#x02264;12&#x02009;cpf), not for high-pass (&#x02265;36&#x02009;cpf), filtered faces. These findings are in line with our amygdala CI: many pixels reached significance for SFs under 6&#x02009;cpf but very few attained significance for SFs above 24&#x02009;cpf [see the summation plot in Figure <xref ref-type="fig" rid="F5">5</xref>B (top)]. However, as discussed above, we did not find any significant SFs for latencies below 200&#x02009;ms in our visible face condition, suggesting that the early emotional effects observed using surface ERPs (Vlamings et al., <xref ref-type="bibr" rid="B56">2009</xref>; see also Pourtois et al., <xref ref-type="bibr" rid="B37">2005</xref>) are probably not driven by the amygdala or insula.</p><p>The SF tuning patterns we found raise the question about the underlying neural mechanisms of SF processing as a function of awareness. Specifically, through which pathways does SF information arrive at the insula and amygdala? Currently two theories are discussed in the emotion-processing literature, namely the subcortical pathway hypothesis (for recent reviews see Tamietto and De Gelder, <xref ref-type="bibr" rid="B50">2010</xref>; De Gelder et al., <xref ref-type="bibr" rid="B11">2011</xref>) and the multiple waves model (Pessoa and Adolphs, <xref ref-type="bibr" rid="B32">2010</xref>, <xref ref-type="bibr" rid="B33">2011</xref>). According to the former, low-SF information from emotional stimuli is conveyed quickly and automatically via a subcortical route through the superior colliculus and the pulvinar nucleus of the thalamus to the amygdala, whereas high SFs are processed more slowly along a cortical route. The multiple waves model, in contrast, suggests that emotional information is processed in parallel by multiple cortical pathways, without reliance on a direct subcortical route to the amygdala. Our study was not designed to test these theories; however, our results appear to be consistent with the multiple waves model, while they challenge the subcortical pathway hypothesis in at least two ways. The first hurdle for the subcortical pathway hypothesis is that the early low-SF clusters revealed to be significant in the invisible face condition are <italic>not</italic> present in the visible face condition. The second hurdle is that the latencies we found in both awareness conditions (140&#x02009;ms in the invisible face condition, and 340&#x02009;ms or 240&#x02009;ms in the visible face condition for the insula and amygdala, respectively) do not appear faster than cortical visual processing (see Pessoa and Adolphs, <xref ref-type="bibr" rid="B32">2010</xref>, for a review). More work will be needed to test these two theories.</p><p>One limitation of the current study is that since we recorded brain signals from patients with epilepsy, we cannot be entirely sure that our data are representative of the healthy population. For Participant 1, epileptic spikes were found in the insula; we therefore recomputed our CIs without her data for the insular contacts. However, we did not find any changes in the main results (see Figure <xref ref-type="supplementary-material" rid="SM3">S3</xref> in Supplementary Material). The structures we recorded from in all participants were structurally normal-appearing on high-resolution MRI. Thus, we think it is reasonable to assume that the results we report here can be generalized. Intracranial recordings from volunteers with epilepsy have previously been used in several studies (e.g., Oya et al., <xref ref-type="bibr" rid="B29">2002</xref>; Krolak-Salmon et al., <xref ref-type="bibr" rid="B22">2003</xref>, <xref ref-type="bibr" rid="B23">2004</xref>; Naccache et al., <xref ref-type="bibr" rid="B28">2005</xref>; Tsuchiya et al., <xref ref-type="bibr" rid="B52">2008</xref>; Pourtois et al., <xref ref-type="bibr" rid="B38">2010</xref>) because they bear a number of advantages &#x02013; specifically, a millisecond temporal resolution combined with a high spatial resolution &#x02013; and are thus considered to provide an important window into the workings of the human brain.</p><p>A second drawback is that in creating the two awareness conditions, we introduced differences in physical stimulation. In the visible face condition, a static face was presented to both eyes, whereas in the invisible face condition, dynamic high-contrast noise replaced the face presented to one eye. This has the disadvantage that we do not know to what extent and how the flashing of the noise patterns influenced our SF results (see Yang and Blake, <xref ref-type="bibr" rid="B66">2012</xref>). We chose suppression noise that was used in several previous studies and found to be very effective (i.e., high-contrast Mondrian patterns; e.g., Tsuchiya and Koch, <xref ref-type="bibr" rid="B53">2005</xref>; Jiang and He, <xref ref-type="bibr" rid="B19">2006</xref>; Jiang et al., <xref ref-type="bibr" rid="B20">2009</xref>). The spectral energy of our Mondrians was highly correlated with that of our base faces (see Figure <xref ref-type="supplementary-material" rid="SM1">S1</xref> in Supplementary Material; the correlation between the average across faces and the average across noise patterns was <italic>r</italic>&#x02009;=&#x02009;0.95). Our Mondrians consisted of elliptical elements (see also Tsuchiya et al., <xref ref-type="bibr" rid="B54">2009</xref>; Adams et al., <xref ref-type="bibr" rid="B1">2010</xref>) and thus contained energy at all orientations. It is not yet known what the optimal suppression noise would be, and basically all methods used to render visual stimuli invisible for normal-sighted observers introduce differences in stimulation. Therefore, this problem is difficult to overcome (see, e.g., the review by Tamietto and De Gelder, <xref ref-type="bibr" rid="B50">2010</xref>). We used CFS because it results in longer suppression than other techniques, such as backward masking or binocular rivalry. Also, it has the advantage that the suppression can be precisely timed.</p><p>Investigating the informational correlates of consciousness from several angles &#x02013; i.e., with different awareness-manipulating techniques and paradigms (e.g., Faivre et al., <xref ref-type="bibr" rid="B14">2012</xref>; for a review see Kim and Blake, <xref ref-type="bibr" rid="B21">2005</xref>) &#x02013; might currently be the best approach to overcome the limitations of the present study. For example, one could combine the SF Bubbles technique with masked priming to examine which SFs of visible vs. invisible primes influence observers&#x02019; responses to a visible target. It might also be a good idea to use SF Bubbles together with a crowding paradigm, which has recently been emphasized as a more ecologically valid approach than masking or CFS (Faivre et al., <xref ref-type="bibr" rid="B14">2012</xref>). Furthermore, it might be insightful to combine SF Bubbles with an attentional blink (e.g., Raymond et al., <xref ref-type="bibr" rid="B39">1992</xref>) paradigm, where physical stimulation remains constant but stimuli can be rendered invisible by diverting the observers&#x02019; attention. This could represent a promising avenue for contrasting SF tuning between conscious and <italic>preconscious</italic> processing (see Dehaene et al., <xref ref-type="bibr" rid="B13">2006</xref>). The present study is meant as a first step toward gathering converging evidence about the informational correlates of consciousness.</p></sec><sec><title>Conclusion</title><p>Using state-of-the art techniques, we mapped the SF tuning of the insula and amygdala as a function of awareness. Our results are consistent with the idea that a wide range of SFs plays a role in the conscious and non-conscious perception of emotional facial expressions, with SFs between 6 and 10&#x02009;cpf appearing particularly important early on (for faces subtending approximately 7&#x000b0;). That being said, qualitative differences in SF tuning were observed between our awareness conditions &#x02013; particularly in the early processing of very low SFs &#x02013; that are consistent with the idea that different neural pathways are employed for conveying visual information to the amygdala and insula under different degrees of awareness. The present study paves the way for future work that investigates the temporal dynamics of SF processing in specific structures of the emotion-processing network and for elucidating the informational correlates of consciousness in general.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><p>The Supplementary Material for this article can be found online at <uri xlink:type="simple" xlink:href="http://www.frontiersin.org/Consciousness_Research/10.3389/fpsyg.2012.00237/abstract">http://www.frontiersin.org/Consciousness_Research/10.3389/fpsyg.2012.00237/abstract</uri></p><supplementary-material content-type="local-data" id="SM1"><label>Supplementary Figure S1</label><caption><p><bold>Rotational average of the power spectra of the 12 base faces and the suppression noise (540 Mondrian patterns)</bold>.</p></caption><media xlink:href="Presentation 1.PDF" mimetype="application" mime-subtype="pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM2"><label>Supplementary Figure S2</label><caption><p><bold>Raw classification images for the two regions (top: insula; bottom: amygdala) and awareness conditions (left: visible face; right: invisible face)</bold>.</p></caption><media xlink:href="Presentation 1.PDF" mimetype="application" mime-subtype="pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="SM3"><label>Supplementary Figure S3</label><caption><p><bold>Insula classification images [(A), thresholded; (B), raw] computed from the data of Participants 2 and 3 only</bold>.</p></caption><media xlink:href="Presentation 1.PDF" mimetype="application" mime-subtype="pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>We thank the technicians at the CERNEC and the H&#x000f4;pital Notre-Dame for help with data collection and the participants for their time and effort. We also thank the two reviewers for their helpful comments. This research was supported by a graduate student scholarship from the Fonds Qu&#x000e9;b&#x000e9;cois de la Recherche sur la Nature et les Technologies (FQRNT) to Verena Willenbockel, as well as by Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery grants awarded to Fr&#x000e9;d&#x000e9;ric Gosselin and to Franco Lepore.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>W. J.</given-names></name><name><surname>Gray</surname><given-names>K. L. H.</given-names></name><name><surname>Garner</surname><given-names>M.</given-names></name><name><surname>Graf</surname><given-names>E. W.</given-names></name></person-group> (<year>2010</year>). <article-title>High-level face adaptation without awareness</article-title>. <source>Psychol. Sci.</source>
<volume>21</volume>, <fpage>205</fpage>&#x02013;<lpage>210</lpage><pub-id pub-id-type="doi">10.1177/0956797609359508</pub-id><pub-id pub-id-type="pmid">20424046</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname><given-names>R.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name><name><surname>Buchanan</surname><given-names>T. W.</given-names></name><name><surname>Tranel</surname><given-names>D.</given-names></name><name><surname>Schyns</surname><given-names>P.</given-names></name><name><surname>Damasio</surname><given-names>A. R.</given-names></name></person-group> (<year>2005</year>). <article-title>A mechanism for impaired fear recognition after amygdala damage</article-title>. <source>Nature</source>
<volume>433</volume>, <fpage>68</fpage>&#x02013;<lpage>72</lpage><pub-id pub-id-type="doi">10.1038/nature03086</pub-id><pub-id pub-id-type="pmid">15635411</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname><given-names>R.</given-names></name><name><surname>Tranel</surname><given-names>D.</given-names></name><name><surname>Damasio</surname><given-names>H.</given-names></name><name><surname>Damasio</surname><given-names>A.</given-names></name></person-group> (<year>1994</year>). <article-title>Impaired recognition of emotion in facial expressions following bilateral damage to the human amygdala</article-title>. <source>Nature</source>
<volume>372</volume>, <fpage>669</fpage>&#x02013;<lpage>672</lpage><pub-id pub-id-type="doi">10.1038/372669a0</pub-id><pub-id pub-id-type="pmid">7990957</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname><given-names>R.</given-names></name><name><surname>Tranel</surname><given-names>D.</given-names></name><name><surname>Damasio</surname><given-names>H.</given-names></name><name><surname>Damasio</surname><given-names>A. R.</given-names></name></person-group> (<year>1995</year>). <article-title>Fear and the human amygdala</article-title>. <source>J. Neurosci.</source>
<volume>15</volume>, <fpage>5879</fpage>&#x02013;<lpage>5891</lpage><pub-id pub-id-type="pmid">7666173</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allard</surname><given-names>R.</given-names></name><name><surname>Faubert</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>The noisy-bit method for digital displays: converting a 256 luminance resolution into a continuous resolution</article-title>. <source>Behav. Res. Methods</source>
<volume>40</volume>, <fpage>735</fpage>&#x02013;<lpage>743</lpage><pub-id pub-id-type="doi">10.3758/BRM.40.3.735</pub-id><pub-id pub-id-type="pmid">18697669</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>A. K.</given-names></name><name><surname>Christoff</surname><given-names>K.</given-names></name><name><surname>Panitz</surname><given-names>D.</given-names></name><name><surname>De Rosa</surname><given-names>E.</given-names></name><name><surname>Gabrieli</surname><given-names>J. D. E.</given-names></name></person-group> (<year>2003</year>). <article-title>Neural correlates of the automatic processing of threat facial signals</article-title>. <source>J. Neurosci.</source>
<volume>23</volume>, <fpage>5627</fpage>&#x02013;<lpage>5633</lpage><pub-id pub-id-type="pmid">12843265</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouthillier</surname><given-names>A.</given-names></name><name><surname>Surbeck</surname><given-names>W.</given-names></name><name><surname>Weil</surname><given-names>A. G.</given-names></name><name><surname>Tayah</surname><given-names>T.</given-names></name><name><surname>Nguyen</surname><given-names>D. K.</given-names></name></person-group> (<year>2012</year>). <article-title>The hybrid operculo-insular electrode: a new electrode for intracranial investigation of peri-sylvian/insular refractory epilepsy</article-title>. <source>Neurosurgery</source>
<volume>70</volume>, <fpage>1574</fpage>&#x02013;<lpage>1580</lpage><pub-id pub-id-type="doi">10.1227/NEU.0b013e318246a3b7</pub-id><pub-id pub-id-type="pmid">22186839</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>D. H.</given-names></name></person-group> (<year>1997</year>). <article-title>The psychophysics toolbox</article-title>. <source>Spat. Vis.</source>
<volume>10</volume>, <fpage>433</fpage>&#x02013;<lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chauvin</surname><given-names>A.</given-names></name><name><surname>Worsley</surname><given-names>K. J.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Arguin</surname><given-names>M.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name></person-group> (<year>2005</year>). <article-title>Accurate statistical tests for smooth classification images</article-title>. <source>J. Vis.</source>
<volume>5</volume>, <fpage>659</fpage>&#x02013;<lpage>667</lpage><pub-id pub-id-type="doi">10.1167/5.9.1</pub-id><pub-id pub-id-type="pmid">16356076</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Gardelle</surname><given-names>V.</given-names></name><name><surname>Kouider</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>How spatial frequencies and visual awareness interact during face processing</article-title>. <source>Psychol. Sci.</source>
<volume>21</volume>, <fpage>58</fpage>&#x02013;<lpage>66</lpage><pub-id pub-id-type="doi">10.1177/0956797609354064</pub-id><pub-id pub-id-type="pmid">20424024</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Gelder</surname><given-names>B.</given-names></name><name><surname>Van Honk</surname><given-names>J.</given-names></name><name><surname>Tamietto</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Emotion in the brain: of low roads, high roads and roads less travelled</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>12</volume>, <fpage>425</fpage><pub-id pub-id-type="doi">10.1038/nrn2889-c2</pub-id><pub-id pub-id-type="pmid">21673722</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>De Valois</surname><given-names>R. L.</given-names></name><name><surname>De Valois</surname><given-names>K. K.</given-names></name></person-group> (<year>1990</year>). <source>Spatial Vision</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S.</given-names></name><name><surname>Changeux</surname><given-names>J.-P.</given-names></name><name><surname>Naccache</surname><given-names>L.</given-names></name><name><surname>Sackur</surname><given-names>J.</given-names></name><name><surname>Sergent</surname><given-names>C.</given-names></name></person-group> (<year>2006</year>). <article-title>Conscious, preconscious, and subliminal processing: a testable taxonomy</article-title>. <source>Trends Cogn. Sci. (Regul. Ed.)</source>
<volume>10</volume>, <fpage>204</fpage>&#x02013;<lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.03.007</pub-id><pub-id pub-id-type="pmid">16603406</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faivre</surname><given-names>N.</given-names></name><name><surname>Berthet</surname><given-names>V.</given-names></name><name><surname>Kouider</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>Nonconscious influences from emotional faces: a comparison of visual crowding, masking, and continuous flash suppression</article-title>. <source>Front. Psychol.</source>
<volume>3</volume>:<fpage>129</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00129</pub-id><pub-id pub-id-type="pmid">22563325</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzgerald</surname><given-names>D. A.</given-names></name><name><surname>Angstadt</surname><given-names>M.</given-names></name><name><surname>Jelsone</surname><given-names>L. M.</given-names></name><name><surname>Nathan</surname><given-names>P. J.</given-names></name><name><surname>Phan</surname><given-names>K. L.</given-names></name></person-group> (<year>2006</year>). <article-title>Beyond threat: amygdala reactivity across multiple expressions of facial affect</article-title>. <source>Neuroimage</source>
<volume>30</volume>, <fpage>1441</fpage>&#x02013;<lpage>1448</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.11.003</pub-id><pub-id pub-id-type="pmid">16368249</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Floyd</surname><given-names>R. W.</given-names></name><name><surname>Steinberg</surname><given-names>L.</given-names></name></person-group> (<year>1976</year>). <article-title>An adaptive algorithm for spatial greyscale</article-title>. <source>Proc. Soc. Inf. Disp.</source>
<volume>17</volume>, <fpage>75</fpage>&#x02013;<lpage>77</lpage></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosselin</surname><given-names>F.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2001</year>). <article-title>Bubbles: a technique to reveal the use of information in recognition tasks</article-title>. <source>Vision Res.</source>
<volume>41</volume>, <fpage>2261</fpage>&#x02013;<lpage>2271</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(01)00097-9</pub-id><pub-id pub-id-type="pmid">11448718</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>A.</given-names></name><name><surname>Winston</surname><given-names>J. S.</given-names></name><name><surname>Eimer</surname><given-names>M.</given-names></name></person-group> (<year>2005</year>). <article-title>The role of spatial frequency information for ERP components sensitive to faces and emotional facial expression</article-title>. <source>Cogn. Brain Res.</source>
<volume>25</volume>, <fpage>508</fpage>&#x02013;<lpage>520</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2005.08.003</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y.</given-names></name><name><surname>He</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). <article-title>Cortical responses to invisible faces: dissociating subsystems for facial-information processing</article-title>. <source>Curr. Biol.</source>
<volume>16</volume>, <fpage>2023</fpage>&#x02013;<lpage>2029</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.08.084</pub-id><pub-id pub-id-type="pmid">17055981</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y.</given-names></name><name><surname>Shannon</surname><given-names>R. W.</given-names></name><name><surname>Vizueta</surname><given-names>N.</given-names></name><name><surname>Bernat</surname><given-names>E. M.</given-names></name><name><surname>Patrick</surname><given-names>C. J.</given-names></name><name><surname>He</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>Dynamics of processing invisible faces in the brain: automatic neural encoding of facial expression information</article-title>. <source>Neuroimage</source>
<volume>44</volume>, <fpage>1171</fpage>&#x02013;<lpage>1177</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.09.038</pub-id><pub-id pub-id-type="pmid">18976712</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>C.-Y.</given-names></name><name><surname>Blake</surname><given-names>R.</given-names></name></person-group> (<year>2005</year>). <article-title>Psychophysical magic: rendering the visible &#x02018;invisible.&#x02019;</article-title>
<source>Trends Cogn. Sci. (Regul. Ed.)</source>
<volume>9</volume>, <fpage>381</fpage>&#x02013;<lpage>388</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.06.012</pub-id><pub-id pub-id-type="pmid">16006172</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krolak-Salmon</surname><given-names>P.</given-names></name><name><surname>H&#x000e9;naff</surname><given-names>M.-A.</given-names></name><name><surname>Isnard</surname><given-names>J.</given-names></name><name><surname>Tallon-Baudry</surname><given-names>C.</given-names></name><name><surname>Gu&#x000e9;not</surname><given-names>M.</given-names></name><name><surname>Vighetto</surname><given-names>A.</given-names></name><name><surname>Bertrand</surname><given-names>O.</given-names></name><name><surname>Maugui&#x000e8;re</surname><given-names>F.</given-names></name></person-group> (<year>2003</year>). <article-title>An attention modulated response to disgust in human ventral anterior insula</article-title>. <source>Ann. Neurol.</source>
<volume>53</volume>, <fpage>446</fpage>&#x02013;<lpage>453</lpage><pub-id pub-id-type="doi">10.1002/ana.10403</pub-id><pub-id pub-id-type="pmid">12666112</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krolak-Salmon</surname><given-names>P.</given-names></name><name><surname>H&#x000e9;naff</surname><given-names>M.-A.</given-names></name><name><surname>Vighetto</surname><given-names>A.</given-names></name><name><surname>Bertrand</surname><given-names>O.</given-names></name><name><surname>Maugui&#x000e8;re</surname><given-names>F.</given-names></name></person-group> (<year>2004</year>). <article-title>Early amygdala reaction to fear spreading in occipital, temporal, and frontal cortex: a depth electrode ERP study in human</article-title>. <source>Neuron</source>
<volume>42</volume>, <fpage>665</fpage>&#x02013;<lpage>676</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(04)00264-8</pub-id><pub-id pub-id-type="pmid">15157426</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morawetz</surname><given-names>C.</given-names></name><name><surname>Baudewig</surname><given-names>J.</given-names></name><name><surname>Treue</surname><given-names>S.</given-names></name><name><surname>Dechent</surname><given-names>P.</given-names></name></person-group> (<year>2011</year>). <article-title>Effects of spatial frequency and location of fearful faces on human amygdala activity</article-title>. <source>Brain Res.</source>
<volume>1371</volume>, <fpage>87</fpage>&#x02013;<lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2010.10.110</pub-id><pub-id pub-id-type="pmid">21059346</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>J. S.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name><name><surname>B&#x000fc;chel</surname><given-names>C.</given-names></name><name><surname>Frith</surname><given-names>C. D.</given-names></name><name><surname>Young</surname><given-names>A. W.</given-names></name><name><surname>Calder</surname><given-names>A. J.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>1998</year>). <article-title>A neuromodulatory role for the human amygdala in processing emotional facial expressions</article-title>. <source>Brain</source>
<volume>121</volume>, <fpage>47</fpage>&#x02013;<lpage>57</lpage><pub-id pub-id-type="doi">10.1093/brain/121.1.47</pub-id><pub-id pub-id-type="pmid">9549487</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>J. S.</given-names></name><name><surname>Frith</surname><given-names>C. D.</given-names></name><name><surname>Perrett</surname><given-names>D. I.</given-names></name><name><surname>Rowland</surname><given-names>D.</given-names></name><name><surname>Young</surname><given-names>A. W.</given-names></name><name><surname>Calder</surname><given-names>A. J.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>1996</year>). <article-title>A differential neural response in the human amygdala to fearful and happy facial expressions</article-title>. <source>Nature</source>
<volume>383</volume>, <fpage>812</fpage>&#x02013;<lpage>815</lpage><pub-id pub-id-type="doi">10.1038/383389a0</pub-id><pub-id pub-id-type="pmid">8893004</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>J. S.</given-names></name><name><surname>&#x000d6;hman</surname><given-names>A.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>1999</year>). <article-title>A subcortical pathway to the right amygdala mediating &#x0201c;unseen&#x0201d; fear</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>96</volume>, <fpage>1680</fpage>&#x02013;<lpage>1685</lpage><pub-id pub-id-type="doi">10.1073/pnas.96.4.1680</pub-id><pub-id pub-id-type="pmid">9990084</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naccache</surname><given-names>L.</given-names></name><name><surname>Gaillard</surname><given-names>R.</given-names></name><name><surname>Adam</surname><given-names>C.</given-names></name><name><surname>Hasboun</surname><given-names>D.</given-names></name><name><surname>Cl&#x000e9;menceau</surname><given-names>S.</given-names></name><name><surname>Baulac</surname><given-names>M.</given-names></name><name><surname>Dehaene</surname><given-names>S.</given-names></name><name><surname>Cohen</surname><given-names>L.</given-names></name></person-group> (<year>2005</year>). <article-title>A direct intracranial record of emotions evoked by subliminal words</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>102</volume>, <fpage>7713</fpage>&#x02013;<lpage>7717</lpage><pub-id pub-id-type="doi">10.1073/pnas.0500542102</pub-id><pub-id pub-id-type="pmid">15897465</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oya</surname><given-names>H.</given-names></name><name><surname>Kawasaki</surname><given-names>H.</given-names></name><name><surname>Howard</surname><given-names>M. A.</given-names><suffix>III</suffix></name><name><surname>Adolphs</surname><given-names>R.</given-names></name></person-group> (<year>2002</year>). <article-title>Electrophysiological responses in the human amygdala discriminate emotion categories of complex visual stimuli</article-title>. <source>J. Neurosci.</source>
<volume>22</volume>, <fpage>9502</fpage>&#x02013;<lpage>9512</lpage><pub-id pub-id-type="pmid">12417674</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasley</surname><given-names>B. N.</given-names></name><name><surname>Mayes</surname><given-names>L. C.</given-names></name><name><surname>Schultz</surname><given-names>R. T.</given-names></name></person-group> (<year>2004</year>). <article-title>Subcortical discrimination of unperceived objects during binocular rivalry</article-title>. <source>Neuron</source>
<volume>42</volume>, <fpage>163</fpage>&#x02013;<lpage>172</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(04)00155-2</pub-id><pub-id pub-id-type="pmid">15066273</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>D. G.</given-names></name></person-group> (<year>1997</year>). <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spat. Vis.</source>
<volume>10</volume>, <fpage>437</fpage>&#x02013;<lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessoa</surname><given-names>L.</given-names></name><name><surname>Adolphs</surname><given-names>R.</given-names></name></person-group> (<year>2010</year>). <article-title>Emotion processing and the amygdala: from a &#x0201c;low road&#x0201d; to &#x0201c;many roads&#x0201d; of evaluating biological significance</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>11</volume>, <fpage>773</fpage>&#x02013;<lpage>782</lpage><pub-id pub-id-type="doi">10.1038/nrg2867</pub-id><pub-id pub-id-type="pmid">20959860</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessoa</surname><given-names>L.</given-names></name><name><surname>Adolphs</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>Emotion and the brain: multiple roads are better than one</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>12</volume>, <fpage>425</fpage>&#x02013;<lpage>426</lpage><pub-id pub-id-type="doi">10.1038/nrn2920-c1</pub-id><pub-id pub-id-type="pmid">21673722</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>M. L.</given-names></name><name><surname>Williams</surname><given-names>L. M.</given-names></name><name><surname>Heining</surname><given-names>M.</given-names></name><name><surname>Herba</surname><given-names>C. M.</given-names></name><name><surname>Russell</surname><given-names>T.</given-names></name><name><surname>Andrew</surname><given-names>C.</given-names></name><name><surname>Bullmore</surname><given-names>E. T.</given-names></name><name><surname>Brammer</surname><given-names>M. J.</given-names></name><name><surname>Williams</surname><given-names>S. C. R.</given-names></name><name><surname>Morgan</surname><given-names>M.</given-names></name><name><surname>Young</surname><given-names>A. W.</given-names></name><name><surname>Gray</surname><given-names>J. A.</given-names></name></person-group> (<year>2004</year>). <article-title>Differential neural responses to overt and covert presentations of facial expressions of fear and disgust</article-title>. <source>Neuroimage</source>
<volume>21</volume>, <fpage>1484</fpage>&#x02013;<lpage>1496</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.12.013</pub-id><pub-id pub-id-type="pmid">15050573</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>M. L.</given-names></name><name><surname>Young</surname><given-names>A. W.</given-names></name><name><surname>Scott</surname><given-names>S. K.</given-names></name><name><surname>Calder</surname><given-names>A. J.</given-names></name><name><surname>Andrew</surname><given-names>C.</given-names></name><name><surname>Giampietro</surname><given-names>V.</given-names></name><name><surname>Williams</surname><given-names>S. C. R.</given-names></name><name><surname>Bullmore</surname><given-names>E. T.</given-names></name><name><surname>Brammer</surname><given-names>M.</given-names></name><name><surname>Gray</surname><given-names>J. A.</given-names></name></person-group> (<year>1998</year>). <article-title>Neural responses to facial and vocal expressions of fear and disgust</article-title>. <source>Proc. R. Soc. Lond. B Biol. Sci.</source>
<volume>265</volume>, <fpage>1809</fpage>&#x02013;<lpage>1817</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0506</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>M. L.</given-names></name><name><surname>Young</surname><given-names>A. W.</given-names></name><name><surname>Senior</surname><given-names>C.</given-names></name><name><surname>Brammer</surname><given-names>M.</given-names></name><name><surname>Andrew</surname><given-names>C.</given-names></name><name><surname>Calder</surname><given-names>A. J.</given-names></name><name><surname>Bullmore</surname><given-names>E. T.</given-names></name><name><surname>Perrett</surname><given-names>D. I.</given-names></name><name><surname>Rowland</surname><given-names>D.</given-names></name><name><surname>Williams</surname><given-names>S. C. R.</given-names></name><name><surname>Gray</surname><given-names>J. A.</given-names></name><name><surname>David</surname><given-names>A. S.</given-names></name></person-group> (<year>1997</year>). <article-title>A specific neural substrate for perceiving facial expressions of disgust</article-title>. <source>Nature</source>
<volume>389</volume>, <fpage>495</fpage>&#x02013;<lpage>498</lpage><pub-id pub-id-type="doi">10.1038/39051</pub-id><pub-id pub-id-type="pmid">9333238</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pourtois</surname><given-names>G.</given-names></name><name><surname>Dan</surname><given-names>E. S.</given-names></name><name><surname>Grandjean</surname><given-names>D.</given-names></name><name><surname>Sander</surname><given-names>D.</given-names></name><name><surname>Vuilleumier</surname><given-names>P.</given-names></name></person-group> (<year>2005</year>). <article-title>Enhanced extrastriate visual response to bandpass spatial frequency filtered fearful faces: time course and topographic evoked-potentials mapping</article-title>. <source>Hum. Brain Mapp.</source>
<volume>26</volume>, <fpage>65</fpage>&#x02013;<lpage>79</lpage><pub-id pub-id-type="doi">10.1002/hbm.20130</pub-id><pub-id pub-id-type="pmid">15954123</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pourtois</surname><given-names>G.</given-names></name><name><surname>Spinelli</surname><given-names>L.</given-names></name><name><surname>Seeck</surname><given-names>M.</given-names></name><name><surname>Vuilleumier</surname><given-names>P.</given-names></name></person-group> (<year>2010</year>). <article-title>Temporal precedence of emotion over attention modulations in the lateral amygdala: intracranial ERP evidence from a patient with temporal lobe epilepsy</article-title>. <source>Cogn. Affect. Behav. Neurosci.</source>
<volume>10</volume>, <fpage>83</fpage>&#x02013;<lpage>93</lpage><pub-id pub-id-type="doi">10.3758/CABN.10.1.83</pub-id><pub-id pub-id-type="pmid">20233957</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raymond</surname><given-names>J. E.</given-names></name><name><surname>Shapiro</surname><given-names>K. L.</given-names></name><name><surname>Arnell</surname><given-names>K. M.</given-names></name></person-group> (<year>1992</year>). <article-title>Temporary suppression of visual processing in an RSVP task: an attentional blink?</article-title>
<source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>18</volume>, <fpage>849</fpage>&#x02013;<lpage>860</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.18.3.849</pub-id><pub-id pub-id-type="pmid">1500880</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabatini</surname><given-names>E.</given-names></name><name><surname>Penna</surname><given-names>S. D.</given-names></name><name><surname>Franciotti</surname><given-names>R.</given-names></name><name><surname>Ferretti</surname><given-names>A.</given-names></name><name><surname>Zoccolotti</surname><given-names>P.</given-names></name><name><surname>Rossini</surname><given-names>P. M.</given-names></name><name><surname>Romani</surname><given-names>G. L.</given-names></name><name><surname>Gainotti</surname><given-names>G.</given-names></name></person-group> (<year>2009</year>). <article-title>Brain structures activated by overt and covert emotional visual stimuli</article-title>. <source>Brain Res. Bull.</source>
<volume>79</volume>, <fpage>258</fpage>&#x02013;<lpage>264</lpage><pub-id pub-id-type="doi">10.1016/j.brainresbull.2009.03.001</pub-id><pub-id pub-id-type="pmid">19480985</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Jentzsch</surname><given-names>I.</given-names></name><name><surname>Johnson</surname><given-names>M.</given-names></name><name><surname>Schweinberger</surname><given-names>S. R.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name></person-group> (<year>2003</year>). <article-title>A principled method for determining the functionality of brain responses</article-title>. <source>Neuroreport</source>
<volume>14</volume>, <fpage>1665</fpage>&#x02013;<lpage>1669</lpage><pub-id pub-id-type="doi">10.1097/00001756-200309150-00002</pub-id><pub-id pub-id-type="pmid">14512834</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Petro</surname><given-names>L. S.</given-names></name><name><surname>Smith</surname><given-names>M. L.</given-names></name></person-group> (<year>2007</year>). <article-title>Dynamics of visual information integration in the brain for categorizing facial expressions</article-title>. <source>Curr. Biol.</source>
<volume>17</volume>, <fpage>1580</fpage>&#x02013;<lpage>1585</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.08.048</pub-id><pub-id pub-id-type="pmid">17869111</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>P. G.</given-names></name><name><surname>Petro</surname><given-names>L. S.</given-names></name><name><surname>Smith</surname><given-names>M. L.</given-names></name></person-group> (<year>2009</year>). <article-title>Transmission of facial expressions of emotion co-evolved with their efficient decoding in the brain: behavioral and brain evidence</article-title>. <source>PLoS ONE</source>
<volume>4</volume>, <fpage>e5625</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0005625</pub-id><pub-id pub-id-type="pmid">19462006</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>M. L.</given-names></name></person-group> (in press). <article-title>Rapid processing of emotional expressions without conscious awareness</article-title>. <source>Cereb. Cortex</source>.<pub-id pub-id-type="doi">10.1093/cercor/bhr250</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>M. L.</given-names></name><name><surname>Cottrell</surname><given-names>G. W.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2005</year>). <article-title>Transmitting and decoding facial expressions</article-title>. <source>Psychol. Sci.</source>
<volume>16</volume>, <fpage>184</fpage>&#x02013;<lpage>189</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2005.00801.x</pub-id><pub-id pub-id-type="pmid">15733197</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>M. L.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2004</year>). <article-title>Receptive fields for flexible face categorizations</article-title>. <source>Psychol. Sci.</source>
<volume>15</volume>, <fpage>753</fpage>&#x02013;<lpage>761</lpage><pub-id pub-id-type="doi">10.1111/j.0956-7976.2004.00752.x</pub-id><pub-id pub-id-type="pmid">15482447</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>M. L.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2007</year>). <article-title>From a face to its category via a few information processing states in the brain</article-title>. <source>Neuroimage</source>
<volume>37</volume>, <fpage>974</fpage>&#x02013;<lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.05.030</pub-id><pub-id pub-id-type="pmid">17611125</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sowden</surname><given-names>P. T.</given-names></name><name><surname>Schyns</surname><given-names>P. G.</given-names></name></person-group> (<year>2006</year>). <article-title>Channel surfing in the visual brain</article-title>. <source>Trends Cogn. Sci. (Regul. Ed.)</source>
<volume>10</volume>, <fpage>538</fpage>&#x02013;<lpage>545</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.10.007</pub-id><pub-id pub-id-type="pmid">17071128</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Surbeck</surname><given-names>W.</given-names></name><name><surname>Bouthillier</surname><given-names>A.</given-names></name><name><surname>Weil</surname><given-names>A. G.</given-names></name><name><surname>Crevier</surname><given-names>L.</given-names></name><name><surname>Carmant</surname><given-names>L.</given-names></name><name><surname>Lortie</surname><given-names>A.</given-names></name><name><surname>Major</surname><given-names>P.</given-names></name><name><surname>Nguyen</surname><given-names>D. K.</given-names></name></person-group> (<year>2011</year>). <article-title>The combination of subdural and depth electrodes for intracranial EEG investigation of suspected insular (perisylvian) epilepsy</article-title>. <source>Epilepsia</source>
<volume>52</volume>, <fpage>458</fpage>&#x02013;<lpage>466</lpage><pub-id pub-id-type="doi">10.1111/j.1528-1167.2010.02910.x</pub-id><pub-id pub-id-type="pmid">21204825</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamietto</surname><given-names>M.</given-names></name><name><surname>De Gelder</surname><given-names>B.</given-names></name></person-group> (<year>2010</year>). <article-title>Neural bases of the non-conscious perception of emotional signals</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>11</volume>, <fpage>697</fpage>&#x02013;<lpage>709</lpage><pub-id pub-id-type="doi">10.1038/nrg2844</pub-id><pub-id pub-id-type="pmid">20811475</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thurman</surname><given-names>S. M.</given-names></name><name><surname>Grossman</surname><given-names>E. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Diagnostic spatial frequencies and human efficiency for discriminating actions</article-title>. <source>Atten. Percept. Psychophys.</source>
<volume>73</volume>, <fpage>572</fpage>&#x02013;<lpage>580</lpage><pub-id pub-id-type="doi">10.3758/s13414-010-0028-z</pub-id><pub-id pub-id-type="pmid">21264736</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuchiya</surname><given-names>N.</given-names></name><name><surname>Kawasaki</surname><given-names>H.</given-names></name><name><surname>Oya</surname><given-names>H.</given-names></name><name><surname>Howard</surname><given-names>M. A.</given-names><suffix>III</suffix></name><name><surname>Adolphs</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>Decoding face information in time, frequency and space from direct intracranial recordings of the human brain</article-title>. <source>PLoS ONE</source>
<volume>3</volume>, <fpage>e3892</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0003892</pub-id><pub-id pub-id-type="pmid">19065268</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuchiya</surname><given-names>N.</given-names></name><name><surname>Koch</surname><given-names>C.</given-names></name></person-group> (<year>2005</year>). <article-title>Continuous Flash Suppression reduces negative afterimages</article-title>. <source>Nat. Neurosci.</source>
<volume>8</volume>, <fpage>1096</fpage>&#x02013;<lpage>1101</lpage><pub-id pub-id-type="doi">10.1038/nn1500</pub-id><pub-id pub-id-type="pmid">15995700</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsuchiya</surname><given-names>N.</given-names></name><name><surname>Moradi</surname><given-names>F.</given-names></name><name><surname>Felsen</surname><given-names>C.</given-names></name><name><surname>Yamazaki</surname><given-names>M.</given-names></name><name><surname>Adolphs</surname><given-names>R.</given-names></name></person-group> (<year>2009</year>). <article-title>Intact rapid detection of fearful faces in the absence of the amygdala</article-title>. <source>Nat. Neurosci.</source>
<volume>12</volume>, <fpage>1224</fpage>&#x02013;<lpage>1225</lpage><pub-id pub-id-type="doi">10.1038/nn.2380</pub-id><pub-id pub-id-type="pmid">19718036</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Gaag</surname><given-names>C.</given-names></name><name><surname>Minderaa</surname><given-names>R. B.</given-names></name><name><surname>Keysers</surname><given-names>C.</given-names></name></person-group> (<year>2007</year>). <article-title>The BOLD signal in the amygdala does not differentiate between dynamic facial expressions</article-title>. <source>Soc. Cogn. Affect. Neurosci.</source>
<volume>2</volume>, <fpage>93</fpage>&#x02013;<lpage>103</lpage><pub-id pub-id-type="doi">10.1093/scan/nsm002</pub-id><pub-id pub-id-type="pmid">18985128</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlamings</surname><given-names>P. H. J. M.</given-names></name><name><surname>Goffaux</surname><given-names>V.</given-names></name><name><surname>Kemner</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>). <article-title>Is the early modulation of brain activity by fearful facial expressions primarily mediated by coarse low spatial frequency information?</article-title>
<source>J. Vis.</source>
<volume>9</volume>, <fpage>1</fpage>&#x02013;<lpage>13</lpage><pub-id pub-id-type="doi">10.1167/9.2.1</pub-id><pub-id pub-id-type="pmid">19757890</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuilleumier</surname><given-names>P.</given-names></name><name><surname>Armony</surname><given-names>J. L.</given-names></name><name><surname>Driver</surname><given-names>J.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>2003</year>). <article-title>Distinct spatial frequency sensitivities for processing faces and emotional expressions</article-title>. <source>Nat. Neurosci.</source>
<volume>6</volume>, <fpage>624</fpage>&#x02013;<lpage>631</lpage><pub-id pub-id-type="doi">10.1038/nn1057</pub-id><pub-id pub-id-type="pmid">12740580</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Agarwal</surname><given-names>R.</given-names></name><name><surname>Nguyen</surname><given-names>D.</given-names></name><name><surname>Domocos</surname><given-names>V.</given-names></name><name><surname>Gotman</surname><given-names>J.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Intracranial electrode visualization in invasive pre-surgical evaluation for epilepsy&#x0201d;</article-title> in <conf-name>Proceedings of the 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference</conf-name>, September 1&#x02013;4, <conf-loc>Shanghai</conf-loc></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whalen</surname><given-names>P. J.</given-names></name><name><surname>Rauch</surname><given-names>S. L.</given-names></name><name><surname>Etcoff</surname><given-names>N. L.</given-names></name><name><surname>McInerney</surname><given-names>S. C.</given-names></name><name><surname>Lee</surname><given-names>M. B.</given-names></name><name><surname>Jenike</surname><given-names>M. A.</given-names></name></person-group> (<year>1998</year>). <article-title>Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge</article-title>. <source>J. Neurosci.</source>
<volume>18</volume>, <fpage>411</fpage>&#x02013;<lpage>418</lpage><pub-id pub-id-type="pmid">9412517</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V.</given-names></name><name><surname>Fiset</surname><given-names>D.</given-names></name><name><surname>Chauvin</surname><given-names>A.</given-names></name><name><surname>Blais</surname><given-names>C.</given-names></name><name><surname>Arguin</surname><given-names>M.</given-names></name><name><surname>Tanaka</surname><given-names>J. W.</given-names></name><name><surname>Bub</surname><given-names>D. N.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name></person-group> (<year>2010a</year>). <article-title>Does face inversion change spatial frequency tuning?</article-title>
<source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>36</volume>, <fpage>122</fpage>&#x02013;<lpage>135</lpage><pub-id pub-id-type="doi">10.1037/a0016465</pub-id><pub-id pub-id-type="pmid">20121299</pub-id></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V.</given-names></name><name><surname>Sadr</surname><given-names>J.</given-names></name><name><surname>Fiset</surname><given-names>D.</given-names></name><name><surname>Horne</surname><given-names>G. O.</given-names></name><name><surname>Gosselin</surname><given-names>F.</given-names></name><name><surname>Tanaka</surname><given-names>J. W.</given-names></name></person-group> (<year>2010b</year>). <article-title>Controlling low-level image properties: the SHINE toolbox</article-title>. <source>Behav. Res. Methods</source>
<volume>42</volume>, <fpage>671</fpage>&#x02013;<lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>L. M.</given-names></name><name><surname>Das</surname><given-names>P.</given-names></name><name><surname>Liddell</surname><given-names>B. J.</given-names></name><name><surname>Kemp</surname><given-names>A. H.</given-names></name><name><surname>Rennie</surname><given-names>C. J.</given-names></name><name><surname>Gordon</surname><given-names>E.</given-names></name></person-group> (<year>2006</year>). <article-title>Mode of functional connectivity in amygdala pathways dissociates level of awareness for signals of fear</article-title>. <source>J. Neurosci.</source>
<volume>26</volume>, <fpage>9264</fpage>&#x02013;<lpage>9271</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0022-06.2006</pub-id><pub-id pub-id-type="pmid">16957082</pub-id></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>M. A.</given-names></name><name><surname>Morris</surname><given-names>A. P.</given-names></name><name><surname>McGlone</surname><given-names>F.</given-names></name><name><surname>Abbott</surname><given-names>D. F.</given-names></name><name><surname>Mattingley</surname><given-names>J. B.</given-names></name></person-group> (<year>2004</year>). <article-title>Amygdala responses to fearful and happy facial expressions under conditions of binocular suppression</article-title>. <source>J. Neurosci.</source>
<volume>24</volume>, <fpage>2898</fpage>&#x02013;<lpage>2904</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2716-04.2004</pub-id><pub-id pub-id-type="pmid">15044528</pub-id></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winston</surname><given-names>J. S.</given-names></name><name><surname>O&#x02019;Doherty</surname><given-names>J.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>2003a</year>). <article-title>Common and distinct neural responses during direct and incidental processing of multiple facial emotions</article-title>. <source>Neuroimage</source>
<volume>20</volume>, <fpage>84</fpage>&#x02013;<lpage>97</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00303-3</pub-id><pub-id pub-id-type="pmid">14527572</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winston</surname><given-names>J. S.</given-names></name><name><surname>Vuilleumier</surname><given-names>P.</given-names></name><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>2003b</year>). <article-title>Effects of low-spatial frequency components of fearful faces on fusiform cortex activity</article-title>. <source>Curr. Biol.</source>
<volume>13</volume>, <fpage>1824</fpage>&#x02013;<lpage>1829</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2003.09.038</pub-id><pub-id pub-id-type="pmid">14561410</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>E.</given-names></name><name><surname>Blake</surname><given-names>R.</given-names></name></person-group> (<year>2012</year>). <article-title>Deconstructing continuous flash suppression</article-title>. <source>J. Vis.</source>
<volume>12</volume>, <fpage>1</fpage>&#x02013;<lpage>14</lpage><pub-id pub-id-type="doi">10.1167/12.5.1</pub-id></mixed-citation></ref></ref-list></back></article>
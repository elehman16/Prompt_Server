<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">24098665</article-id><article-id pub-id-type="pmc">3786990</article-id><article-id pub-id-type="publisher-id">PONE-D-13-05373</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0074746</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Phonetic Imitation from an Individual-Difference Perspective: Subjective Attitude, Personality and &#x0201c;Autistic&#x0201d; Traits</article-title><alt-title alt-title-type="running-head">Individual Differences in Phonetic Imitation</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Alan C. L.</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Abrego-Collier</surname><given-names>Carissa</given-names></name><xref ref-type="aff" rid="aff1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Sonderegger</surname><given-names>Morgan</given-names></name><xref ref-type="aff" rid="aff2">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="aff1">
<label>1</label>
<addr-line>Phonology Laboratory, Department of Linguistics, University of Chicago, Chicago, Illinois, United States of America</addr-line>
</aff><aff id="aff2">
<label>2</label>
<addr-line>Department of Linguistics, McGill University, Montreal, Quebec, Canada</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Snyder</surname><given-names>Joel</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>UNLV, United States of America</addr-line>
</aff><author-notes><corresp id="cor1">* E-mail: <email>aclyu@uchicago.edu</email></corresp><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p>Conceived and designed the experiments: AY. Performed the experiments: AY CAC. Analyzed the data: AY CAC MS. Contributed reagents/materials/analysis tools: AY CAC MS. Wrote the paper: AY MS.</p></fn></author-notes><pub-date pub-type="collection"><year>2013</year></pub-date><pub-date pub-type="epub"><day>30</day><month>9</month><year>2013</year></pub-date><volume>8</volume><issue>9</issue><elocation-id>e74746</elocation-id><history><date date-type="received"><day>1</day><month>2</month><year>2013</year></date><date date-type="accepted"><day>6</day><month>8</month><year>2013</year></date></history><permissions><copyright-statement>&#x000a9; 2013 Yu et al</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Yu et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>Numerous studies have documented the phenomenon of phonetic imitation: the process by which the production patterns of an individual become more similar on some phonetic or acoustic dimension to those of her interlocutor. Though social factors have been suggested as a motivator for imitation, few studies has established a tight connection between language-external factors and a speaker&#x02019;s likelihood to imitate. The present study investigated the phenomenon of phonetic imitation using a within-subject design embedded in an individual-differences framework. Participants were administered a phonetic imitation task, which included two speech production tasks separated by a perceptual learning task, and a battery of measures assessing traits associated with Autism-Spectrum Condition, working memory, and personality. To examine the effects of subjective attitude on phonetic imitation, participants were randomly assigned to four experimental conditions, where the perceived sexual orientation of the narrator (homosexual vs. heterosexual) and the outcome (positive vs. negative) of the story depicted in the exposure materials differed. The extent of phonetic imitation by an individual is significantly modulated by the story outcome, as well as by the participant&#x02019;s subjective attitude toward the model talker, the participant&#x02019;s personality trait of openness and the autistic-like trait associated with attention switching.</p></abstract><funding-group><funding-statement>The authors have no support or funding to report.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Imitation has been observed in many domains of human behavior, including postures, gestures, and facial expressions <xref rid="pone.0074746-Nielsen2" ref-type="bibr">[9]</xref>. In the domain of language and speech, imitation has been observed for many properties, such as lexical and syntactic alignment <xref rid="pone.0074746-Pickering1" ref-type="bibr">[2]</xref>, speech rate <xref rid="pone.0074746-Webb1" ref-type="bibr">[3]</xref>, pause and utterance duration <xref rid="pone.0074746-Jaffe1" ref-type="bibr">[4]</xref>, vocal intensity <xref rid="pone.0074746-Natale1" ref-type="bibr">[5]</xref>, vowel quality <xref rid="pone.0074746-Babel1" ref-type="bibr">[6]</xref>, <xref rid="pone.0074746-Babel2" ref-type="bibr">[7]</xref>, and voice onset time (VOT) <xref rid="pone.0074746-Nielsen1" ref-type="bibr">[8]</xref>&#x02013;<xref rid="pone.0074746-Nielsen3" ref-type="bibr">[10]</xref>. When the speech production patterns of an individual become more similar on some phonetic or acoustic dimension to those of her interlocutor, phonetic <italic>imitation</italic> or <italic>convergence</italic> obtains; phonetic <italic>divergence</italic> refers to the reverse process. For example, studies using a &#x0201c;shadowing&#x0201d; paradigm (e.g., <xref rid="pone.0074746-Goldinger1" ref-type="bibr">[11]</xref>, <xref rid="pone.0074746-Mitterer1" ref-type="bibr">[12]</xref>) show that subjects shift their speech production (evaluated using perceptual measures) in the direction of speech they are asked to repeat, either immediately or after a delay. Several previous studies have considered imitation of VOT in particular. A significant VOT imitation effect was reported in a single-word shadowing task using words with artificially-lengthened initial VOTs <xref rid="pone.0074746-Shockley1" ref-type="bibr">[13]</xref>. VOT imitation was also observed even when subjects were not explicitly asked to shadow: their VOTs became longer after listening to a period of speech with extended VOTs; subjects also generalized the extended VOT pattern to words they were not exposed to during the passive listening task <xref rid="pone.0074746-Nielsen1" ref-type="bibr">[8]</xref>, <xref rid="pone.0074746-Nielsen2" ref-type="bibr">[9]</xref>. Given the prevalence of imitation effects in language, some scholars have hypothesized that studies of phonetic imitation and convergence can inform the understanding of sound change. In particular, phonetic imitation found in the laboratory setting is taken to be similar to phonetic convergence in conversational interaction, which has been hypothesized as an important source of propagation of sound changes throughout speech communities <xref rid="pone.0074746-Babel2" ref-type="bibr">[7]</xref>, <xref rid="pone.0074746-Nielsen3" ref-type="bibr">[10]</xref>, <xref rid="pone.0074746-Pardo1" ref-type="bibr">[14]</xref>&#x02013;<xref rid="pone.0074746-Pardo2" ref-type="bibr">[16]</xref>.</p><p>Phonetic imitation is not an entirely automatic (i.e., it can occur without the speaker&#x02019;s intention or control) or unrestricted process <xref rid="pone.0074746-Nielsen2" ref-type="bibr">[9]</xref>. Contrast preservation, which has been argued to be an essential part of the phonological grammar <xref rid="pone.0074746-Flemming1" ref-type="bibr">[17]</xref>&#x02013;<xref rid="pone.0074746-Lubowicz1" ref-type="bibr">[19]</xref>, may constrain phonetic imitation. For example, one study found that lengthened VOTs were imitated but shortened VOTs were not <xref rid="pone.0074746-Dijksterhuis1" ref-type="bibr">[1]</xref>, suggesting that speakers may not imitate if the novel phonetic feature (shortened VOT) endangers phonetic contrasts (unaspirated vs. aspirated). In the case of vowels, subjects in one study imitated only low vowels but not higher ones <xref rid="pone.0074746-Babel1" ref-type="bibr">[6]</xref>, which might be due to the influence of speaker experience; unlike the higher vowels, subjects encounter more varieties of low vowels due to differences in jaw height in accented and unaccented syllables.</p><p>Beyond linguistic factors, &#x0201c;macro&#x0201d; socio-biological factors, such as gender/sex, have often been suggested as important moderators of imitation <xref rid="pone.0074746-Dijksterhuis1" ref-type="bibr">[1]</xref>, <xref rid="pone.0074746-Babel1" ref-type="bibr">[6]</xref>, although the exact nature of this modulation is not clear. Men have been found to imitate more than women in a map task <xref rid="pone.0074746-Pardo1" ref-type="bibr">[14]</xref>, but less than women in a shadowing task <xref rid="pone.0074746-Namy1" ref-type="bibr">[20]</xref>. These mixed results suggest that gender/sex may not be the appropriate predictive factor in modulating likelihood of imitation.</p><p>Situational variables also affect the degree of imitation. Speakers vary in the degree of phonetic convergence depending on their gender as well as their role in a particular conversation. In a study where dyads participated in a map task, the degree of overall phonetic imitation (assessed perceptually) depended on the speaker&#x02019;s conversational role, as well as gender <xref rid="pone.0074746-Pardo1" ref-type="bibr">[14]</xref>, <xref rid="pone.0074746-Pardo2" ref-type="bibr">[16]</xref>. The language distance between interlocutors can also affect the likelihood of imitation; greater imitation is found in same-dialect conversational pairs than in either different-dialect pairs or different-L1 pairs <xref rid="pone.0074746-Kim1" ref-type="bibr">[21]</xref>.</p><p>Accommodation research, particularly work within the framework of Communication Accommodation Theory (CAT), which sees speech convergence phenomena as motivated by an individual&#x02019;s desire for social acceptance and identification with a particular social group <xref rid="pone.0074746-Giles1" ref-type="bibr">[22]</xref>, has repeatedly demonstrated the centrality of subjective attitude and ideology for predicting the likelihood of linguistic convergence and divergence between speakers at multiple linguistic levels <xref rid="pone.0074746-Giles2" ref-type="bibr">[23]</xref>&#x02013;<xref rid="pone.0074746-Bourhis1" ref-type="bibr">[25]</xref>. Many dialect convergence studies have found that speaker attitude and language ideologies strongly influence the degree of convergence between languages in contact. Labov, in his seminal study of/ay/and/aw/centralization in Martha&#x02019;s Vineyard <xref rid="pone.0074746-Labov1" ref-type="bibr">[26]</xref>, showed that individuals who had a positive orientation toward the island were more likely to exhibit centralized diphthongs than those who did not. Another seminal study by Bourhis and Giles found that Welsh adults who were invested in Welsh language and culture would adopt a Welsh-accented dialect during the interview when talking to an out-group speaker, who questioned the vitality and function of the Welsh language in modern times <xref rid="pone.0074746-Bourhis1" ref-type="bibr">[25]</xref>. On the other hand, Welsh adults who adopted a more utilitarian view of Welsh language and culture were more likely to accommodate to the interviewer. More recently, Babel found that speakers of New Zealand English were more likely to accommodate to an Australian talker in a speech production task when the New Zealand English speaker had a pro-Australia bias <xref rid="pone.0074746-Babel3" ref-type="bibr">[27]</xref>. While these studies show that the extent of phonetic accommodation may be dependent on speaker attitudes, whether this type of attitude-based modulation of phonetic convergence would only arise from deep-seated attitudes formed over a long period or whether such modulation could be induced by impressions formed after short exposure (e.g., over the course of a conversation or over the course of a laboratory experiment) remains an open question. To the extent that subjects listening to a monologue may form an opinion of the narrator on account of attributes of the monologue, we hypothesize that listeners might show more accommodation to the narrator if the impression formed were positive. In addition to attitudinal differences, the effects of other individual-level factors on accommodation at the phonetic level have been investigated in recent years. Subjective evaluation of model talker attractiveness has been implicated as a potential modulating factor in vocalic imitation, as assessed using acoustic measures <xref rid="pone.0074746-Babel1" ref-type="bibr">[6]</xref>, <xref rid="pone.0074746-Babel2" ref-type="bibr">[7]</xref>. &#x0201c;Phonetic talent&#x0201d; in L2 acquisition may also serve as a good predictor of phonetic convergence, as assessed by both perceptual and instrumental measures <xref rid="pone.0074746-Lewandowski1" ref-type="bibr">[28]</xref>.</p><p>With the important exception of these studies, little is known about the factors which modulate interspeaker differences in the extent of phonetic imitation. Previous studies on phonetic imitation have largely focused on group-level effects, that is, effects observed in a (sub-)population as a whole. Understanding sources of individual differences in phonetic imitation is important for two reasons. While huge variability between speakers in the amount of phonetic imitation is often reported <xref rid="pone.0074746-Babel2" ref-type="bibr">[7]</xref>, <xref rid="pone.0074746-Nielsen3" ref-type="bibr">[10]</xref>, <xref rid="pone.0074746-Pardo1" ref-type="bibr">[14]</xref>, there has been little discussion of what sources could underlie this variability. Given that sound change propagation hinges heavily on the attitude and stance of an individual within a locally-defined social reality, understanding why individuals differ in the extent of phonetic imitation is crucial for understanding the role of imitation phenomena in sound change. An individual&#x02019;s personality profile and social distribution, for example, has been argued to play a significant role in contributing to the socially-structured distribution of linguistic innovations <xref rid="pone.0074746-Cheshire1" ref-type="bibr">[29]</xref>&#x02013;<xref rid="pone.0074746-Yu2" ref-type="bibr">[32]</xref>. To the extent that personality traits and social dispositions are influenced, if only partially, by cognitive and neuropsychological factors <xref rid="pone.0074746-Yu2" ref-type="bibr">[32]</xref>, understanding the neuro-cognitive contribution to variation in phonetic imitation must also be seen as an integral part of unveiling the full picture of sound change actuation and propagation.</p><p>In this article, we aim to contribute to the existing literature on phonetic imitation by considering the range and relative magnitude of situational and individual-level factors that mediate phonetic imitation. To examine the effects of situational variables on phonetic convergence, we manipulate the nature of the model talker. Earlier studies have examined how phonetic imitation is affected by the conversational role of the participants <xref rid="pone.0074746-Pardo1" ref-type="bibr">[14]</xref>, <xref rid="pone.0074746-Pardo2" ref-type="bibr">[16]</xref>, the model talker&#x02019;s perceived race/ethnicity <xref rid="pone.0074746-Babel2" ref-type="bibr">[7]</xref>, and speakers&#x02019; national identity and attitudes toward other countries <xref rid="pone.0074746-Babel3" ref-type="bibr">[27]</xref>. In this study, we vary the talker&#x02019;s perceived sexual orientation and the narrative&#x02019;s outcome. The present investigation also considers the effects of individual-difference dimensions on phonetic imitation. As mentioned above, in addition to manipulating the nature of exposure materials, we consider the attitude of the listeners toward the narrator as a potential variable influencing phonetic imitation (see also <xref rid="pone.0074746-Babel3" ref-type="bibr">[27]</xref>). Besides attitudinal differences, however, individuals also vary in terms of their intrinsic neuropsychological and cognitive predisposition. One important individual-difference dimension from this perspective is working memory capacity (WMC). WMC represents the ability to control attention and deal with irrelevant information, and not simply the amount of information that can reside in working memory <xref rid="pone.0074746-Conway1" ref-type="bibr">[33]</xref>. The underlying premise of this controlled-attention viewpoint is that individual differences are not due to some limited amount of activation available to the working memory system but, rather, to an individual&#x02019;s ability to ignore irrelevant information (on the basis of a specific relevant goal) through the control of attention <xref rid="pone.0074746-Conway1" ref-type="bibr">[33]</xref>, <xref rid="pone.0074746-Kane1" ref-type="bibr">[34]</xref>. Individuals who possess lower WMC are generally less able to utilize executive control to ignore irrelevant or interfering information and maintain focus on a specific goal, whereas the opposite is true for high-WMC individuals. Availability of WM resources has been found to affect speech processing <xref rid="pone.0074746-Francis1" ref-type="bibr">[35]</xref>. Increased WM load (thus reduced WM resources), for example, has been shown to slow down spoken word recognition <xref rid="pone.0074746-Francis2" ref-type="bibr">[36]</xref>. High-WM individuals exhibit less perceptual compensation for coarticulation and are less biased toward hearing legal sound sequences than low-WM individuals <xref rid="pone.0074746-Yu3" ref-type="bibr">[37]</xref>. WMC also affects sentence processing (see <xref rid="pone.0074746-MacDonald1" ref-type="bibr">[38]</xref>) as well as success in learning artificial grammars <xref rid="pone.0074746-Misyak1" ref-type="bibr">[39]</xref>, <xref rid="pone.0074746-Ettlinger1" ref-type="bibr">[40]</xref>. Thus to the extent that the type of perceptual retuning in phonetic imitation requires selective attention to the fine-grained phonetic details of the training materials, low-WM individuals might have more difficulty with phonetic imitation than high-WM ones.</p><p>Another individual-difference dimension is cognitive processing style and, by extension, personality traits. Cognitive processing style refers to psychological dimensions representing preferences and consistencies in an individual&#x02019;s particular manner of cognitive functioning, with respect to acquiring and processing information <xref rid="pone.0074746-Ausburn1" ref-type="bibr">[41]</xref>&#x02013;<xref rid="pone.0074746-Witkin1" ref-type="bibr">[43]</xref>. Recent studies have suggested that across-individual variation in perceptual and production norms is determined in part by individual variability in cognitive processing style, such as traits associated with the Autism-Spectrum Condition (ASC), which includes autistic disorder, Asperger&#x02019;s disorder, and pervasive developmental disorders, are characterized by deficits in social interaction, communication, and behavioral flexibility, and affects about 1% of the population. Autistic-like traits, as measured by the total Autism-Spectrum Quotient (AQ; <xref rid="pone.0074746-BaronCohen1" ref-type="bibr">[44]</xref>) taken from within the neurotypical population (i.e., individuals who are not clinically autistic), have been found to correlate negatively with the extent of identification shift associated with the `Ganong effect&#x02019; (i.e., the bias in categorization in the direction of a known word) <xref rid="pone.0074746-Stewart1" ref-type="bibr">[45]</xref>. Recent studies have also found significant associations between autistic-like traits and perceptual compensation for contextual variation in speech (vocalic context, <xref rid="pone.0074746-Yu1" ref-type="bibr">[31]</xref>; talker voice <xref rid="pone.0074746-Yu1" ref-type="bibr">[31]</xref>; and phonotactic contexts <xref rid="pone.0074746-Yu3" ref-type="bibr">[37]</xref>), although the nature of the autistic-like trait effects varied depending on the type of contextual information. For example, individuals with high AQ exhibited stronger perceptual compensation for coarticulation than those with low AQ, while low-AQ individuals exhibited stronger phonotactic effects on speech perception than high-AQ ones. Individuals with ASC have been shown to exhibit enhanced perceptual processing of fine-grained auditory information <xref rid="pone.0074746-Bonnel1" ref-type="bibr">[46]</xref>, <xref rid="pone.0074746-Mottron1" ref-type="bibr">[47]</xref>. This suggests that individuals with more autistic-like trait-related cognitive processing styles might be particularly sensitive to fine phonetic differences. On the other hand, given that cognitive theories of autism hold that individuals with autism have difficulties integrating perceptual information with higher order language processing (weak central coherence; <xref rid="pone.0074746-Happ1" ref-type="bibr">[48]</xref>, <xref rid="pone.0074746-Happ2" ref-type="bibr">[49]</xref>), neurotypical individuals with more pronounced autistic-like traits (e.g., high AQ), even if they might be better at detecting fine phonetic details, might nonetheless have difficulties utilizing the perceived fine phonetic differences in his/her own speech production to affect discernible phonetic imitation. Finally, variability in cognitive processing style has been shown to correlate with individual differences in social and personality traits. In particular, Autistic traits, as measured by the Autism-Spectrum Quotient and the Empathy Quotient <xref rid="pone.0074746-BaronCohen2" ref-type="bibr">[50]</xref>, <xref rid="pone.0074746-Wheelwright1" ref-type="bibr">[51]</xref>, has been shown to significantly correlated with individual personality traits <xref rid="pone.0074746-Yu2" ref-type="bibr">[32]</xref>, <xref rid="pone.0074746-Austin1" ref-type="bibr">[52]</xref> and social network characteristics <xref rid="pone.0074746-Yu2" ref-type="bibr">[32]</xref>, <xref rid="pone.0074746-Nettle1" ref-type="bibr">[53]</xref>. Given that social and personality traits may influence how an individual interacts with other members of his/her social network or community of practice <xref rid="pone.0074746-Cheshire1" ref-type="bibr">[29]</xref>, <xref rid="pone.0074746-StuartSmith1" ref-type="bibr">[30]</xref>, <xref rid="pone.0074746-Yu2" ref-type="bibr">[32]</xref>, we also gathered information regarding participants&#x02019; personality traits in an attempt to identify potential significant personality predictors of phonetic imitation.</p></sec><sec sec-type="materials|methods" id="s2"><title>Materials and Methods</title><sec id="s2a"><title>Ethics Statement</title><p>The study was approved by the Social and Behavioral Sciences Institutional Review Board at the University of Chicago and written informed consent was obtained from all participants.</p></sec><sec id="s2b"><title>Procedure</title><p>The production task consisted of three blocks: First, there was a <italic>baseline</italic> production block where subjects produced a list of 72/p t k/&#x02212;initial target words (randomized order) in the carrier sentence, &#x0201c;say ___ again&#x0201d;. Target words, given in <xref ref-type="table" rid="pone-0074746-t001">Table 1</xref>, were selected from <sc>celex</sc>2 <xref rid="pone.0074746-Baayen1" ref-type="bibr">[54]</xref>, evenly distributed by token frequency quartile and by initial consonant. A subsequent post-exposure <italic>test</italic> block consisted of subjects producing the same word list again in a different randomized order. In between the two production tasks was a <italic>exposure</italic> block where subjects heard a constructed first-person narrative in which the same 72 p/t/k-initial words were embedded. VOTs of the target words in the story were extended by 100% using Praat <xref rid="pone.0074746-Boersma1" ref-type="bibr">[55]</xref>. VOTs were extended by selecting stable medial portions of the aspiration, copying, and pasting them back into the aspiration selection of the waveform (see also <xref rid="pone.0074746-Shockley1" ref-type="bibr">[13]</xref>). Intervals including sudden bursts of acoustic energy were avoided to minimize unnaturalness of aspiration noise. Care was also taken to select stretches of VOT that did not lead to perception of clicks or other evidence of the splicing operation when pasted back into the utterance. To achieve natural sounding extended VOT, duration and placement of selections varied across and within tokens.</p><table-wrap id="pone-0074746-t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.t001</object-id><label>Table 1</label><caption><title>Stimuli for the baseline and test production blocks.</title></caption><alternatives><graphic id="pone-0074746-t001-1" xlink:href="pone.0074746.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Q1</td><td align="left" rowspan="1" colspan="1">Q2</td><td align="left" rowspan="1" colspan="1">Q3</td><td align="left" rowspan="1" colspan="1">Q4</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Bilabial</td><td align="left" rowspan="1" colspan="1">picky</td><td align="left" rowspan="1" colspan="1">Pearl</td><td align="left" rowspan="1" colspan="1">patio</td><td align="left" rowspan="1" colspan="1">personal</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Pisces</td><td align="left" rowspan="1" colspan="1">pointlessness</td><td align="left" rowspan="1" colspan="1">pale</td><td align="left" rowspan="1" colspan="1">picture</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">pensively</td><td align="left" rowspan="1" colspan="1">perfect</td><td align="left" rowspan="1" colspan="1">peppermints</td><td align="left" rowspan="1" colspan="1">put</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">pink</td><td align="left" rowspan="1" colspan="1">purpose</td><td align="left" rowspan="1" colspan="1">panic-stricken</td><td align="left" rowspan="1" colspan="1">piece</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">pimpled</td><td align="left" rowspan="1" colspan="1">peck</td><td align="left" rowspan="1" colspan="1">pain</td><td align="left" rowspan="1" colspan="1">pork</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">poker-faced</td><td align="left" rowspan="1" colspan="1">pigsty</td><td align="left" rowspan="1" colspan="1">pair</td><td align="left" rowspan="1" colspan="1">pulse</td></tr><tr><td align="left" rowspan="1" colspan="1">Coronal</td><td align="left" rowspan="1" colspan="1">tingle</td><td align="left" rowspan="1" colspan="1">table</td><td align="left" rowspan="1" colspan="1">talker</td><td align="left" rowspan="1" colspan="1">tenth</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">taunting</td><td align="left" rowspan="1" colspan="1">teasingly</td><td align="left" rowspan="1" colspan="1">tasteless</td><td align="left" rowspan="1" colspan="1">typical</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">turn-on</td><td align="left" rowspan="1" colspan="1">temptingly</td><td align="left" rowspan="1" colspan="1">toffee</td><td align="left" rowspan="1" colspan="1">town</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">teensy</td><td align="left" rowspan="1" colspan="1">tipsy</td><td align="left" rowspan="1" colspan="1">total</td><td align="left" rowspan="1" colspan="1">tolerable</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">tearlessly</td><td align="left" rowspan="1" colspan="1">tigerish</td><td align="left" rowspan="1" colspan="1">tactful</td><td align="left" rowspan="1" colspan="1">terribly</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">terror-stricken</td><td align="left" rowspan="1" colspan="1">tubby</td><td align="left" rowspan="1" colspan="1">timidly</td><td align="left" rowspan="1" colspan="1">two</td></tr><tr><td align="left" rowspan="1" colspan="1">Velar</td><td align="left" rowspan="1" colspan="1">cod</td><td align="left" rowspan="1" colspan="1">kinda</td><td align="left" rowspan="1" colspan="1">course</td><td align="left" rowspan="1" colspan="1">cocktail</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">kissable</td><td align="left" rowspan="1" colspan="1">corpulent</td><td align="left" rowspan="1" colspan="1">cauliflower</td><td align="left" rowspan="1" colspan="1">contact</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">chiropractor</td><td align="left" rowspan="1" colspan="1">captivate</td><td align="left" rowspan="1" colspan="1">candlelight</td><td align="left" rowspan="1" colspan="1">chemistry</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">cusp</td><td align="left" rowspan="1" colspan="1">coaxing</td><td align="left" rowspan="1" colspan="1">candid</td><td align="left" rowspan="1" colspan="1">confidence</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">concrete</td><td align="left" rowspan="1" colspan="1">cop-out</td><td align="left" rowspan="1" colspan="1">coolness</td><td align="left" rowspan="1" colspan="1">calm</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">killjoy</td><td align="left" rowspan="1" colspan="1">cordial</td><td align="left" rowspan="1" colspan="1">courtship</td><td align="left" rowspan="1" colspan="1">compliment</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>Stimuli are arranged by frequency (by quartile in columns) and place of articulation.</p></fn></table-wrap-foot></table-wrap><p>All subjects took a post-experiment survey which included questions about the subject&#x02019;s age, second language knowledge, assessment of own sexual orientation (from 1&#x0200a;=&#x0200a;exclusively heterosexual to 7&#x0200a;=&#x0200a;exclusively homosexual), feelings towards the talker (from 1&#x0200a;=&#x0200a;very positive to 7&#x0200a;=&#x0200a;very negative), likelihood of behaving in the same way in a similar situation (yes/no), and whether anything unusual was noticed in the talker&#x02019;s speech. Subjects also completed several neurocognitive and personality measures. Subjects filled out the Autism-Spectrum Quotient (AQ; <xref rid="pone.0074746-BaronCohen1" ref-type="bibr">[44]</xref>). The AQ is a short, self-administered scale for identifying the degree to which any individual adult of normal IQ may have traits associated with the ASC, of which classic autism and Asperger&#x02019;s Syndrome are the clearest subgroups. The AQ is not a diagnostic measure, although it has been clinically tested as a screening tool; traits as assessed by the AQ show high heritability and are stable cross-culturally. The test consists of 50 items, made up of 10 questions assessing five subscales: social skills (SS), communication (CM), attention to detail (AD), attention-switching (AS), and imagination (IM). The AQ items were scored on a Likert scale (1&#x02013;4). A total AQ score was calculated by summing all the scores for each of the items, with a maximum score of 200 and a minimum score of 50. Scores for the subscales (AS, CM, AD, AS, IM) have a maximum score of 40 and a minimum score of 10. All scales were scored in such a way that high scores indicated traits associated with ASC: lower social skills, difficulty in attention switching/strong focus of attention, higher attention to detail and patterns, lower ability to communicate, and lower imagination. Subjects also took the Big Five Inventory, which consists of five broad personality dimensions: Openness (O), Conscientiousness (C), Extraversion (E), Agreeableness (A), and Neuroticism (N) <xref rid="pone.0074746-John1" ref-type="bibr">[56]</xref>, <xref rid="pone.0074746-John2" ref-type="bibr">[57]</xref>. The score for each personality dimension was computed as the mean score for questions associated with the dimension. High AQ individuals are associated with high Neuroticism, low Extraversion, and low Agreeableness <xref rid="pone.0074746-Yu2" ref-type="bibr">[32]</xref>, <xref rid="pone.0074746-Austin1" ref-type="bibr">[52]</xref> or low Conscientiousness <xref rid="pone.0074746-Yu2" ref-type="bibr">[32]</xref>, <xref rid="pone.0074746-Wakabayashi1" ref-type="bibr">[58]</xref>. Subjects also completed the Automated Reading Span Task (RSPAN; <xref rid="pone.0074746-Unsworth1" ref-type="bibr">[59]</xref>), a widely-used instrument for assessing working memory capacity. In this test, subjects were presented with a series of sentences on a computer (e.g., &#x0201c;The ranger told the hiker to look out for snakes.&#x0201d;) and were asked to indicate whether the sentence makes sense by clicking &#x0201c;TRUE&#x0201d; or &#x0201c;FALSE&#x0201d; on the screen. A letter was then presented for participants to hold in memory. These sentence-letter trials were presented in sets, with three to seven trials per set, for a total of 75 letters in 15 sets. At the end of each set, a screen with 12 letters appeared, and participants used the mouse to select the letters they remembered in the correct order. Scores for the RSPAN were calculated with the partial-credit unit scoring method <xref rid="pone.0074746-Conway2" ref-type="bibr">[60]</xref>. The order in which the battery of personality/socio-cognitive tests was administered was random.</p></sec><sec id="s2c"><title>Materials</title><p>The narrative consisted of a male talker recounting the experience of a recent blind date. The narrative contained no other stressed syllable-initial voiceless aspirated stops aside from the target words. Two versions of the narrative were created: in one version, the narrator abandoned his date at the restaurant and went home alone (the &#x0201c;negative&#x0201d; version); in the other version, the narrator hit it off with the date and was happy about it (the &#x0201c;positive&#x0201d; version). In order to manipulate the perceived sexual orientation of the narrator (as either &#x0201c;heterosexual&#x0201d; or &#x0201c;homosexual&#x0201d;), the gender of the date was varied for each storyline. A total of four possible storylines were created (i.e., two date outcomes (&#x0201c;positive&#x0201d; vs. &#x0201c;negative&#x0201d;) &#x000d7; two perceived narrator sexual orientations (&#x0201c;heterosexual&#x0201d; vs. &#x0201c;homosexual&#x0201d;)). The full texts of the heterosexual version of the &#x0201c;positive&#x0201d; and &#x0201c;negative&#x0201d; storylines are given in the <xref ref-type="supplementary-material" rid="pone.0074746.s001">Supporting Information S1</xref>. To create the narrative recording, a native English-speaking male talker was recorded reading all four versions of the story. The VOTs of the target words in the &#x0201c;homosexual&#x0201d; version of the narratives (both &#x0201c;positive&#x0201d; and &#x0201c;negative&#x0201d; outcomes) were extended as described above. The &#x0201c;heterosexual&#x0201d; version of the narratives was created by replacing the proper names and pronouns in the extended-VOT recordings with the gender-appropriate names and pronouns from the &#x0201c;heterosexual&#x0201d; versions of the recording.</p></sec><sec id="s2d"><title>Measurements</title><p>A team of five labelers delineated and labeled target-word VOTs in Praat, using both waveforms and spectrograms to determine the extent of prevocalic aspiration. VOTs were then calculated using a script. Seven sets of test block recordings were measured by all five labelers and VOT measurements were compared in order to check for inter-researcher consistency. No single VOT had more than 6 msec of variation among the five measurements. Considering that the inter-researcher variation reported in previous VOT studies ranges from 2 msec to 10 msec <xref rid="pone.0074746-Allen1" ref-type="bibr">[61]</xref>&#x02013;<xref rid="pone.0074746-Fischer1" ref-type="bibr">[64]</xref>, a difference in 6 msec of VOT variation appears to be reasonable. Moreover, given that our main focus is in the amount of within-individual VOT difference across blocks, variation in VOT across individuals is also less of a factor in the final analysis.</p></sec><sec id="s2e"><title>Participants</title><p>Ninety-three subjects completed the study, and received either course credits or a nominal cash payment (USD$10). Participants were assigned to one of the four conditions. Approximately equal numbers of subjects participated in each of the conditions. (Note that our analysis below uses mixed-effects regression models, which are robust to unbalanced designs.).</p></sec></sec><sec id="s3"><title>Results</title><p>While 93 subjects participated in the study, data from two subjects were lost due to problems with the recording procedure. In addition, data were excluded from one subject who said many words without the carrier phrase, as well as from six subjects who did not complete the RSPAN or one of the questionnaires (AQ or Big Five). While subjects read 72 words, due to problems with the stimuli presentation script, the words &#x0201c;pair&#x0201d; and &#x0201c;pearl&#x0201d; did not appear consistently across pre-exposure and post-exposure block and were excluded from the final analysis. The following analysis was performed on VOTs for the remaining 70 words by 84 subjects.</p><p>Descriptive statistics of subjects&#x02019; age, and attitude scores, as well as their AQ and Big Five scores are given in <xref ref-type="table" rid="pone-0074746-t002">Table 2</xref>. The distributions of AQ scores are typical of normally developing populations. As a general comparison, the mean total AQ of fifty-five native speakers of English at a British university in <xref rid="pone.0074746-Stewart1" ref-type="bibr">[45]</xref> was 102 (SD&#x0200a;=&#x0200a;14.5, range&#x0200a;=&#x0200a;71&#x02013;150) and the mean total AQ of sixty native speakers of English at an American university in <xref rid="pone.0074746-Yu1" ref-type="bibr">[31]</xref> was 110.05 (SD&#x0200a;=&#x0200a;18, range&#x0200a;=&#x0200a;78&#x02013;155).</p><table-wrap id="pone-0074746-t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.t002</object-id><label>Table 2</label><caption><title>Descriptive statistics of subject-level variables.</title></caption><alternatives><graphic id="pone-0074746-t002-2" xlink:href="pone.0074746.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Condition</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">gay</td><td align="left" rowspan="1" colspan="1">straight</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<bold>Positive</bold>
</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">20 subjects, 9F</td><td align="left" rowspan="1" colspan="1">24 subjects, 13F</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">AGE</td><td align="left" rowspan="1" colspan="1">20.37 (4.39)</td><td align="left" rowspan="1" colspan="1">20.33 (2.16)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">ATTITUDE</td><td align="left" rowspan="1" colspan="1">2.80 (1.24)</td><td align="left" rowspan="1" colspan="1">3.63 (1.395)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">TOTAL AUTISM-SPECTRUM QUOTIENT</td><td align="left" rowspan="1" colspan="1">113 (13.85)</td><td align="left" rowspan="1" colspan="1">104.17 (15.88)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SOCIAL SKILLS</td><td align="left" rowspan="1" colspan="1">21.60 (4.92)</td><td align="left" rowspan="1" colspan="1">20.38 (5.55)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">ATTENTION SWITCHING</td><td align="left" rowspan="1" colspan="1">25.65 (3.50)</td><td align="left" rowspan="1" colspan="1">23.96 (3.77)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">ATTENTION TO DETAIL</td><td align="left" rowspan="1" colspan="1">25.60 (5.98)</td><td align="left" rowspan="1" colspan="1">23.42 (5.40)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">COMMUNICATION SKILLS</td><td align="left" rowspan="1" colspan="1">21.30 (2.99)</td><td align="left" rowspan="1" colspan="1">18.46 (3.95)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">IMAGINATION</td><td align="left" rowspan="1" colspan="1">18.85 (4.63)</td><td align="left" rowspan="1" colspan="1">17.96 (3.67)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td colspan="3" align="left" rowspan="1">BIG FIVE INVENTORY</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">EXTROVERSION</td><td align="left" rowspan="1" colspan="1">3.06 (0.77)</td><td align="left" rowspan="1" colspan="1">3.03 (0.73)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">AGREEABLENESS</td><td align="left" rowspan="1" colspan="1">3.41 (0.68)</td><td align="left" rowspan="1" colspan="1">3.67 (0.52)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">CONSCIENTIOUSNESS</td><td align="left" rowspan="1" colspan="1">3.04 (0.71)</td><td align="left" rowspan="1" colspan="1">3.38 (0.60)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">NEUROTICISM</td><td align="left" rowspan="1" colspan="1">2.77 (0.85)</td><td align="left" rowspan="1" colspan="1">2.96 (0.84)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">OPENNESS</td><td align="left" rowspan="1" colspan="1">3.90 (0.60)</td><td align="left" rowspan="1" colspan="1">3.84 (0.46)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">RSPAN</td><td align="left" rowspan="1" colspan="1">64.65 (7.03)</td><td align="left" rowspan="1" colspan="1">61.96 (9.82)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Pre-exposure VOT (MS)</td><td align="left" rowspan="1" colspan="1">80.91 (16.57)</td><td align="left" rowspan="1" colspan="1">83.78 (14.80)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Post-exposure VOT (MS)</td><td align="left" rowspan="1" colspan="1">79.79 (17.62)</td><td align="left" rowspan="1" colspan="1">82.25 (16.54)</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Negative</bold>
</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">19 subjects, 10F</td><td align="left" rowspan="1" colspan="1">21 subjects, 13F</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">AGE</td><td align="left" rowspan="1" colspan="1">19.26 (1.19)</td><td align="left" rowspan="1" colspan="1">20.24 (2.98)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">ATTITUDE</td><td align="left" rowspan="1" colspan="1">3.32 (1.63)</td><td align="left" rowspan="1" colspan="1">4.05 (1.563)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">TOTAL AUTISM-SPECTRUM QUOTIENT</td><td align="left" rowspan="1" colspan="1">108.30 (10.09)</td><td align="left" rowspan="1" colspan="1">109.90 (13.69)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">SOCIAL SKILLS</td><td align="left" rowspan="1" colspan="1">20.16 (4.71)</td><td align="left" rowspan="1" colspan="1">20.19 (4.35)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">ATTENTION SWITCHING</td><td align="left" rowspan="1" colspan="1">24.84 (3.72)</td><td align="left" rowspan="1" colspan="1">24.81 (2.91)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">ATTENTION TO DETAILS</td><td align="left" rowspan="1" colspan="1">25.63 (4.70)</td><td align="left" rowspan="1" colspan="1">25.48 (3.92)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">COMMUNICATION SKILLS</td><td align="left" rowspan="1" colspan="1">19.58 (3.72)</td><td align="left" rowspan="1" colspan="1">20.19 (3.31)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">IMAGINATION</td><td align="left" rowspan="1" colspan="1">18.05 (2.86)</td><td align="left" rowspan="1" colspan="1">19.24 (5.12)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td colspan="3" align="left" rowspan="1">BIG FIVE INVENTORY</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">EXTROVERSION</td><td align="left" rowspan="1" colspan="1">3.28 (0.81)</td><td align="left" rowspan="1" colspan="1">3.08 (0.70)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">AGREEABLENESS</td><td align="left" rowspan="1" colspan="1">3.65 (0.68)</td><td align="left" rowspan="1" colspan="1">3.43 (0.72)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">CONSCIENTIOUSNESS</td><td align="left" rowspan="1" colspan="1">3.26 (0.90)</td><td align="left" rowspan="1" colspan="1">3.16 (0.67)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">NEUROTICISM</td><td align="left" rowspan="1" colspan="1">2.74 (0.82)</td><td align="left" rowspan="1" colspan="1">3.07 (0.79)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">OPENNESS</td><td align="left" rowspan="1" colspan="1">3.70 (0.62)</td><td align="left" rowspan="1" colspan="1">3.86 (0.57)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">RSPAN</td><td align="left" rowspan="1" colspan="1">63.11 (8.21)</td><td align="left" rowspan="1" colspan="1">63.9 (11.09)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Pre-exposure VOT (MS)</td><td align="left" rowspan="1" colspan="1">86.72 (13.29)</td><td align="left" rowspan="1" colspan="1">84.00 (14.83)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Post-exposure VOT (MS)</td><td align="left" rowspan="1" colspan="1">86.49 (13.44)</td><td align="left" rowspan="1" colspan="1">84.45 (15.20)</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt102"><label/><p>Mean and standard deviation of variables measured for subjects in each narrative condition, including age, attitude towards the narrator, total AQ, AQ subscores, Big 5 subscores, and RSPAN, as well as the VOT values during the pre- and post-exposure blocks.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s4"><title>Analysis</title><p>We are interested in two questions about subjects&#x02019; VOT productions. First, how does VOT shift as a result of hearing the narrative, across subjects, after controlling for other factors? Second, what factors affect how much a subject&#x02019;s VOT shifts?</p><p>Our analysis addresses these questions using a two-step modeling procedure. We first model the effects of properties of all factors on VOT <italic>except</italic> whether the subject has heard the narrative yet or not (Model 1). The residuals of this model are VOT values normalized for speaking rate, properties of the host word, and idiosyncratic by-subject and by-word differences. For each word for each subject, we then calculate the <italic>normalized VOT shift</italic> (just <italic>shift</italic> henceforth): the difference between the subject&#x02019;s pre-narrative and post-narrative normalized VOT values for the word. We then model the effects of subject-level variables (such as <sc>rspan</sc> and attitude towards the narrator) on the amount of shift (Model 2). The results of Model 2 address both questions: the value of its intercept corresponds to how much overall VOT shift occurs, and the values of its coefficients describe how different subject-level variables affect the amount of shift.</p><sec id="s4a"><title>Model Preliminaries</title><p>The models include several types of predictors, summarized in <xref ref-type="table" rid="pone-0074746-t003">Table 3</xref>, corresponding to properties of the host word (word-level predictors), the individual utterance (utterance-level predictors), the narrative condition (<sc>outcome</sc>, <sc>sexuality</sc>), or the subject (all other predictors). For the purposes of modeling it is convenient to call all predictors indexing either properties of the narrative or of the subject &#x0201c;subject-level&#x0201d;, meaning that they have fixed values for all tokens from a given subject.</p><table-wrap id="pone-0074746-t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.t003</object-id><label>Table 3</label><caption><title>Predictors used in Models 1 and 2.</title></caption><alternatives><graphic id="pone-0074746-t003-3" xlink:href="pone.0074746.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Predictor type</td><td align="left" rowspan="1" colspan="1">Predictor</td><td align="left" rowspan="1" colspan="1">Abbreviation</td><td align="left" rowspan="1" colspan="1">Type</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic>Word-level</italic>
</td><td align="left" rowspan="1" colspan="1">Number of syllables</td><td align="left" rowspan="1" colspan="1">
<sc>syllables</sc>
</td><td align="left" rowspan="1" colspan="1">ordered factor (1, 2, 3, 4)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Log <sc>celex</sc> frequency</td><td align="left" rowspan="1" colspan="1">
<sc>frequency</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Initial consonant</td><td align="left" rowspan="1" colspan="1">
<sc>consonant</sc>
</td><td align="left" rowspan="1" colspan="1">factor (/p/,/t/,/k/)</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic>Utterance-level</italic>
</td><td align="left" rowspan="1" colspan="1">Syllables/second in the word</td><td align="left" rowspan="1" colspan="1">
<sc>rate</sc>1</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Syllables/second in the carrier phrase</td><td align="left" rowspan="1" colspan="1">
<sc>rate</sc>2</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Within-block order</td><td align="left" rowspan="1" colspan="1">
<sc>trial</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Stimulus block</td><td align="left" rowspan="1" colspan="1">
<sc>block</sc>
</td><td align="left" rowspan="1" colspan="1">factor (pre-, post-narrative)</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic>Subject-level</italic>
</td><td align="left" rowspan="1" colspan="1">Narrative outcome</td><td align="left" rowspan="1" colspan="1">
<sc>outcome</sc>
</td><td align="left" rowspan="1" colspan="1">factor (positive, negative)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Narrator&#x02019;s sexual orientation</td><td align="left" rowspan="1" colspan="1">
<sc>sexuality</sc>
</td><td align="left" rowspan="1" colspan="1">factor (gay, straight)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Subject gender</td><td align="left" rowspan="1" colspan="1">
<sc>gender</sc>
</td><td align="left" rowspan="1" colspan="1">factor (male, female)</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Subject attitude</td><td align="left" rowspan="1" colspan="1">
<sc>attitude</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">RSPAN score</td><td align="left" rowspan="1" colspan="1">
<sc>rspan</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Openness</td><td align="left" rowspan="1" colspan="1">
<sc>o</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Conscientiousness</td><td align="left" rowspan="1" colspan="1">
<sc>c</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Extraversion</td><td align="left" rowspan="1" colspan="1">
<sc>e</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Neuroticism</td><td align="left" rowspan="1" colspan="1">
<sc>n</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Agreeableness</td><td align="left" rowspan="1" colspan="1">
<sc>a</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Attention switching</td><td align="left" rowspan="1" colspan="1">
<sc>as</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Social skills</td><td align="left" rowspan="1" colspan="1">
<sc>ss</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Communication</td><td align="left" rowspan="1" colspan="1">
<sc>cm</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Imagination</td><td align="left" rowspan="1" colspan="1">
<sc>im</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Attention to detail</td><td align="left" rowspan="1" colspan="1">
<sc>ad</sc>
</td><td align="left" rowspan="1" colspan="1">continuous</td></tr></tbody></table></alternatives></table-wrap><p>Model 1 describes how VOT depends on word-level and utterance-level predictors, with the exception of <sc>block</sc> : the frequency, initial consonant, and length in syllables of the host word, as well as two measures of speaking rate and the within-block position of the utterance. Speaking rate (syllables per second) within the host word (<sc>rate</sc>1) and within the carrier phrase (<sc>rate</sc>2) were calculated using word boundaries from forced alignment obtained using the Penn Forced Aligner (<ext-link ext-link-type="uri" xlink:href="http://www.ling.upenn.edu/phonetics/p2fa/">http://www.ling.upenn.edu/phonetics/p2fa/</ext-link>; <xref rid="pone.0074746-Yuan1" ref-type="bibr">[65]</xref>) with the number of syllables in a word or carrier phrase determined assuming canonical American English pronunciations from the CMU pronunciation dictionary (<ext-link ext-link-type="uri" xlink:href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict">http://www.speech.cs.cmu.edu/cgi-bin/cmudict</ext-link>). To ensure that each force-aligned chunk corresponded to the number of syllables indicated by its transcription, all disfluencies (such as false starts, repetitions, and non-speech noises) which deviated from the prompts were segmented out by hand prior to forced alignment. All resulting force-aligned word and phrase boundaries were very good in a visual inspection of several speakers&#x02019; files; as a result, we were confident in the results of forced alignment, and made no explicit comparison with manually-labeled boundaries.</p><p>Model 2 describes how the amount of normalized VOT shift depends on subject-level predictors: the narrative outcome and the narrator&#x02019;s sexual orientation, as well as the subject&#x02019;s gender, attitude towards the narrator, RSPAN, Big 5 personality scores (<sc>c</sc>, <sc>o</sc>, <sc>e</sc>, <sc>a</sc>, <sc>n</sc>) and AQ subscores (<sc>ss</sc>, <sc>cs</sc>, <sc>im</sc>, <sc>ad</sc>, <sc>as</sc>).</p></sec><sec id="s4b"><title>Model 1</title><p>VOT in the dataset was modeled using a linear mixed-effects model fit in R, using the lmer() function from the lme4 package <xref rid="pone.0074746-Bates1" ref-type="bibr">[66]</xref>.</p><p>Fixed-effect terms were included for <sc>frequency</sc>, <sc>consonant</sc>, and <sc>syllables</sc>, to allow for the possibility that VOT is negatively correlated with frequency, to control for the well-known effect of place of articulation on VOT (/p/&#x0003c;/t/&#x0003c;/k/; e.g., <xref rid="pone.0074746-Lisker1" ref-type="bibr">[67]</xref>), and to account for a trend observed in our data for VOT to depend on the number of syllables in the word. Fixed-effect terms were also included for <sc>rate</sc>1 and <sc>rate</sc>2, to control for the large negative effects of speaking rate on VOT (e.g., <xref rid="pone.0074746-Summerfield1" ref-type="bibr">[68]</xref>, <xref rid="pone.0074746-Miller1" ref-type="bibr">[69]</xref>). By-subject random slopes were included for all five variables, to allow for by-subject variability in the effect of each variable on VOT <xref rid="pone.0074746-Theodore1" ref-type="bibr">[63]</xref>, <xref rid="pone.0074746-Sonderegger1" ref-type="bibr">[70]</xref>.</p><p>Exploratory data analysis suggested that some subjects&#x02019; VOT values steadily increased or decreased over the course of a block (pre or post-narrative), and that the slope of this change could differ by block. To control for this possibility, we included both fixed-effect terms and random slopes for the main effect of <sc>trial</sc> and for its interaction with <sc>block</sc>.</p><p>The model also included by-subject and by-word random intercepts, to allow for subject-specific variation in VOT <xref rid="pone.0074746-Allen1" ref-type="bibr">[61]</xref>, as well as word-specific variation in VOT beyond the effects of the word-level predictors. Finally, the model included all possible correlations between random effect terms. The model formula in lme4 style was: <sc>vot</sc> &#x0223c; <sc>consonant</sc>+<sc>frequency</sc>+<sc>syllables</sc>+<sc>rate</sc>1+ <sc>rate</sc>2+ <sc>trial</sc>+<sc>trial</sc> : <sc>block</sc>+(1+ <sc>consonant</sc>+<sc>frequency</sc>+<sc>syllables</sc>+<sc>rate</sc>1+ <sc>rate</sc>2+ <sc>trial</sc>+<sc>trial</sc> : <sc>block</sc>
<inline-formula><inline-graphic xlink:href="pone.0074746.e001.jpg"/></inline-formula>
<sc>subject</sc>)+(1<inline-formula><inline-graphic xlink:href="pone.0074746.e002.jpg"/></inline-formula>
<sc>word</sc>). By-word random slopes for utterance-level predictors could also be included for the maximal random effect structure, following <xref rid="pone.0074746-Barr1" ref-type="bibr">[71]</xref>. A model with by-word random slopes added failed to converge, and was extremely similar to the model without them; the latter is reported for simplicity.</p><p>Outliers were trimmed from the dataset prior to fitting the model. A token for a given word and subject was excluded if its VOT was more than 3 standard deviations from the mean across all tokens of the word, all tokens from the subject, or the entire dataset. Because of the extremely strong effect of speaking rate on VOT, we also excluded speaking rate outliers (either <sc>rate</sc>1 or <sc>rate</sc>2) by the same criterion. Out of 11573 tokens in the dataset described above, we excluded 134 tokens (1.1%) as VOT outliers and 216 tokens (1.9%) as rate outliers. To reduce multicollinearity between predictors, <sc>rate</sc>1, <sc>rate</sc>2, and <sc>trial</sc> were standardized (centered and divided by one standard deviation), <sc>block</sc> was sum-coded, <sc>consonant</sc> was Helmert-coded, and <sc>syllables</sc> was treated as an ordered factor using orthogonal polynomial coding. <sc>syllables</sc> was treated as a factor rather than a continuous predictor because of its small number of unique values (4). Exploratory plots suggested the relationship between <sc>syllables</sc> and VOT was roughly quadratic, so only the linear and quadratic trend terms (and not the cubic term) for <sc>syllables</sc> were included.</p><p>The residuals of an initial fit of the model had a distribution which deviated strongly from normality. We trimmed 80 tokens (0.7%) with residuals which were more than 3 standard deviations from the mean, and refit the model to the trimmed dataset. The new model had a residual distribution much closer to normality, and it is the residuals of the new model which were used as the input to Model 2.</p><sec id="s4b1"><title>Model 1: Results</title><p>
<xref ref-type="table" rid="pone-0074746-t004">Table 4</xref> lists the estimated value for each fixed-effect coefficient, along with its standard error, <inline-formula><inline-graphic xlink:href="pone.0074746.e003.jpg"/></inline-formula> statistic, and corresponding significance value using a Wald test. Each coefficient&#x02019;s <inline-formula><inline-graphic xlink:href="pone.0074746.e004.jpg"/></inline-formula> statistic should be normally distributed given the size of the current dataset, making a Wald test appropriate <xref rid="pone.0074746-Hox1" ref-type="bibr">[72]</xref>. Because all predictors have been centered, the intercept (86.3 msec) can be interpreted as the predicted mean VOT across the three places of articulations, for a word with average values of each other word-level predictor, for an average subject.</p><table-wrap id="pone-0074746-t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.t004</object-id><label>Table 4</label><caption><title>Model 1 summary.</title></caption><alternatives><graphic id="pone-0074746-t004-4" xlink:href="pone.0074746.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Predictor</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e005.jpg"/></inline-formula>
</td><td align="left" rowspan="1" colspan="1">s.e.(<inline-formula><inline-graphic xlink:href="pone.0074746.e006.jpg"/></inline-formula>)</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e007.jpg"/></inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e008.jpg"/></inline-formula>
</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Intercept</td><td align="left" rowspan="1" colspan="1">86.31</td><td align="left" rowspan="1" colspan="1">1.93</td><td align="left" rowspan="1" colspan="1">44.61</td><td align="left" rowspan="1" colspan="1">&#x0003c;0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Consonant (t vs. p)</td><td align="left" rowspan="1" colspan="1">10.41</td><td align="left" rowspan="1" colspan="1">1.28</td><td align="left" rowspan="1" colspan="1">8.13</td><td align="left" rowspan="1" colspan="1">&#x0003c;0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Initial consonant (k vs. p/t)</td><td align="left" rowspan="1" colspan="1">3.10</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">4.40</td><td align="left" rowspan="1" colspan="1">&#x0003c;0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Log <sc>celex</sc> frequency</td><td align="left" rowspan="1" colspan="1">1.24</td><td align="left" rowspan="1" colspan="1">1.12</td><td align="left" rowspan="1" colspan="1">1.11</td><td align="left" rowspan="1" colspan="1">0.27</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of syllables (linear)</td><td align="left" rowspan="1" colspan="1">17.99</td><td align="left" rowspan="1" colspan="1">3.15</td><td align="left" rowspan="1" colspan="1">5.71</td><td align="left" rowspan="1" colspan="1">&#x0003c;0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of syllables (quadratic)</td><td align="left" rowspan="1" colspan="1">6.93</td><td align="left" rowspan="1" colspan="1">2.39</td><td align="left" rowspan="1" colspan="1">2.91</td><td align="left" rowspan="1" colspan="1">0.0037</td></tr><tr><td align="left" rowspan="1" colspan="1">Syllables/second (word)</td><td align="left" rowspan="1" colspan="1">&#x02212;9.77</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">&#x02212;14.21</td><td align="left" rowspan="1" colspan="1">&#x0003c;0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Syllables/second (phrase)</td><td align="left" rowspan="1" colspan="1">&#x02212;3.58</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">&#x02212;5.00</td><td align="left" rowspan="1" colspan="1">&#x0003c;0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Within-block order</td><td align="left" rowspan="1" colspan="1">0.53</td><td align="left" rowspan="1" colspan="1">0.23</td><td align="left" rowspan="1" colspan="1">2.35</td><td align="left" rowspan="1" colspan="1">0.019</td></tr><tr><td align="left" rowspan="1" colspan="1">Within-block order&#x000d7;stimulus block</td><td align="left" rowspan="1" colspan="1">0.07</td><td align="left" rowspan="1" colspan="1">0.48</td><td align="left" rowspan="1" colspan="1">0.14</td><td align="left" rowspan="1" colspan="1">0.89</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt103"><label/><p>Estimate (<inline-formula><inline-graphic xlink:href="pone.0074746.e009.jpg"/></inline-formula>), standard error (s.e.(<inline-formula><inline-graphic xlink:href="pone.0074746.e010.jpg"/></inline-formula>)), <inline-formula><inline-graphic xlink:href="pone.0074746.e011.jpg"/></inline-formula>-value, and significance value (Wald test) for each fixed-effect coefficient in Model 1.</p></fn></table-wrap-foot></table-wrap><p>Most word-level predictors have significant effects on VOT. <xref ref-type="fig" rid="pone-0074746-g001">Figure 1</xref> illustrates the empirical relationships between the significant predictors and VOT. As shown in the lower-left panel of <xref ref-type="fig" rid="pone-0074746-g001">Figure 1</xref>, VOT is heavily modulated by the place of articulation of the consonant. The model predicts that the VOT of velars is 9.30 msec longer than the VOT of more anterior stops (<inline-formula><inline-graphic xlink:href="pone.0074746.e012.jpg"/></inline-formula> 0.0001) while the VOT for coronals is 20.82 msec longer than for labials (<inline-formula><inline-graphic xlink:href="pone.0074746.e013.jpg"/></inline-formula> 0.0001). Because the interpretation of the two Helmert contrasts for <sc>consonant</sc> is &#x0201c;half the difference between/t/and/p/&#x0201d; and &#x0201c;one third the difference between/k/and the mean of/p/and/t/&#x0201d;, i.e., 9.30 msec&#x0200a;=&#x0200a;3.3.10. As illustrated by the lower-right panel of <xref ref-type="fig" rid="pone-0074746-g001">Figure 1</xref>, VOT depends on word length (in syllables: linear trend <inline-formula><inline-graphic xlink:href="pone.0074746.e014.jpg"/></inline-formula>, quadratic trend <inline-formula><inline-graphic xlink:href="pone.0074746.e015.jpg"/></inline-formula>), with predicted VOT of 77.7/78.8/86.9/101.8 msec for words of 1/2/3/4 syllables. The upper-left two panels of <xref ref-type="fig" rid="pone-0074746-g001">Figure 1</xref> show that VOT is strongly negatively affected by both measures of speaking rate (<inline-formula><inline-graphic xlink:href="pone.0074746.e016.jpg"/></inline-formula>): the model predicts a decrease of 9.77 msec per increase of one standard deviation (<inline-formula><inline-graphic xlink:href="pone.0074746.e017.jpg"/></inline-formula>) in syllables/second within the host word, and a decrease of 3.58 msec per <inline-formula><inline-graphic xlink:href="pone.0074746.e018.jpg"/></inline-formula> increase in syllables/second within the carrier phrase. The host word&#x02019;s within-block order, as illustrated by the top-right panel of <xref ref-type="fig" rid="pone-0074746-g001">Figure 1</xref>, also has a small but significant effect on VOT (<inline-formula><inline-graphic xlink:href="pone.0074746.e019.jpg"/></inline-formula>), which is predicted to increase by roughly 2.12 msec over the course of each block (corresponding to a 4<inline-formula><inline-graphic xlink:href="pone.0074746.e020.jpg"/></inline-formula> increase in <sc>trial</sc>); this effect does not differ significantly between the pre- and post-narrative blocks (<inline-formula><inline-graphic xlink:href="pone.0074746.e021.jpg"/></inline-formula>). The effect of log frequency did not reach significance (<inline-formula><inline-graphic xlink:href="pone.0074746.e022.jpg"/></inline-formula>).</p><fig id="pone-0074746-g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.g001</object-id><label>Figure 1</label><caption><title>Empirical plots of VOT versus significant predictors in Model 1.</title><p>For the utterance-level predictors <sc>rate</sc>1, <sc>rate</sc>2, and <sc>trial</sc> (top row), the line and shading show a linear fit and 95% confidence intervals (CIs) to the empirical data, represented by one point per observation (points omitted in the <sc>trial</sc> plot for legibility). For the word-level predictors <sc>consonant</sc> (bottom left) and <sc>syllables</sc> (bottom right), each point and vertical line show the mean and its 95% CI for VOT over all tokens of one word. The error bars for <sc>consonant</sc> are 95% CIs on the mean of the word-level means; the curve and shading show a quadratic fit to the word-level means and its 95% CIs, corresponding to the coding of <sc>syllables</sc> (see text).</p></caption><graphic xlink:href="pone.0074746.g001"/></fig><p>The by-subject and by-word random intercept variances have estimated values <inline-formula><inline-graphic xlink:href="pone.0074746.e023.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0074746.e024.jpg"/></inline-formula>, and are both highly significant (i.e., <inline-formula><inline-graphic xlink:href="pone.0074746.e025.jpg"/></inline-formula> using a likelihood ratio test <xref rid="pone.0074746-Hox1" ref-type="bibr">[72]</xref>). Thus, subjects and words differ in their average VOT, <italic>after</italic> controlling for all word-level and utterance-level predictors (except for <sc>block</sc>): 95% of words are predicted to have offsets of less than 16 msec (<inline-formula><inline-graphic xlink:href="pone.0074746.e026.jpg"/></inline-formula>) and 95% of subjects are predicted to have offsets of less than 28 msec (<inline-formula><inline-graphic xlink:href="pone.0074746.e027.jpg"/></inline-formula>), relative to the grand mean.</p></sec></sec><sec id="s4c"><title>Model 2</title><p>The residuals of Model 1 give a measure of normalized VOT for each token, after controlling for word-level and utterance-level factors (except for <sc>block</sc>). The difference in normalized VOT between the post-narrative and pre-narrative blocks, or <italic>normalized VOT shift</italic>, measures how much a subject shifted her VOT for a particular word as a result of hearing the narrative.</p><p>Normalized VOT shift was again modeled using a linear mixed-effects model, using the lmer() function in lme4. To determine the effect of properties of the narrative and the subject on the amount of shift, a fixed-effect term was included for each subject-level predictor (<xref ref-type="table" rid="pone-0074746-t003">Table 3</xref>). By-word random intercepts were included to allow for word-specific variation in the amount of shift, and by-subject random intercepts were included to allow for subject-specific variation in the amount of shift, beyond the effects of subject-level predictors. The model formula in lme4 style was: <sc>shift</sc> &#x0223c; <sc>gender</sc> + <sc>attitude</sc> + <sc>sexuality</sc> + <sc>outcome</sc> + <sc>rspan</sc> + <sc>o</sc> + <sc>c</sc> + <sc>e</sc> + <sc>n</sc> + <sc>a</sc> + <sc>as</sc> + <sc>ss</sc> + <sc>cm</sc> + <sc>im</sc> + <sc>ad</sc> + (1<inline-formula><inline-graphic xlink:href="pone.0074746.e028.jpg"/></inline-formula>
<sc>subject</sc>) + (1<inline-formula><inline-graphic xlink:href="pone.0074746.e029.jpg"/></inline-formula>
<sc>word</sc>). As for Model 1 (see note ??), the maximal model would incorporate by-word random slopes. (By-speaker random slopes are not possible since the model contains only speaker-level predictors.) A model with by-word random slopes added was not significantly different (using a likelihood ratio test) to the model without them; the latter is reported for simplicity.</p><p>Due to the data trimming steps taken in Model 1, as well as the exclusion of some tokens from the full dataset (see above), in 231 cases either the pre-narrative or post-narrative token for a given subject and word was not assigned a normalized VOT, in which case the normalized VOT shift was undefined. Model 2 was fit to the 5348 tokens for which normalized VOT shift was defined, each corresponding to one of 70 words for one of 84 subjects.</p><p>To reduce multicollinearity between predictors, continuous predictors (<sc>attitude</sc>, <sc>rspan</sc>, Big 5 scores, AQ subscores) were centered, and <sc>gender</sc>, <sc>sexuality</sc>, and outcome were sum-coded (e.g., for <sc>outcome</sc> : positive&#x0200a;=&#x0200a;0.5, negative&#x0200a;=&#x0200a;&#x02212;0.5). Each continuous predictor was scaled by twice its standard deviation, in order to make the fixed-effect coefficients for continuous predictors comparable to those for categorical predictors <xref rid="pone.0074746-Gelman1" ref-type="bibr">[73]</xref>.</p><p>The condition number of the (centered) predictors was 4.8, indicating minimal multicollinearity in the full set of predictors <xref rid="pone.0074746-Belsley1" ref-type="bibr">[74]</xref>. Nonetheless, some moderate correlations exist among Big 5 scores and AQ subscores, such as between ss and cm (<inline-formula><inline-graphic xlink:href="pone.0074746.e030.jpg"/></inline-formula>).</p><sec id="s4c1"><title>Model 2: Results</title><p>
<xref ref-type="table" rid="pone-0074746-t005">Table 5</xref> lists the estimated value for each fixed-effect coefficient, along with its standard error, <inline-formula><inline-graphic xlink:href="pone.0074746.e031.jpg"/></inline-formula>-value, and significance values obtained by MCMC sampling from the posterior distribution of the model parameters, with 50000 samples, using mcmcsamp() in lme4. Because all predictors have been centered, the intercept can be interpreted as the predicted amount of normalized VOT shift for an average subject and average word. The predicted amount of shift is very small (0.47 msec) and is not significant (<inline-formula><inline-graphic xlink:href="pone.0074746.e032.jpg"/></inline-formula>), meaning that there is no evidence that VOT is lengthened, on average, as a result of listening to the narrative. However, five predictors did have significant effects on the amount of normalized VOT shift (<inline-formula><inline-graphic xlink:href="pone.0074746.e033.jpg"/></inline-formula>), suggesting that individual subjects did shift towards or away from the narrator, in part as a function of some subject-level predictors. One of these predictors (i.e. <sc>conscientiousness</sc>) was not consistently significant under different model parameterizations and data trimming procedures we tried before arriving at a final model. We will therefore only consider the four other predictors (<sc>attitude</sc>, <sc>outcome</sc>, <sc>openness</sc>, <sc>attention</sc>-<sc>switching</sc>) as significantly affecting the amount of normalized VOT shift.</p><table-wrap id="pone-0074746-t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.t005</object-id><label>Table 5</label><caption><title>Model 2 summary.</title></caption><alternatives><graphic id="pone-0074746-t005-5" xlink:href="pone.0074746.t005"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Predictor</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e034.jpg"/></inline-formula>
</td><td align="left" rowspan="1" colspan="1">s.e.(<inline-formula><inline-graphic xlink:href="pone.0074746.e035.jpg"/></inline-formula>)</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e036.jpg"/></inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e037.jpg"/></inline-formula>
</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Intercept</td><td align="left" rowspan="1" colspan="1">0.47</td><td align="left" rowspan="1" colspan="1">0.47</td><td align="left" rowspan="1" colspan="1">1.00</td><td align="left" rowspan="1" colspan="1">0.30</td></tr><tr><td align="left" rowspan="1" colspan="1">Subject gender</td><td align="left" rowspan="1" colspan="1">0.94</td><td align="left" rowspan="1" colspan="1">0.99</td><td align="left" rowspan="1" colspan="1">0.95</td><td align="left" rowspan="1" colspan="1">0.33</td></tr><tr><td align="left" rowspan="1" colspan="1">Subject attitude</td><td align="left" rowspan="1" colspan="1">&#x02212;4.53</td><td align="left" rowspan="1" colspan="1">1.02</td><td align="left" rowspan="1" colspan="1">&#x02212;4.44</td><td align="left" rowspan="1" colspan="1">&#x0003c;0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Narrator sexual orientation</td><td align="left" rowspan="1" colspan="1">1.63</td><td align="left" rowspan="1" colspan="1">1.06</td><td align="left" rowspan="1" colspan="1">1.54</td><td align="left" rowspan="1" colspan="1">0.11</td></tr><tr><td align="left" rowspan="1" colspan="1">Narrative outcome</td><td align="left" rowspan="1" colspan="1">&#x02212;2.58</td><td align="left" rowspan="1" colspan="1">0.96</td><td align="left" rowspan="1" colspan="1">&#x02212;2.69</td><td align="left" rowspan="1" colspan="1">0.0082</td></tr><tr><td align="left" rowspan="1" colspan="1">RSPAN</td><td align="left" rowspan="1" colspan="1">1.36</td><td align="left" rowspan="1" colspan="1">1.02</td><td align="left" rowspan="1" colspan="1">1.33</td><td align="left" rowspan="1" colspan="1">0.17</td></tr><tr><td align="left" rowspan="1" colspan="1">Openness</td><td align="left" rowspan="1" colspan="1">3.53</td><td align="left" rowspan="1" colspan="1">1.05</td><td align="left" rowspan="1" colspan="1">3.36</td><td align="left" rowspan="1" colspan="1">0.0010</td></tr><tr><td align="left" rowspan="1" colspan="1">Conscientiousness</td><td align="left" rowspan="1" colspan="1">&#x02212;2.13</td><td align="left" rowspan="1" colspan="1">1.11</td><td align="left" rowspan="1" colspan="1">&#x02212;1.92</td><td align="left" rowspan="1" colspan="1">0.043</td></tr><tr><td align="left" rowspan="1" colspan="1">Extraversion</td><td align="left" rowspan="1" colspan="1">0.24</td><td align="left" rowspan="1" colspan="1">1.38</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">0.84</td></tr><tr><td align="left" rowspan="1" colspan="1">Neuroticism</td><td align="left" rowspan="1" colspan="1">&#x02212;1.47</td><td align="left" rowspan="1" colspan="1">1.20</td><td align="left" rowspan="1" colspan="1">&#x02212;1.23</td><td align="left" rowspan="1" colspan="1">0.21</td></tr><tr><td align="left" rowspan="1" colspan="1">Agreeableness</td><td align="left" rowspan="1" colspan="1">&#x02212;2.06</td><td align="left" rowspan="1" colspan="1">1.11</td><td align="left" rowspan="1" colspan="1">&#x02212;1.86</td><td align="left" rowspan="1" colspan="1">0.059</td></tr><tr><td align="left" rowspan="1" colspan="1">Attention switching</td><td align="left" rowspan="1" colspan="1">2.58</td><td align="left" rowspan="1" colspan="1">1.10</td><td align="left" rowspan="1" colspan="1">2.35</td><td align="left" rowspan="1" colspan="1">0.014</td></tr><tr><td align="left" rowspan="1" colspan="1">Social skills</td><td align="left" rowspan="1" colspan="1">&#x02212;2.02</td><td align="left" rowspan="1" colspan="1">1.87</td><td align="left" rowspan="1" colspan="1">&#x02212;0.01</td><td align="left" rowspan="1" colspan="1">0.99</td></tr><tr><td align="left" rowspan="1" colspan="1">Communication</td><td align="left" rowspan="1" colspan="1">&#x02212;1.29</td><td align="left" rowspan="1" colspan="1">1.56</td><td align="left" rowspan="1" colspan="1">&#x02212;0.83</td><td align="left" rowspan="1" colspan="1">0.39</td></tr><tr><td align="left" rowspan="1" colspan="1">Imagination</td><td align="left" rowspan="1" colspan="1">1.29</td><td align="left" rowspan="1" colspan="1">1.19</td><td align="left" rowspan="1" colspan="1">1.08</td><td align="left" rowspan="1" colspan="1">0.26</td></tr><tr><td align="left" rowspan="1" colspan="1">Attention to detail</td><td align="left" rowspan="1" colspan="1">0.07</td><td align="left" rowspan="1" colspan="1">1.11</td><td align="left" rowspan="1" colspan="1">0.06</td><td align="left" rowspan="1" colspan="1">0.95</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt104"><label/><p>Estimate (<inline-formula><inline-graphic xlink:href="pone.0074746.e038.jpg"/></inline-formula>), standard error (s.e.(<inline-formula><inline-graphic xlink:href="pone.0074746.e039.jpg"/></inline-formula>)), <inline-formula><inline-graphic xlink:href="pone.0074746.e040.jpg"/></inline-formula>-value, and simulation-based significance value for each fixed-effect coefficient in Model 2.</p></fn></table-wrap-foot></table-wrap><p>
<xref ref-type="fig" rid="pone-0074746-g002">Figure 2</xref> illustrates the effects of these predictors on the amount of shift. The effects of <sc>attitude</sc> and <sc>openness</sc> are highly significant. As shown in the top-right panel of <xref ref-type="fig" rid="pone-0074746-g002">Figure 2</xref>, subjects with a more positive attitude towards the narrator (lower <sc>attitude</sc> score) shift towards him (increased VOT) while those with a negative attitude shift away from him (decreased VOT) (<inline-formula><inline-graphic xlink:href="pone.0074746.e041.jpg"/></inline-formula>), with an increase of 2<inline-formula><inline-graphic xlink:href="pone.0074746.e042.jpg"/></inline-formula> in <sc>attitude</sc> corresponding to a decrease of 4.53 msec in predicted VOT shift (<inline-formula><inline-graphic xlink:href="pone.0074746.e043.jpg"/></inline-formula>). The lower-left panel of <xref ref-type="fig" rid="pone-0074746-g002">Figure 2</xref> illustrates that subjects with higher Openness scores increased VOT, while those with lower Openness scores decreased VOT (<inline-formula><inline-graphic xlink:href="pone.0074746.e044.jpg"/></inline-formula>), with an increase of 2<inline-formula><inline-graphic xlink:href="pone.0074746.e045.jpg"/></inline-formula> in Openness corresponding to an increase of 3.53 msec in predicted VOT shift. The lower-right panel of <xref ref-type="fig" rid="pone-0074746-g002">Figure 2</xref> shows that the amount of VOT shift differed by 2.58 msec between subjects in the negative and positive outcome conditions (negative condition&#x0003e;positive condition; <inline-formula><inline-graphic xlink:href="pone.0074746.e046.jpg"/></inline-formula>). There was also a significant effect of Attention Switching (<inline-formula><inline-graphic xlink:href="pone.0074746.e047.jpg"/></inline-formula>). As shown in the top-left panel of <xref ref-type="fig" rid="pone-0074746-g002">Figure 2</xref>, an increase of 2<inline-formula><inline-graphic xlink:href="pone.0074746.e048.jpg"/></inline-formula> in as score is predicted to increase VOT shift by 2.58 msec. No other predictor, including subject gender and perceived narrator sexual orientation, had a significant effect.</p><fig id="pone-0074746-g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.g002</object-id><label>Figure 2</label><caption><title>Empirical plots of normalized VOT shift versus significant by-subject predictors in Model 2.</title><p>Each point and vertical line show the mean and its 95% confidence interval for one subject&#x02019;s shift across all words. For as (top left), <sc>attitude</sc> (top right), and o (bottom left), lines and shading show a linear fit and 95% CIs of normalized VOT shift vs. the predictor, across all tokens. For <sc>outcome</sc> (bottom right), the error bars are 95% CIs on the mean of normalized VOT shift across all tokens.</p></caption><graphic xlink:href="pone.0074746.g002"/></fig><p>The by-word random intercept has estimated value of 0, meaning there is no evidence that words differed in the amount of VOT shift. The by-subject random intercept variance has an estimated value of <inline-formula><inline-graphic xlink:href="pone.0074746.e049.jpg"/></inline-formula>, and is highly significant (i.e., <inline-formula><inline-graphic xlink:href="pone.0074746.e050.jpg"/></inline-formula> using a likelihood ratio test <xref rid="pone.0074746-Hox1" ref-type="bibr">[72]</xref>. Thus, subjects differ in how much they shift VOT, <italic>after</italic> controlling for all subject-level predictors, with 95% of subjects expected to shift VOT between &#x02212;6.83 msec and 7.77 msec (&#x0200a;=&#x0200a;<inline-formula><inline-graphic xlink:href="pone.0074746.e051.jpg"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="pone.0074746.e052.jpg"/></inline-formula> is the predicted value of the intercept), beyond the effect of subject-level predictors included in the model. These by-subject offsets could be due to systematic variability between subjects in the amount of VOT shift (i.e., effects of subject-level predictors which were not included in the model), truly idiosyncratic variability among subjects in the amount of VOT shift, or a combination of the two. Thus, although subjects do not show any VOT shift <italic>on average</italic>, there are substantial differences in the amount of VOT shift shown by different subjects, due in part to characteristics of subjects and the narrative.</p><p>A natural question is how important the full set of subject-level predictors is for predicting the amount of VOT shift. One metric of the importance of a set of predictors for linear mixed models is the reduction in mean-squared prediction error of a full model which includes these predictors, relative to a baseline model which does not <xref rid="pone.0074746-Snijders1" ref-type="bibr">[75]</xref>; this quantity, denoted <inline-formula><inline-graphic xlink:href="pone.0074746.e053.jpg"/></inline-formula>, lies between 0 and 1. In our case, this measure is <inline-formula><inline-graphic xlink:href="pone.0074746.e054.jpg"/></inline-formula> relative to a baseline model with only by-word and by-subject random intercepts, indicating that while the subject-level predictors make significant contributions to the model of VOT shift, they explain very little of the total observed variability in VOT shift.</p><p>Also of interest is the relative importance of the different predictors in the model. Because the predictors were standardized, the coefficient values are comparable, and one measure of a predictor&#x02019;s importance is simply the absolute value of its coefficient (<inline-formula><inline-graphic xlink:href="pone.0074746.e055.jpg"/></inline-formula>). Another measure of a predictor&#x02019;s importance is the percentage change in <inline-formula><inline-graphic xlink:href="pone.0074746.e056.jpg"/></inline-formula> when the predictor is dropped from the full model. Both measures are shown in <xref ref-type="table" rid="pone-0074746-t006">Table 6</xref>, with predictors sorted in order of <inline-formula><inline-graphic xlink:href="pone.0074746.e057.jpg"/></inline-formula>. The four significant predictors in Model 2 have the same ordering under both measures, and come out as more important than other predictors by both measures. (For other predictors the two measures of importance disagree somewhat.).</p><table-wrap id="pone-0074746-t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0074746.t006</object-id><label>Table 6</label><caption><title>Relative importance of predictors in Model 2.</title></caption><alternatives><graphic id="pone-0074746-t006-6" xlink:href="pone.0074746.t006"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">Predictor</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e058.jpg"/></inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula><inline-graphic xlink:href="pone.0074746.e059.jpg"/></inline-formula>
</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Subject attitude</td><td align="left" rowspan="1" colspan="1">4.53</td><td align="left" rowspan="1" colspan="1">65.6</td></tr><tr><td align="left" rowspan="1" colspan="1">Openness</td><td align="left" rowspan="1" colspan="1">3.53</td><td align="left" rowspan="1" colspan="1">36.6</td></tr><tr><td align="left" rowspan="1" colspan="1">Narrative outcome</td><td align="left" rowspan="1" colspan="1">2.58</td><td align="left" rowspan="1" colspan="1">22.1</td></tr><tr><td align="left" rowspan="1" colspan="1">Attention switching</td><td align="left" rowspan="1" colspan="1">2.58</td><td align="left" rowspan="1" colspan="1">16.0</td></tr><tr><td align="left" rowspan="1" colspan="1">Conscientiousness</td><td align="left" rowspan="1" colspan="1">2.13</td><td align="left" rowspan="1" colspan="1">9.3</td></tr><tr><td align="left" rowspan="1" colspan="1">Agreeableness</td><td align="left" rowspan="1" colspan="1">2.06</td><td align="left" rowspan="1" colspan="1">8.6</td></tr><tr><td align="left" rowspan="1" colspan="1">Narrator sexual orientation</td><td align="left" rowspan="1" colspan="1">1.63</td><td align="left" rowspan="1" colspan="1">4.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Neuroticism</td><td align="left" rowspan="1" colspan="1">1.47</td><td align="left" rowspan="1" colspan="1">2.1</td></tr><tr><td align="left" rowspan="1" colspan="1">RSPAN</td><td align="left" rowspan="1" colspan="1">1.36</td><td align="left" rowspan="1" colspan="1">2.8</td></tr><tr><td align="left" rowspan="1" colspan="1">Communication</td><td align="left" rowspan="1" colspan="1">1.29</td><td align="left" rowspan="1" colspan="1">&#x02212;1.4</td></tr><tr><td align="left" rowspan="1" colspan="1">Imagination</td><td align="left" rowspan="1" colspan="1">1.29</td><td align="left" rowspan="1" colspan="1">0.6</td></tr><tr><td align="left" rowspan="1" colspan="1">Subject gender</td><td align="left" rowspan="1" colspan="1">0.94</td><td align="left" rowspan="1" colspan="1">&#x02212;0.3</td></tr><tr><td align="left" rowspan="1" colspan="1">Extraversion</td><td align="left" rowspan="1" colspan="1">0.24</td><td align="left" rowspan="1" colspan="1">&#x02212;3.5</td></tr><tr><td align="left" rowspan="1" colspan="1">Attention to detail</td><td align="left" rowspan="1" colspan="1">0.07</td><td align="left" rowspan="1" colspan="1">&#x02212;3.6</td></tr><tr><td align="left" rowspan="1" colspan="1">Social skills</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">&#x02212;3.6</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt105"><label/><p>Absolute value of the fixed effect coefficient (<inline-formula><inline-graphic xlink:href="pone.0074746.e060.jpg"/></inline-formula>) for each predictor, and percent change in <inline-formula><inline-graphic xlink:href="pone.0074746.e061.jpg"/></inline-formula> when it is dropped from the model.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s4c2"><title>Comparison to one-step model</title><p>Note that instead of the two-step modeling procedure used here, it is also possible to address both questions of interest (how much overall VOT shift occurs, and which subject-level variables affect the amount of shift) using a single more complex model, with terms <italic>both</italic> corresponding to the amount of VOT shift (as in Model 2), and controls for other factors which affect VOT (as in Model 1). In such a `one-step&#x02019; model, the main effect for <sc>block</sc> would correspond to an overall shift in VOT (the intercept in Model 2), and interactions of <sc>block</sc> with subject-level predictors would correspond to factors which affect the amount of VOT shift (the main effects in Model 2). A pilot version of this study <xref rid="pone.0074746-AbregoCollier1" ref-type="bibr">[76]</xref> used a one-step modeling procedure. We have used the two-step procedure here for ease of presentation, since it has allowed us to focus our discussion on the results of Model 2 in detail. However, we note that fitting a one-step model to the current dataset yields broadly similar results to Model 2, with respect to how much shift occurs and which factors affect the amount of shift. In the one-step model, as in Model 2, there is not a significant overall shift in VOT, and there are significant effects of <sc>outcome</sc>, <sc>o</sc>, <sc>attitude</sc>, and as on the amount of shift, all in the same directions as in Model 2. However, in the one-step model two additional subject-level predictors significantly affect the amount of shift: agreeableness and narrator sexual orientation, both predictors which were near at least marginal significance (<inline-formula><inline-graphic xlink:href="pone.0074746.e062.jpg"/></inline-formula>) in Model 2. One possible explanation for this discrepancy is that the two-step model has less statistical power than the one-step model to detect factors affecting the amount of VOT shift, since it is based on less data: one observation per subject/word pair instead of two, and more points discarded as a result of trimming outliers. In any event, while further investigation is needed to ascertain the significance of these effects, the potential significance of agreeableness and narrator sexual orientation suggest that individuals who are less agreeable and those who participated in the gay narrator condition tend to show a positive VOT shift.</p></sec></sec></sec><sec id="s5"><title>General Discussion</title><p>Our findings show that phonetic imitation is highly variable, both in terms of contexts and across individuals. Before discussing the implications of such findings, however, it is worth noting that the significance of these findings is partly contingent on understanding that any observed VOT shift between production blocks <italic>is</italic> in fact due to exposure to the narrative with extended VOT. To be sure, since the design of this study did not include a separate condition where the VOTs were unchanged in the narrative, one might question whether the observed VOT shifts were imitations at all. That is, would the VOT shifts take place even if the model talker&#x02019;s VOTs had not been extended? For example, subjects with higher o scores (openness to new experiences) might have increased their VOTs from the first to the second reading of the word list more than subjects with lower o scores, regardless of whether there were an intervening narrative. While we acknowledge the possibility of such an interpretation, we believe that this alternative interpration is difficult to reconcile with the general findings of this study. To begin with, we found no <italic>unmediated</italic> VOT shifts overall, suggesting that the participants did not change their VOTs between blocks in general. To the extend that VOT shifts are observed, how much a subject&#x02019;s VOT shifted is predicted in part by subject-level predictors, some of which crucially referenced the presence of the narrative. For example, the amount of VOT shift is determined, if only partially, by subjects&#x02019; attitude toward the narrator. These results point to a significant awareness on the part of the participants of the content of the narrative. The direction of VOT shifts is also nonrandom; subjects with a positive attitude towards the narrator increase VOT more than subjects with a negative attitude towards the narrator. If we did not assume subjects&#x02019; VOTs shifted as a result of exposure to the narrative, we would be forced to conclude that the strong effect of attitude on the amount of VOT shift (subjects who like the narrator more shift towards him more) is actually spurious: subjects who liked the narrator more tended to be those who increase VOT more, purely by chance. Given these reasons, we assume for the remainder of this discussion that between-subject differences in the amount of VOT shift reflect between-subject differences in the effect of the narrative on their speech, in line with previous studies of phonetic imitation, which mostly have not used a control condition. However, future work using a control condition should test the assumption implicit in much of the phonetic imitation literature, that effects of covariates on how subjects imitate a speech stimulus are in fact due to exposure to the stimulus.</p><p>With the above caveat in place, our findings point to the fact that phonetic imitation, defined here as shifts in VOT between production blocks, may be modulated by disincentives and obstacles that conflict with goals, attention, and liking <xref rid="pone.0074746-Dijksterhuis1" ref-type="bibr">[1]</xref>. To begin with, in line with previous literature on phonetic convergence and imitation, which showed significant social modulation of imitation effects (e.g., conversational role <xref rid="pone.0074746-Pardo1" ref-type="bibr">[14]</xref>, <xref rid="pone.0074746-Pardo2" ref-type="bibr">[16]</xref>, national identity <xref rid="pone.0074746-Babel3" ref-type="bibr">[27]</xref>, race/ethnicity <xref rid="pone.0074746-Babel2" ref-type="bibr">[7]</xref>), the present study also found that the extent of phonetic imitation crucially depends on what impression the listener has toward the person who produces the phonetic variant. In particular, subjects who liked the narrator more imitated his extended VOT more. Relative to other predictors considered in Model 2, this attitudinal measure is also one of a small number of predictors that significantly influence the amount of phonetic imitation, highlighting the prominent role &#x0201c;liking&#x0201d; has in mediating the perception-production linkage.</p><p>Our study also found that the extent of phonetic imitation depends on where the phonetic variant is embedded (i.e. the content of the narrative), even if the effect of narrative outcome on imitation might seem surprising at first glance. Recall that there were two possible outcomes to the blind date as recounted by the narrator during the <italic>listening</italic> phase of the experiment. In the positive scenario, the narrator and his date went on well, while in the negative scenario, the narrator behaved rudely by leaving the blind date in a lurch. Perhaps counterintuitively, subjects who heard the negative narrative show an increase in VOT in the post-exposure block. To be sure, there is no correlation between subject&#x02019;s attitude toward the narrator and the narrative outcome (i.e., <inline-formula><inline-graphic xlink:href="pone.0074746.e063.jpg"/></inline-formula>0.29 for a Wilcoxson rank-sum test of the hypothesis that attitude differs between the positive and negative narrative conditions). That is, it is not the case that subjects tended to react positively toward the narrator under the negative scenario. Why should subjects imitate more when they heard the negative story compared to those who heard the positive one? One, admittedly speculative, possibility is that subjects who heard the negative story paid more attention to the narrative than those who heard the positive one on account of the fact that the negative story is more engaging than the positive account. This interpretation may be related to the notion of <italic>automatic vigilance</italic>, which has been argued to be a mechanism that serves to direct attentional capacity to undesirable stimuli without the perceiver&#x02019;s intention or control <xref rid="pone.0074746-Pratto1" ref-type="bibr">[77]</xref>. If this is the case, it would suggest that subjects&#x02019; attention and focus is driving this narrative outcome effect.</p><p>We found that the dynamics of VOT imitation were modulated by speaker attitude and narrative outcome, but not speaker gender or perceived sexual orientation of the narrator. The former can be thought of as variables which are constructed situationally, and the latter as &#x0201c;macro&#x0201d; social variables describing pre-existing categories. To the extent that our experiment is representative of phonetic imitation behavior more generally, this asymmetry highlights the importance of taking into account variables which are defined relative to a particular social situation when studying phonetic imitation, in line with other recent work in sociophonetics (e.g., <xref rid="pone.0074746-Pardo2" ref-type="bibr">[16]</xref>, <xref rid="pone.0074746-Hay1" ref-type="bibr">[78]</xref>, <xref rid="pone.0074746-Podesva1" ref-type="bibr">[79]</xref>).</p><p>Beyond these situation-specific social factors, our findings also show that certain aspects of the social and cognitive makeup of the subject strongly influence the extent of phonetic imitation. In particular, individuals whose personality reflects a greater sense of openness and those with strong attention focus, as measured by the <sc>attention</sc>
<sc>switching</sc> subscore of the AQ, tend to approximate the narrator&#x02019;s VOT more than those with the opposite personality and autistic-like traits. The personality facet of O<sc>penness</sc> (also known as O<sc>penness</sc>/Intellect) indexes an individual&#x02019;s level of engagement with perceptual, sensory, as well as abstract and semantic information. The fact that O<sc>penness</sc> modulates phonetic imitation suggests that the level of engagement with the exposure materials matters. That is, a higher level of engagement with the narrator&#x02019;s speech may have led to greater attention paid to how the utterances are produced by the narrator. Similarly, individuals who are more focused and are not accustomed to constant attention switching, as indexed by a high <sc>attention</sc>
<sc>switching</sc> subscore of the AQ, might likewise be more attuned to the fine-grained phonetic fluctuations in the exposure materials, thus increasing the chance of such phonetic attributes being imitated. To be sure, further investigation is required to understand the way selective attention and attentional-resource allocation may influence phonetic imitation.</p><p>While certain individual-difference dimensions may influence phonetic imitation, it is also important to point out that not all individual-difference dimensions have such an influence, as shown by the lack of an effect of most of the other AQ subscores and personality traits on the amount of VOT shift. Our finding that the extent of phonetic imitation is in part governed by a personality facet that indexes willingness to engage with new information, and a cognitive processing style that favors focused attention and eschews spreading attentional resources thinly, points to the potential role of attention in mediating the perception-production link. To that end, it came as a surprise that working memory capacity, as measured by RSPAN, did not emerge as a significant predictor in phonetic imitation, given that selective attention is highly influenced by working memory resources. This result suggests that success in phonetic imitation might not be related to the availability of attentional resources <italic>per se</italic>, but more related to the monitoring and allocation of attentional resources; both are within the purview of the executive-function process. Further research is needed to elucidate the role of executive-function ability in phonetic imitation.</p><p>The present findings have implications for models of speech perception and production, particularly for exemplar-based models of speech production and perception, which assume some form of perception-production feedback loop where exposure to a production activates similar stored exemplars, leading to a subsequent production by the listener-turned-speaker that is more like that of the model talker. Such models are able to account for imitation results <xref rid="pone.0074746-Goldinger2" ref-type="bibr">[80]</xref>&#x02013;<xref rid="pone.0074746-Kirby1" ref-type="bibr">[82]</xref>, although the inclusion of an attention-weighting component in the model (see, for example, <xref rid="pone.0074746-Johnson1" ref-type="bibr">[81]</xref>) is needed to account for attention-related inter-individual variation. Further modification is also needed to take into account situationally-based social information in mediating the perception-production link. As noted in <xref rid="pone.0074746-Babel2" ref-type="bibr">[7]</xref>, simple automatic exemplar-models that predict cumulative imitation effects as a result of increased activation from increased exposure are not tenable in light of findings like those reported here where exposure to the model talker does not necessarily lead to significant overall imitation.</p><p>Given that our experiment focused on VOT as the imitation target and employed a very similar methodology to the experiment reported in Nielsen&#x02019;s study <xref rid="pone.0074746-Nielsen3" ref-type="bibr">[10]</xref>, the fact that we did not observe an overall effect of phonetic imitation but she did deserves some qualification. To begin with, the exposure materials in Nielsen&#x02019;s study were English words presented in isolation, while our exposure materials were embedded in a meaningful narrative. The marked difference in experimental results might be partly attributable to the decontextualization of the exposure materials in Nielsen&#x02019;s study; imitation might be more automatic (i.e., they can occur without the speaker&#x02019;s intention or control) in a context where the words are presented in isolation devoid of social significance. The narrative in the present study, in contrast, allows participants to make evaluative judgements on the narrator as he recounts his blind date. The difference in the presence of baseline imitation could also be related to the substantially different statistical analyses used in Nielsen&#x02019;s study and in the current study. Differences in the model talkers in our experiment and Nielsen&#x02019;s also might have contributed to the differences in results, in light of previous work showing that shadowers are more likely to accommodate to particular individuals than to others <xref rid="pone.0074746-Namy1" ref-type="bibr">[20]</xref>.</p><p>Another possibility not directly explored here concerns the subject&#x02019;s interpretation of the social meaning of the extended VOT. Recent studies on the social meaning of released/t/argue that/t/release is associated with qualities such as being educated, elegance, articulateness, and prissiness <xref rid="pone.0074746-Eckert1" ref-type="bibr">[83]</xref>. This feature has apparently been recruited by particular social groups in their construction of an articulate persona (e.g., nerd girls <xref rid="pone.0074746-Bucholtz1" ref-type="bibr">[84]</xref>, Orthodox Jewish boys <xref rid="pone.0074746-Benor1" ref-type="bibr">[85]</xref>, gay divas <xref rid="pone.0074746-Podesva1" ref-type="bibr">[79]</xref>, United States politicians <xref rid="pone.0074746-Podesva2" ref-type="bibr">[86]</xref>). The association of released consonants with articulateness is partly confirmed in the results of the post-experiment survey in our study. When asked whether they noticed anything unusual about the narrator&#x02019;s speech, many subjects characterized his way of speaking as &#x0201c;articulate&#x0201d;, &#x0201c;aspirated&#x0201d;, or &#x0201c;robotic&#x0201d;. Given the complex social meaning associated with the released/t/variable, we cannot discount the possibility that the social meaning of an especially strong consonantal release (lengthened VOT) might have an effect on phonetic convergence or divergence. While the indexical meanings associated with released/t/are not intrinsically positive or negative, some subjects might nonetheless resist extending their VOTs in order to avoid projecting an articulate persona.</p></sec><sec id="s6"><title>Conclusion</title><p>This study offers further evidence that the extent of phonetic imitation is highly regulated by individual-level variables, such as an individual&#x02019;s evaluative judgement of her interlocutor, and the social and cognitive profile of the individual. In particular, such cross-individual variability strongly affects the likelihood and directionality of phonetic imitation.</p></sec><sec sec-type="supplementary-material" id="s7"><title>Supporting Information</title><supplementary-material content-type="local-data" id="pone.0074746.s001"><label>Supporting Information S1</label><caption><p>
<bold>The full texts of the heterosexual version of the &#x0201c;positive&#x0201d; and &#x0201c;negative&#x0201d; storylines.</bold>
</p><p>(PDF)</p></caption><media xlink:href="pone.0074746.s001.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>We thank James Kirby, the editor, and the two reviewers (Molly Babel and an anonymous reviewer) for valuable comments. Thanks also go to audiences at the International Congress of Phonetic Sciences in Hong Kong in 2011 and the 2012 annual meeting of the Linguistics Society of America.</p></ack><ref-list><title>References</title><ref id="pone.0074746-Dijksterhuis1"><label>1</label><mixed-citation publication-type="other">Dijksterhuis A, Bargh JA (2001) The perception-behavior expressway: Automatic effects of social perception on social behavior. In: Zanna MP, editor, Advances in experimental social psychology. Vol. 33, San Diego: Academic Press. 1&#x02013;40.</mixed-citation></ref><ref id="pone.0074746-Pickering1"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Pickering</surname><given-names>M</given-names></name>, <name><surname>Garrod</surname><given-names>S</given-names></name> (<year>2004</year>) <article-title>Toward a mechanistic psychology of dialogue</article-title>. <source>Behavioral and Brain Sciences</source>
<volume>27</volume>: <fpage>169</fpage>&#x02013;<lpage>225</lpage>.<pub-id pub-id-type="pmid">15595235</pub-id></mixed-citation></ref><ref id="pone.0074746-Webb1"><label>3</label><mixed-citation publication-type="other">Webb JT (1970) Interview synchrony: An investigation of two speech rate measures in an auto mated standardized interview. In: Siegman AW, Pope B, editors, Studies in dyadic communication: Proceedings of a research conference on the interview, New York: Pergamon. 115&#x02013;133.</mixed-citation></ref><ref id="pone.0074746-Jaffe1"><label>4</label><mixed-citation publication-type="other">Jaffe J, Feldstein S (1970) Rhythms of dialogue. New York: Academic Press.</mixed-citation></ref><ref id="pone.0074746-Natale1"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Natale</surname><given-names>M</given-names></name> (<year>1975</year>) <article-title>Convergence of mean vocal intensity in dyadic communication as a function of social desirability</article-title>. <source>Journal of Personality and Social Psychology</source>
<volume>32</volume>: <fpage>790</fpage>&#x02013;<lpage>804</lpage>.</mixed-citation></ref><ref id="pone.0074746-Babel1"><label>6</label><mixed-citation publication-type="other">Babel ME (2009) Phonetic and social selectivity in speech accomodation. Ph.D. thesis, University of California, Berkeley.</mixed-citation></ref><ref id="pone.0074746-Babel2"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Babel</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Evidence for phonetic and social selectivity in spontaneous phonetic imitation</article-title>. <source>Journal of Phonetics</source>
<volume>40</volume>: <fpage>177</fpage>&#x02013;<lpage>189</lpage>.</mixed-citation></ref><ref id="pone.0074746-Nielsen1"><label>8</label><mixed-citation publication-type="other">Nielsen KY (2007) Implicit phonetic imitation is constrained by phonemic contrast. In: Proceedings of the 16th International Congress of the Phonetic Sciences, Saarbrcken, Germany, Saarbrcken, Germany: The International Congress of the Phonetic Sciences. 1961&#x02013;1964.</mixed-citation></ref><ref id="pone.0074746-Nielsen2"><label>9</label><mixed-citation publication-type="other">Nielsen K (2008) Word-level and feature-level effects in phonetic imitation. Ph.D. thesis, UCLA.</mixed-citation></ref><ref id="pone.0074746-Nielsen3"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Nielsen</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>Specificity and abstractness of VOT imitation</article-title>. <source>Journal of Phonetics</source>
<volume>39</volume>: <fpage>132</fpage>&#x02013;<lpage>142</lpage>.</mixed-citation></ref><ref id="pone.0074746-Goldinger1"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Goldinger</surname><given-names>S</given-names></name> (<year>1998</year>) <article-title>Echoes of echoes? an episodic theory of lexical access</article-title>. <source>Psychological Review</source>
<volume>105</volume>: <fpage>251</fpage>&#x02013;<lpage>279</lpage>.<pub-id pub-id-type="pmid">9577239</pub-id></mixed-citation></ref><ref id="pone.0074746-Mitterer1"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Mitterer</surname><given-names>H</given-names></name>, <name><surname>Ernestus</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>The link between speech perception and production is phonological and abstract: Evidence from a shadowing task</article-title>. <source>Cognition</source>
<volume>109</volume>: <fpage>168</fpage>&#x02013;<lpage>173</lpage>.<pub-id pub-id-type="pmid">18805522</pub-id></mixed-citation></ref><ref id="pone.0074746-Shockley1"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Shockley</surname><given-names>K</given-names></name>, <name><surname>Sabadini</surname><given-names>L</given-names></name>, <name><surname>Fowler</surname><given-names>CA</given-names></name> (<year>2004</year>) <article-title>Imitation in shadowing words</article-title>. <source>Perception &#x00026; Psychophysics</source>
<volume>66</volume>: <fpage>422</fpage>&#x02013;<lpage>429</lpage>.<pub-id pub-id-type="pmid">15283067</pub-id></mixed-citation></ref><ref id="pone.0074746-Pardo1"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Pardo</surname><given-names>JS</given-names></name> (<year>2006</year>) <article-title>On phonetic convergence during conversational interaction</article-title>. <source>Journal of the Acoustical Society of America</source>
<volume>119</volume>: <fpage>2382</fpage>&#x02013;<lpage>2393</lpage>.<pub-id pub-id-type="pmid">16642851</pub-id></mixed-citation></ref><ref id="pone.0074746-Delvaux1"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Delvaux</surname><given-names>V</given-names></name>, <name><surname>Soquet</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>The inuence of ambient speech on adult speech productions through unintentional imitation</article-title>. <source>Phonetica</source>
<volume>64</volume>: <fpage>145</fpage>&#x02013;<lpage>173</lpage>.<pub-id pub-id-type="pmid">17914281</pub-id></mixed-citation></ref><ref id="pone.0074746-Pardo2"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Pardo</surname><given-names>JS</given-names></name>, <name><surname>Jay</surname><given-names>IC</given-names></name>, <name><surname>Krauss</surname><given-names>RM</given-names></name> (<year>2010</year>) <article-title>Converstaional role inuences speech imitation</article-title>. <source>Attention, Perception, &#x00026; Psychophysics</source>
<volume>72</volume>: <fpage>2254</fpage>&#x02013;<lpage>2264</lpage>.</mixed-citation></ref><ref id="pone.0074746-Flemming1"><label>17</label><mixed-citation publication-type="other">Flemming E (2001) Auditory representations in phonology. New York: Garland Press.</mixed-citation></ref><ref id="pone.0074746-Flemming2"><label>18</label><mixed-citation publication-type="other">Flemming E (2004) Contrast and perceptual distinctiveness. In: Hayes B, Kirchner R, Steriade D, editors, Phonetically based phonology, Cambridge, UK: Cambridge University Press. 232&#x02013;276.</mixed-citation></ref><ref id="pone.0074746-Lubowicz1"><label>19</label><mixed-citation publication-type="other">Lubowicz A (2003) Contrast preservation in phonological mappings. Ph.D. thesis, University of Massachusetts, Amherst.</mixed-citation></ref><ref id="pone.0074746-Namy1"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Namy</surname><given-names>LL</given-names></name>, <name><surname>Nygaard</surname><given-names>LC</given-names></name>, <name><surname>Sauerteig</surname><given-names>D</given-names></name> (<year>2002</year>) <article-title>Gender differences in vocal accommodation: The role of perception</article-title>. <source>Journal of Language and Social Psychology</source>
<volume>21</volume>: <fpage>422</fpage>&#x02013;<lpage>432</lpage>.</mixed-citation></ref><ref id="pone.0074746-Kim1"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Kim</surname><given-names>M</given-names></name>, <name><surname>Horton</surname><given-names>WS</given-names></name>, <name><surname>Bradlow</surname><given-names>AR</given-names></name> (<year>2011</year>) <article-title>Phonetic convergence in spontaneous conversation as a function of interlocutor language distance</article-title>. <source>Journal of Laboratory Phonology</source>
<volume>2</volume>: <fpage>125</fpage>&#x02013;<lpage>156</lpage>.<pub-id pub-id-type="pmid">23637712</pub-id></mixed-citation></ref><ref id="pone.0074746-Giles1"><label>22</label><mixed-citation publication-type="other">Giles H, Coupland N (1991) Language: Contexts and consequences. Milton Keynes: Open University Press.</mixed-citation></ref><ref id="pone.0074746-Giles2"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Giles</surname><given-names>H</given-names></name> (<year>1973</year>) <article-title>Accent mobility: A model and some data</article-title>. <source>Anthropological Linguistics</source>
<volume>15</volume>: <fpage>87</fpage>&#x02013;<lpage>105</lpage>.</mixed-citation></ref><ref id="pone.0074746-Giles3"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Giles</surname><given-names>H</given-names></name>, <name><surname>Taylor</surname><given-names>D</given-names></name>, <name><surname>Bourhis</surname><given-names>R</given-names></name> (<year>1973</year>) <article-title>Towards a theory of interpersonal accommodation through language: Some Canadian data</article-title>. <source>Language in Society</source>
<volume>2</volume>: <fpage>177</fpage>&#x02013;<lpage>192</lpage>.</mixed-citation></ref><ref id="pone.0074746-Bourhis1"><label>25</label><mixed-citation publication-type="other">Bourhis RY, Giles H (1977) The language of intergroup distinctiveness. In: Giles H, editor, Language, ethnicity, and intergroup relations, London: Academic Press. 119&#x02013;136.</mixed-citation></ref><ref id="pone.0074746-Labov1"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Labov</surname><given-names>W</given-names></name> (<year>1963</year>) <article-title>The social motivation of a sound change</article-title>. <source>Word</source>
<volume>19</volume>: <fpage>273</fpage>&#x02013;<lpage>309</lpage>.</mixed-citation></ref><ref id="pone.0074746-Babel3"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Babel</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Dialect convergence and divergence in New Zealand English</article-title>. <source>Language in Society</source>
<volume>39</volume>: <fpage>437</fpage>&#x02013;<lpage>456</lpage>.</mixed-citation></ref><ref id="pone.0074746-Lewandowski1"><label>28</label><mixed-citation publication-type="other">Lewandowski N, Jilka M, Rota G, Reiterer S, Dogil G (2007) Phonetic convergence as a paradigm of showing phonetic talent in foreign language acquisition. In: Zimmer HD, editor, Cognitive Science 2007: Proceedings of the 8th Annual Conference of the Cognitive Science Society of Germany, Saarbr&#x000fc;cken.</mixed-citation></ref><ref id="pone.0074746-Cheshire1"><label>29</label><mixed-citation publication-type="journal">
<name><surname>Cheshire</surname><given-names>J</given-names></name>, <name><surname>Fox</surname><given-names>S</given-names></name>, <name><surname>Kerswill</surname><given-names>P</given-names></name>, <name><surname>Torgersen</surname><given-names>E</given-names></name> (<year>2008</year>) <article-title>Ethnicity, friendship network and social practices as the motor of dialect change: linguistic innovation in London</article-title>. <source>In: Sociolinguistica : International Yearbook of European Sociolinguistics, Max Niemeyer Verlag, volume</source>
<volume>22</volume>: <fpage>1</fpage>&#x02013;<lpage>23</lpage>.</mixed-citation></ref><ref id="pone.0074746-StuartSmith1"><label>30</label><mixed-citation publication-type="other">Stuart-Smith J, Timmins C (2009) The role of the individual in language variation and change. In: Llamas C, Watt D, editors, Language and Identities., Edinburgh, UK: Edinburgh University Press. 39&#x02013;54.</mixed-citation></ref><ref id="pone.0074746-Yu1"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Yu</surname><given-names>ACL</given-names></name> (<year>2010</year>) <article-title>Perceptual compensation is correlated with individuals&#x02019; &#x0201c;autistic&#x0201d; traits: Implica1tions for models of sound change</article-title>. <source>PLoS One</source>
<volume>5</volume>: <fpage>e11950</fpage>.<pub-id pub-id-type="pmid">20808859</pub-id></mixed-citation></ref><ref id="pone.0074746-Yu2"><label>32</label><mixed-citation publication-type="other">Yu ACL (2013) Individual differences in socio-cognitive processing and the actuation of sound change. In: Yu ACL, editor, Origins of Sound Change: Approaches to Phonologization, Oxford, UK: Oxford University Press. 201&#x02013;227.</mixed-citation></ref><ref id="pone.0074746-Conway1"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Conway</surname><given-names>AR</given-names></name>, <name><surname>Engle</surname><given-names>R</given-names></name> (<year>1994</year>) <article-title>Working memory and retrieval: A resource-dependent inhibition model</article-title>. <source>Journal of Experimental Psychology: General</source>
<volume>123</volume>: <fpage>154</fpage>&#x02013;<lpage>173</lpage>.</mixed-citation></ref><ref id="pone.0074746-Kane1"><label>34</label><mixed-citation publication-type="journal">
<name><surname>Kane</surname><given-names>MJ</given-names></name>, <name><surname>Bleckley</surname><given-names>KM</given-names></name>, <name><surname>Conway</surname><given-names>ARA</given-names></name>, <name><surname>Engle</surname><given-names>RW</given-names></name> (<year>2001</year>) <article-title>A controlled-attention view of working memory capacity</article-title>. <source>Journal of Experimental Psychology: General</source>
<volume>130</volume>: <fpage>169</fpage>&#x02013;<lpage>183</lpage>.<pub-id pub-id-type="pmid">11409097</pub-id></mixed-citation></ref><ref id="pone.0074746-Francis1"><label>35</label><mixed-citation publication-type="journal">
<name><surname>Francis</surname><given-names>AL</given-names></name>, <name><surname>Nusbaum</surname><given-names>HC</given-names></name> (<year>2002</year>) <article-title>Selective attention and the acquisition of new phonetic categories</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>
<volume>28</volume>: <fpage>349</fpage>&#x02013;<lpage>366</lpage>.<pub-id pub-id-type="pmid">11999859</pub-id></mixed-citation></ref><ref id="pone.0074746-Francis2"><label>36</label><mixed-citation publication-type="journal">
<name><surname>Francis</surname><given-names>AL</given-names></name>, <name><surname>Nusbaum</surname><given-names>HC</given-names></name> (<year>2009</year>) <article-title>Effects of intelligibility on working memory demand for speech perception</article-title>. <source>Attention, Perception, &#x00026; Psychophysics</source>
<volume>71</volume>: <fpage>1360</fpage>&#x02013;<lpage>1374</lpage>.</mixed-citation></ref><ref id="pone.0074746-Yu3"><label>37</label><mixed-citation publication-type="other">Yu ACL, Grove J, Martinovic M, Sonderegger M (2011) Effects of working memory capacity and &#x0201c;autistic&#x0201d; traits on phonotactic effects in speech perception. In: Zee E, editor, Proceedings of the International Congress of the Phonetic Sciences XVII, Hong Kong: International Congress of the Phonetic Sciences. 2236&#x02013;2239.</mixed-citation></ref><ref id="pone.0074746-MacDonald1"><label>38</label><mixed-citation publication-type="journal">
<name><surname>MacDonald</surname><given-names>MC</given-names></name>, <name><surname>Christiansen</surname><given-names>MH</given-names></name> (<year>2002</year>) <article-title>Reassessing working memory: Comment on Just and Carpenter (1992) and Waters and Caplan (1996)</article-title>. <source>Psychological Review</source>
<volume>109</volume>: <fpage>35</fpage>&#x02013;<lpage>54</lpage>.<pub-id pub-id-type="pmid">11863041</pub-id></mixed-citation></ref><ref id="pone.0074746-Misyak1"><label>39</label><mixed-citation publication-type="journal">
<name><surname>Misyak</surname><given-names>JB</given-names></name>, <name><surname>Christiansen</surname><given-names>MH</given-names></name> (<year>2012</year>) <article-title>Statistical learning and language: An individual differences study</article-title>. <source>Language Learning</source>
<volume>62</volume>: <fpage>302</fpage>&#x02013;<lpage>331</lpage>.</mixed-citation></ref><ref id="pone.0074746-Ettlinger1"><label>40</label><mixed-citation publication-type="journal">
<name><surname>Ettlinger</surname><given-names>M</given-names></name>, <name><surname>Margulis</surname><given-names>EH</given-names></name>, <name><surname>Wong</surname><given-names>PCM</given-names></name> (<year>2011</year>) <article-title>Implicit memory in music and language</article-title>. <source>Frontiers in Cognitive Neuroscience</source>
<volume>2</volume>: <fpage>211</fpage>.</mixed-citation></ref><ref id="pone.0074746-Ausburn1"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Ausburn</surname><given-names>LJ</given-names></name>, <name><surname>Ausburn</surname><given-names>FB</given-names></name> (<year>1978</year>) <article-title>Cognitive styles: Some information and implications for instructional design</article-title>. <source>Educational Communication &#x00026; Technology</source>
<volume>26</volume>: <fpage>337</fpage>&#x02013;<lpage>354</lpage>.</mixed-citation></ref><ref id="pone.0074746-Messick1"><label>42</label><mixed-citation publication-type="other">Messick S (1976) Individuality in learning. Oxford, UK: Jossey-Bass.</mixed-citation></ref><ref id="pone.0074746-Witkin1"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Witkin</surname><given-names>HA</given-names></name>, <name><surname>Moore</surname><given-names>CA</given-names></name>, <name><surname>Goodenough</surname><given-names>DR</given-names></name>, <name><surname>Cox</surname><given-names>PW</given-names></name> (<year>1977</year>) <article-title>Field-dependent and field-independent cognitive styles and their educational implications</article-title>. <source>Review of Educational Research</source>
<volume>47</volume>: <fpage>1</fpage>&#x02013;<lpage>64</lpage>.</mixed-citation></ref><ref id="pone.0074746-BaronCohen1"><label>44</label><mixed-citation publication-type="journal">
<name><surname>Baron-Cohen</surname><given-names>S</given-names></name>, <name><surname>Wheelwright</surname><given-names>S</given-names></name>, <name><surname>Skinner</surname><given-names>R</given-names></name>, <name><surname>Martin</surname><given-names>J</given-names></name>, <name><surname>Clubley</surname><given-names>E</given-names></name> (<year>2001</year>) <article-title>The autism-spectrum quotient (AQ): Evidence from Asperger syndrome/high-functioning autism, males, females, scientists and mathematicians</article-title>. <source>Journal of Autism &#x00026; Developmental Disorders</source>
<volume>31</volume>: <fpage>5</fpage>&#x02013;<lpage>17</lpage>.<pub-id pub-id-type="pmid">11439754</pub-id></mixed-citation></ref><ref id="pone.0074746-Stewart1"><label>45</label><mixed-citation publication-type="journal">
<name><surname>Stewart</surname><given-names>ME</given-names></name>, <name><surname>Ota</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Lexical effects on speech perception in individuals with &#x0201c;autistic&#x0201d; traits</article-title>. <source>Cognition</source>
<volume>109</volume>: <fpage>157</fpage>&#x02013;<lpage>162</lpage>.<pub-id pub-id-type="pmid">18834977</pub-id></mixed-citation></ref><ref id="pone.0074746-Bonnel1"><label>46</label><mixed-citation publication-type="other">Bonnel A, Mottron L, Peretz I, Trudel M, Gallun E, <etal>et al</etal>.. (2003) Enhanced pitch sensitivity in individuals with autism: A signal detection analysis. Journal of Cognitive Neuroscience 15: 21 226235.</mixed-citation></ref><ref id="pone.0074746-Mottron1"><label>47</label><mixed-citation publication-type="journal">
<name><surname>Mottron</surname><given-names>L</given-names></name>, <name><surname>Dawson</surname><given-names>M</given-names></name>, <name><surname>Soulifieres</surname><given-names>I</given-names></name>, <name><surname>Hubert</surname><given-names>B</given-names></name>, <name><surname>Burack</surname><given-names>J</given-names></name> (<year>2006</year>) <article-title>Enhanced perceptual functioning in autism: An update, and eight principles of autistic perception</article-title>. <source>Journal of Autism &#x00026; Developmental Disorders</source>
<volume>36</volume>: <fpage>27</fpage>&#x02013;<lpage>43</lpage>.<pub-id pub-id-type="pmid">16453071</pub-id></mixed-citation></ref><ref id="pone.0074746-Happ1"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Happ&#x000e8;</surname><given-names>F</given-names></name>, <name><surname>Briskman</surname><given-names>J</given-names></name>, <name><surname>Frith</surname><given-names>U</given-names></name> (<year>2001</year>) <article-title>Exploring the cognitive phenotype of autism: Weak &#x0201c;central coherence&#x0201d; in parents and siblings of children with autism: I. experimental tests</article-title>. <source>Journal of Child Psychology &#x00026; Psychiatry &#x00026; Allied Disciplines</source>
<volume>42</volume>: <fpage>299</fpage>&#x02013;<lpage>307</lpage>.</mixed-citation></ref><ref id="pone.0074746-Happ2"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Happ&#x000e8;</surname><given-names>F</given-names></name>, <name><surname>Frith</surname><given-names>U</given-names></name> (<year>2006</year>) <article-title>The weak coherence account: Detail-focused cognitive style in autism spectrum disorders</article-title>. <source>Journal of Autism &#x00026; Developmental Disorders</source>
<volume>36</volume>: <fpage>5</fpage>&#x02013;<lpage>25</lpage>.<pub-id pub-id-type="pmid">16450045</pub-id></mixed-citation></ref><ref id="pone.0074746-BaronCohen2"><label>50</label><mixed-citation publication-type="journal">
<name><surname>Baron-Cohen</surname><given-names>S</given-names></name>, <name><surname>Wheelwright</surname><given-names>S</given-names></name> (<year>2004</year>) <article-title>The Empathy Quotient: An investigation of adults with Asperger Syndrome or High Functioning Autism and normal sex differences</article-title>. <source>Journal of Autism and Developmental Disorders</source>
<volume>34</volume>: <fpage>163</fpage>&#x02013;<lpage>175</lpage>.<pub-id pub-id-type="pmid">15162935</pub-id></mixed-citation></ref><ref id="pone.0074746-Wheelwright1"><label>51</label><mixed-citation publication-type="journal">
<name><surname>Wheelwright</surname><given-names>S</given-names></name>, <name><surname>Baron-Cohen</surname><given-names>S</given-names></name>, <name><surname>Goldenfeld</surname><given-names>N</given-names></name>, <name><surname>Delaney</surname><given-names>J</given-names></name>, <name><surname>Fine</surname><given-names>D</given-names></name>, <etal>et al</etal> (<year>2006</year>) <article-title>Predicting autism spectrum quotient (AQ) from the systemizing quotient-revised (SQ-R) and empathy quotient (EQ)</article-title>. <source>Brain Research</source>
<volume>1079</volume>: <fpage>47</fpage>&#x02013;<lpage>56</lpage>.<pub-id pub-id-type="pmid">16473340</pub-id></mixed-citation></ref><ref id="pone.0074746-Austin1"><label>52</label><mixed-citation publication-type="journal">
<name><surname>Austin</surname><given-names>EJ</given-names></name> (<year>2005</year>) <article-title>Personality correlates of the broader autism phenotype as assessed by the Autism Spectrum Quotient (AQ)</article-title>. <source>Personality and Individual Differences</source>
<volume>38</volume>: <fpage>451</fpage>&#x02013;<lpage>460</lpage>.</mixed-citation></ref><ref id="pone.0074746-Nettle1"><label>53</label><mixed-citation publication-type="journal">
<name><surname>Nettle</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>Empathizing and systemizing: What are they, and what do they contribute to our understanding of psychological sex differences?</article-title>
<source>British Journal of Psychology</source>
<volume>98</volume>: <fpage>237</fpage>&#x02013;<lpage>255</lpage>.<pub-id pub-id-type="pmid">17456271</pub-id></mixed-citation></ref><ref id="pone.0074746-Baayen1"><label>54</label><mixed-citation publication-type="other">Baayen R, Piepenbrock R, Gulikers L (1996) CELEX2 (CD-ROM). Philadelphia: Linguistic Data Consortium.</mixed-citation></ref><ref id="pone.0074746-Boersma1"><label>55</label><mixed-citation publication-type="other">Boersma P, Weenink D (2007) Praat. Software Package.</mixed-citation></ref><ref id="pone.0074746-John1"><label>56</label><mixed-citation publication-type="other">John OP, Donahue EM, Kentle RL (1991) The Big Five Inventory&#x02013;Versions 4a and 54. Berkeley, CA: Institute of Personality and Social Research, University of California, Berkeley.</mixed-citation></ref><ref id="pone.0074746-John2"><label>57</label><mixed-citation publication-type="other">John OP, Naumann LP, Soto CJ (2008) Paradigm shift to the integrative Big-Five trait taxonomy: History, measurement, and conceptual issues. In: John OP, Robins RW, Pervin LA, editors, Handbook of personality: Theory and research, New York, NY: Guilford Press. 114&#x02013;158.</mixed-citation></ref><ref id="pone.0074746-Wakabayashi1"><label>58</label><mixed-citation publication-type="journal">
<name><surname>Wakabayashi</surname><given-names>A</given-names></name>, <name><surname>Baron-Cohen</surname><given-names>S</given-names></name>, <name><surname>Wheelwright</surname><given-names>S</given-names></name> (<year>2006</year>) <article-title>Are autistic traits an independent personality dimension? A study of the Autism-Spectrum Quotient (AQ) and the NEO-PI-R</article-title>. <source>Personality and Individual Differences</source>
<volume>41</volume>: <fpage>873</fpage>&#x02013;<lpage>883</lpage>.</mixed-citation></ref><ref id="pone.0074746-Unsworth1"><label>59</label><mixed-citation publication-type="journal">
<name><surname>Unsworth</surname><given-names>N</given-names></name>, <name><surname>Heitz</surname><given-names>RP</given-names></name>, <name><surname>Schrock</surname><given-names>JC</given-names></name>, <name><surname>Engle</surname><given-names>RW</given-names></name> (<year>2005</year>) <article-title>An automated version of the operation spantask</article-title>. <source>Behavior Research Methods</source>
<volume>37</volume>: <fpage>498</fpage>&#x02013;<lpage>505</lpage>.<pub-id pub-id-type="pmid">16405146</pub-id></mixed-citation></ref><ref id="pone.0074746-Conway2"><label>60</label><mixed-citation publication-type="journal">
<name><surname>Conway</surname><given-names>ARA</given-names></name>, <name><surname>Kane</surname><given-names>M</given-names></name>, <name><surname>Bunting</surname><given-names>M</given-names></name>, <name><surname>Hambrick</surname><given-names>ZZ</given-names></name>, <name><surname>Wilhelm</surname><given-names>O</given-names></name>, <etal>et al</etal> (<year>2005</year>) <article-title>Working memory span tasks: A methodological review and user&#x02019;s guide</article-title>. <source>Psychonomic Bulletin and Review</source>
<volume>12</volume>: <fpage>769</fpage>&#x02013;<lpage>786</lpage>.<pub-id pub-id-type="pmid">16523997</pub-id></mixed-citation></ref><ref id="pone.0074746-Allen1"><label>61</label><mixed-citation publication-type="journal">
<name><surname>Allen</surname><given-names>J</given-names></name>, <name><surname>Miller</surname><given-names>J</given-names></name>, <name><surname>DeSteno</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>Individual talker differences in voice-onset-time</article-title>. <source>Journal of the Acoustic Society of America</source>
<volume>113</volume>: <fpage>544</fpage>&#x02013;<lpage>552</lpage>.</mixed-citation></ref><ref id="pone.0074746-AbdelliBeruh1"><label>62</label><mixed-citation publication-type="other">Abdelli-Beruh NB (2004) The stop voicing contrast in French sentences: Contextual sensitivity of vowel duration, closure duration, voice onset time, stop release and closure voicing. Phonetica 61: 7 201&#x02013;219.</mixed-citation></ref><ref id="pone.0074746-Theodore1"><label>63</label><mixed-citation publication-type="journal">
<name><surname>Theodore</surname><given-names>R</given-names></name>, <name><surname>Miller</surname><given-names>J</given-names></name>, <name><surname>DeSteno</surname><given-names>D</given-names></name> (<year>2009</year>) <article-title>Individual talker differences in voice-onset-time: Contextual inuences</article-title>. <source>Journal of the Acoustical Society of America</source>
<volume>125</volume>: <fpage>3974</fpage>&#x02013;<lpage>3982</lpage>.<pub-id pub-id-type="pmid">19507979</pub-id></mixed-citation></ref><ref id="pone.0074746-Fischer1"><label>64</label><mixed-citation publication-type="journal">
<name><surname>Fischer</surname><given-names>E</given-names></name>, <name><surname>Goberman</surname><given-names>AM</given-names></name> (<year>2010</year>) <article-title>Voice onset time in Parkinson disease</article-title>. <source>Journal of Communication Disorders</source>
<volume>43</volume>: <fpage>21</fpage>&#x02013;<lpage>34</lpage>.<pub-id pub-id-type="pmid">19717164</pub-id></mixed-citation></ref><ref id="pone.0074746-Yuan1"><label>65</label><mixed-citation publication-type="journal">
<name><surname>Yuan</surname><given-names>J</given-names></name>, <name><surname>Liberman</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Speaker identification on the SCOTUS corpus</article-title>. <source>In: Proceedings of Acoustics &#x02018;</source>
<volume>08</volume>: <fpage>5687</fpage>&#x02013;<lpage>5690</lpage>.</mixed-citation></ref><ref id="pone.0074746-Bates1"><label>66</label><mixed-citation publication-type="other">Bates D, Maechler M, Bolker B (2011) lme4. R package version 0.999375&#x02013;38.</mixed-citation></ref><ref id="pone.0074746-Lisker1"><label>67</label><mixed-citation publication-type="journal">
<name><surname>Lisker</surname><given-names>L</given-names></name>, <name><surname>Abramson</surname><given-names>A</given-names></name> (<year>1964</year>) <article-title>A cross-language study of voicing in initial stops: acoustical measurements</article-title>. <source>Word</source>
<volume>20</volume>: <fpage>384</fpage>&#x02013;<lpage>422</lpage>.</mixed-citation></ref><ref id="pone.0074746-Summerfield1"><label>68</label><mixed-citation publication-type="journal">
<name><surname>Summerfield</surname><given-names>A</given-names></name> (<year>1975</year>) <article-title>Aerodynamics versus mechanics in the control of voicing onset in consonant-vowel syllables</article-title>. <source>Speech Perception</source>
<volume>2</volume>: <fpage>61</fpage>&#x02013;<lpage>72</lpage>.</mixed-citation></ref><ref id="pone.0074746-Miller1"><label>69</label><mixed-citation publication-type="journal">
<name><surname>Miller</surname><given-names>J</given-names></name>, <name><surname>Green</surname><given-names>K</given-names></name>, <name><surname>Reeves</surname><given-names>A</given-names></name> (<year>1986</year>) <article-title>Speaking rate and segments: A look at the relation between speech production and speech perception for the voicing contrast</article-title>. <source>Phonetica</source>
<volume>43</volume>: <fpage>106</fpage>&#x02013;<lpage>115</lpage>.</mixed-citation></ref><ref id="pone.0074746-Sonderegger1"><label>70</label><mixed-citation publication-type="other">Sonderegger M (2012) Phonetic and Phonological Dynamics on Reality Television. Ph.D. thesis, University of Chicago.</mixed-citation></ref><ref id="pone.0074746-Barr1"><label>71</label><mixed-citation publication-type="journal">
<name><surname>Barr</surname><given-names>DJ</given-names></name>, <name><surname>Levy</surname><given-names>R</given-names></name>, <name><surname>Scheepers</surname><given-names>C</given-names></name>, <name><surname>Tily</surname><given-names>HJ</given-names></name> (<year>2013</year>) <article-title>Random effects structure for confirmatory hypothesis testing: Keep it maximal</article-title>. <source>Journal of Memory and Language</source>
<volume>68</volume>: <fpage>255</fpage>&#x02013;<lpage>278</lpage>.</mixed-citation></ref><ref id="pone.0074746-Hox1"><label>72</label><mixed-citation publication-type="other">Hox J (2010) Multilevel analysis: Techniques and applications. New York: Routledge.</mixed-citation></ref><ref id="pone.0074746-Gelman1"><label>73</label><mixed-citation publication-type="other">Gelman A, Hill J (2006) Data analysis using regression and multilevel/hierarchical models. Cambridge, UK: Cambridge University Press.</mixed-citation></ref><ref id="pone.0074746-Belsley1"><label>74</label><mixed-citation publication-type="other">Belsley D, Kuh E,Welsch R (1980) Regression Diagnostics: Identifying Inuential Data and Sources of Collinearity. Wiley-Interscience.</mixed-citation></ref><ref id="pone.0074746-Snijders1"><label>75</label><mixed-citation publication-type="other">Snijders T, Bosker R (2011) Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage.</mixed-citation></ref><ref id="pone.0074746-AbregoCollier1"><label>76</label><mixed-citation publication-type="other">Abrego-Collier C, Grove J, Sonderegger M, Yu ACL (2011) Effects of speaker evaluation on phonetic convergence. In: Zee E, editor, Proceedings of the International Congress of the Phonetic Sciences XVII, Hong Kong: International Congress of the Phonetic Sciences. 192&#x02013;195.</mixed-citation></ref><ref id="pone.0074746-Pratto1"><label>77</label><mixed-citation publication-type="journal">
<name><surname>Pratto</surname><given-names>F</given-names></name>, <name><surname>John</surname><given-names>OP</given-names></name> (<year>1991</year>) <article-title>Automatic vigilance: The attention-grabbing power of negative social information</article-title>. <source>Journal of Personality and Social Psychology</source>
<volume>61</volume>: <fpage>380</fpage>&#x02013;<lpage>391</lpage>.<pub-id pub-id-type="pmid">1941510</pub-id></mixed-citation></ref><ref id="pone.0074746-Hay1"><label>78</label><mixed-citation publication-type="journal">
<name><surname>Hay</surname><given-names>J</given-names></name>, <name><surname>Warren</surname><given-names>P</given-names></name>, <name><surname>Drager</surname><given-names>K</given-names></name> (<year>2006</year>) <article-title>Factors inuencing speech perception in the context of a merger-in-progress</article-title>. <source>Journal of Phonetics</source>
<volume>34</volume>: <fpage>458</fpage>&#x02013;<lpage>484</lpage>.</mixed-citation></ref><ref id="pone.0074746-Podesva1"><label>79</label><mixed-citation publication-type="journal">
<name><surname>Podesva</surname><given-names>R</given-names></name> (<year>2007</year>) <article-title>Phonation type as a stylistic variable: The use of falsetto in constructing a persona</article-title>. <source>Journal of Sociolinguistics</source>
<volume>11</volume>: <fpage>478</fpage>&#x02013;<lpage>504</lpage>.</mixed-citation></ref><ref id="pone.0074746-Goldinger2"><label>80</label><mixed-citation publication-type="journal">
<name><surname>Goldinger</surname><given-names>S</given-names></name>, <name><surname>Azuma</surname><given-names>T</given-names></name> (<year>2003</year>) <article-title>Puzzle-solving science: the quixotic questfor units in speech perception</article-title>. <source>Journal of Phonetics</source>
<volume>31</volume>: <fpage>305</fpage>&#x02013;<lpage>320</lpage>.<pub-id pub-id-type="pmid">29093608</pub-id></mixed-citation></ref><ref id="pone.0074746-Johnson1"><label>81</label><mixed-citation publication-type="other">Johnson K (1997b) Speech perception without speaker normalization. In: Johnson K, Mullenix J, editors, Talker variability in speech processing, San Diego: Academic Press. 146&#x02013;165.</mixed-citation></ref><ref id="pone.0074746-Kirby1"><label>82</label><mixed-citation publication-type="other">Kirby JP (2013) The role of probabilistic enhancement in phonologization. In: Yu ACL, editor, Origins of Sound Change: Approaches to Phonologization, Oxford, UK: Oxford University Press. 22 228&#x02013;246.</mixed-citation></ref><ref id="pone.0074746-Eckert1"><label>83</label><mixed-citation publication-type="journal">
<name><surname>Eckert</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Variation and the indexical field</article-title>. <source>Journal of Sociolinguistics</source>
<volume>12</volume>: <fpage>453</fpage>&#x02013;<lpage>476</lpage>.</mixed-citation></ref><ref id="pone.0074746-Bucholtz1"><label>84</label><mixed-citation publication-type="journal">
<name><surname>Bucholtz</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>The whiteness of nerds: Superstandard English and racial markedness</article-title>. <source>Journal of Linguistic Anthropology</source>
<volume>11</volume>: <fpage>84</fpage>&#x02013;<lpage>100</lpage>.</mixed-citation></ref><ref id="pone.0074746-Benor1"><label>85</label><mixed-citation publication-type="other">Benor S (2001) The learned/t/: Phonological variation in orthodox Jewish English. In: Sanchez T, Johnson DE, editors, Penn Working Papers in Linguistics: Selected Papers from NWAV 2000, Philadelphia, Pennsylvania: University of Pennsylvania Department of Linguistics. 1&#x02013;16.</mixed-citation></ref><ref id="pone.0074746-Podesva2"><label>86</label><mixed-citation publication-type="other">Podesva RJ, Jamsu J, Callier P (to appear) Constraints on the social meaning of released/t/: A production and perception study of U.S. politicians. Language Variation and Change (to appear).</mixed-citation></ref></ref-list></back></article>
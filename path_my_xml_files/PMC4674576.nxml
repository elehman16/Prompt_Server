<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Comput Math Methods Med</journal-id><journal-id journal-id-type="iso-abbrev">Comput Math Methods Med</journal-id><journal-id journal-id-type="publisher-id">CMMM</journal-id><journal-title-group><journal-title>Computational and Mathematical Methods in Medicine</journal-title></journal-title-group><issn pub-type="ppub">1748-670X</issn><issn pub-type="epub">1748-6718</issn><publisher><publisher-name>Hindawi Publishing Corporation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26693249</article-id><article-id pub-id-type="pmc">4674576</article-id><article-id pub-id-type="doi">10.1155/2015/202934</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A Vid<sc>E</sc>o-Based Intelligent Recognition and Decision System for the Phacoemulsification Cataract Surgery</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4910-6071</contrib-id><name><surname>Tian</surname><given-names>Shu</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Yin</surname><given-names>Xu-Cheng</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhi-Bin</given-names></name><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Fang</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref><xref ref-type="corresp" rid="cor2">
<sup>*</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Hao</surname><given-names>Hong-Wei</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref></contrib></contrib-group><aff id="I1"><sup>1</sup>Department of Computer Science and Technology, School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China</aff><aff id="I2"><sup>2</sup>National Engineering Research Center for Information Technology in Agriculture, Beijing 100089, China</aff><aff id="I3"><sup>3</sup>Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China</aff><author-notes><corresp id="cor1">*Xu-Cheng Yin: <email>xuchengyin@ustb.edu.cn</email> and </corresp><corresp id="cor2">*Fang Zhou: <email>zhoufang@ies.ustb.edu.cn</email></corresp><fn fn-type="other"><p>Academic Editor: Chuangyin Dang</p></fn></author-notes><pub-date pub-type="ppub"><year>2015</year></pub-date><pub-date pub-type="epub"><day>26</day><month>11</month><year>2015</year></pub-date><volume>2015</volume><elocation-id>202934</elocation-id><history><date date-type="received"><day>9</day><month>6</month><year>2015</year></date><date date-type="accepted"><day>8</day><month>11</month><year>2015</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2015 Shu Tian et al.</copyright-statement><copyright-year>2015</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/3.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>The phacoemulsification surgery is one of the most advanced surgeries to treat cataract. However, the conventional surgeries are always with low automatic level of operation and over reliance on the ability of surgeons. Alternatively, one imaginative scene is to use video processing and pattern recognition technologies to automatically detect the cataract grade and intelligently control the release of the ultrasonic energy while operating. Unlike cataract grading in the diagnosis system with static images, complicated background, unexpected noise, and varied information are always introduced in dynamic videos of the surgery. Here we develop a Vid<sc>e</sc>o-Based Intelligent Recognitionand Decision (V<sc>e</sc>BIRD) system, which breaks new ground by providing a generic framework for automatically tracking the operation process and classifying the cataract grade in microscope videos of the phacoemulsification cataract surgery. V<sc>e</sc>BIRD comprises a robust eye (iris) detector with randomized Hough transform to precisely locate the eye in the noise background, an effective probe tracker with Tracking-Learning-Detection to thereafter track the operation probe in the dynamic process, and an intelligent decider with discriminative learning to finally recognize the cataract grade in the complicated video. Experiments with a variety of real microscope videos of phacoemulsification verify V<sc>e</sc>BIRD's effectiveness.</p></abstract></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>As the foremost cause of blindness, cataract, the &#x0201c;clouding&#x0201d; or opacity developed in the crystalline lens of human eyes, blinds or is weakening tens of thousands of peoples' vision all over the world [<xref rid="B1" ref-type="bibr">1</xref>]. It is most commonly due to aging but has many other causes such as genetics [<xref rid="B2" ref-type="bibr">2</xref>], trauma, drug use, radiation, and medications. The world health reports published by World Health Organization (WHO) show that the proportion of cataract patients in the global blindness increases quickly from 43% in year 1998 [<xref rid="B3" ref-type="bibr">3</xref>] to 47.8% in year 2002 [<xref rid="B4" ref-type="bibr">4</xref>], which means that about 18 million people suffer from cataract in year 2002. Based on the locations of developed opacity, age-related cataracts are categorized into three classes: posterior subcapsular cataract, cortical cataract, and nuclear cataract [<xref rid="B5" ref-type="bibr">5</xref>]. Among them, the nuclear cataract is one of the most common types, which is the focus in this paper.</p><p>Medically speaking, extracapsular cataract extraction is a conventional manual cataract surgery for dealing with hard cataracts; meanwhile phacoemulsification is the most widely used surgery for relatively soft cataracts. There are several typical steps of operation for the phacoemulsification surgery, for example, anesthetic, corneal incision, capsulorhexis, phacoemulsification, and irrigation and aspiration. The procedure of phacoemulsification, one key step of the surgery, can be summarized as follows [<xref rid="B6" ref-type="bibr">6</xref>, <xref rid="B7" ref-type="bibr">7</xref>]. First, the surgeon observes the situation of the patient's cataract through microscope. Next, the grade of the opacity severity of the nuclear cataract is decided and assigned by the surgeon based on his own clinical experience. Simultaneously, a proper amount of ultrasonic energy is released with his feet. The cataract in front of the probe is gradually emulsified. Compared to the conventional extracapsular cataract extraction surgery, the phacoemulsification surgery has many advantages: small incision, short operative time, quick recovery, no need of hospitalization, small amounts of astigmatism, and fewer postoperative visits [<xref rid="B7" ref-type="bibr">7</xref>]. However, the phacoemulsification step is strongly dependent on the ophthalmologist's high skills and his long-term clinical experience, which nowadays is a main barrier to popularize the commonly used phacoemulsification operation instruments, especially in small and rural hospitals.</p><p>One vital factor is about the key equipment, the phacoemulsification instrument, which is not an automatically controlled machine. In some computer-assisted systems, only the images of the patient's eye are captured to the legibility for the surgeons, but the hardness level of lens nuclear and the amount of ultrasonic energy released are still manually determined by surgery operators by comparing the image with several standard pictures [<xref rid="B8" ref-type="bibr">8</xref>&#x02013;<xref rid="B10" ref-type="bibr">10</xref>]. These semiautomatic systems require the surgeons to have a high skill with a rich experience to determine the hardness level and thereafter to release the right amount of ultrasonic energy. At the same time, there are also some other research efforts with several automatic grading systems established to measure the opacity severity quantitatively [<xref rid="B11" ref-type="bibr">11</xref>&#x02013;<xref rid="B14" ref-type="bibr">14</xref>]. Specifically, Duncan et al. first extracted three features, that is, nuclear mean gray level, slope at the posterior point of profile, and the fractional residual of the polynomial least-square regression, and then trained a neural network classifier for determining the grade of nuclear opacity [<xref rid="B13" ref-type="bibr">13</xref>]. Fan et al. detected the visual axis, identified the ocular landmark features, and constructed a linear regression model for classifying the nuclear hardness [<xref rid="B14" ref-type="bibr">14</xref>]. Similarly, Li et al. proposed a computer-assisted method for nuclear cataract grading from slit-lamp images using ranking in the cataract diagnosis system [<xref rid="B15" ref-type="bibr">15</xref>, <xref rid="B16" ref-type="bibr">16</xref>]. In their method, a grade indicating the severity of nuclear cataract is via ranking with some learning and prediction techniques rather not manually assigned by a trained ophthalmologist. After the grade ranking, the conventional clinical decision-making process is performed. Actually, all these automatic grading systems are based on static images, which are helpful to diagnose the cataract and recognize the cataract grade before treatment. However, as we know, the cataract grade usually varies over the procedure of the phacoemulsification surgery. Consequently, it is really challenging to intelligently track the cataract and classify the grade in the dynamic video of the operation.</p><p>Ideally, the eventual goal is to develop an intelligent system to support the phacoemulsification surgery for automatically recognizing the cataract grade and releasing the ultrasonic energy during its operation but not to simply diagnose the disease before treatment. This first step with &#x0201c;virtual reality&#x0201d; is to automatically detect the cataract location, track the operation probe, and classify the cataract in the real microscope videos of the phacoemulsification surgery. Consequently, unlike cataract grading by the opacity severity in the cataract diagnosis system with static slit-lamp images, in multimedia understanding of the phacoemulsification surgery, there are several typical challenges. First, the complicated background is always generated not only by the cataract eye but also by the surgery operation. Second, the dynamic surgery introduces complex noise with the ophthalmologist operation, for example, tissue changing, illumination varying, and probe moving. Third, the cataract grade varies over the surgery procedure. For example, the cataract grade will become lower and lower with the energy releasing on the same location. In some other cases, the cataract grade will suddenly change from one place to another when moving the probe in the surgery [<xref rid="B6" ref-type="bibr">6</xref>].</p><p>To our knowledge, there are very few research efforts [<xref rid="B17" ref-type="bibr">17</xref>] by utilizing intelligent video analysis and pattern recognition technologies to improve the automatic level of this phacoemulsification surgery. In this paper, we develop a Vid<sc>e</sc>o-Based Intelligent Recognition and Decision (V<sc>e</sc>BIRD) system, which provides a generic framework for automatically tracking the operation process and classifying the cataract grade in microscope videos of the phacoemulsification cataract surgery. These microscope videos record the operation process in the patient's eye by the surgeon in real time. In V<sc>e</sc>BIRD, first a robust eye (iris) detector is constructed with randomized Hough transform and precisely locates the eye in the noise background. Next, an effective probe tracker is learned with Tracking-Learning-Detection (TLD) [<xref rid="B18" ref-type="bibr">18</xref>] and dynamically tracks the operation probe in the surgery process. Finally, an intelligent decider is trained with a discriminative learning algorithm and accurately recognizes the cataract grade in the complicated video. We make full use of the recent video analysis and pattern classification techniques to be adaptable to the phacoemulsification surgery situation. Specifically, we propose an improved randomized Hough transform method to robustly detect ellipse with noise. An adaptive TLD approach is also constructed to handle the problem that the probe is easily recognized as background. Additionally, V<sc>e</sc>BIRD has been experimented with a variety of real microscope videos of the phacoemulsification surgery, and the experimental results show that V<sc>e</sc>BIRD has the potential ability to reduce the complexity of the cataract operation and may be used to raise the automatic level of phacoemulsification instruments in the future.</p></sec><sec id="sec2"><title>2. Methods</title><sec id="sec2.1"><title>2.1. Framework of V<sc>e</sc>BIRD</title><p>Summarily, V<sc>e</sc>BIRD uses image processing, object tracking, and pattern recognition technologies to detect the eye (eye detection), track the emulsification probe (probe tracking), and recognize the cataract grade (cataract grading). The framework of this system is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, where cataract grading is composed of feature representation, cataract identification, and lens nuclear hardness classification. In <xref ref-type="fig" rid="fig1">Figure 1</xref>, the green ellipse represents the detected eye (iris), the yellow rectangle shows the tracked probe, and the red rectangles are the location of extracted tissues for cataract grading.</p><p>In V<sc>e</sc>BIRD, first, eye detection is performed by an eye detector which is constructed with an improved randomized Hough transform approach. Ellipse detection with this improved Hough transform is robust to noise and shape distortion. Then, a probe tracker is conducted to effectively track the emulsification probe in the eye region. Here, the tracker is built by an adaptive TLD method, which can clearly distinguish between foreground and background and therefore precisely track the object (probe) in the video. Moreover, this online tracking algorithm can be conveniently adapted and easily used in the real phacoemulsification surgery in the future. Next, an intelligent decider is used to recognize the cataract grade. In this final cataract grading step, features are extracted from the tissue image in front of the emulsification probe location; and the hardness level of the tissue (cataract grade) is predicted through a discriminative learning classifier with Support Vector Machines (SVMs) [<xref rid="B19" ref-type="bibr">19</xref>, <xref rid="B20" ref-type="bibr">20</xref>]. Specifically, the last prediction procedure is divided into 2 substeps. The first substep is cataract identification, which identifies whether the tissue is normal or not. The other one is lens nuclear hardness classification, which classifies the hardness grade of the cataract. In the proposed system, the training databases for these classifiers are composed of images taken from real microscope videos of phacoemulsification and the grade of nuclear opacity is annotated by the experienced ophthalmologists.</p><p>Our system can be explained by the probability theory as follows. We define <italic>I</italic>
<sub>1:<italic>t</italic></sub> as all the images from frame 1 to <italic>t</italic>, <italic>L</italic>
<sub>1:<italic>t</italic></sub> as the grade of nuclear opacity from frame 1 to <italic>t</italic>, <italic>D</italic>
<sub>1:<italic>t</italic></sub> as the results of eye detection from frame 1 to <italic>t</italic>, and <italic>T</italic>
<sub>1:<italic>t</italic></sub> as the results of probe tracking from frame 1 to <italic>t</italic>. For simplicity, the subscript can be dropped. Then, our goal is to get <italic>P</italic>(<italic>L</italic>&#x02223;<italic>I</italic>). Based on Total Probability Theorem, we get the following equation:<disp-formula id="EEq1"><label>(1)</label><mml:math id="M1"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>L</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>L</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Then, we assume only one result of eye detection and probe tracking is possible. We get the following equation:<disp-formula id="EEq2"><label>(2)</label><mml:math id="M2"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>L</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>L</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>
<italic>P</italic>(<italic>D</italic>, <italic>T</italic>&#x02223;<italic>I</italic>) can be further decomposed according to Total Probability Theorem and the assumption that only one result of eye detection is possible. Consider<disp-formula id="EEq3"><label>(3)</label><mml:math id="M3"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>D</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>T</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>At last, we get<disp-formula id="EEq4"><label>(4)</label><mml:math id="M4"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>L</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>D</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>T</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="1.61pt"/><mml:mi>L</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mspace height="7.08pt" depth="1.61pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>The terms <italic>P</italic>(<italic>D</italic>&#x02223;<italic>I</italic>), <italic>P</italic>(<italic>T</italic>&#x02223;<italic>D</italic>, <italic>I</italic>), and <italic>P</italic>(<italic>L</italic>&#x02223;<italic>D</italic>, <italic>T</italic>, <italic>I</italic>) correspond to the eye detection, probe tracking, and the cataract grading.</p></sec><sec id="sec2.2"><title>2.2. Eye Detection with Improved Randomized Hough Transform</title><p>As we know, the Hough transform [<xref rid="B21" ref-type="bibr">21</xref>] is an essential method for geometric shape recognition in images and is widely used in image processing and computer vision. Meanwhile, the contour of the eye (iris) in microscope videos of phacoemulsification can approximately be regarded as a circle. Hence, the Hough transform is certainly suitable for identifying the position of the iris. The standard Hough transform finds the circle by a voting strategy which is carried out by an accumulation process in the parameter space. Generally speaking, when the dimension of the parameter space is small, the Hough transform is efficient. But the computational burden becomes heavy as the dimension of the parameter space increases. To alleviate this problem, a number of researchers have developed numerous variations of the Hough transform to decompose the high-dimension parameter space into a low-dimension one by using the geometric properties, for example, symmetry [<xref rid="B22" ref-type="bibr">22</xref>, <xref rid="B23" ref-type="bibr">23</xref>]. In these variations, the storage space and the processing time are largely reduced. On the contrary, the real peaks in the low-dimension parameter space are more easily covered by noise than the high-dimension parameter space. The randomized Hough transform [<xref rid="B24" ref-type="bibr">24</xref>, <xref rid="B25" ref-type="bibr">25</xref>] is a solution to alleviate the problem to some extent. However, it is still easily affected by noise in complicated images and videos. Another drawback of the Hough transform is that it may be failed for shape detection and recognition when shape distortion occurs. For example, the shape of patient's iris may vary (from a circle to an ellipse) during the surgery, and the contour of the iris changes to be similar to an ellipse sometimes. As a result, the circle detection may not recognize the shape of the iris in the dynamic situation.</p><p>In order to deal with the above challenges, we propose an improvement of the randomized Hough transform for both circle detection and ellipse detection in V<sc>e</sc>BIRD. This improved randomized Hough transform restricts the distance between the sampling points with domain priors. Moreover, we also designed a cascaded detection method combining the circle detection and ellipse detection to largely reduce the computational burden of the eye detection system.</p><sec id="sec2.2.1"><title>2.2.1. Improved Randomized Hough Transform</title><p>In the Cartesian coordinate system, the equation of conic curve is<disp-formula id="EEq5"><label>(5)</label><mml:math id="M5"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mi>B</mml:mi><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mi>D</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mi>E</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where (<italic>x</italic>, <italic>y</italic>) represents the coordinate of a pixel in the two-dimension space (e.g., an image), <italic>A</italic> ~ <italic>F</italic> are the parameters, and |<italic>A</italic>| + |<italic>B</italic>| + |<italic>C</italic>| &#x02260; 0. The discriminant <italic>B</italic>
<sup>2</sup> &#x02212; 4<italic>AC</italic> can be used to classify the conic curve. If <italic>B</italic>
<sup>2</sup> &#x02212; 4<italic>AC</italic> &#x0003c; 0, the equation represents an ellipse. For an ellipse, because <italic>B</italic>
<sup>2</sup> &#x02212; 4<italic>AC</italic> &#x0003c; 0, <italic>A</italic> &#x02260; 0, (<xref ref-type="disp-formula" rid="EEq5">5</xref>) can be converted into<disp-formula id="EEq6"><label>(6)</label><mml:math id="M6"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mfrac><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mfrac><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn mathvariant="normal">0</mml:mn><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>After this simple mathematic transform, the number of relevant parameters is reduced to five. Then, only five different points will determine a parameter vector (<italic>B</italic>/<italic>A</italic>, <italic>C</italic>/<italic>A</italic>, <italic>D</italic>/<italic>A</italic>, <italic>E</italic>/<italic>A</italic>, <italic>F</italic>/<italic>A</italic>)<sup><italic>T</italic></sup>. The randomized Hough transform randomly samples groups of points which are composed by five points from the image's edges. If a parameter vector and one group of points can satisfy (<xref ref-type="disp-formula" rid="EEq6">6</xref>), accumulation occurs in the parameter space. Afterwards, the peak of the parameter space is found to represent the real existing ellipse on the image. In order to reduce the consuming time and the storage size by finding peaks of the parameter space, the 5-dimension parameter space can be displaced by five 1-dimension parameter spaces, which is well suited for the image containing one real ellipse [<xref rid="B26" ref-type="bibr">26</xref>].</p><p>In general cases where the noise of image is slight and the ellipse to be detected is relatively clear, the randomized Hough transform has a pretty good performance. However, when the image is with heavy noise (see an example shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>), the randomized Hough transform is always failed. In the randomized Hough transform, once five noncollinear sampling points are selected randomly, the five parameters of (<xref ref-type="disp-formula" rid="EEq6">6</xref>) are calculated and the voting procedure is conducted in the parameter space. Actually, this aimless random sampling step introduces a plenty of invalid sampling and accumulation steps which further result in the decrease of precision and runtime in eye detection. In particular, we intuitively show this disadvantage in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Here, the solid line ellipse represents the real existing ellipse on which the points <italic>P</italic>
<sub>1</sub> ~ <italic>P</italic>
<sub>4</sub> fall. The point <italic>P</italic>
<sub>5</sub> is a noise point which affects the ellipse detection result. If all five points are sampled together, the dashed line ellipse is detected and the wrong parameters are accumulated.</p><p>Consequently, we aim to improve this algorithm with a sampling control strategy and propose an improved randomized Hough transform approach. Unlike the aimless random sampling strategy in the conventional method, we add a constraint that each pair of sampling points must not be less than a specific threshold (shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> by a dashed line circle). We can observe that the error which occurred in the randomized Hough transform is avoided because point <italic>P</italic>
<sub>4</sub> and point <italic>P</italic>
<sub>5</sub> are too close to be sampled together. The distance between the real point and the noise point is often short. It is easy to get a large deviation when the parameters are calculated by a set of points which are near to each other. Because we do not sample the points which are too closer, the accuracy of eye detection is largely improved.</p><p>Please note that the runtime of circle detection is much faster than the ellipse detection. Hence, it is time-consuming to use ellipse detection all the time. To alleviate this problem, we design a cascaded detection method combining ellipse detection and circle detection. In this simple cascaded detection strategy, circle detection is first used to locate the eye (iris). If circle detection failed, ellipse detection is then utilized to further detect and locate the eye.</p></sec></sec><sec id="sec2.3"><title>2.3. Probe Tracking with Adaptive Tracking-Learning-Detection</title><p>In the literature, there are few techniques on probe tracking for the phacoemulsification surgery. Baldas et al. proposed a simple and direct tracking algorithm [<xref rid="B27" ref-type="bibr">27</xref>]. They first get the eye region by thresholding the color. And then the probe is detected as a straight line in the eye region. Meanwhile, object tracking is a widely researched field in computer vision. A huge number of methods and systems of object tracking are investigated. Many effective algorithms are proposed, such as pyramidal Lucas-Kanade (LK) tracker [<xref rid="B28" ref-type="bibr">28</xref>], particle filter [<xref rid="B29" ref-type="bibr">29</xref>], and tracking-by-detection [<xref rid="B30" ref-type="bibr">30</xref>]. Specifically, Tracking-Learning-Detection (TLD) method [<xref rid="B18" ref-type="bibr">18</xref>] is the recent highlighted one. In V<sc>e</sc>BIRD, we design an adaptive TLD method to locate and track the probe by combing a priori knowledge to adaptively distinguish between foreground and background.</p><sec id="sec2.3.1"><title>2.3.1. Tracking-Learning-Detection</title><p>In the first frame, the real location of probe is annotated manually. Afterwards the probe is correspondingly tracked. There are 3 components in the framework of Tracking-Learning-Detection, that is,<italic> Tracker</italic>,<italic> Detector</italic>, and<italic> Learning</italic>. The task of<italic> Tracker</italic> is to estimate the object's motion between the consecutive frames and predict the location of object in the next frame.<italic> Detector</italic> is used to detect object in one frame without any information of other frames.<italic> Learning</italic> component estimates error of<italic> Tracker</italic> and<italic> Detector</italic> and generates training data for<italic> Detector</italic>. The final output of TLD is a combination of the results of<italic> Tracker</italic> and<italic> Detector</italic>.</p><p>
<italic>Tracker</italic> in the TLD framework is the Median Flow tracker [<xref rid="B31" ref-type="bibr">31</xref>]. The object's location is estimated by a novel measure, Forward-Backward error. The object is represented by a bounding box in which a number of points are generated randomly. First, the location of the points in the next frame is predicted by a pyramidal Lucas-Kanande (LK) tracker [<xref rid="B28" ref-type="bibr">28</xref>]. Then the locations of the points in the current frame are predicted backward based on the predicted locations in the next frame by a pyramidal LK tracker too. At last, the distances of pairs in these two sets of points are compared, and the points are considered incorrect if they differ significantly. The bounding box enclosing all correct points is the final output of the tracker.</p><p>
<italic>Detector</italic> in the TLD framework scans the input image by a sliding window. Then a cascaded classifier is trained to decide whether the image patches contain the object. In the first stage, the patch variance is computed and analyzed. All patches, for which the gray-value variance is smaller than half of the variance of the patch that was selected for tracking, are rejected. Next, in the second stage, left patches are to be further classified by an ensemble classifier. The feature is a binary code computed by comparing pairs of pixels which are generated offline randomly and stay fixed in runtime. Each base classifier is based on each feature independently. The output of the base classifier is the posterior probability which is gotten from the training data. The label output by the ensemble classifier is judged by whether the sum of all posterior probabilities is more than a given threshold. The last stage of the cascade is a nearest neighbor classifier for reconfirmation.</p><p>
<italic>Learning</italic> component in the TLD framework is a P-N learning algorithm [<xref rid="B32" ref-type="bibr">32</xref>]. Using the conservative similarity and temporal and spatial information, the reliability of the object's location is decided. Then based on the object's location with very high confidence, the samples classified incorrectly by the detector are utilized to update the object model and the ensemble classifier.</p></sec><sec id="sec2.3.2"><title>2.3.2. Adaptive Tracking-Learning-Detection</title><p>In V<sc>e</sc>BIRD, we improve the TLD method to deal with the specificity of probe tracking. In many object tracking algorithms, and also in the TLD method, an object is always modeled as a rectangle which is not the shape of the probe. As a result, a large region of the background is contained in the rectangle. The main effect for TLD is that the ensemble classifier often outputs the wrong label especially when the probe is still in the rectangle but the background is with a small change. This error easily leads TLD to regard the object as absent while tracking.</p><p>Here, we construct an adaptive Tracking-Learning-Detection (TLD) approach, where the idea is to force the ensemble classifier to output the positive label easily. The second stage of the cascaded detector is the key as the first stage is to eliminate the incredible samples and the last stage of cascade is to verify. The second stage is an ensemble classifier. Each base classifier is based on each feature independently. The output of a base classifier is the posterior probability. But the samples are gotten from the<italic> Learning</italic> component; hence several features are not covered in the long term of tracking. The corresponding base classifier will output 0 for a long time. To avoid the decrease of the posterior probability, we empirically select one-fifth of base classifiers randomly to set their initial posterior probability to 0.5. Then the ratio of number of positive samples and total number of samples gotten from the P-N learning algorithm is updated to the posterior probability of these base classifiers frame by frame. This simple and adaptive improvement avoids the wrong absence by considering the overall conditions of training data.</p></sec></sec><sec id="sec2.4"><title>2.4. Cataract Grading with Support Vector Machines</title><p>After eye detection and probe tracking, the following task is cataract grading. Here, tissue image at the top of the emulsification probe is extracted and feature representation is constructed. Next, an intelligent decider with SVMs first identifies whether the tissue is normal or not and then classifies the hardness degree (grade) of the patient's cataract. Actually, the cataract grade decides the release lever of ultrasonic energy in the phacoemulsification surgery.</p><sec id="sec2.4.1"><title>2.4.1. Tissue Image Extraction and Feature Representation</title><p>In V<sc>e</sc>BIRD, the cataract grading system must be accurate and in real time to recognize the tissue image and classify the hardness grade at the top of the emulsification probe. In general, tissue image extraction and feature representation of the tissue image are the fundamental step.</p><p>In cataract grading, V<sc>e</sc>BIRD extracts features with color characteristics of the images, since the color is the key information for classifying the grade of the lens nuclear hardness by surgery operators. The original RGB values of image pixels are usually used to construct features. However, in real situations, they are always with varied noise in the image. Hence, we use the following strategy to extract the feature vector of the tissue image. First, the extracted tissue image part is normalized to 32 &#x000d7; 32. Then, we further uniformly divide the image into 8 &#x000d7; 8 grids. Next, we calculate the average color value of each grid as one feature. Finally, the color feature vector is computed as follows:<disp-formula id="EEq7"><label>(7)</label><mml:math id="M7"><mml:mtable style="T3"><mml:mtr><mml:mtd><mml:malignmark/><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="6.57999pt" depth="0.0pt"/><mml:mi>I</mml:mi><mml:mspace height="6.57999pt" depth="0.0pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="5.7337646484375pt"/><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mspace height="9.58pt" depth="3.045pt"/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mspace height="9.58pt" depth="3.045pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>I</italic> is an image, (<italic>r</italic>
<sub><italic>i</italic></sub>, <italic>g</italic>
<sub><italic>i</italic></sub>, <italic>b</italic>
<sub><italic>i</italic></sub>) is the color value of <italic>i</italic>th pixel of <italic>I</italic>, <italic>n</italic> is the total number of pixels, <inline-formula><mml:math id="M8"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the average color value of the <italic>j</italic>th grid, and <italic>m</italic> is the total number of grids.</p></sec><sec id="sec2.4.2"><title>2.4.2. Identification and Classification with Support Vector Machines</title><p>According to the Emery-Little classification in phacoemulsification, the grade standard is shown in <xref ref-type="table" rid="tab1">Table 1</xref>. The color features (feature representation) with some examples are also shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. We follow the principal criteria and categorize all images into six types. As shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, the cataract grading stage includes two recognition substeps, that is, cataract identification and lens nuclear hardness classification. Here, identification decides whether the tissue is normal or not, and classification determines the hardness grade of the cataract. Here we introduce the cataract identification function for avoiding the hurt of the normal tissue in the operation (in the future applications). Moreover, the lens nuclear hardness classification function can be used to control the release of ultrasonic energy while operating in the phacoemulsification surgery. Obviously, a cascaded classifier is suitable to fulfill the classification task. That is, if a tissue image is classified into the normal tissue (by cataract identification), the second classifier (grade classification) will not be applied. Otherwise, the grade of hardness is recognized by the second stage of the cascaded classifier. Such a strategy can not only reduce the computation complexity but also avoid hurting the normal tissue. Specifically, the designed classifiers for both identification and classification are SVMs with a RBF (Radial Basis Function) kernel. As described before, all the training and testing images are normalized to the same size to ensure the same dimension of extracted features (see (<xref ref-type="disp-formula" rid="EEq7">7</xref>)).</p></sec></sec></sec><sec id="sec3"><title>3. Results</title><p>In this section, we will sequentially describe the experimental datasets and experiments of eye detection, probe tracking, and cataract grading techniques in V<sc>e</sc>BIRD. Our eye detection method is compared with the randomized Hough transform method. Our probe tracking method is compared with TLD. At last, we compare our cataract grading method with <italic>K</italic>NN.</p><sec id="sec3.1"><title>3.1. Implementation and Data Subsets for Evaluation</title><p>Our system has been implemented in VC++ using the OpenCV library on a 2.6&#x02009;GHz Quad-Core Intel Core i5&#x02009;CPU with 8&#x02009;GB&#x02009;RAM. We use a variety of real videos of the phacoemulsification surgery in the patient's eye acquired with the microscope in one month, which are captured from the operation procedure of the real phacoemulsification surgery from Beijing Tongren Hospital of China [<xref rid="B6" ref-type="bibr">6</xref>]. The research on the data has been approved by the ethics committee of Beijing Tongren Hospital. All data have been anonymized and deidentified.</p><p>We perform 3 types of experiments, that is, experiments with eye detection, experiments with probe tracking, and experiments with cataract grading. Hence 3 different subsets of microscope videos/frames are constructed and annotated, respectively. Specifically, for experiments with eye detection, the focus is to evaluate the precision of the eye detection. So, we randomly sample 2000 video frames of all these videos and then perform the proposed eye detection method on this dataset. For experiments with probe tracking, 5 typical videos with more than 100,000 frame are used to evaluate the tracking performance quantitatively. For experiments with cataract grading, the identification/classification accuracy is our target. Similarly, 2000 video frames are randomly selected at proper intervals from videos and the hardness level of cataract for each frame is annotated by the experts (skilled ophthalmologists). All datasets are divided into the training and testing sets. The detailed information about them are shown in <xref ref-type="table" rid="tab2">Table 2</xref>.</p></sec><sec id="sec3.2"><title>3.2. Experimental Results with Eye Detection</title><p>In V<sc>e</sc>BIRD, the detected region of the eye (iris) is to constrain the location of the probe, and afterwards the abnormal tissue in front of the probe is extracted for analysis. Obviously, the detection error will be accumulated to the next step. One example of the eye detection procedure is shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p>First, we show some qualitative comparison results of the randomized Hough transform method and the improved approach (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). From <xref ref-type="fig" rid="fig5">Figure 5</xref>, we can see that when the shape of iris varies, the proposed method is much better than the standard randomized Hough transform. Specifically, if the iris is represented by a circle, some parts of the normal tissue are likely in the circle and other parts are easily outside the circle. Moreover, the whole iris is located by the proposed approach since the improved standard randomized Hough transform technique can adaptively detect shapes varying from circle to ellipse.</p><p>Second, we also present a quantitative performance. As the contour of the iris is not a standard circle or a standard ellipse, the error of the detected results is difficult to quantify. Here, we propose a simple specific-domain strategy to evaluate the detection result. That is, if the detected ellipse encircles the iris, and at the same time it is encircled by the spatia anguli iridocornealis (the line separating the iris and the &#x0201c;white eyeball&#x0201d;), then the result is regarded as a correct one. Consequently, the detection accuracy of the proposed technique is described in <xref ref-type="table" rid="tab3">Table 3</xref> on the testing set. In most cases, this method can correctly detect the whole iris (with 92% accuracy). <xref ref-type="table" rid="tab3">Table 3</xref> also shows that the detection performance can be affected by the number of sampling points in the Hough transform algorithm. Generally speaking, the larger number of sampling points the better performance.</p></sec><sec id="sec3.3"><title>3.3. Experimental Results with Probe Tracking</title><p>We use a quantitative metric to evaluate the tracking algorithm, namely, precision (<italic>P</italic>). Define the overlapping rate (OR) as<disp-formula id="EEq8"><label>(8)</label><mml:math id="M9"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mtext>OR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>area</mml:mtext><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="{" close="}"><mml:mrow><mml:mtext>area</mml:mtext><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mtext>area</mml:mtext><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>ROI</italic>
<sub><italic>T</italic></sub> is the result of the tracker and <italic>ROI</italic>
<sub><italic>G</italic></sub> is the ground truth. The tracking result is considered as a correct one when OR is larger than a specific threshold. In V<sc>e</sc>BIRD, we set the threshold as 0.5. Note that because we cannot annotate the ground truth exactly since the scale of the probe is defined as unclear the metric &#x0201c;OR&#x0201d; is defined with a little relaxation compared to the conventional one. Here, the precision <italic>P</italic> is calculated as<disp-formula id="EEq9"><label>(9)</label><mml:math id="M10"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>#</mml:mo></mml:mrow><mml:mrow><mml:mtext>correct</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>#</mml:mo></mml:mrow><mml:mrow><mml:mtext>frames</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where #<sub>right</sub> is the number of frames in which the tracking result is correct and #<sub>frames</sub> is the total number of all frames in the video.</p><p>The quantitative results on the testing video set of TLD and the adaptive TLD methods are compared in <xref ref-type="table" rid="tab4">Table 4</xref>. We can see that the improvement of our method is pretty large by increasing the precision of more than 20%. The adaptive TLD method obtains a better performance than TLD for all videos. Specifically, for Video 3, the object is lost by TLD and never recovered. But our adaptive TLD can effectively track the probe accurately. We also show some typical different results between the original TLD and the adaptive TLD methods in <xref ref-type="fig" rid="fig6">Figure 6</xref>. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows that the biggest problem of TLD is that it tends to classify some of the images of the probe as negative samples. For example, the difference of the background's appearance and the probe's one is small between frame 1222 and frame 1223. The tracker of TLD is disabled to track the probe and outputs a wrong negative label, which results in the absence of the tracking result (the red rectangle in <xref ref-type="fig" rid="fig6">Figure 6</xref>). In the adaptive TLD method, though the tracker fails to track the probe, the detector works well. Hence, the probe region is correspondingly retrieved. Moreover, the detector is updated by more correct samples to be more accurate in the long term of probe tracking in V<sc>e</sc>BIRD.</p></sec><sec id="sec3.4"><title>3.4. Experimental Results with Cataract Grading</title><p>In experiments with cataract grading, there are two tasks, that is, cataract identification and hardness classification, which are performed and analyzed with numerous video frames. In the experiment of hardness classification, we compare two types of classifiers for identification and classification, that is, <italic>K</italic> Nearest Neighbor Classifiers (<italic>K</italic>NN) and Support Vector Machines (SVMs), where SVM is a recent representative discriminative approach in pattern recognition and machine learning fields. Moreover, the <italic>K</italic>NN classifier is with 5 neighbors, and the parameters of SVMs with RBF kernel are decided with cross-validation by LIBSVM [<xref rid="B19" ref-type="bibr">19</xref>]. However, in the experiment of cataract identification, we only use the SVM classifiers as it is a simple two-class recognition task.</p><p>The experimental results with the average recognition rate are shown in <xref ref-type="table" rid="tab5">Table 5</xref>, which show that the identification and classification techniques have an impressive performance. First, the identification accuracy is very high, that is, 99.2%, because of the simple task. Second, classification accuracies (recognition rates) of both classification methods (<italic>K</italic>NN and SVM) are more than 90%. SVM has a much better performance with 96.3%. Consequently, the SVM classifier is finally selected as lens nuclear hardness classification in V<sc>e</sc>BIRD. We also analyze the failed cases for hardness classification. As we know, the successive and adjacent grades (e.g., 2 and 3 or 3 and 4) are easy to be confusedly classified by surgeons, even skilled experts. Hence, this observation also can explain most of the failed cases in the experiments. Namely, in some times, the hardness classification decider will classify a very few successive and adjacent grades in video frames.</p></sec></sec><sec id="sec4"><title>4. Conclusion</title><p>In this paper, we introduce design principles, key technologies, and experimental results of V<sc>e</sc>BIRD (a Vid<sc>e</sc>o-Based Intelligent Recognition and Decision system) for the phacoemulsification cataract surgery. By utilizing a variety of video analysis and pattern recognition techniques, this new technology can intelligently analyze real videos from the operation process; that is, it robustly detects the eye (iris), effectively tracks the emulsification probe, and precisely grades the hardness of lens nuclear. Specifically, we highlight three major contributions of V<sc>e</sc>BIRD. First, V<sc>e</sc>BIRD provides a generic framework (with eye detection, probe tracking, and cataract grading) for tracking the operation process and classifying the cataract grade in microscope videos of the phacoemulsification surgery. Second, several novel techniques are designed in V<sc>e</sc>BIRD. For example, the proposed randomized Hough transform is robust to noise and shape distortion. The adaptive TLD method for object (probe) tracking can easily distinguish between foreground and background. Third, a variety of experiments on real microscope videos of the phacoemulsification surgery verify that V<sc>e</sc>BIRD can automatically and effectively detect the eye, track the probe, identify the intraocular tissue, and classify the hardness of lens nucleus.</p><p>Imagine that, in the computer-aided system with intelligent programs of the real phacoemulsification surgery in the future, the cataract grade (the hardness of lens nucleus) is automatically decided, and correspondingly the release of the ultrasonic energy is also automatically controlled. In such cases, the surgeons only need to simply move and control the probe. Obviously, V<sc>e</sc>BIRD is one potential technology for these intelligent computer programs. Actually, only 2 steps need to be changed or added in V<sc>e</sc>BIRD. First, the images/frames are not from the prestorage videos but from the real-time captured ones with the phacoemulsification instrument. Second, one controlling module should be added to decide the release of proper ultrasonic energy according to the hardness degree recognized by cataract grading and afterwards to emulsify the cataract. With the help of the computer-aided system, the difficulty level of the cataract operation and its heavy reliance on highly skilled surgeons should be significantly reduced. Hence, it will help promote the spread of the surgery, which will be possible for operating in not only big and advanced hospitals but also small and rural ones. Of course, the proposed system could still be improved in some aspects. First, the hardness of the nucleus depends not only on color but also on some other factors such as shape and scale. We will collect and annotate more data to improve the system. Second, the amounts of ultrasound use are mainly decided by the hardness of the nucleus, but they are not linearly dependent. We will mine their relation in the next step.</p><p>We hope that continuous efforts will help improve the automatic level of phacoemulsification instruments and intelligentize the phacoemulsification surgery which can be really operated by general surgeons in the future.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank Professor Siquan Zhu at Beijing Tongren Hospital for the data support. This work was supported by the National Natural Science Foundation of China (61403035, 61175020, and 61473036).</p></ack><sec sec-type="conflict"><title>Conflict of Interests</title><p>The authors declare that there is no conflict of interests regarding the publication of this paper.</p></sec><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brian</surname><given-names>G.</given-names></name><name><surname>Taylor</surname><given-names>H.</given-names></name></person-group><article-title>Cataract blindness&#x02014;challenges for the 21st century</article-title><source><italic>Bulletin of the World Health Organization</italic></source><year>2001</year><volume>79</volume><issue>3</issue><fpage>249</fpage><lpage>256</lpage><pub-id pub-id-type="other">2-s2.0-0035088540</pub-id><pub-id pub-id-type="pmid">11285671</pub-id></element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>D.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>Q.</given-names></name><etal/></person-group><article-title>Identification and functional analysis of GJA8 mutation in a chinese family with autosomal dominant perinuclear cataracts</article-title><source><italic>PLoS ONE</italic></source><year>2013</year><volume>8</volume><issue>3</issue><pub-id pub-id-type="publisher-id">e59926</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0059926</pub-id><pub-id pub-id-type="other">2-s2.0-84875538485</pub-id></element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="book"><collab>World Health Organization</collab><source><italic>The World Health Report: Life in the 21st Century&#x02014;A Vision for All</italic></source><year>1998</year><publisher-loc>Geneva, Switzerland</publisher-loc><publisher-name>World Health Organization</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="http://www.who.int/whr/1998/en/whr98_en.pdf">http://www.who.int/whr/1998/en/whr98_en.pdf</ext-link></comment></element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="other"><collab>World Health Organization</collab><comment>Magnitude and causes of visual impairments, 2002, <ext-link ext-link-type="uri" xlink:href="http://www.who.int/mediacentre/factsheets/fs282/en/index.html">http://www.who.int/mediacentre/factsheets/fs282/en/index.html</ext-link></comment></element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coleman</surname><given-names>A.</given-names></name><name><surname>Morrison</surname><given-names>J.</given-names></name></person-group><source><italic>Management of Cataracts and Glaucoma</italic></source><year>2005</year><publisher-loc>Oxfordshire, UK</publisher-loc><publisher-name>Taylor &#x00026; Francis</publisher-name></element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.-B.</given-names></name><name><surname>Hao</surname><given-names>H.-W.</given-names></name><name><surname>Yin</surname><given-names>X.-C.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name></person-group><article-title>An intelligent recognition system for the phacoemulsification cataract surgery</article-title><conf-name>Proceedings of the Chinese Conference on Pattern Recognition (CCPR &#x02019;09)</conf-name><conf-date>November 2009</conf-date><conf-loc>Nanjing, China</conf-loc><fpage>741</fpage><lpage>745</lpage><pub-id pub-id-type="doi">10.1109/ccpr.2009.5344004</pub-id><pub-id pub-id-type="other">2-s2.0-74549160463</pub-id></element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurapkiene</surname><given-names>S.</given-names></name><name><surname>Raitelaitiene</surname><given-names>R.</given-names></name><name><surname>Paunksnis</surname><given-names>A.</given-names></name><etal/></person-group><article-title>The relationship of ultrasonic and mechanical properties of human nuclear cataract: a pilot study</article-title><source><italic>Ultragarsas</italic></source><year>2005</year><volume>54</volume><fpage>39</fpage><lpage>43</lpage></element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>B. E. K.</given-names></name><name><surname>Klein</surname><given-names>R.</given-names></name><name><surname>Lee Pedula Linton</surname><given-names>K.</given-names></name><name><surname>Magli</surname><given-names>Y. L.</given-names></name><name><surname>Neider</surname><given-names>M. W.</given-names></name></person-group><article-title>Assessment of cataracts from photographs in the Beaver Dam Eye Study</article-title><source><italic>Ophthalmology</italic></source><year>1990</year><volume>97</volume><issue>11</issue><fpage>1428</fpage><lpage>1433</lpage><pub-id pub-id-type="doi">10.1016/s0161-6420(90)32391-6</pub-id><pub-id pub-id-type="other">2-s2.0-0025054487</pub-id><pub-id pub-id-type="pmid">2255515</pub-id></element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Martinyi</surname><given-names>C.</given-names></name><name><surname>Bahn</surname><given-names>C.</given-names></name><name><surname>Meyer</surname><given-names>R.</given-names></name></person-group><source><italic>Slit Lamp: Examination and Photography</italic></source><year>2007</year><publisher-loc>Flagstaff, Ariz, USA</publisher-loc><publisher-name>Time One Ink Ltd Press</publisher-name></element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>West</surname><given-names>S. K.</given-names></name><name><surname>Rosenthal</surname><given-names>F.</given-names></name><name><surname>Newland</surname><given-names>H. S.</given-names></name><name><surname>Taylor</surname><given-names>H. R.</given-names></name></person-group><article-title>Use of photographic techniques to grade nuclear cataracts</article-title><source><italic>Investigative Ophthalmology &#x00026; Visual Science</italic></source><year>1988</year><volume>29</volume><issue>1</issue><fpage>73</fpage><lpage>77</lpage><pub-id pub-id-type="other">2-s2.0-0023873863</pub-id><pub-id pub-id-type="pmid">3335435</pub-id></element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chylack</surname><given-names>L. T.</given-names><suffix>Jr.</suffix></name><name><surname>Wolfe</surname><given-names>J. K.</given-names></name><name><surname>Singer</surname><given-names>D. M.</given-names></name><etal/></person-group><article-title>The lens opacities classification system III</article-title><source><italic>Archives of Ophthalmology</italic></source><year>1993</year><volume>111</volume><issue>6</issue><fpage>831</fpage><lpage>836</lpage><pub-id pub-id-type="pmid">8512486</pub-id></element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thylefors</surname><given-names>B.</given-names></name><name><surname>Chylack</surname><given-names>L. T.</given-names><suffix>Jr.</suffix></name><name><surname>Konyama</surname><given-names>K.</given-names></name><etal/></person-group><article-title>A simplified cataract grading system</article-title><source><italic>Ophthalmic Epidemiology</italic></source><year>2002</year><volume>9</volume><issue>2</issue><fpage>83</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1076/opep.9.2.83.1523</pub-id><pub-id pub-id-type="other">2-s2.0-0036107838</pub-id><pub-id pub-id-type="pmid">11821974</pub-id></element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>D. D.</given-names></name><name><surname>Shukla</surname><given-names>O. B.</given-names></name><name><surname>West</surname><given-names>S. K.</given-names></name><name><surname>Schein</surname><given-names>O. D.</given-names></name></person-group><article-title>New objective classification system for nuclear opacification</article-title><source><italic>Journal of the Optical Society of America</italic></source><year>1997</year><volume>14</volume><issue>6</issue><fpage>1197</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1364/josaa.14.001197</pub-id><pub-id pub-id-type="other">2-s2.0-0031150156</pub-id><pub-id pub-id-type="pmid">9168593</pub-id></element-citation></ref><ref id="B14"><label>14</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>S.</given-names></name><name><surname>Dyer</surname><given-names>C.</given-names></name><name><surname>Hubbard</surname><given-names>L.</given-names></name><name><surname>Klein</surname><given-names>B.</given-names></name></person-group><article-title>An automatic system for classification of nuclear sclerosis from slit-lamp photographs</article-title><source><italic>Medical Image Computing and Computer-Assisted Intervention&#x02014;MICCAI 2003</italic></source><year>2003</year><volume>2878</volume><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name><fpage>592</fpage><lpage>601</lpage><series>Lecture Notes in Computer Science</series><pub-id pub-id-type="doi">10.1007/978-3-540-39899-8_73</pub-id></element-citation></ref><ref id="B15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Lim</surname><given-names>J. H.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><etal/></person-group><article-title>A computer-aided diagnosis system of nuclear cataract</article-title><source><italic>IEEE Transactions on Biomedical Engineering</italic></source><year>2010</year><volume>57</volume><issue>7</issue><fpage>1690</fpage><lpage>1698</lpage><pub-id pub-id-type="doi">10.1109/tbme.2010.2041454</pub-id><pub-id pub-id-type="other">2-s2.0-77953758058</pub-id><pub-id pub-id-type="pmid">20172776</pub-id></element-citation></ref><ref id="B16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>W.</given-names></name><name><surname>Chan</surname><given-names>K. L.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Lim</surname><given-names>J. H.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Wong</surname><given-names>T. Y.</given-names></name></person-group><article-title>A computer assisted method for nuclear cataract grading from slit-lamp images using ranking</article-title><source><italic>IEEE Transactions on Medical Imaging</italic></source><year>2011</year><volume>30</volume><issue>1</issue><fpage>94</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2062197</pub-id><pub-id pub-id-type="other">2-s2.0-78650873881</pub-id><pub-id pub-id-type="pmid">20679026</pub-id></element-citation></ref><ref id="B17"><label>17</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Baldas</surname><given-names>V.</given-names></name><name><surname>Tang</surname><given-names>L.</given-names></name><name><surname>Bountris</surname><given-names>P.</given-names></name><name><surname>Saleh</surname><given-names>G.</given-names></name><name><surname>Koutsouris</surname><given-names>D.</given-names></name></person-group><article-title>A real-time automatic instrument tracking system on cataract surgery videos for dexterity assessment</article-title><conf-name>Proceedings of the 10th International Conference on Information Technology and Applications in Biomedicine (ITAB &#x02019;10)</conf-name><conf-date>November 2010</conf-date><conf-loc>Corfu, Greece</conf-loc><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/itab.2010.5687643</pub-id><pub-id pub-id-type="other">2-s2.0-79951662198</pub-id></element-citation></ref><ref id="B18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalal</surname><given-names>Z.</given-names></name><name><surname>Mikolajczyk</surname><given-names>K.</given-names></name><name><surname>Matas</surname><given-names>J.</given-names></name></person-group><article-title>Tracking-learning-detection</article-title><source><italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic></source><year>2012</year><volume>34</volume><issue>7</issue><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2011.239</pub-id><pub-id pub-id-type="other">2-s2.0-84861312439</pub-id><pub-id pub-id-type="pmid">22156098</pub-id></element-citation></ref><ref id="B19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C.-C.</given-names></name><name><surname>Lin</surname><given-names>C.-J.</given-names></name></person-group><article-title>LIBSVM: a library for support vector machines</article-title><source><italic>ACM Transactions on Intelligent Systems and Technology</italic></source><year>2011</year><volume>2</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id><pub-id pub-id-type="other">2-s2.0-79955702502</pub-id></element-citation></ref><ref id="B20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>X.-C.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name><name><surname>Hao</surname><given-names>H.-W.</given-names></name><name><surname>Wang</surname><given-names>Z.-B.</given-names></name><name><surname>Huang</surname><given-names>K.</given-names></name></person-group><article-title>FMI image based rock structure classification using classifier combination</article-title><source><italic>Neural Computing and Applications</italic></source><year>2011</year><volume>20</volume><issue>7</issue><fpage>955</fpage><lpage>963</lpage><pub-id pub-id-type="doi">10.1007/s00521-010-0395-3</pub-id><pub-id pub-id-type="other">2-s2.0-80052804953</pub-id></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duda</surname><given-names>R.</given-names></name><name><surname>Hart</surname><given-names>P.</given-names></name></person-group><article-title>Use of the hough transformation to detect lines and curves in pictures</article-title><source><italic>Communications of the ACM</italic></source><year>1972</year><volume>15</volume><issue>1</issue><fpage>11</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1145/361237.361242</pub-id><pub-id pub-id-type="other">2-s2.0-0015285440</pub-id></element-citation></ref><ref id="B22"><label>22</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Efficient technique for ellipse detection using restricted randomized hough transform</article-title><conf-name>Proceedings of the International Conference on Information Technology: Coding Computing</conf-name><conf-date>April 2004</conf-date><fpage>714</fpage><lpage>718</lpage><pub-id pub-id-type="other">2-s2.0-3042547050</pub-id></element-citation></ref><ref id="B23"><label>23</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T. M.</given-names></name><name><surname>Ahuja</surname><given-names>S.</given-names></name><name><surname>Wu</surname><given-names>Q.</given-names></name></person-group><article-title>A real-time ellipse detection based on edge grouping</article-title><conf-name>Proceedings of the IEEE International Conference on Systems, Man and Cybernetics (SMC &#x02019;09)</conf-name><conf-date>October 2009</conf-date><conf-loc>San Antonio, Tex, USA</conf-loc><fpage>3280</fpage><lpage>3286</lpage><pub-id pub-id-type="doi">10.1109/icsmc.2009.5346226</pub-id><pub-id pub-id-type="other">2-s2.0-74849086594</pub-id></element-citation></ref><ref id="B24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>L.</given-names></name><name><surname>Oja</surname><given-names>E.</given-names></name><name><surname>Kultanen</surname><given-names>P.</given-names></name></person-group><article-title>A new curve detection method: randomized Hough transform (RHT)</article-title><source><italic>Pattern Recognition Letters</italic></source><year>1990</year><volume>11</volume><issue>5</issue><fpage>331</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1016/0167-8655(90)90042-z</pub-id><pub-id pub-id-type="other">2-s2.0-33747992562</pub-id></element-citation></ref><ref id="B25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiryati</surname><given-names>N.</given-names></name><name><surname>K&#x000e4;lvi&#x000e4;inen</surname><given-names>H.</given-names></name><name><surname>Alaoutinen</surname><given-names>S.</given-names></name></person-group><article-title>Randomized or probabilistic Hough transform: unified performance evaluation</article-title><source><italic>Pattern Recognition Letters</italic></source><year>2000</year><volume>21</volume><issue>13-14</issue><fpage>1157</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/s0167-8655(00)00077-5</pub-id><pub-id pub-id-type="other">2-s2.0-0343826098</pub-id></element-citation></ref><ref id="B26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>W.</given-names></name><name><surname>Tan</surname><given-names>J.</given-names></name></person-group><article-title>Detection of incomplete ellipse in images with strong noise by iterative randomized Hough transform (IRHT)</article-title><source><italic>Pattern Recognition</italic></source><year>2008</year><volume>41</volume><issue>4</issue><fpage>1268</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2007.09.006</pub-id></element-citation></ref><ref id="B27"><label>27</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Baldas</surname><given-names>V.</given-names></name><name><surname>Tang</surname><given-names>L.</given-names></name><name><surname>Bountris</surname><given-names>P.</given-names></name><name><surname>Saleh</surname><given-names>G.</given-names></name><name><surname>Koutsouris</surname><given-names>D.</given-names></name></person-group><article-title>A real-time automatic instrument tracking system on cataract surgery videos for dexterity assessment</article-title><conf-name>Proceedings of the 10th IEEE International Conference on Information Technology and Applications in Biomedicine (ITAB &#x02019;10)</conf-name><conf-date>November 2010</conf-date><conf-loc>Corfu Island, Greece</conf-loc><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/itab.2010.5687643</pub-id><pub-id pub-id-type="other">2-s2.0-79951662198</pub-id></element-citation></ref><ref id="B28"><label>28</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bouguet</surname><given-names>J. Y.</given-names></name></person-group><source><italic>Pyramidal Implementation of the Lucas Kanade Feature Tracker</italic></source><year>2000</year><publisher-name>Intel Corporation, Microprocessor Research Labs</publisher-name></element-citation></ref><ref id="B29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arulampalam</surname><given-names>M. S.</given-names></name><name><surname>Maskell</surname><given-names>S.</given-names></name><name><surname>Gordon</surname><given-names>N.</given-names></name><name><surname>Clapp</surname><given-names>T.</given-names></name></person-group><article-title>A tutorial on particle filters for online nonlinear/non-gaussian bayesian tracking</article-title><source><italic>IEEE Transactions on Signal Processing</italic></source><year>2002</year><volume>50</volume><issue>2</issue><fpage>174</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1109/78.978374</pub-id><pub-id pub-id-type="other">2-s2.0-0036475447</pub-id></element-citation></ref><ref id="B30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avidan</surname><given-names>S.</given-names></name></person-group><article-title>Support vector tracking</article-title><source><italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic></source><year>2004</year><volume>26</volume><issue>8</issue><fpage>1064</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.1109/tpami.2004.53</pub-id><pub-id pub-id-type="other">2-s2.0-3242681758</pub-id><pub-id pub-id-type="pmid">15641735</pub-id></element-citation></ref><ref id="B31"><label>31</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kalal</surname><given-names>Z.</given-names></name><name><surname>Mikolajczyk</surname><given-names>K.</given-names></name><name><surname>Matas</surname><given-names>J.</given-names></name></person-group><article-title>Forward-backward error: automatic detection of tracking failures</article-title><conf-name>Proceedings of the 20th International Conference on Pattern Recognition (ICPR &#x02019;10)</conf-name><conf-date>August 2010</conf-date><conf-loc>Istanbul, Turkey</conf-loc><publisher-name>IEEE</publisher-name><fpage>2756</fpage><lpage>2759</lpage><pub-id pub-id-type="doi">10.1109/icpr.2010.675</pub-id><pub-id pub-id-type="other">2-s2.0-78149481816</pub-id></element-citation></ref><ref id="B32"><label>32</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kalal</surname><given-names>Z.</given-names></name><name><surname>Matas</surname><given-names>J.</given-names></name><name><surname>Mikolajczyk</surname><given-names>K.</given-names></name></person-group><article-title>P-N learning: bootstrapping binary classifiers by structural constraints</article-title><conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR &#x02019;10)</conf-name><conf-date>June 2010</conf-date><conf-loc>San Francisco, Calif, USA</conf-loc><publisher-name>IEEE</publisher-name><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1109/cvpr.2010.5540231</pub-id><pub-id pub-id-type="other">2-s2.0-77956005443</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>The framework V<sc>e</sc>BIRD, a Vid<sc>e</sc>o-Based Intelligent Recognition and Decision system, for the phacoemulsification cataract surgery, where the microscope videos record the operation process in the patient's eye by the surgeon in real time. This framework mainly comprises eye detection, probe tracking, and cataract grading.</p></caption><graphic xlink:href="CMMM2015-202934.001"/></fig><fig id="fig2" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The detection results with the randomized Hough transform methods: (a) the input image with heavy noise, (b) the detection result (with red color) with the standard randomized Hough transform, and (c) the result with the adaptive randomized Hough transform.</p></caption><graphic xlink:href="CMMM2015-202934.002"/></fig><fig id="fig3" orientation="portrait" position="float"><label>Figure 3</label><caption><p>The adaptive strategy in the proposed randomized Hough transform method, where the ellipse with the full line exists while the one with the dashed line does not, and the dashed one is determined by <italic>P</italic>
<sub>1</sub> ~ <italic>P</italic>
<sub>5</sub>.</p></caption><graphic xlink:href="CMMM2015-202934.003"/></fig><fig id="fig4" orientation="portrait" position="float"><label>Figure 4</label><caption><p>The (example) eye detection procedure in V<sc>e</sc>BIRD: (a) the original image of the cataract surgery, (b) the result with edge detection, (c) the denoised result, and (d) the final result with the improved randomized Hough transform approach.</p></caption><graphic xlink:href="CMMM2015-202934.004"/></fig><fig id="fig5" orientation="portrait" position="float"><label>Figure 5</label><caption><p>Comparison results between the standard randomized Hough transform method and the improved one: (a) and (b) are the results from the standard method, while (c) and (d) are the corresponding results detected by the improved method.</p></caption><graphic xlink:href="CMMM2015-202934.005"/></fig><fig id="fig6" orientation="portrait" position="float"><label>Figure 6</label><caption><p>Probe tracking example results in V<sc>e</sc>BIRD: (a) and (b) are the results from Tracking-Learning-Detection (TLD) and the adaptive TLD methods, respectively. The two rows (from left to right) show the tracking results in frames 1130, 1207, 1222, and 1223 of Video 1.</p></caption><graphic xlink:href="CMMM2015-202934.006"/></fig><fig id="fig7" orientation="portrait" position="float"><label>Figure 7</label><caption><p>Feature representation with different color information for cataract grades.</p></caption><graphic xlink:href="CMMM2015-202934.007"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Grades of the lens nuclear hardness with the Emery-Little classification strategy in phacoemulsification and feature representation (color information) of the tissue image.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Classification </th><th align="left" rowspan="1" colspan="1"> Medicine name </th><th align="left" rowspan="1" colspan="1"> Lens nuclear color </th><th align="center" rowspan="1" colspan="1">Label </th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"> Normal tissue </td><td align="left" rowspan="1" colspan="1">&#x02014; </td><td align="left" rowspan="1" colspan="1">&#x02014;</td><td align="center" rowspan="1" colspan="1"> 0 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade I </td><td align="left" rowspan="1" colspan="1"> Very soft nuclear </td><td align="left" rowspan="1" colspan="1"> Transparent and nonnuclear </td><td align="center" rowspan="1" colspan="1"> 1 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade II </td><td align="left" rowspan="1" colspan="1"> Soft nuclear </td><td align="left" rowspan="1" colspan="1"> Yellow or yellow-white </td><td align="center" rowspan="1" colspan="1"> 2 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade III </td><td align="left" rowspan="1" colspan="1"> Medium hard nuclear </td><td align="left" rowspan="1" colspan="1"> Dark yellow </td><td align="center" rowspan="1" colspan="1"> 3 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade IV </td><td align="left" rowspan="1" colspan="1"> Hard nuclear </td><td align="left" rowspan="1" colspan="1"> Brown or amber </td><td align="center" rowspan="1" colspan="1"> 4 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade V </td><td align="left" rowspan="1" colspan="1"> Extremely hard nuclear </td><td align="left" rowspan="1" colspan="1"> Dark brown or black </td><td align="center" rowspan="1" colspan="1"> 5 </td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>The training and testing sets in the experiments for eye detection, probe tracking, and cataract grading.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1"> Training set </th><th align="center" rowspan="1" colspan="1"> Testing set </th></tr></thead><tbody><tr><td align="center" colspan="3" rowspan="1"> Eye detection (video frames) </td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1"> 1000 </td><td align="center" rowspan="1" colspan="1"> 1000 </td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="center" colspan="3" rowspan="1"> Probe tracking (videos) </td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02009;</td><td align="center" rowspan="1" colspan="1"> Video 1 </td><td align="center" rowspan="1" colspan="1"> Videos 2, 3, 4, and 5 </td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="center" colspan="3" rowspan="1"> Cataract grading (video frames) </td></tr><tr><td align="left" rowspan="1" colspan="1"> Normal tissue </td><td align="center" rowspan="1" colspan="1"> 446 </td><td align="center" rowspan="1" colspan="1"> 444 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade I </td><td align="center" rowspan="1" colspan="1"> 293 </td><td align="center" rowspan="1" colspan="1"> 296 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade II </td><td align="center" rowspan="1" colspan="1"> 184 </td><td align="center" rowspan="1" colspan="1"> 183 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade III </td><td align="center" rowspan="1" colspan="1"> 37 </td><td align="center" rowspan="1" colspan="1"> 33 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade IV </td><td align="center" rowspan="1" colspan="1"> 28 </td><td align="center" rowspan="1" colspan="1"> 30 </td></tr><tr><td align="left" rowspan="1" colspan="1">Grade V </td><td align="center" rowspan="1" colspan="1"> 12 </td><td align="center" rowspan="1" colspan="1"> 14 </td></tr><tr><td align="center" colspan="3" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1"> Total </td><td align="center" rowspan="1" colspan="1"> 1000 </td><td align="center" rowspan="1" colspan="1"> 1000 </td></tr></tbody></table></table-wrap><table-wrap id="tab3" orientation="portrait" position="float"><label>Table 3</label><caption><p>The accuracy of eye detection with different numbers of sampling points in video frames on the testing set.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" rowspan="1" colspan="1"> Sampling number </td><td align="center" rowspan="1" colspan="1"> 3000 </td><td align="center" rowspan="1" colspan="1"> 5000 </td><td align="center" rowspan="1" colspan="1"> 8000 </td><td align="center" rowspan="1" colspan="1"> 10000 </td></tr><tr><td align="center" colspan="5" rowspan="1">
<hr/>
</td></tr><tr><td align="left" rowspan="1" colspan="1"> Accuracy (%) </td><td align="center" rowspan="1" colspan="1"> 77.2 </td><td align="center" rowspan="1" colspan="1"> 84.1 </td><td align="center" rowspan="1" colspan="1"> 89.4 </td><td align="center" rowspan="1" colspan="1"> 92.3 </td></tr></tbody></table></table-wrap><table-wrap id="tab4" orientation="portrait" position="float"><label>Table 4</label><caption><p>The performance of probe tracking on the testing set of the standard Tacking-Learning-Detection (TLD) and the adaptive TLD methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1"> Video </th><th align="center" rowspan="1" colspan="1"> Precision of adaptive TLD (%) </th><th align="center" rowspan="1" colspan="1"> Precision of TLD (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"> 2 </td><td align="center" rowspan="1" colspan="1"> 84.5 </td><td align="center" rowspan="1" colspan="1"> 61.2</td></tr><tr><td align="left" rowspan="1" colspan="1">3 </td><td align="center" rowspan="1" colspan="1"> 91.4 </td><td align="center" rowspan="1" colspan="1"> 15.5</td></tr><tr><td align="left" rowspan="1" colspan="1">4 </td><td align="center" rowspan="1" colspan="1"> 98.2 </td><td align="center" rowspan="1" colspan="1"> 82.0</td></tr><tr><td align="left" rowspan="1" colspan="1">5 </td><td align="center" rowspan="1" colspan="1"> 76.2 </td><td align="center" rowspan="1" colspan="1"> 64.4</td></tr></tbody></table></table-wrap><table-wrap id="tab5" orientation="portrait" position="float"><label>Table 5</label><caption><p>The recognition rates of cataract grading (including cataract identification and hardness classification) with <italic>K</italic> Nearest Neighbor Classifiers (<italic>K</italic>NN) and Support Vector Machines (SVM).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1"> Identification </th><th align="center" rowspan="1" colspan="1"> Classification </th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic>K</italic>NN </td><td align="center" rowspan="1" colspan="1">&#x02014; </td><td align="center" rowspan="1" colspan="1"> 92.5% </td></tr><tr><td align="left" rowspan="1" colspan="1">SVM </td><td align="center" rowspan="1" colspan="1"> 99.2% </td><td align="center" rowspan="1" colspan="1"> 96.3% </td></tr></tbody></table></table-wrap></floats-group></article>
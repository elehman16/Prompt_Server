<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Med Internet Res</journal-id><journal-id journal-id-type="iso-abbrev">J. Med. Internet Res</journal-id><journal-id journal-id-type="publisher-id">JMIR</journal-id><journal-title-group><journal-title>Journal of Medical Internet Research</journal-title></journal-title-group><issn pub-type="ppub">1439-4456</issn><issn pub-type="epub">1438-8871</issn><publisher><publisher-name>JMIR Publications Inc.</publisher-name><publisher-loc>Toronto, Canada</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26531850</article-id><article-id pub-id-type="pmc">4642384</article-id><article-id pub-id-type="publisher-id">v17i11e247</article-id><article-id pub-id-type="doi">10.2196/jmir.5072</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group><subj-group subj-group-type="article-type"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>Analysis of Documentation Speed Using Web-Based Medical Speech Recognition Technology: Randomized Controlled Trial</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Eysenbach</surname><given-names>Gunther</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Johnson</surname><given-names>Maree</given-names></name></contrib></contrib-group><contrib-group><contrib id="contrib1" contrib-type="author" corresp="yes"><name><surname>Vogel</surname><given-names>Markus</given-names></name><degrees>Dr med</degrees><xref ref-type="aff" rid="aff1">1</xref><address><institution>University Children&#x02019;s Hospital D&#x000fc;sseldorf</institution><institution>Department of General Pediatrics, Neonatology and Pediatric Cardiology</institution><institution>Heinrich-Heine-University</institution><addr-line>Moorenstrasse 5</addr-line><addr-line>D&#x000fc;sseldorf, 40225</addr-line><country>Germany</country><phone>49 21181 ext 16984</phone><fax>49 2118101516984</fax><email>markus.vogel@med.uni-duesseldorf.de</email></address><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7946-3160</contrib-id></contrib><contrib id="contrib2" contrib-type="author"><name><surname>Kaisers</surname><given-names>Wolfgang</given-names></name><degrees>Dr med</degrees><xref ref-type="aff" rid="aff2">2</xref><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1923-3088</contrib-id></contrib><contrib id="contrib3" contrib-type="author"><name><surname>Wassmuth</surname><given-names>Ralf</given-names></name><degrees>Dr med</degrees><xref ref-type="aff" rid="aff3">3</xref><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9481-3533</contrib-id></contrib><contrib id="contrib4" contrib-type="author"><name><surname>Mayatepek</surname><given-names>Ertan</given-names></name><degrees>Dr med</degrees><xref ref-type="aff" rid="aff1">1</xref><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8460-3738</contrib-id></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>University Children&#x02019;s Hospital D&#x000fc;sseldorf</institution><institution>Department of General Pediatrics, Neonatology and Pediatric Cardiology</institution><institution>Heinrich-Heine-University</institution><addr-line>D&#x000fc;sseldorf</addr-line><country>Germany</country></aff><aff id="aff2"><sup>2</sup><institution>Center of Bioinformatics and Biostatistics</institution><institution>Mathematical Institute</institution><institution>Heinrich-Heine-University</institution><addr-line>D&#x000fc;sseldorf</addr-line><country>Germany</country></aff><aff id="aff3"><sup>3</sup><institution>University Hospital D&#x000fc;sseldorf</institution><institution>Staff Unit Quality Management and Patient Safety</institution><institution>Heinrich-Heine-University</institution><addr-line>D&#x000fc;sseldorf</addr-line><country>Germany</country></aff><author-notes><corresp>Corresponding Author: Markus Vogel <email>markus.vogel@med.uni-duesseldorf.de</email></corresp></author-notes><pub-date pub-type="collection"><month>11</month><year>2015</year></pub-date><pub-date pub-type="epub"><day>03</day><month>11</month><year>2015</year></pub-date><volume>17</volume><issue>11</issue><elocation-id>e247</elocation-id><history><date date-type="received"><day>26</day><month>8</month><year>2015</year></date><date date-type="rev-request"><day>30</day><month>9</month><year>2015</year></date><date date-type="rev-recd"><day>11</day><month>10</month><year>2015</year></date><date date-type="accepted"><day>13</day><month>10</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9;Markus Vogel, Wolfgang Kaisers, Ralf Wassmuth, Ertan Mayatepek. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 03.11.2015. </copyright-statement><copyright-year>2015</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0/"><license-p><!--CREATIVE COMMONS-->This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0/">http://creativecommons.org/licenses/by/2.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on <ext-link ext-link-type="uri" xlink:href="http://www.jmir.org/,">http://www.jmir.org/,</ext-link> as well as this copyright and license information must be included.</license-p></license></permissions><self-uri xlink:type="simple" xlink:href="http://www.jmir.org/2015/11/e247/"/><abstract><sec sec-type="background"><title>Background</title><p>Clinical documentation has undergone a change due to the usage of electronic health records. The core element is to capture clinical findings and document therapy electronically. Health care personnel spend a significant portion of their time on the computer. Alternatives to self-typing, such as speech recognition, are currently believed to increase documentation efficiency and quality, as well as satisfaction of health professionals while accomplishing clinical documentation, but few studies in this area have been published to date.</p></sec><sec sec-type="objective"><title>Objective</title><p>This study describes the effects of using a Web-based medical speech recognition system for clinical documentation in a university hospital on (1) documentation speed, (2) document length, and (3) physician satisfaction.</p></sec><sec sec-type="methods"><title>Methods</title><p>Reports of 28 physicians were randomized to be created with (intervention) or without (control) the assistance of a Web-based system of medical automatic speech recognition (ASR) in the German language. The documentation was entered into a browser&#x02019;s text area and the time to complete the documentation including all necessary corrections, correction effort, number of characters, and mood of participant were stored in a database. The underlying time comprised text entering, text correction, and finalization of the documentation event. Participants self-assessed their moods on a scale of 1-3 (1=good, 2=moderate, 3=bad). Statistical analysis was done using permutation tests.</p></sec><sec sec-type="results"><title>Results</title><p>The number of clinical reports eligible for further analysis stood at 1455. Out of 1455 reports, 718 (49.35%) were assisted by ASR and 737 (50.65%) were not assisted by ASR. Average documentation speed without ASR was 173 (SD 101) characters per minute, while it was 217 (SD 120) characters per minute using ASR. The overall increase in documentation speed through Web-based ASR assistance was 26% (<italic>P</italic>=.04). Participants documented an average of 356 (SD 388) characters per report when not assisted by ASR and 649 (SD 561) characters per report when assisted by ASR. Participants' average mood rating was 1.3 (SD 0.6) using ASR assistance compared to 1.6 (SD 0.7) without ASR assistance (<italic>P</italic>&#x0003c;.001).</p></sec><sec sec-type="conclusions"><title>Conclusions</title><p>We conclude that medical documentation with the assistance of Web-based speech recognition leads to an increase in documentation speed, document length, and participant mood when compared to self-typing. Speech recognition is a meaningful and effective tool for the clinical documentation process.</p></sec></abstract><kwd-group><kwd>electronic health record</kwd><kwd>automatic speech recognition</kwd><kwd>randomized controlled trial</kwd></kwd-group></article-meta></front><body><sec sec-type="introduction"><title>Introduction</title><p>The diagnostic and therapeutic procedures of medical professionals lead to a vast number of observations and decisions, which must be documented correctly to ensure the documentation of the medical course, fulfillment of legal aspects, quality reporting, and billing. The electronic health record (EHR) system plays a critical role in documenting the clinical treatment procedure. The electronic availability of clinical data improves readability, administration, safety, and communication during the course of treatment. On the other hand, electronic health records can interrupt clinical workflows and the treatment procedure, conceivably because of limited availability at bedside. These aspects lead to different beliefs and experiences of health care professionals concerning the general quality of electronic health record systems, clinical day-to-day usability, and user satisfaction [<xref rid="ref1" ref-type="bibr">1</xref>].</p><p>Automatic speech recognition (ASR) systems are believed to facilitate documentation while using the EHR. In clinical specialties with high demands for structured documentation (eg, radiology, pathology), ASR systems are a standard tool today although several studies on ASR in the field of radiology reflect a large amount of heterogeneity [<xref rid="ref2" ref-type="bibr">2</xref>]. In general, when using a current front-end ASR system, the dictated text immediately appears in visible characters on the screen, and medical documentation can be finalized as soon as it has been entered. As a result, the report is available without delays due to corrections or transport of the report.</p><p>Without the availability of a front-end ASR system, the user is forced to wait for a transcriptionist to enter content, to wait for a back-end ASR system to finish the job, or to enter it manually through a keyboard, mouse, or touch screen including all necessary corrections. This already leads to an avoidance of individual documentation by inserting copied text blocks (ie, copy and paste). In an analysis of clinical documentation of an intensive care unit, 82% of documentation contained at least 20% inserted text blocks [<xref rid="ref3" ref-type="bibr">3</xref>]. Clinical information can be lost due to insufficient adaptation and weighting of the inserted content. Besides, unforeseeable legal issues could be suspected [<xref rid="ref4" ref-type="bibr">4</xref>].</p><p>Front-end ASR systems require the user to interact directly. Since ASR systems compare the user&#x02019;s audio information with predefined patterns, recognition accuracy depends on correct grammar, consistent pronunciation, and constant feedback on new words or abbreviations. Therefore, users are urged to correct errors during report generation interactively in order to feed machine learning mechanisms.</p><p>Comparative analysis and synthesis of studies covering the usefulness of ASR in various clinical settings is challenging due to a narrative presentation of the results [<xref rid="ref5" ref-type="bibr">5</xref>]. In 2003, a randomized controlled trial was undertaken outside radiology to compare back-end speech recognition with standard transcription, which failed to find an overall benefit [<xref rid="ref6" ref-type="bibr">6</xref>]. Now, with the advent of new technologies, front-end ASR is available instantly in all areas and specialties of medicine in different languages [<xref rid="ref7" ref-type="bibr">7</xref>]. But the effects of using current front-end ASR in a clinical setting on documentation speed, document length, and physician satisfaction are not known, thereby indicating the need for explorative studies on the topic.</p><p>We hypothesize that the addition of a Web-based, front-end ASR system to the clinical documentation process leads to an increase in documentation speed and documentation amount, and thereby increased physician satisfaction. To measure the effects of using a Web-based, front-end ASR system on documentation speed, document length, and physician satisfaction, we conducted a prospective randomized controlled trial. Documentation time, the number of documented characters, and physician satisfaction have been analyzed for keyboard and speech input in the German language. No changes have been made to any other aspect of the clinical documentation process.</p></sec><sec sec-type="methods"><title>Methods</title><p>The study was not registered in a World Health Organization (WHO)-accredited trial registry since there was no applicable biomedical or health outcome conforming to any human subject or ethics review regulations, or regulations of the national or regional health authority.</p><sec><title>Study Design</title><p>Physicians from the Department of Pediatrics and the Department of Trauma Surgery, D&#x000fc;sseldorf University Hospital, Germany, were asked to participate in morning meetings. Two participants were asked to participate via personal communication. Enrollment was possible over a period of 30 days. The inclusion criteria were clinical activity of the participating physicians and documentation of at least two clinical reports within the study period.</p><p>All participants signed their informed consent forms. Each participant was known to the study team in person. Through the enrollment, the physician chose an individual username and password, not known to the study team, to access the study website. Thereafter, the users identified themselves using a username and a password. After written informed consent was obtained and the privacy policy signed, the password-protected, browser-based, study analysis home page was activated. The username was replaced with a number when storing study information in the database. The study was conducted by approval from, and according to requirements of, the Health Privacy Commissioner of D&#x000fc;sseldorf University Hospital (see <xref ref-type="app" rid="app1">Multimedia Appendix 1</xref> for the CONSORT-EHEALTH checklist).</p><p>During a short, standardized technical training session, all participants documented a uniform standard text once using speech and once using a keyboard to assess individual speed levels. After that, no further contact between the study team and the participants occurred until the end of the study period of 120 days.</p><p>All participants were asked to do everyday clinical documentation in a browser&#x02019;s text area. To complete a study step, the participant opened a webpage, logged in, and documented the clinical finding, report, or discharge letter in the browser&#x02019;s text area. After completion, the text was manually copied into the EHR. For each study step, the length of the text, documentation time including necessary corrections, correction-associated usage of keyboard, and physician&#x02019;s moods were captured using JavaScript. Each participant received the intervention in a random sequence. For each study step, log-in, or refresh of the webpage, a randomization occurred between the availability of speech recognition and the keyboard, or the keyboard alone.</p><p>After all necessary corrections (misspellings, misrecognitions, etc) to achieve a correct text, the study step (ie, the clinical report) was finished by hitting one of three smileys to indicate the mood. The physician's mood was measured using a 3-point scale (1 = good, 2 = moderate, 3 = bad) by online self-assessment on the study webpage. Hitting one of three smileys lead to the appearance of a copy button. Finalization of a study step and the transfer of captured data to the storage database were achieved by hitting the copy button. This action placed the text in the clipboard and simultaneously triggered a new randomization. Depending on the randomization result, the browser loaded a script that enabled medical speech recognition in addition to conventional keyboard text entry. Closing the session without hitting the copy button or direct log-out lead to exclusion of the report for further analysis (see <xref ref-type="fig" rid="figure1">Figure 1</xref>).</p><p>Writing speed was calculated by the number of characters per minute. The underlying time frame was the text entry time and corrections until finalization of the document. Numerical measures are mentioned in the text as mean (SD). To reduce the impact of technical artifacts (eg, by inserting text blocks or by interrupting text entry without finalizing the study documentation step), documentations with greater than 1000 characters per minute, more than 1-hour documentation time, or fewer than 10 characters have been excluded.</p><p>During the study period, a Web-based medical speech recognition system has been used (Nuance SpeechAnywhereServices Browser SDK, SpeechAnywhereServices 1.6/ SpeechMagic Version 7 Release 4 FP4, MultiMed 510.706). The system was available on any clinical desktop computer having a microphone, Microsoft Silverlight installed, and access to the network. No modifications to the physician&#x02019;s computer were made except the addition of a USB microphone (Samson Go Mic clip-on USB microphone). The only limitation was the restriction of usage for medical documentation only. The participants conducted their documentation based on their own needs. They were not allowed to use the system for private communication. It was not possible to insert text blocks by voice commands.</p><p>For each report, information including a time stamp had been saved for further analysis. The study information contained the following: current number, user ID, time stamp, session time, delete key count, backspace key count, arrow key count, mouse left-click count, total number of characters, self-assessment of mood, and type of session (intervention or control). The information was transmitted to a database using a Secure Sockets Layer (SSL) protocol. The end point of the study was the end of the study period.</p><fig id="figure1" position="float"><label>Figure 1</label><caption><p>Completion of a study step (ie, clinical report), webpage layout of intervention and control, randomization procedure, and time measurement. Please note the existence of a speech plug-in during the intervention (grey bar, lower left corner of intervention webpage). Starting time count also starts the other counters used (ie, delete key, backspace key, arrow keys, mouse left click). The copy button will appear after the participant hits a smiley for self-assessment of mood. This action copies the text onto the clipboard for further usage in the EHR.</p></caption><graphic xlink:href="jmir_v17i11e247_fig1"/></fig></sec><sec><title>Statistical Analysis</title><p>Calculation of numerical results, statistical tests, and creation of images were performed using R version 3.2.1 (The R Foundation, Vienna, Austria) [<xref rid="ref8" ref-type="bibr">8</xref>] using a permutation test.</p></sec></sec><sec sec-type="results"><title>Results</title><p>During the recruitment period, 40 physicians asked for participation. Out of 40 physicians, 37 (93%) met the inclusion criteria. Out of 37 participants, 7 (19%) could not participate after initial enrollment because of organizational reasons. Out of 30 participants, 2 (7%) were excluded later because fewer than two documents were completed using the study system.</p><p>The 28 (100%) final participants were comprised of 21 (75%) interns and 7 (25%) senior physicians. A total of 17 out of 28 (61%) participants were male and 11 (39%) were female. A total of 22 out of 28 (79%) participants were from a nonsurgery department and 6 (21%) were from a surgery department (see <xref ref-type="table" rid="table1">Table 1</xref>). All participants were native German speakers. No participant was a trained typist or had professional exposure to speech recognition systems before (see <xref ref-type="fig" rid="figure2">Figure 2</xref>).</p><table-wrap id="table1" position="float"><label>Table 1</label><caption><p>Participant characteristics (n=28).</p></caption><table frame="hsides" rules="groups" width="454" border="1" cellpadding="8" cellspacing="0"><col width="276" span="1"/><col width="144" span="1"/><thead><tr valign="top"><td rowspan="1" colspan="1">Participant characteristics</td><td rowspan="1" colspan="1">n (%)</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">All participants</td><td rowspan="1" colspan="1">28 (100)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Male</td><td rowspan="1" colspan="1">17 (61)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Female</td><td rowspan="1" colspan="1">11 (39)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Number of senior physicians</td><td rowspan="1" colspan="1">7 (25)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Surgery</td><td rowspan="1" colspan="1">6 (21)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Nonsurgery</td><td rowspan="1" colspan="1">22 (79)</td></tr></tbody></table></table-wrap><p>Over a period of 120 days, 1455 of 1689 (86.15%) recorded clinical reports from 28 participants met the inclusion criteria. A total of 234 reports out of 1455 (16.08%) were excluded because documentation speed was greater than 1000 characters per minute, documentation time was more than 1 hour, or the report contained fewer than 10 characters (see <xref ref-type="fig" rid="figure2">Figure 2</xref>). A total of 718 out of 1455 (49.35%) clinical reports were done using speech and 737 (50.65%) were done using the keyboard alone. <xref ref-type="fig" rid="figure3">Figure 3</xref> shows the number of documentations per participant.</p><p>The average documentation speeds until the finalization of the report, including all corrections, were 173 (SD 101) characters per minute in the keyboard only (control) group, and 217 (SD 120) characters per minute in the speech-assisted (intervention) group. The documentation speed was increased by 25.7% in the speech group (<italic>P</italic>=.04, permutation test). The distribution of speed values is shown in <xref ref-type="fig" rid="figure4">Figure 4</xref>. Using the keyboard exclusively, an average of 356 (SD 388) characters per report were entered compared to 649 (SD 561) characters using speech entry. After the documentation, the physicians' average mood ratings were 1.6 (SD 0.7) using keyboard alone and 1.3 (SD 0.6) when using speech recognition (<italic>P</italic>&#x0003c;.001, permutation test). <xref ref-type="table" rid="table2">Table 2</xref> shows a complete reference of captured data for productive use during the study period, and <xref ref-type="table" rid="table3">Table 3</xref> shows a complete reference of captured data of standardized text.</p><table-wrap id="table2" position="float"><label>Table 2</label><caption><p>Captured data during productive use (n=1455)<sup>a</sup>.</p></caption><table frame="hsides" rules="groups" width="595" border="1" cellpadding="7" cellspacing="0"><col width="345" span="1"/><col width="100" span="1"/><col width="106" span="1"/><thead><tr valign="top"><td rowspan="1" colspan="1">Captured data: productive use</td><td rowspan="1" colspan="1">Keyboard only</td><td rowspan="1" colspan="1">Speech assisted</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">Number of reports/documentations, n (%)</td><td rowspan="1" colspan="1">737 (50.65)</td><td rowspan="1" colspan="1">718 (49.35)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Total number of characters</td><td rowspan="1" colspan="1">262,080</td><td rowspan="1" colspan="1">465,785</td></tr><tr valign="top"><td rowspan="1" colspan="1">Total documentation time</td><td rowspan="1" colspan="1">37h 18min</td><td rowspan="1" colspan="1">55h 24min</td></tr><tr valign="top"><td rowspan="1" colspan="1">Number of characters per report, mean (SD)</td><td rowspan="1" colspan="1">356 (388)</td><td rowspan="1" colspan="1">649 (561)</td></tr><tr valign="bottom"><td rowspan="1" colspan="1">Number of delete key strokes<sup>b</sup>, mean (SD)</td><td rowspan="1" colspan="1">0.3 (1.2)</td><td rowspan="1" colspan="1">4.2 (9.5)</td></tr><tr valign="bottom"><td rowspan="1" colspan="1">Number of backspace key strokes<sup>b</sup>, mean (SD)</td><td rowspan="1" colspan="1">25.7 (41.8)</td><td rowspan="1" colspan="1">10.3 (16.5)</td></tr><tr valign="bottom"><td rowspan="1" colspan="1">Number of arrow key strokes<sup>b</sup>, mean (SD)</td><td rowspan="1" colspan="1">3.0 (7.1)</td><td rowspan="1" colspan="1">5.8 (15.2)</td></tr><tr valign="bottom"><td rowspan="1" colspan="1">Number of mouse left clicks<sup>b</sup>, mean (SD)</td><td rowspan="1" colspan="1">2.8 (4.0)</td><td rowspan="1" colspan="1">11.4 (13.6)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Mood rating (1=good, 2=moderate, 3=bad), mean (SD)</td><td rowspan="1" colspan="1">1.6 (0.7)</td><td rowspan="1" colspan="1">1.3 (0.6)</td></tr></tbody></table><table-wrap-foot><fn id="table2fn1"><p>
<sup>a</sup>Please note the absolute numbers in <xref ref-type="table" rid="table2">Table 2</xref> versus the relative numbers in <xref ref-type="fig" rid="figure6">Figure 6</xref>.</p></fn><fn id="table2fn2"><p>
<sup>b</sup>The listed key strokes are necessary correction events to produce a final report.</p></fn></table-wrap-foot></table-wrap><table-wrap id="table3" position="float"><label>Table 3</label><caption><p>Captured data during standard text entry (n=60)<sup>a</sup>.</p></caption><table frame="hsides" rules="groups" width="661" border="1" cellpadding="7" cellspacing="0"><col width="339" span="1"/><col width="136" span="1"/><col width="142" span="1"/><thead><tr valign="bottom"><td rowspan="1" colspan="1">Captured data: standard text<sup>b</sup>
</td><td rowspan="1" colspan="1">Keyboard only (n=30),<break/>mean (SD)</td><td rowspan="1" colspan="1">Speech assisted (n=30),<break/>mean (SD)</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">Duration (s)</td><td rowspan="1" colspan="1">376 (176)</td><td rowspan="1" colspan="1">339 (175)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Number of characters</td><td rowspan="1" colspan="1">939 (10)</td><td rowspan="1" colspan="1">956 (8)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Number of delete key strokes</td><td rowspan="1" colspan="1">0.8 (2.3)</td><td rowspan="1" colspan="1">5.6 (8.0)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Number of backspace key strokes</td><td rowspan="1" colspan="1">26.8 (15.0)</td><td rowspan="1" colspan="1">14.8 (15.7)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Number of arrow key strokes</td><td rowspan="1" colspan="1">6.2 (10.6)</td><td rowspan="1" colspan="1">13.9 (22.5)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Number of mouse left clicks</td><td rowspan="1" colspan="1">3.8 (4.4)</td><td rowspan="1" colspan="1">11.3 (10.1)</td></tr><tr valign="top"><td rowspan="1" colspan="1">Mood rating (1=good, 2=moderate, 3=bad)</td><td rowspan="1" colspan="1">1.6 (0.7)</td><td rowspan="1" colspan="1">1.3 (0.6)</td></tr></tbody></table><table-wrap-foot><fn id="table3fn1"><p>
<sup>a</sup>Please note the absolute numbers in <xref ref-type="table" rid="table3">Table 3</xref> versus the relative numbers in <xref ref-type="fig" rid="figure6">Figure 6</xref>.</p></fn><fn id="table3fn2"><p>
<sup>b</sup>Each participant entered the standard text twice (control method and intervention method) and applied corrections to generate a correct text.</p></fn></table-wrap-foot></table-wrap><p>While documenting a standardized text, 17 out of 28 (61%) participants were faster using speech. During productive use, 22 out of 28 participants (79%) were faster when using speech recognition (see <xref ref-type="fig" rid="figure5">Figure 5</xref>). The individual rate of corrections&#x02014;sum of correction actions (delete, backspace, arrow keys, and mouse left click) per number of characters per report&#x02014;was lower for speech-assisted documentation. The total number of characters per report was higher in the intervention group (speech recognition) (see <xref ref-type="fig" rid="figure6">Figure 6</xref>).</p><p>The measured total time of documentation was 37 hours and 18 minutes for the control group and 55 hours and 24 minutes for the intervention group. Using keyboard alone, 262,080 characters were entered into the study system compared to 465,785 characters during speech recognition availability. <xref ref-type="table" rid="table1">Tables 1</xref> and <xref ref-type="table" rid="table2">2</xref> show an overview of captured data, including correction effort which is defined as the recorded keyboard strokes of the delete, backspace, and arrow keys, as well as mouse left clicks. Comparing control and intervention groups, the results show a significant increase in documentation speed, document length, and physician satisfaction. They also show a decreased correction rate and an increase in total documentation time secondary to the increased documentation amount.</p><fig id="figure2" position="float"><label>Figure 2</label><caption><p>CONSORT-EHEALTH flowchart of enrollment, participants, and report status.</p></caption><graphic xlink:href="jmir_v17i11e247_fig2"/></fig><fig id="figure3" position="float"><label>Figure 3</label><caption><p>Box plot of the number of documentations per participant (total n=1455).</p></caption><graphic xlink:href="jmir_v17i11e247_fig3"/></fig><fig id="figure4" position="float"><label>Figure 4</label><caption><p>Distribution of documentation speed in characters per minute.</p></caption><graphic xlink:href="jmir_v17i11e247_fig4"/></fig><fig id="figure5" position="float"><label>Figure 5</label><caption><p>Per person analysis of documentation speed while documenting a standard text and during productive use in characters per minute. Each dot represents one participant. The location of the dot indicates the documentation speed of control and intervention. A location above the dotted line indicates a gain in speed using the intervention (speech-assisted documentation). The dots representing the standard text consist of one initial documentation pair while the dots representing productive use consist of all available data for each participant. The labeling of dots with numbers is for better comparison of both plots within the figure. Please note that documentation of the standard text reflects the individual&#x02019;s typing capabilities on the x-axis and the individual&#x02019;s initial capabilities in using the ASR system on the y-axis.</p></caption><graphic xlink:href="jmir_v17i11e247_fig5"/></fig><fig id="figure6" position="float"><label>Figure 6</label><caption><p>Per person analysis of correction rate and number of characters. Each dot represents one participant. The location of the dot indicates the sum of corrections per documented character (see text for further details) of control and intervention and the number of characters per report of control and intervention. A location above the dotted line indicates increased correction effort and increased number of characters per report when using the intervention.</p></caption><graphic xlink:href="jmir_v17i11e247_fig6"/></fig></sec><sec sec-type="discussion"><title>Discussion</title><sec><title>Principal Findings</title><p>Digital communication and electronic information exchange have become a part of everyday communication. Clinical documentation is a core aspect of the clinical profession and is more than a tool for efficient, maximized billing [<xref rid="ref9" ref-type="bibr">9</xref>]. Nevertheless, in medicine the emphasis lies on paper-based documentation, being only assisted by electronic documentation. The reasons for this are the deficient quality and usability of electronic clinical documentation tools [<xref rid="ref1" ref-type="bibr">1</xref>].</p><p>To enter information into the EHR, physicians rely on their ability to type or on the assistance of transcriptionists, but medical transcriptionists are a limited resource (eg, usually not available at the point of care or during nighttime). Therefore, the only alternatives for clinicians are pen-and-paper documentation, self-typing, or avoidance of documentation.</p><p>The objective of this study was to compare the impact of electronic speech recognition with self-typing based on measurement of documentation speed and volume, and user satisfaction. This explorative study based on 1455 captured medical documents demonstrated that the availability of Web-based medical speech recognition led to increased documentation speed, increased documentation amount, and higher physician satisfaction.</p><p>The study group consisted of native German speakers which favors good results in this German-based speech recognition system. Due to the correct grammar of a native speaker, word chains can be predicted by an ASR system thereby enhancing the recognition result. Foreign speakers face difficulties in using ASR systems mainly because of using incorrect grammar versus having an accent [<xref rid="ref10" ref-type="bibr">10</xref>].</p><p>The study group was more satisfied when speech recognition was used in documentation. The reasons for increased satisfaction could be individual physiological factors like hand posture and typing speed. These factors greatly influence fatigability, finger pain, and various physiological aspects, which could explain different satisfaction levels [<xref rid="ref11" ref-type="bibr">11</xref>]. On the other hand, due to the study group&#x02019;s explorative composition, blocking and selection as well as stratification bias cannot be precluded.</p></sec><sec><title>Requirements for Extensive Automatic Speech Recognition Usage in Clinical Routine</title><p>Electronic documentation tools are available on any desktop computer in hospitals. But the usual technical requirements for using speech recognition drastically reduce availability. The Web-based design of the study made speech recognition available on virtually any microphone-equipped desktop computer with a network connection. Documentation which is available everywhere in a hospital (also at the point of care) and simplified by suitable electronic tools leads to rapidly available individual texts. At the point of care, ASR systems have the greatest advantage over audio recordings since the documented text is electronically available day and night in real time [<xref rid="ref12" ref-type="bibr">12</xref>].</p><p>Assuming availability issues were to be solved, remaining obstacles for widespread use of ASR in hospital settings include insufficient identification of employees with the new technology, slow learning curve, correction efforts, costs, and limited availability of microphones. Individual barriers and different usage types are reflected in the different gains when analyzing Web-based study ASRs on an individual level [<xref rid="ref2" ref-type="bibr">2</xref>,<xref rid="ref13" ref-type="bibr">13</xref>-<xref rid="ref15" ref-type="bibr">15</xref>].</p><p>Improved human-machine interfaces, such as speech recognition or touch screen entry, could change the paradigm of paper-based documentation to full electronic documentation. This study addresses the question of whether typing alone or a combination of typing and speaking is a suitable human-machine interface in a general clinical setting. Autonomous clinical observations documented in real time are of utmost importance for the treatment process, and the acceptance of electronic documentation depends on the availability of these observations [<xref rid="ref16" ref-type="bibr">16</xref>].</p></sec><sec><title>Electronic Documentation</title><p>Despite obvious advantages, the change to complete electronic documentation is a matter of ongoing discussion; on one hand, electronic documentation promises increased efficiency and improvements in patient treatment through availability and readability. On the other hand, there are concerns of exchanging information electronically, such as the tracking of data to an individual, general data security, or compliance with local regulatory requirements [<xref rid="ref4" ref-type="bibr">4</xref>]. An insufficient adaptation of systems for specific clinical requirements (eg, in pediatrics) immediately reduces the clinical usefulness, leading to avoidance and thereby manifesting the status quo [<xref rid="ref17" ref-type="bibr">17</xref>].</p><p>In general, the use of electronic documentation in creating a clinical document is a multistage process that starts even before the patient has been seen by the physician: copying and pasting of personal information, importing of lab values, importing of the radiologist&#x02019;s reports, or findings and reports of colleagues [<xref rid="ref18" ref-type="bibr">18</xref>]. This can lead to fast but insufficiently individualized documentation, which can be troublesome. It is likely that certain clinical documentation tasks like informed consent will be exclusively documented electronically in the near future [<xref rid="ref19" ref-type="bibr">19</xref>]. Electronic documentation may lead to a more complete documentation [<xref rid="ref20" ref-type="bibr">20</xref>]. Consistent with this finding, the availability of a tool for the production of more complete documentation may be an explanation for the observed increase in documentation volume in our study.</p></sec><sec><title>Strengths and Limitations</title><p>The strength of this study was being able to describe the effects of ASR availability on the clinical documentation process based on detailed data recorded by a computer program. This contrasts a significant number of studies on the topic which rely on perception-based data [<xref rid="ref1" ref-type="bibr">1</xref>]. We present no proposal for a generally optimized clinical documentation process. The intervention affected only the self-typing by the medical personnel [<xref rid="ref2" ref-type="bibr">2</xref>]. Provision via browser windows limited ASR availability issues to the presence of microphones. For study purposes, all computers involved in the study were equipped with a clip-on USB microphone.</p><p>Until recently, ASR training phases for new users were common. This was not done in this study system. Corrective user actions induce ASR system adaptation. Depending on the dictation style and the contents of the dictation, the errors made by ASR can be numerous. Therefore, different correction efforts and usage types could explain the scattering in <xref ref-type="fig" rid="figure6">Figure 6</xref>. This scattering was induced by the user, and the reaction of the system to the user. The reports created by the study participants were not evaluated for typos or misspellings. It was not the intention of the study to check the grammar quality of a medical report directly; this might be an objective for a follow-up trial. Another strength of the study was being able to measure the correction effort indirectly by counting correction-related user interactions. Effects resulting from insertion of text blocks were eliminated by limiting documentation speed to a physiologically sensible range.</p><p>As part of the personalization process, corrections have great impact on speech recognition systems. They modify statistical models and potentially add new words to the system. Corrective actions include deletion and insertion of words, replacement of system errors, inserting correct words through the keyboard, and modifying and changing text because of errors caused by voice. Personalization is the basis for good recognition results that are superior to the general recognition results of mobile devices [<xref rid="ref21" ref-type="bibr">21</xref>]. The different usage patterns of using the mouse left-click button and the delete, backspace, and arrow keys were taken as an indicator of errors within the documentation. We emphasize that all necessary corrections, either using keyboard alone or ASR-assisted keyboard usage, were captured and included in the speed measurements. Maximum increased productivity can be reached if a trained system causing few errors is readily available [<xref rid="ref12" ref-type="bibr">12</xref>,<xref rid="ref22" ref-type="bibr">22</xref>].</p><p>Any study on the topic has to consider recruitment-induced biases. For this study, no staff groups or specializations were selected&#x02014;recruitment was based on voluntary participation. A bias can arise easily owing to different specialties, different clinical knowledge, and different experiences with dictation in general [<xref rid="ref10" ref-type="bibr">10</xref>]. <xref ref-type="fig" rid="figure5">Figure 5</xref> illustrates the individual loss or gain when using the speech recognition system compared to self-typing. Overall, 6 individuals lost speed during the study period (productive use speed) while using the speech recognition system. Users 1 and 4 increased speed during productive use compared to standard text when using speech recognition, but due to high typing speed, there was no overall speed effect. User 28 was consistently slow compared to the rest of the group. For Users 18, 24, and 26, both typing speed and speech speed decreased. The underlying reasons for these observations were not explored. Although these 6 participants individually did not increase documentation speed, there was still an overall time-saving effect. Individual factors like slurred speech, dictation style, and dictation content may heavily influence the recognition result.</p><p>Together, all participants documented 727,865 characters (total documentation volume). Documenting this amount by the study group using the keyboard alone would have taken 104 hours. By adding speech to the documentation process, this time decreased to 87 hours. Therefore, the gain on the total documentation time would have been 17 hours. Despite this gain in documentation speed, the notable effect might not be decreased documentation time, but increased documentation volume.</p><p>Tools for efficient capturing of patient data and clinical observations are still underrepresented in clinical practice [<xref rid="ref23" ref-type="bibr">23</xref>]. Even a trained typist will not achieve the same efficiency in capturing patient data on a mobile device&#x02019;s touch screen as speech recognition or a digitizer pen could do day and night.</p></sec><sec><title>Conclusions</title><p>We conclude that medical documentation with the assistance of Web-based speech recognition leads to an increase in documentation speed and amount, and enhances the participant&#x02019;s mood when compared to self-typing. The remarkable effect might not be the time savings, but the increase in documentation volume. This study may be a starting point for further investigations where the overall efficiency of the documentation process, differences due to personal preferences, as well as aspects concerning quality of care and patient safety related to clinical documentation are explored.</p><p>The way medical documentation influences treatment quality needs to be understood better to choose the right mode of documentation and to help both the doctor and the patient. The continued exchange between health care personnel and technicians can facilitate a technological change in hospitals, and encourage technical advances leading to a more patient-centered treatment.</p></sec></sec></body><back><ack><p>Nuance Healthcare International provided the ASR software for this study, but they or their affiliates did not have any influence on the conduction of the study, access to the study data, or writing of the manuscript. The authors thank Sandeep Kosta deeply for skillful programming, and for creating the webpage and the SQL database.</p></ack><fn-group><fn fn-type="con"><p>Authors' Contributions: MV was the principal investigator of the study. MV, WK, RW, and EM were responsible for the study conception and design, interpretation of data, revising the manuscript for intellectual content, and final approval of the published version. MV and WK were responsible for the acquisition of data. WK performed data analysis and MV and WK drafted the paper. MV is the guarantor of this work and, as such, had full access to all the data in the study and takes responsibility for the integrity of the data and the accuracy of the data analysis.</p></fn><fn fn-type="conflict"><p>Conflicts of Interest: MV works as a clinical consultant at Nuance Healthcare International.</p></fn></fn-group><app-group><app id="app1"><title>Multimedia Appendix 1</title><p>CONSORT-EHEALTH (v. 1.6.1) checklist.</p><fig id="d36e817" position="anchor"><media xlink:href="jmir_v17i11e247_app1.pdf"/></fig></app></app-group><glossary><title>Abbreviations</title><def-list><def-item><term id="abb1">ASR</term><def><p>automatic speech recognition</p></def></def-item><def-item><term id="abb2">EHR</term><def><p>electronic health record</p></def></def-item><def-item><term id="abb3">SSL</term><def><p>Secure Sockets Layer</p></def></def-item><def-item><term id="abb4">WHO</term><def><p>World Health Organization</p></def></def-item></def-list></glossary><ref-list><ref id="ref1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>L</given-names></name><name><surname>Bellucci</surname><given-names>E</given-names></name><name><surname>Nguyen</surname><given-names>LT</given-names></name></person-group><article-title>Electronic health records implementation: An evaluation of information system impact and contingency factors</article-title><source>Int J Med Inform</source><year>2014</year><month>11</month><volume>83</volume><issue>11</issue><fpage>779</fpage><lpage>796</lpage><pub-id pub-id-type="doi">10.1016/j.ijmedinf.2014.06.011</pub-id><pub-id pub-id-type="medline">25085286</pub-id><!--<pub-id pub-id-type="pii">S1386-5056(14)00123-3</pub-id>--><pub-id pub-id-type="pmid">25085286</pub-id></element-citation></ref><ref id="ref2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammana</surname><given-names>I</given-names></name><name><surname>Lepanto</surname><given-names>L</given-names></name><name><surname>Poder</surname><given-names>T</given-names></name><name><surname>Bellemare</surname><given-names>C</given-names></name><name><surname>Ly</surname><given-names>M</given-names></name></person-group><article-title>Speech recognition in the radiology department: A systematic review</article-title><source>HIM J</source><year>2015</year><volume>44</volume><issue>2</issue><fpage>4</fpage><lpage>10</lpage><pub-id pub-id-type="medline">26157081</pub-id><pub-id pub-id-type="pmid">26157081</pub-id></element-citation></ref><ref id="ref3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thornton</surname><given-names>JD</given-names></name><name><surname>Schold</surname><given-names>JD</given-names></name><name><surname>Venkateshaiah</surname><given-names>L</given-names></name><name><surname>Lander</surname><given-names>B</given-names></name></person-group><article-title>Prevalence of copied information by attendings and residents in critical care progress notes</article-title><source>Crit Care Med</source><year>2013</year><month>2</month><volume>41</volume><issue>2</issue><fpage>382</fpage><lpage>388</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://europepmc.org/abstract/MED/23263617"/></comment><pub-id pub-id-type="doi">10.1097/CCM.0b013e3182711a1c</pub-id><pub-id pub-id-type="medline">23263617</pub-id><!--<pub-id pub-id-type="pmcid">PMC3718042</pub-id>--><pub-id pub-id-type="pmid">23263617</pub-id></element-citation></ref><ref id="ref4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weis</surname><given-names>JM</given-names></name><name><surname>Levy</surname><given-names>PC</given-names></name></person-group><article-title>Copy, paste, and cloned notes in electronic health records: Prevalence, benefits, risks, and best practice recommendations</article-title><source>Chest</source><year>2014</year><month>3</month><day>1</day><volume>145</volume><issue>3</issue><fpage>632</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1378/chest.13-0886</pub-id><pub-id pub-id-type="medline">24590024</pub-id><!--<pub-id pub-id-type="pii">1833461</pub-id>--><pub-id pub-id-type="pmid">24590024</pub-id></element-citation></ref><ref id="ref5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>M</given-names></name><name><surname>Lapkin</surname><given-names>S</given-names></name><name><surname>Long</surname><given-names>V</given-names></name><name><surname>Sanchez</surname><given-names>P</given-names></name><name><surname>Suominen</surname><given-names>H</given-names></name><name><surname>Basilakis</surname><given-names>J</given-names></name><name><surname>Dawson</surname><given-names>L</given-names></name></person-group><article-title>A systematic review of speech recognition technology in health care</article-title><source>BMC Med Inform Decis Mak</source><year>2014</year><month>10</month><volume>14</volume><fpage>94</fpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1472-6947/14/94"/></comment><pub-id pub-id-type="doi">10.1186/1472-6947-14-94</pub-id><pub-id pub-id-type="medline">25351845</pub-id><!--<pub-id pub-id-type="pii">1472-6947-14-94</pub-id>--><!--<pub-id pub-id-type="pmcid">PMC4283090</pub-id>--><pub-id pub-id-type="pmid">25351845</pub-id></element-citation></ref><ref id="ref6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohr</surname><given-names>D</given-names></name><name><surname>Turner</surname><given-names>D</given-names></name><name><surname>Pond</surname><given-names>GR</given-names></name><name><surname>Kamath</surname><given-names>JS</given-names></name><name><surname>De Vos</surname><given-names>CB</given-names></name><name><surname>Carpenter</surname><given-names>PC</given-names></name></person-group><article-title>Speech recognition as a transcription aid: A randomized comparison with standard transcription</article-title><source>J Am Med Inform Assoc</source><year>2003</year><volume>10</volume><issue>1</issue><fpage>85</fpage><lpage>93</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://jamia.oxfordjournals.org/cgi/pmidlookup?view=long&#x00026;pmid=12509359"/></comment><pub-id pub-id-type="medline">12509359</pub-id><!--<pub-id pub-id-type="pmcid">PMC150361</pub-id>--><pub-id pub-id-type="pmid">12509359</pub-id></element-citation></ref><ref id="ref7"><label>7</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Scheuer</surname><given-names>A</given-names></name></person-group><source>Deutsches &#x000c4;rzteblatt</source><year>2013</year><date-in-citation>2015-08-25</date-in-citation><publisher-loc>Cologne, Germany</publisher-loc><publisher-name>Deutsches &#x000c4;rzteblatt</publisher-name><comment>Spracherkennung in der cloud: Arzt an wolke: Zum diktat, bitte!<ext-link ext-link-type="uri" xlink:href="http://www.aerzteblatt.de/archiv/134911">http://www.aerzteblatt.de/archiv/134911</ext-link></comment><ext-link ext-link-type="webcite" xlink:href="6b3NQha5h"/></element-citation></ref><ref id="ref8"><label>8</label><element-citation publication-type="book"><person-group person-group-type="author"><collab>R Core Team</collab></person-group><source>R Project</source><date-in-citation>2015-10-26</date-in-citation><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><comment>The R Project for statistical computing<ext-link ext-link-type="uri" xlink:href="https://www.r-project.org/">https://www.r-project.org/</ext-link></comment><ext-link ext-link-type="webcite" xlink:href="6cZeJI64W"/></element-citation></ref><ref id="ref9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barr</surname><given-names>MS</given-names></name></person-group><article-title>The clinical record: A 200-year-old 21st-century challenge</article-title><source>Ann Intern Med</source><year>2010</year><month>11</month><day>16</day><volume>153</volume><issue>10</issue><fpage>682</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.7326/0003-4819-153-10-201011160-00015</pub-id><pub-id pub-id-type="medline">21079228</pub-id><!--<pub-id pub-id-type="pii">153/10/682</pub-id>--><pub-id pub-id-type="pmid">21079228</pub-id></element-citation></ref><ref id="ref10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benzeghiba</surname><given-names>M</given-names></name><name><surname>De Mori</surname><given-names>M</given-names></name><name><surname>Deroo</surname><given-names>O</given-names></name><name><surname>Dupont</surname><given-names>S</given-names></name><name><surname>Erbes</surname><given-names>T</given-names></name><name><surname>Jouvet</surname><given-names>D</given-names></name><name><surname>Fissore</surname><given-names>L</given-names></name><name><surname>Laface</surname><given-names>P</given-names></name><name><surname>Mertins</surname><given-names>A</given-names></name><name><surname>Ris</surname><given-names>C</given-names></name><name><surname>Rose</surname><given-names>R</given-names></name><name><surname>Tyagi</surname><given-names>V</given-names></name><name><surname>Wellekens</surname><given-names>C</given-names></name></person-group><article-title>Automatic speech recognition and speech variability: A review</article-title><source>Speech Commun</source><year>2007</year><volume>49</volume><issue>10-11</issue><fpage>763</fpage><lpage>786</lpage></element-citation></ref><ref id="ref11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoneau</surname><given-names>GG</given-names></name><name><surname>Marklin</surname><given-names>RW</given-names></name><name><surname>Berman</surname><given-names>JE</given-names></name></person-group><article-title>Effect of computer keyboard slope on wrist position and forearm electromyography of typists without musculoskeletal disorders</article-title><source>Phys Ther</source><year>2003</year><month>9</month><volume>83</volume><issue>9</issue><fpage>816</fpage><lpage>830</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.ptjournal.org/cgi/pmidlookup?view=long&#x00026;pmid=12940768"/></comment><pub-id pub-id-type="medline">12940768</pub-id><pub-id pub-id-type="pmid">12940768</pub-id></element-citation></ref><ref id="ref12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prevedello</surname><given-names>LM</given-names></name><name><surname>Ledbetter</surname><given-names>S</given-names></name><name><surname>Farkas</surname><given-names>C</given-names></name><name><surname>Khorasani</surname><given-names>R</given-names></name></person-group><article-title>Implementation of speech recognition in a community-based radiology practice: Effect on report turnaround times</article-title><source>J Am Coll Radiol</source><year>2014</year><month>4</month><volume>11</volume><issue>4</issue><fpage>402</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/j.jacr.2013.07.008</pub-id><pub-id pub-id-type="medline">24161456</pub-id><!--<pub-id pub-id-type="pii">S1546-1440(13)00431-6</pub-id>--><pub-id pub-id-type="pmid">24161456</pub-id></element-citation></ref><ref id="ref13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaser</surname><given-names>C</given-names></name><name><surname>Trumm</surname><given-names>C</given-names></name><name><surname>Nissen-Meyer</surname><given-names>S</given-names></name><name><surname>Francke</surname><given-names>M</given-names></name><name><surname>K&#x000fc;ttner</surname><given-names>B</given-names></name><name><surname>Reiser</surname><given-names>M</given-names></name></person-group><article-title>[Speech recognition: Impact on workflow and report availability] [Article in German]</article-title><source>Radiologe</source><year>2005</year><month>8</month><volume>45</volume><issue>8</issue><fpage>735</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1007/s00117-005-1253-7</pub-id><pub-id pub-id-type="medline">16049714</pub-id><pub-id pub-id-type="pmid">16049714</pub-id></element-citation></ref><ref id="ref14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thumann</surname><given-names>P</given-names></name><name><surname>Topf</surname><given-names>S</given-names></name><name><surname>Feser</surname><given-names>A</given-names></name><name><surname>Erfurt</surname><given-names>C</given-names></name><name><surname>Schuler</surname><given-names>G</given-names></name><name><surname>Mahler</surname><given-names>V</given-names></name></person-group><article-title>[Digital speech recognition in dermatology: A pilot study with regard to medical and economic aspects] [Article in German]</article-title><source>Hautarzt</source><year>2008</year><month>2</month><volume>59</volume><issue>2</issue><fpage>131</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1007/s00105-007-1450-6</pub-id><pub-id pub-id-type="medline">18210002</pub-id><pub-id pub-id-type="pmid">18210002</pub-id></element-citation></ref><ref id="ref15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issenman</surname><given-names>RM</given-names></name><name><surname>Jaffer</surname><given-names>IH</given-names></name></person-group><article-title>Use of voice recognition software in an outpatient pediatric specialty practice</article-title><source>Pediatrics</source><year>2004</year><month>9</month><volume>114</volume><issue>3</issue><fpage>e290</fpage><lpage>e293</lpage><pub-id pub-id-type="doi">10.1542/peds.2003-0724-L</pub-id><pub-id pub-id-type="medline">15342888</pub-id><!--<pub-id pub-id-type="pii">114/3/e290</pub-id>--><pub-id pub-id-type="pmid">15342888</pub-id></element-citation></ref><ref id="ref16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiff</surname><given-names>GD</given-names></name><name><surname>Bates</surname><given-names>DW</given-names></name></person-group><article-title>Can electronic clinical documentation help prevent diagnostic errors?</article-title><source>N Engl J Med</source><year>2010</year><month>3</month><day>25</day><volume>362</volume><issue>12</issue><fpage>1066</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1056/NEJMp0911734</pub-id><pub-id pub-id-type="medline">20335582</pub-id><!--<pub-id pub-id-type="pii">362/12/1066</pub-id>--><pub-id pub-id-type="pmid">20335582</pub-id></element-citation></ref><ref id="ref17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehmann</surname><given-names>CU</given-names></name><collab>Council on Clinical Information Technology</collab></person-group><article-title>Pediatric aspects of inpatient health information technology systems</article-title><source>Pediatrics</source><year>2015</year><month>3</month><volume>135</volume><issue>3</issue><fpage>e756</fpage><lpage>e768</lpage><pub-id pub-id-type="doi">10.1542/peds.2014-4148</pub-id><pub-id pub-id-type="medline">25713282</pub-id><!--<pub-id pub-id-type="pii">peds.2014-4148</pub-id>--><pub-id pub-id-type="pmid">25713282</pub-id></element-citation></ref><ref id="ref18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirschtick</surname><given-names>RE</given-names></name></person-group><article-title>A piece of my mind. John Lennon's elbow</article-title><source>JAMA</source><year>2012</year><month>8</month><day>1</day><volume>308</volume><issue>5</issue><fpage>463</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1001/jama.2012.8331</pub-id><pub-id pub-id-type="medline">22851112</pub-id><!--<pub-id pub-id-type="pii">1273021</pub-id>--><pub-id pub-id-type="pmid">22851112</pub-id></element-citation></ref><ref id="ref19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grady</surname><given-names>C</given-names></name></person-group><article-title>Enduring and emerging challenges of informed consent</article-title><source>N Engl J Med</source><year>2015</year><month>2</month><day>26</day><volume>372</volume><issue>9</issue><fpage>855</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1056/NEJMra1411250</pub-id><pub-id pub-id-type="medline">25714163</pub-id><pub-id pub-id-type="pmid">25714163</pub-id></element-citation></ref><ref id="ref20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000e4;yrinen</surname><given-names>K</given-names></name><name><surname>Saranto</surname><given-names>K</given-names></name><name><surname>Nyk&#x000e4;nen</surname><given-names>P</given-names></name></person-group><article-title>Definition, structure, content, use and impacts of electronic health records: A review of the research literature</article-title><source>Int J Med Inform</source><year>2008</year><month>5</month><volume>77</volume><issue>5</issue><fpage>291</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1016/j.ijmedinf.2007.09.001</pub-id><pub-id pub-id-type="medline">17951106</pub-id><!--<pub-id pub-id-type="pii">S1386-5056(07)00168-2</pub-id>--><pub-id pub-id-type="pmid">17951106</pub-id></element-citation></ref><ref id="ref21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makhoul</surname><given-names>J</given-names></name><name><surname>Schwartz</surname><given-names>R</given-names></name></person-group><article-title>State of the art in continuous speech recognition</article-title><source>Proc Natl Acad Sci U S A</source><year>1995</year><month>10</month><day>24</day><volume>92</volume><issue>22</issue><fpage>9956</fpage><lpage>9963</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/cgi/pmidlookup?view=long&#x00026;pmid=7479809"/></comment><pub-id pub-id-type="medline">7479809</pub-id><!--<pub-id pub-id-type="pmcid">PMC40718</pub-id>--><pub-id pub-id-type="pmid">7479809</pub-id></element-citation></ref><ref id="ref22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyons</surname><given-names>JP</given-names></name><name><surname>Sanders</surname><given-names>SA</given-names></name><name><surname>Fredrick</surname><given-names>CD</given-names></name><name><surname>Palmer</surname><given-names>C</given-names></name><name><surname>Mihalik</surname><given-names>VL</given-names></name><name><surname>Weigel</surname><given-names>T</given-names></name></person-group><article-title>Speech recognition acceptance by physicians: A temporal replication of a survey of expectations and experiences</article-title><source>Health Informatics J</source><year>2015</year><month>7</month><day>17</day><fpage>1</fpage><pub-id pub-id-type="doi">10.1177/1460458215589600</pub-id><pub-id pub-id-type="medline">26187989</pub-id><!--<pub-id pub-id-type="pii">1460458215589600</pub-id>--></element-citation></ref><ref id="ref23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>T</given-names></name><name><surname>Basch</surname><given-names>P</given-names></name><name><surname>Barr</surname><given-names>M</given-names></name><name><surname>Yackel</surname><given-names>T</given-names></name><collab>Medical Informatics Committee of the American College of Physicians</collab></person-group><article-title>Clinical documentation in the 21st century: Executive summary of a policy position paper from the American College of Physicians</article-title><source>Ann Intern Med</source><year>2015</year><month>2</month><day>17</day><volume>162</volume><issue>4</issue><fpage>301</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.7326/M14-2128</pub-id><pub-id pub-id-type="medline">25581028</pub-id><!--<pub-id pub-id-type="pii">2089368</pub-id>--><pub-id pub-id-type="pmid">25581028</pub-id></element-citation></ref><ref id="ref24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eysenbach</surname><given-names>G</given-names></name><collab>CONSORT-EHEALTH Group</collab></person-group><article-title>CONSORT-EHEALTH: Improving and standardizing evaluation reports of Web-based and mobile health interventions</article-title><source>J Med Internet Res</source><year>2011</year><volume>13</volume><issue>4</issue><fpage>e126</fpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.jmir.org/2011/4/e126/"/></comment><pub-id pub-id-type="doi">10.2196/jmir.1923</pub-id><pub-id pub-id-type="medline">22209829</pub-id><!--<pub-id pub-id-type="pii">v13i4e126</pub-id>--><!--<pub-id pub-id-type="pmcid">PMC3278112</pub-id>--><pub-id pub-id-type="pmid">22209829</pub-id></element-citation></ref></ref-list></back></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Comput Math Methods Med</journal-id><journal-id journal-id-type="iso-abbrev">Comput Math Methods Med</journal-id><journal-id journal-id-type="publisher-id">CMMM</journal-id><journal-title-group><journal-title>Computational and Mathematical Methods in Medicine</journal-title></journal-title-group><issn pub-type="ppub">1748-670X</issn><issn pub-type="epub">1748-6718</issn><publisher><publisher-name>Hindawi Publishing Corporation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26858773</article-id><article-id pub-id-type="pmc">4698527</article-id><article-id pub-id-type="doi">10.1155/2015/576413</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Application of Random Forest Survival Models to Increase Generalizability of Decision Trees: A Case Study in Acute Myocardial Infarction</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yosefian</surname><given-names>Iman</given-names></name><xref ref-type="aff" rid="I1">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Mosa Farkhani</surname><given-names>Ehsan</given-names></name><xref ref-type="aff" rid="I2">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Baneshi</surname><given-names>Mohammad Reza</given-names></name><xref ref-type="aff" rid="I3">
<sup>3</sup>
</xref><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib></contrib-group><aff id="I1"><sup>1</sup>Regional Knowledge Hub and WHO Collaborating Center for HIV Surveillance, Institute for Futures Studies in Health, Kerman University of Medical Sciences, Kerman 7616911317, Iran</aff><aff id="I2"><sup>2</sup>Department of Epidemiology, University of Tehran, Tehran, Iran</aff><aff id="I3"><sup>3</sup>Research Center for Modeling in Health, Institute for Futures Studies in Health, Kerman University of Medical Sciences, Kerman 7616911317, Iran</aff><author-notes><corresp id="cor1">*Mohammad Reza Baneshi: <email>rbaneshi2@gmail.com</email></corresp><fn fn-type="other"><p>Academic Editor: Issam El Naqa</p></fn></author-notes><pub-date pub-type="ppub"><year>2015</year></pub-date><pub-date pub-type="epub"><day>21</day><month>12</month><year>2015</year></pub-date><volume>2015</volume><elocation-id>576413</elocation-id><history><date date-type="received"><day>12</day><month>9</month><year>2015</year></date><date date-type="rev-recd"><day>23</day><month>11</month><year>2015</year></date><date date-type="accepted"><day>24</day><month>11</month><year>2015</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2015 Iman Yosefian et al.</copyright-statement><copyright-year>2015</copyright-year><license xlink:href="https://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>
<italic>Background</italic>. Tree models provide easily interpretable prognostic tool, but instable results. Two approaches to enhance the generalizability of the results are pruning and random survival forest (RSF). The aim of this study is to assess the generalizability of saturated tree (ST), pruned tree (PT), and RSF.<italic> Methods</italic>. Data of 607 patients was randomly divided into training and test set applying 10-fold cross-validation. Using training sets, all three models were applied. Using Log-Rank test, ST was constructed by searching for optimal cutoffs. PT was selected plotting error rate versus minimum sample size in terminal nodes. In construction of RSF, 1000 bootstrap samples were drawn from the training set.<italic> C</italic>-index and integrated Brier score (IBS) statistic were used to compare models.<italic> Results</italic>. ST provides the most overoptimized statistics. Mean difference between<italic> C</italic>-index in training and test set was 0.237. Corresponding figure in PT and RSF was 0.054 and 0.007. In terms of IBS, the difference was 0.136 in ST, 0.021 in PT, and 0.0003 in RSF.<italic> Conclusion</italic>. Pruning of tree and assessment of its performance of a test set partially improve the generalizability of decision trees. RSF provides results that are highly generalizable.</p></abstract></article-meta></front><body><sec id="sec1"><title>1. Introduction</title><p>The prediction of survival rate is a major aim in survival analysis. In the case of time-to-event data, Log-Rank test and Cox regression models are the most frequently used method. The Cox model can be used to identify the variables that significantly affect the outcome of interest and presents the results in terms of Hazard Ratio (HR) [<xref rid="B14" ref-type="bibr">1</xref>]. However, this model does not provide an easily interpretable decision rule to be used in clinical practice. In addition, exploration of presence of high order interactions needs inclusion of interaction terms in the model which makes the interpretation of results more difficult [<xref rid="B19" ref-type="bibr">2</xref>].</p><p>An alternative strategy which easily handles both these problems is decision tree analysis [<xref rid="B2" ref-type="bibr">3</xref>]. The trees consist of root, internal, or daughter nodes and terminal nodes. At the first step, all subjects are put in the root node. Subjects should be categorized into two daughter nodes with maximum difference between them. This will achieve by extensive search among all independent variables to find the variable (and cutoff) that maximizes the difference [<xref rid="B6" ref-type="bibr">4</xref>]. All possible cutoffs of all independent variables are tried to explore which one leads to the highest Log-Rank statistics (corresponding to the lowest <italic>P</italic> value). Once the first split is created, a similar approach is applied to each internal node. This leads to a tree structure which divides the subjects into the final terminal nodes [<xref rid="B8" ref-type="bibr">5</xref>&#x02013;<xref rid="B20" ref-type="bibr">8</xref>]. These models provide pictorial decision rules and therefore can be easily used in medical decision making.</p><p>Once a model has been created some measures of model performance are required. For example, in the case of logistic regression, sensitivity and specificity, or area under ROC curve, should be reported. These statistics show how well the model discriminates between cases and controls.</p><p>In the case of survival analysis,<italic> C</italic>-index and Brier statistics are usually reported.<italic> C</italic>-index is a generalization of the area under ROC curve which compares survival rate of those who experienced the event with those who did not [<xref rid="B10" ref-type="bibr">9</xref>]. Brier score (BS) compares predicted survival rate with the actual status of patients [<xref rid="B17" ref-type="bibr">10</xref>]. High<italic> C</italic>-index and low BS indicate adequate fit of the model to the data.</p><p>In the process of model building, researchers usually fit a model using a given data set and then assess its performance using the same data set. Regardless of the method of model building, an important aim in risk prediction models is to construct models which accurately predicts the risk for future patients. It has been argued that use of a training set to construct the model and to assess its performance leads to overoptimized statistics with low generalizability [<xref rid="B22" ref-type="bibr">11</xref>]. The level of overoptimization in the case of decision tree models is even higher, due to extensive search at each node [<xref rid="B5" ref-type="bibr">12</xref>].</p><p>One of the easiest approaches to tackle the problem of overoptimized statistics is to randomly divide the data into training and test set. In this case, the model can be constructed on the training set. The model derived will then be applied on the test set to calculate the performance statistics [<xref rid="B22" ref-type="bibr">11</xref>]. This approach, however, leads to decrease in sample size and power.</p><p>Alternative approaches suggest bootstrap aggregation of the results [<xref rid="B4" ref-type="bibr">13</xref>, <xref rid="B11" ref-type="bibr">14</xref>]. This means to construct the model on a number of randomly derived bootstrap samples (say 1000) and to test them using the same sample and to report the mean and standard deviation of the statistics of interest.</p><p>One of the aggregation methods which has been proposed is random survival forest models. This method controls for overoptimization by two mechanisms [<xref rid="B13" ref-type="bibr">15</xref>]. Firstly, it draws multiple bootstrap samples from the initial data. In addition to that, to construct each tree, a random sample of independent variables would be selected and used. It has been argued that using two forms of randomization in growing the trees and combination of them cause sensible reduction instability of a single tree. The objective of this study was to compare the performance of survival tree and random survival forest for predicting survival probability patients admitted with acute myocardial infarction.</p></sec><sec id="sec2"><title>2. Material and Methods</title><p>We used information of 607 acute myocardial infarction (AMI) patients aged &#x0003e;25 years, admitted to the CCU of Imam Reza Hospital Mashhad, Iran, in 2007. Patients were identified according to the International Classification of Diseases (ICD-10) with 12.0 to 12.9 codes. In the current study, the main outcome was death due to AMI. Time from admission to discharge or death was considered as follow-up time. Information of 11 predictor variables was as follows: age (in years), sex, hypertension disease (no and yes) (patients with systolic blood pressure &#x02265;140&#x02009;mmg or diastolic blood pressure &#x02265;90&#x02009;mmg were considered as &#x0201c;yes&#x0201d;), hyperlipidemia (no and yes), history of ischemic heart disease at admission (no and yes), diabetes (no and yes), smoking status (no and yes), family history of AMI disease, Q wave status (presence or absence of pathologic Q waves in electrocardiogram (ECG)), streptokinase treatment (no and yes), and intervention (angioplasty, pacemaker surgery, bypass surgery, and drug therapy).</p><p>We compared four methods as explained below: saturated survival tree, pruned survival tree, and Random Forest Survival (RFS) (see detail below). We randomly divided our data set into two parts, training and test sets, by using 10-fold cross-validation; then models were constructed using the training set. In saturated and pruned survival trees, performance was assessed on both training and test sets. In random survival forest, performance was assessed on out-of-bag and test sets (explained later).</p><sec id="sec2.1"><title>2.1. Saturated Survival Tree</title><p>In construction of the survival tree, using training set, Log-Rank statistics was used as split criterion. A saturated tree was constructed under the restriction that a terminal node has at least 1 death. The performance of the final tree (in terms of IBS and<italic> C</italic>-index) was tested on both training and test samples.</p></sec><sec id="sec2.2"><title>2.2. Pruned Survival Tree</title><p>Secondly, the tree constructed using training sample was pruned. The tree size was plotted against error in test set (1 &#x02212; <italic>C</italic> index) to select the optimal tree. Sampling variation was addressed as explained above.</p></sec><sec id="sec2.3"><title>2.3. Random Survival Forest</title><p>RSF is an ensemble method that introduces 2 forms of randomization into the tree growing process: bootstrap sampling from the data and selection of a limited number of independent variables to construct the tree [<xref rid="B12" ref-type="bibr">16</xref>].</p><p>Using the training set, RSF procedure was applied. Its performance was then assessed using OOB training and the test set. This procedure has been repeated 1000 times, as explained below.</p><p>First, an independent bootstrap sample is used for growing the tree. Second, to split each node of the tree into 2 daughter nodes, a limited number of covariates are selected. It has been shown that each sample would be selected in about 63% of samples. The samples not being selected are referred to as out-of-bag (OOB) sample. This means that, in 1000 bootstrap samples, each subject is a part of OOB 370 times. We followed the procedure below:<list list-type="simple"><list-item><label>(1)</label><p>1000 bootstrap samples were drawn.</p></list-item><list-item><label>(2)</label><p>In each sample, a survival tree was constructed. At each node of the tree, <inline-formula><mml:math id="M1"><mml:mrow><mml:msqrt><mml:mi>p</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> candidate variables were selected. The node is split using the candidate variable that maximizes survival difference between daughter nodes.</p></list-item><list-item><label>(3)</label><p>Based on the rules derived from trees, survival curves for OOB patients were plotted.</p></list-item><list-item><label>(4)</label><p>For each subject, the average survival curves are calculated to be considered as subject's final <inline-formula><mml:math id="M2"><mml:mover accent="true"><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p></list-item></list>In all three approaches, 10-fold cross-validation was applied. To capture additional variations, the process of cross-validation was repeated 20 times, therefore creating 200 training and 200 test data sets at each method.</p></sec><sec id="sec2.4"><title>2.4. Performance Statistics</title><sec id="sec2.4.1"><title>2.4.1.
<italic>C</italic>-Index</title><p>Let (<italic>T</italic>
<sub>1,<italic>h</italic></sub>, <italic>&#x003c3;</italic>
<sub>1,<italic>h</italic></sub>), (<italic>T</italic>
<sub>2,<italic>h</italic></sub>, <italic>&#x003c3;</italic>
<sub>2,<italic>h</italic></sub>),&#x02026;, (<italic>T</italic>
<sub><italic>n</italic>,<italic>h</italic></sub>, <italic>&#x003c3;</italic>
<sub><italic>n</italic>,<italic>h</italic></sub>) be the survival times and the censoring status for <italic>n</italic> subjects in a terminal node <italic>h</italic>. Also, let <italic>t</italic>
<sub>1,<italic>h</italic></sub> &#x0003c; <italic>t</italic>
<sub>2,<italic>h</italic></sub> &#x0003c; &#x022ef;&#x0003c;<italic>t</italic>
<sub><italic>m</italic>,<italic>h</italic></sub> be the <italic>m</italic> distinct event times in terminal node <italic>h</italic>. Define <italic>d</italic>
<sub><italic>l</italic>,<italic>h</italic></sub> and <italic>Y</italic>
<sub><italic>l</italic>,<italic>h</italic></sub> to be the number of deaths and subjects at risk at time <italic>t</italic>
<sub><italic>l</italic>,<italic>h</italic></sub>. The cumulative hazard function (CHF) estimate for terminal node <italic>h</italic> is the Nelson-Aalen estimator<disp-formula id="eq1"><label>(1)</label><mml:math id="M3"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.95pt" depth="0.12pt"/><mml:mi>t</mml:mi><mml:mspace height="5.95pt" depth="0.12pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>for the subject <italic>i</italic> with a <italic>d</italic>-dimensional covariate <bold>x</bold>
<sub><bold>i</bold></sub>
<disp-formula id="eq2"><label>(2)</label><mml:math id="M4"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.4pt"/><mml:mi>t</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="2.4pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.95pt" depth="0.12pt"/><mml:mi>t</mml:mi><mml:mspace height="5.95pt" depth="0.12pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="10pt"/><mml:mtext>if</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>h</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>In RSF procedure, to estimate CHF of subject <italic>i</italic>, define <italic>I</italic>
<sub><italic>i</italic>,<italic>b</italic></sub> = 1 if <italic>i</italic> is an OOB case for <italic>b</italic>th bootstrap sample; otherwise, <italic>I</italic>
<sub><italic>i</italic>,<italic>b</italic></sub> = 0. Let <italic>H</italic>
<sub><italic>b</italic></sub>&#x02329;<italic>t</italic>&#x02223;<bold>x</bold>
<sub><bold>i</bold></sub>&#x0232a; denote the CHF for subject <italic>i</italic> in a tree grown from the <italic>b</italic>th bootstrap sample. The ensemble CHF for <italic>i</italic> is<disp-formula id="eq3"><label>(3)</label><mml:math id="M5"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02217;</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.4pt"/><mml:mi>t</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="2.4pt"/></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The<italic> C</italic>-index is calculated using the following steps:<list list-type="simple"><list-item><label>(1)</label><p>Form all possible pairs of subjects.</p></list-item><list-item><label>(2)</label><p>Consider permissible pairs, by eliminating those pairs whose shorter survival time is censored, and by eliminating pairs (<italic>i</italic>, <italic>j</italic>) if <italic>T</italic>
<sub><italic>i</italic></sub> = <italic>T</italic>
<sub><italic>j</italic></sub> and both are deaths.</p></list-item><list-item><label>(3)</label><p>For each permissible pair where <italic>T</italic>
<sub><italic>i</italic></sub> = <italic>T</italic>
<sub><italic>j</italic></sub>, count 1 if the shorter survival time has high risk predicted; count 0.5 if risk predicted is tied. For each permissible pair, where <italic>T</italic>
<sub><italic>i</italic></sub> = <italic>T</italic>
<sub><italic>j</italic></sub> and both are deaths, count 1 if risk predicted is tied; otherwise, count 0.5. For each permissible pair where <italic>T</italic>
<sub><italic>i</italic></sub> = <italic>T</italic>
<sub><italic>j</italic></sub>, but at least one is not a death, count 1 if the death has high risk predicted; otherwise, count 0.5. Let Concordance denote the sum over all permissible pairs.</p></list-item><list-item><label>(4)</label><p>
<italic>C</italic>-index = Concordance/permissible.</p></list-item></list>In the survival tree, we say <italic>i</italic> has a high risk predicted than <italic>j</italic> if <disp-formula id="eq4"><label>(4)</label><mml:math id="M6"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="2.484pt"/><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="2.484pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>H</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.08pt" depth="4.07298pt"/><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02223;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">j</mml:mi></mml:mrow></mml:msub><mml:mspace height="7.08pt" depth="4.07298pt"/></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>t</italic>
<sub>1</sub> &#x0003c; <italic>t</italic>
<sub>2</sub> &#x0003c; &#x022ef;&#x0003c;<italic>t</italic>
<sub><italic>m</italic></sub> are the unique event times in the data set. In RSF ensemble CHF (<italic>H</italic>
<sup><italic>&#x02217;</italic></sup>(<italic>t</italic>&#x02223;<bold>x</bold>)) is used instead of <italic>H</italic>(<italic>t</italic>&#x02223;<bold>x</bold>) [<xref rid="B12" ref-type="bibr">16</xref>].</p><p>A value of 0.5 for<italic> C</italic>-index is not better than random guessing and a value of 1 denotes full-discriminative ability. Percentiles 2.5 and 97.5 were considered as lower and upper bounds of CI for final statistics.</p></sec><sec id="sec2.4.2"><title>2.4.2. IBS Statistics</title><p>The Brier score at time <italic>t</italic> is given by<disp-formula id="eq5"><graphic xlink:href="CMMM2015-576413.e001.jpg" position="float" orientation="portrait"/><label>(5)</label></disp-formula>where <inline-formula><mml:math id="M8"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> denote the Kaplan-Meier estimate of the censoring survival function [<xref rid="B7" ref-type="bibr">17</xref>, <xref rid="B9" ref-type="bibr">18</xref>].</p><p>The prediction error curve is gotten by calculating of Brier score across the times. In addition, the integrated Brier score (IBS) that is cumulative prediction error curves over time is given by <disp-formula id="eq6"><label>(6)</label><mml:math id="M9"><mml:mtable style="T1"><mml:mtr><mml:mtd><mml:mtext>IBS</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="false">&#x0222b;</mml:mo></mml:mstyle><mml:mrow><mml:mn mathvariant="normal">0</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:mtext>BS</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mspace height="5.95pt" depth="0.12pt"/><mml:mi>t</mml:mi><mml:mspace height="5.95pt" depth="0.12pt"/></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Lower values of IBS indicate better predictive performances. Percentiles 2.5 and 97.5 were considered as lower and upper bounds of CI for final statistics.</p></sec></sec><sec id="sec2.5"><title>2.5. Impact of Method of Tree Construction and Data Set on Performance Statistics</title><p>As explained above, three methods were applied to construct the tree (ST, PT, and RSF). In addition, two data sets (training and testing) were used to assess the performance. These two factors together created six scenarios with 200 replications in each. In each of 1200 samples, values IBS and<italic> C</italic>-index were recorded. Two way ANOVA was applied to assess the impact of method of tree construction and data used for validation on performance statistics.</p></sec><sec id="sec2.6"><title>2.6. Software</title><p>We used randomForestSRC and pec R-package for analyses of this study.</p></sec></sec><sec id="sec3"><title>3. Results</title><p>Our data set comprised 607 patients with mean age of 61.34 years (SD = 13.46). In total, 204 patients experienced the outcome of interest (death due to AIM). <xref ref-type="table" rid="tab1"> Table 1</xref> provides information for the other 10 independent variables collected.</p><p>As summarized in <xref ref-type="table" rid="tab2">Table 2</xref>, saturated tree provides the most overoptimized statistics in training set. While<italic> C</italic>-index in saturated tree was 0.872, corresponding figure for RSF is 0.710. In addition, difference of<italic> C</italic>-index in training and test sets in saturated tree was much higher than other methods (0.24 in saturated tree, 0.05 in pruned tree, and 0.006 in RSF).</p><p>Similarly, in saturated tree, estimation of IBS using training set provides results which were not replicated in test set (0.088 versus 0.224). Pruned tree partially tackles the problem. RSF provides the most comparable results.</p><sec id="sec3.1"><title>3.1. Saturated Tree</title><p>Once the saturated tree was applied to the training set, IBS was 0.088, indicating very low prediction error (<xref ref-type="table" rid="tab2">Table 2</xref>). However, when this model was applied to the test set, IBS was increased by a factor of about 1.5 and reached to 0.224. In addition, about 27% reduction in<italic> C</italic>-index was seen. The<italic> C</italic>-index in training and test sets was 0.872 and 0.634, respectively. CIs suggested significant difference between these statistics in training and test sets. <xref ref-type="fig" rid="fig1"> Figure 1(a)</xref> shows BS values over time in training and test sets. BS in training set is consistently higher than the corresponding figure in test set.</p></sec><sec id="sec3.2"><title>3.2. Pruned Tree</title><p>Pruning the tree, still difference between performance on training and test sets was seen. However, the magnitude of the difference was much in comparison with saturated tree. Assessing the performance of pruned tree on the training set yields IBS of 0.145 (<xref ref-type="table" rid="tab2">Table 2</xref>). Corresponding figure in test set was 0.166, corresponding to 17% increase.<italic> C</italic>-index values in training and test sets were 0.753 and 0.699, respectively. This indicates only 7% reduction. No significant difference between training and test sets was seen in terms of performance statistics. However, statistics corresponding to test set was much wider. <xref ref-type="fig" rid="fig1"> Figure 1(b)</xref> indicates that the difference between two lines (corresponding to training and test sets) is much lower than that of the saturated tree (<xref ref-type="fig" rid="fig1">Figure 1(a)</xref>).</p></sec><sec id="sec3.3"><title>3.3. RSF</title><p>In RSF performance on both training and test sets is approximately the same (<xref ref-type="table" rid="tab2">Table 2</xref>). The IBS values were 0.163 and 0.163, respectively.<italic> C</italic>-index values were 0.710 and 0.716. Based on <xref ref-type="fig" rid="fig1">Figure 1(c)</xref>, two lines cannot be distinguished. This indicates the high generalizability of RSF results. Similar to PT, performance statistics in training and test sets were not significantly different.</p></sec><sec id="sec3.4"><title>3.4. Impact of Method of Tree Construction and Validation Set on Performance Statistics</title><p>Both of these factors significantly affect the statistics. In addition, significant interaction between them was seen (all <italic>P</italic> values &#x0003c; 0.001):<disp-formula id="eq7"><label>(7)</label><mml:math id="M10"><mml:mtable style="T18"><mml:mtr><mml:mtd><mml:maligngroup/><mml:mi>C</mml:mi><mml:mtext>-index</mml:mtext><mml:malignmark/><mml:mo>=</mml:mo><mml:mn>0.716</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mn>0.082</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="0.14pt"/><mml:mtext>model = ST</mml:mtext><mml:mspace height="7.09499pt" depth="0.14pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>&#x02212;</mml:mo><mml:mn>0.017</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="0.09999pt"/><mml:mtext>model = PT</mml:mtext><mml:mspace height="7.09499pt" depth="0.09999pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>&#x02212;</mml:mo><mml:mn>0.006</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="2.38998pt"/><mml:mtext>sample = Train</mml:mtext><mml:mspace height="7.09499pt" depth="2.38998pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>+</mml:mo><mml:mn>0.244</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="2.38998pt"/><mml:mtext>model = ST &#x00026; sample = Train</mml:mtext><mml:mspace height="7.09499pt" depth="2.38998pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>+</mml:mo><mml:mn>0.060</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="2.38998pt"/><mml:mtext>model = PT &#x00026; sample = Test</mml:mtext><mml:mspace height="7.09499pt" depth="2.38998pt"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:mtext>IBS</mml:mtext><mml:malignmark/><mml:mo>=</mml:mo><mml:mn>0.162</mml:mn><mml:mo>+</mml:mo><mml:mn>0.062</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="0.14pt"/><mml:mtext>model = ST</mml:mtext><mml:mspace height="7.09499pt" depth="0.14pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>+</mml:mo><mml:mn>0.004</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="0.09999pt"/><mml:mtext>model = PT</mml:mtext><mml:mspace height="7.09499pt" depth="0.09999pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>+</mml:mo><mml:mn>0.0003</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="2.38998pt"/><mml:mtext>sample = Train</mml:mtext><mml:mspace height="7.09499pt" depth="2.38998pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>&#x02212;</mml:mo><mml:mn>0.137</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="2.38998pt"/><mml:mtext>model = ST &#x00026; sample = Train</mml:mtext><mml:mspace height="7.09499pt" depth="2.38998pt"/></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:malignmark/><mml:mspace width="11.436553955078125pt"/><mml:mo>&#x02212;</mml:mo><mml:mn>0.021</mml:mn><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mspace height="7.09499pt" depth="2.38998pt"/><mml:mtext>model = PT &#x00026; sample = Test</mml:mtext><mml:mspace height="7.09499pt" depth="2.38998pt"/></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p></sec></sec><sec id="sec4"><title>4. Discussion</title><p>Using an empirical data set, our results showed that assessment of performance of decision trees using training set led to huge overoptimized statistics. In particular, when a saturated tree was constructed difference between<italic> C</italic>-index in training and test set was as high as 0.24. Pruning the tree partially tackled the overoptimization where the difference was reached to 0.05. We should emphasize that 0.05 difference in<italic> C</italic>-index is considered as huge since this statistic varies between 0.50 and 1. On the other hand, RSF was the only method that provides comparable results, no matter which data set was used. Performance of methods in terms of IBS led to the same conclusion. Again RSF and saturated trees provided the most accurate and the most overoptimized statistics.</p><p>We have not implemented extensive simulation studies. However, similar manuscripts also suggested that RSF works better than other algorithms. Austin et al. used data of AMI patients data to compare performance of PT and RF. The main outcome was whether the patient died within 30 days of hospital admission. The number of independent variables was 33.<italic> C</italic>-index in training and test sets was 0.768 and 0.767, respectively. Applying RF, corresponding figures were 0.823 and 0.843. We guess that the closeness of results of PT, in training and test sets, was due to very large sample size in training set (9298) [<xref rid="B1" ref-type="bibr">19</xref>].</p><p>Opitz and Maclin used 23 data sets from University of Wisconsin Machine Learning Repository (UCI data) to compare bootstrap aggregated and pruned trees. Performance of models was checked using 10-fold cross-validation. In all data sets, error rates corresponding to bagged trees were lower than pruned trees [<xref rid="B18" ref-type="bibr">20</xref>].</p><p>Walschaerts et al. used data of 144 breast cancer patients to compare PT and RSF. Data was randomly divided into training and test sets 30 times. Models were constructed on training and its performance was checked on test sets. The number of independent variables was 75: five clinical risk factors and 70 gene expression measurements. Mean error rate (1 &#x02212;<italic> C</italic>-index) in PT and RSF was 0.389 and 0.279, respectively [<xref rid="B21" ref-type="bibr">21</xref>].</p><p>Bou-Hamad et al. used information of 312 patients who suffered from primary biliary cirrhosis of the liver. The number of independent variables was 12. They compared Cox regression, PT, bagging trees, and RSF in terms of IBS. 10-fold cross-validation was applied to assess the performance of models. Results were presented graphically and suggested that RSF provides the best results, followed by bagging. PT provided the poorest results. The performance of the Cox regression model was in between [<xref rid="B3" ref-type="bibr">22</xref>].</p><p>As expected, results from literature and ours indicate higher generalizability of ensemble methods such as RSF. One of the strengths of our study is that we compared 3 different approaches on both training and test sets. We also calculated<italic> C</italic>-index and IBS statistics to compare the performance of different approaches. Majority of articles only compared pruned tree with RSF using test sets.</p><p>One of the limitations of our study was that we were not able to plot the mean of prediction error curve oversamples. We simply selected one of randomly generated samples to monitor the trend of BS over time. However, we reported the mean values to take into account the sampling variations. In addition, in our empirical data set the Event Per Variable (EPV) was about 20. We expect poorer performance for saturated and pruned trees at low EPVs. The impact of EPV on performance of alternative methods remains to be addressed.</p></sec><sec id="sec5"><title>5. Conclusion</title><p>We do recommend use of a training set to assess the performance of statistical models including decision trees. Pruning of tree partially tackles the degree of overoptimization. However, still high difference between training and test sets is expected. On the other hand, RSF provides statistics which can be generalized to independent samples.</p></sec></body><back><ack><title>Acknowledgment</title><p>This work is a part of M.S. thesis of Iman Yosefian which has been granted by Kerman University of Medical Sciences.</p></ack><sec sec-type="conflict"><title>Conflict of Interests</title><p>There was no conflict of interests.</p></sec><ref-list><ref id="B14"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kleinbaum</surname><given-names>D. G.</given-names></name><name><surname>Klein</surname><given-names>M.</given-names></name></person-group><source><italic>Survival Analysis</italic></source><year>2012</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4419-6646-9</pub-id><pub-id pub-id-type="other">MR2882858</pub-id></element-citation></ref><ref id="B19"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radespiel-Tr&#x000f6;ger</surname><given-names>M.</given-names></name><name><surname>Rabenstein</surname><given-names>T.</given-names></name><name><surname>Schneider</surname><given-names>H. T.</given-names></name><name><surname>Lausen</surname><given-names>B.</given-names></name></person-group><article-title>Comparison of tree-based methods for prognostic stratification of survival data</article-title><source><italic>Artificial Intelligence in Medicine</italic></source><year>2003</year><volume>28</volume><issue>3</issue><fpage>323</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/S0933-3657(03)00060-5</pub-id><pub-id pub-id-type="other">2-s2.0-0042069898</pub-id><pub-id pub-id-type="pmid">12927339</pub-id></element-citation></ref><ref id="B2"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banerjee</surname><given-names>M.</given-names></name><name><surname>George</surname><given-names>J.</given-names></name><name><surname>Song</surname><given-names>E. Y.</given-names></name><name><surname>Roy</surname><given-names>A.</given-names></name><name><surname>Hryniuk</surname><given-names>W.</given-names></name></person-group><article-title>Tree-based model for breast cancer prognostication</article-title><source><italic>Journal of Clinical Oncology</italic></source><year>2004</year><volume>22</volume><issue>13</issue><fpage>2567</fpage><lpage>2575</lpage><pub-id pub-id-type="doi">10.1200/JCO.2004.11.141</pub-id><pub-id pub-id-type="other">2-s2.0-4344683673</pub-id><pub-id pub-id-type="pmid">15226324</pub-id></element-citation></ref><ref id="B6"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L.</given-names></name><name><surname>Friedman</surname><given-names>J. H.</given-names></name><name><surname>Stone</surname><given-names>C. J.</given-names></name><name><surname>Olshen</surname><given-names>R. A.</given-names></name></person-group><source><italic>Classification and Regression Trees</italic></source><year>1984</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Wadsworth</publisher-name><pub-id pub-id-type="other">MR726392</pub-id></element-citation></ref><ref id="B8"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>L.</given-names></name><name><surname>Olshen</surname><given-names>R. A.</given-names></name></person-group><article-title>Tree-structured survival analysis</article-title><source><italic>Cancer Treatment Reports</italic></source><year>1985</year><volume>69</volume><issue>10</issue><fpage>1065</fpage><lpage>1068</lpage><pub-id pub-id-type="other">2-s2.0-0021875130</pub-id><pub-id pub-id-type="pmid">4042086</pub-id></element-citation></ref><ref id="B15"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeBlanc</surname><given-names>M.</given-names></name><name><surname>Crowley</surname><given-names>J.</given-names></name></person-group><article-title>Relative risk trees for censored survival data</article-title><source><italic>Biometrics</italic></source><year>1992</year><volume>48</volume><issue>2</issue><fpage>411</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.2307/2532300</pub-id><pub-id pub-id-type="other">2-s2.0-0026770594</pub-id><pub-id pub-id-type="pmid">1637970</pub-id></element-citation></ref><ref id="B16"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeBlanc</surname><given-names>M.</given-names></name><name><surname>Crowley</surname><given-names>J.</given-names></name></person-group><article-title>Survival trees by goodness of split</article-title><source><italic>Journal of the American Statistical Association</italic></source><year>1993</year><volume>88</volume><issue>422</issue><fpage>457</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1080/01621459.1993.10476296</pub-id><pub-id pub-id-type="other">MR1224370</pub-id></element-citation></ref><ref id="B20"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Segal</surname><given-names>M. R.</given-names></name></person-group><article-title>Regression trees for censored data</article-title><source><italic>Biometrics</italic></source><year>1988</year><volume>44</volume><issue>1</issue><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.2307/2531894</pub-id><pub-id pub-id-type="other">2-s2.0-0023942063</pub-id></element-citation></ref><ref id="B10"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heagerty</surname><given-names>P. J.</given-names></name><name><surname>Lumley</surname><given-names>T.</given-names></name><name><surname>Pepe</surname><given-names>M. S.</given-names></name></person-group><article-title>Time-dependent ROC curves for censored survival data and a diagnostic marker</article-title><source><italic>Biometrics</italic></source><year>2000</year><volume>56</volume><issue>2</issue><fpage>337</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1111/j.0006-341X.2000.00337.x</pub-id><pub-id pub-id-type="other">ZBL1060.62622</pub-id><pub-id pub-id-type="other">2-s2.0-0033936550</pub-id><pub-id pub-id-type="pmid">10877287</pub-id></element-citation></ref><ref id="B17"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mogensen</surname><given-names>U. B.</given-names></name><name><surname>Ishwaran</surname><given-names>H.</given-names></name><name><surname>Gerds</surname><given-names>T. A.</given-names></name></person-group><article-title>Evaluating random forests for survival analysis using prediction error curves</article-title><source><italic>Journal of Statistical Software</italic></source><year>2012</year><volume>50</volume><issue>11</issue><pub-id pub-id-type="doi">10.18637/jss.v050.i11</pub-id></element-citation></ref><ref id="B22"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.-H.</given-names></name></person-group><source><italic>Ensemble Methods: Foundations and Algorithms</italic></source><year>2012</year><publisher-loc>Boca Raton, Fla, USA</publisher-loc><publisher-name>Chapman &#x00026; Hall/CRC</publisher-name><pub-id pub-id-type="other">MR3184068</pub-id></element-citation></ref><ref id="B5"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L.</given-names></name></person-group><article-title>Heuristics of instability and stabilization in model selection</article-title><source><italic>The Annals of Statistics</italic></source><year>1996</year><volume>24</volume><issue>6</issue><fpage>2350</fpage><lpage>2383</lpage><pub-id pub-id-type="doi">10.1214/aos/1032181158</pub-id><pub-id pub-id-type="other">MR1425957</pub-id></element-citation></ref><ref id="B4"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L.</given-names></name></person-group><article-title>Bagging predictors</article-title><source><italic>Machine Learning</italic></source><year>1996</year><volume>24</volume><issue>2</issue><fpage>123</fpage><lpage>140</lpage><pub-id pub-id-type="other">ZBL0858.68080</pub-id><pub-id pub-id-type="other">2-s2.0-0030211964</pub-id></element-citation></ref><ref id="B11"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hothorn</surname><given-names>T.</given-names></name><name><surname>Lausen</surname><given-names>B.</given-names></name><name><surname>Benner</surname><given-names>A.</given-names></name><name><surname>Radespiel-Tr&#x000f6;ger</surname><given-names>M.</given-names></name></person-group><article-title>Bagging survival trees</article-title><source><italic>Statistics in Medicine</italic></source><year>2004</year><volume>23</volume><issue>1</issue><fpage>77</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1002/sim.1593</pub-id><pub-id pub-id-type="other">2-s2.0-0346656880</pub-id><pub-id pub-id-type="pmid">14695641</pub-id></element-citation></ref><ref id="B13"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishwaran</surname><given-names>H.</given-names></name><name><surname>Kogalur</surname><given-names>U. B.</given-names></name><name><surname>Blackstone</surname><given-names>E. .</given-names></name><name><surname>Lauer</surname><given-names>M. S.</given-names></name></person-group><article-title>Random survival forests</article-title><source><italic>The Annals of Applied Statistics</italic></source><year>2008</year><volume>2</volume><issue>3</issue><fpage>841</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1214/08-AOAS169</pub-id><pub-id pub-id-type="other">MR2516796</pub-id><pub-id pub-id-type="other">ZBL1149.62331</pub-id><pub-id pub-id-type="other">2-s2.0-57449111248</pub-id></element-citation></ref><ref id="B12"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishwaran</surname><given-names>H.</given-names></name><name><surname>Blackstone</surname><given-names>E. H.</given-names></name><name><surname>Apperson-Hansen</surname><given-names>C.</given-names></name><name><surname>Rice</surname><given-names>T. W.</given-names></name></person-group><article-title>A novel approach to cancer staging: application to esophageal cancer</article-title><source><italic>Biostatistics</italic></source><year>2009</year><volume>10</volume><issue>4</issue><fpage>603</fpage><lpage>620</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxp016</pub-id><pub-id pub-id-type="other">2-s2.0-74349113853</pub-id><pub-id pub-id-type="pmid">19502615</pub-id></element-citation></ref><ref id="B7"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerds</surname><given-names>T. A.</given-names></name><name><surname>Schumacher</surname><given-names>M.</given-names></name></person-group><article-title>Consistent estimation of the expected Brier score in general survival models with right-censored event times</article-title><source><italic>Biometrical Journal</italic></source><year>2006</year><volume>48</volume><issue>6</issue><fpage>1029</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1002/bimj.200610301</pub-id><pub-id pub-id-type="other">MR2312613</pub-id><pub-id pub-id-type="other">2-s2.0-33846223169</pub-id><pub-id pub-id-type="pmid">17240660</pub-id></element-citation></ref><ref id="B9"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graf</surname><given-names>E.</given-names></name><name><surname>Schmoor</surname><given-names>C.</given-names></name><name><surname>Sauerbrei</surname><given-names>W.</given-names></name><name><surname>Schumacher</surname><given-names>M.</given-names></name></person-group><article-title>Assessment and comparison of prognostic classification schemes for survival data</article-title><source><italic>Statistics in Medicine</italic></source><year>1999</year><volume>18</volume><issue>17-18</issue><fpage>2529</fpage><lpage>2545</lpage><pub-id pub-id-type="other">2-s2.0-0033619170</pub-id><pub-id pub-id-type="pmid">10474158</pub-id></element-citation></ref><ref id="B1"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Austin</surname><given-names>P. C.</given-names></name><name><surname>Lee</surname><given-names>D. S.</given-names></name><name><surname>Steyerberg</surname><given-names>E. W.</given-names></name><name><surname>Tu</surname><given-names>J. V.</given-names></name></person-group><article-title>Regression trees for predicting mortality in patients with cardiovascular disease: what improvement is achieved by using ensemble-based methods?</article-title><source><italic>Biometrical Journal</italic></source><year>2012</year><volume>54</volume><issue>5</issue><fpage>657</fpage><lpage>673</lpage><pub-id pub-id-type="doi">10.1002/bimj.201100251</pub-id><pub-id pub-id-type="other">MR2967766</pub-id><pub-id pub-id-type="other">2-s2.0-84865535937</pub-id><pub-id pub-id-type="pmid">22777999</pub-id></element-citation></ref><ref id="B18"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Opitz</surname><given-names>D.</given-names></name><name><surname>Maclin</surname><given-names>R.</given-names></name></person-group><article-title>Popular ensemble methods: an empirical study</article-title><source><italic>Journal of Artificial Intelligence Research</italic></source><year>1999</year><volume>11</volume><fpage>169</fpage><lpage>198</lpage><pub-id pub-id-type="other">2-s2.0-0000551189</pub-id></element-citation></ref><ref id="B21"><label>21</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Walschaerts</surname><given-names>M.</given-names></name><name><surname>Leconte</surname><given-names>E.</given-names></name><name><surname>Besse</surname><given-names>P.</given-names></name></person-group><article-title>Stable variable selection for right censored data: comparison of methods</article-title><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1203.4928">http://arxiv.org/abs/1203.4928</ext-link></comment></element-citation></ref><ref id="B3"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bou-Hamad</surname><given-names>I.</given-names></name><name><surname>Larocque</surname><given-names>D.</given-names></name><name><surname>Ben-Ameur</surname><given-names>H.</given-names></name></person-group><article-title>A review of survival trees</article-title><source><italic>Statistics Surveys</italic></source><year>2011</year><volume>5</volume><fpage>44</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1214/09-SS047</pub-id><pub-id pub-id-type="other">MR3018509</pub-id><pub-id pub-id-type="other">ZBL1274.62648</pub-id><pub-id pub-id-type="other">2-s2.0-84857308440</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="fig1" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Comparison of Brier score (BS), over time, in training and test sets: (a) saturated tree, (b) pruned tree, and (c) random survival forest.</p></caption><graphic xlink:href="CMMM2015-576413.001"/></fig><table-wrap id="tab1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Demographic characteristics of patients.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Predictor variables</th><th align="center" rowspan="1" colspan="1">Levels</th><th align="center" rowspan="1" colspan="1">Number (percent%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Sex</td><td align="center" rowspan="1" colspan="1">Male/female</td><td align="center" rowspan="1" colspan="1">423 (69.7)/184 (30.3)</td></tr><tr><td align="left" rowspan="1" colspan="1">Hypertension disease</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">245 (40.4)/362 (59.6)</td></tr><tr><td align="left" rowspan="1" colspan="1">Hyperlipidemia</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">135 (22.2)/472 (77.8)</td></tr><tr><td align="left" rowspan="1" colspan="1">History of ischemic heart disease</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">184 (30.3)/423 (69.7)</td></tr><tr><td align="left" rowspan="1" colspan="1">Diabetes</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">150 (24.7)/457 (75.3)</td></tr><tr><td align="left" rowspan="1" colspan="1">Smoking status</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">216 (35.6)/391 (64.4)</td></tr><tr><td align="left" rowspan="1" colspan="1">Family history of AMI disease</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">63 (10.4)/544 (89.6)</td></tr><tr><td align="left" rowspan="1" colspan="1">Q wave status</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">159 (26.2)/448 (73.8)</td></tr><tr><td align="left" rowspan="1" colspan="1">Streptokinase treatment</td><td align="center" rowspan="1" colspan="1">Yes/no</td><td align="center" rowspan="1" colspan="1">278 (45.8)/329 (54.2)</td></tr><tr><td align="left" rowspan="4" colspan="1">Intervention</td><td align="center" rowspan="1" colspan="1">Angioplasty</td><td align="center" rowspan="1" colspan="1">32 (5.3)</td></tr><tr><td align="center" rowspan="1" colspan="1">Pacemaker surgery</td><td align="center" rowspan="1" colspan="1">36 (5.9)</td></tr><tr><td align="center" rowspan="1" colspan="1">Bypass surgery</td><td align="center" rowspan="1" colspan="1">45 (7.4)</td></tr><tr><td align="center" rowspan="1" colspan="1"> Drug therapy</td><td align="center" rowspan="1" colspan="1">494 (81.4)</td></tr></tbody></table></table-wrap><table-wrap id="tab2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Assessment of performance of different tree construction methods using either training or test sets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" colspan="3" rowspan="1">
<italic>C</italic>-index</th><th align="center" colspan="3" rowspan="1">IBS</th></tr><tr><th align="left" rowspan="1" colspan="1">&#x02009;</th><th align="center" rowspan="1" colspan="1">Training set</th><th align="center" rowspan="1" colspan="1">Test set</th><th align="center" rowspan="1" colspan="1">Percent change</th><th align="center" rowspan="1" colspan="1">Training set</th><th align="center" rowspan="1" colspan="1">Test set</th><th align="center" rowspan="1" colspan="1">Percent change</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Saturated tree</td><td align="center" rowspan="1" colspan="1">0.872 (0.863, 0.882)</td><td align="center" rowspan="1" colspan="1">0.634 (0.528, 0.743)</td><td align="center" rowspan="1" colspan="1">27%</td><td align="center" rowspan="1" colspan="1">0.088 (0.082, 0.094)</td><td align="center" rowspan="1" colspan="1">0.224 (0.157, 0.298)</td><td align="center" rowspan="1" colspan="1">150%</td></tr><tr><td align="left" rowspan="1" colspan="1">Pruned tree</td><td align="center" rowspan="1" colspan="1">0.753 (0.740, 0.768)</td><td align="center" rowspan="1" colspan="1">0.699 (0.570, 0.824)</td><td align="center" rowspan="1" colspan="1">7%</td><td align="center" rowspan="1" colspan="1">0.145 (0.138, 0.151)</td><td align="center" rowspan="1" colspan="1">0.166 (0.113, 0.221)</td><td align="center" rowspan="1" colspan="1">14%</td></tr><tr><td align="left" rowspan="1" colspan="1">RSF</td><td align="center" rowspan="1" colspan="1">0.710 (0.693, 0.729)</td><td align="center" rowspan="1" colspan="1">0.716 (0.609, 0.857)</td><td align="center" rowspan="1" colspan="1">0.08%</td><td align="center" rowspan="1" colspan="1">0.163 (0.156, 0.169)</td><td align="center" rowspan="1" colspan="1">0.163 (0.114, 0.210)</td><td align="center" rowspan="1" colspan="1">0.1%</td></tr></tbody></table></table-wrap></floats-group></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">18364998</article-id><article-id pub-id-type="pmc">2267489</article-id><article-id pub-id-type="publisher-id">07-PONE-RA-02227R2</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0001840</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neurological Disorders/Neuro-Ophthalmology and Neuro-Otology</subject></subj-group></article-categories><title-group><article-title>Seeing &#x02018;Where&#x02019; through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution</article-title><alt-title alt-title-type="running-head">Sensory Substitution</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Proulx</surname><given-names>Michael J.</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">
<sup>&#x0002a;</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Stoerig</surname><given-names>Petra</given-names></name><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author"><name><surname>Ludowig</surname><given-names>Eva</given-names></name><xref ref-type="aff" rid="aff1"/></contrib><contrib contrib-type="author"><name><surname>Knoll</surname><given-names>Inna</given-names></name><xref ref-type="aff" rid="aff1"/></contrib></contrib-group><aff id="aff1">
<addr-line>Institute of Experimental Psychology, Heinrich-Heine-University D&#x000fc;sseldorf, D&#x000fc;sseldorf, Germany</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Auvray</surname><given-names>Malika</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University of Oxford, United Kingdom</aff><author-notes><corresp id="cor1">&#x0002a; E-mail: <email>Michael.Proulx@uni-duesseldorf.de</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: PS. Performed the experiments: PS EL IK. Analyzed the data: MP. Wrote the paper: MP PS. Other: Financed the study: PS.</p></fn></author-notes><pub-date pub-type="collection"><year>2008</year></pub-date><pub-date pub-type="epub"><day>26</day><month>3</month><year>2008</year></pub-date><volume>3</volume><issue>3</issue><elocation-id>e1840</elocation-id><history><date date-type="received"><day>13</day><month>9</month><year>2007</year></date><date date-type="accepted"><day>14</day><month>2</month><year>2008</year></date></history><permissions><copyright-statement>Proulx et al.</copyright-statement><copyright-year>2008</copyright-year><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><sec><title>Background</title><p>Sensory substitution devices for the blind translate inaccessible visual information into a format that intact sensory pathways can process. We here tested image-to-sound conversion-based localization of visual stimuli (LEDs and objects) in 13 blindfolded participants.</p></sec><sec><title>Methods and Findings</title><p>Subjects were assigned to different roles as a function of two variables: visual deprivation (blindfolded continuously (Bc) for 24 hours per day for 21 days; blindfolded for the tests only (Bt)) and system use (system not used (Sn); system used for tests only (St); system used continuously for 21 days (Sc)). The effect of learning-by-doing was assessed by comparing the performance of eight subjects (BtSt) who only used the mobile substitution device for the tests, to that of three subjects who, in addition, practiced with it for four hours daily in their normal life (BtSc and BcSc); two subjects who did not use the device at all (BtSn and BcSn) allowed assessment of its use in the tasks we employed. The impact of long-term sensory deprivation was investigated by blindfolding three of those participants throughout the three week-long experiment (BcSn, BcSn/c, and BcSc); the other ten subjects were only blindfolded during the tests (BtSn, BtSc, and the eight BtSt subjects). Expectedly, the two subjects who never used the substitution device, while fast in finding the targets, had chance accuracy, whereas subjects who used the device were markedly slower, but showed much better accuracy which improved significantly across our four testing sessions. The three subjects who freely used the device daily as well as during tests were faster and more accurate than those who used it during tests only; however, long-term blindfolding did not notably influence performance.</p></sec><sec><title>Conclusions</title><p>Together, the results demonstrate that the device allowed blindfolded subjects to increasingly know where something was by listening, and indicate that practice in naturalistic conditions effectively improved &#x0201c;visual&#x0201d; localization performance.</p></sec></abstract><counts><page-count count="8"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Vision is &#x0201c;to know what is where by looking&#x0201d; (p. 3 <xref rid="pone.0001840-Marr1" ref-type="bibr">[1]</xref>). This definition is intuitively appealing because it describes two central purposes of vision: object recognition and localization. The blind have to rely largely on auditory and tactile information for finding and identifying objects. Sensory substitution aims at supplementing the available aids (such as the cane, echolocation devices, and Braille script) by converting visual information into a tactile or auditory format (see <xref rid="pone.0001840-BachyRita1" ref-type="bibr">[2]</xref> for a general review). The resultant tactile arrays or sound patterns inform a blind person whether, where, and what silent objects fall within the field of view of the camera whose input they represent. Although substitution devices are capable of providing both what and where information, most studies have explored the potential of sensory substitution for stimulus discrimination, often using very simple stimuli <xref rid="pone.0001840-Arno1" ref-type="bibr">[3]</xref>&#x02013;<xref rid="pone.0001840-Ptito1" ref-type="bibr">[7]</xref>. As only one study <xref rid="pone.0001840-Auvray1" ref-type="bibr">[8]</xref> has had their subjects localize and explore real objects with a hand-held camera whose signals were converted into sound patterns, localization performance has hardly been addressed (however see <xref rid="pone.0001840-Renier1" ref-type="bibr">[9]</xref> for a study of the estimation of distance in depth for simple stimuli using a joystick and computer interface and <xref rid="pone.0001840-Segond1" ref-type="bibr">[10]</xref> for a study of spatial navigation). Moreover, studies to date have exclusively employed in-session learning to show that training improves discrimination performance over sessions in blind participants as well as in subjects blindfolded during training <xref rid="pone.0001840-Arno1" ref-type="bibr">[3]</xref>&#x02013;<xref rid="pone.0001840-Auvray1" ref-type="bibr">[8]</xref>.</p><p>The present study used an image-to-sound conversion program, The vOICe <xref rid="pone.0001840-Meijer1" ref-type="bibr">[11]</xref>, to examine the perceptual learning of manual localization based on the sounds generated by translating the images from a video camera hidden in sunglasses (see <xref ref-type="fig" rid="pone-0001840-g001">Figure 1</xref>). The use of a head-mounted rather than a handheld camera (cf. <xref rid="pone.0001840-Auvray1" ref-type="bibr">[8]</xref>) requires a different coordinate system and different sensory-motor contingencies for using the camera to allow one to grasp objects. Localization was assessed in three experiments. In the first, the participants had to manually indicate the location of a lit LED in a horizontal array of 18 possible target locations. The second experiment examined whether the learning would transfer to a more challenging LED task where there were 164 possible target locations. In the third experiment objects that were placed singly on a large table had to be located and grasped. On the basis of subjects' grasping precision we were also able to consider the ability of participants to take account of features of the object, such as its size.</p><fig id="pone-0001840-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001840.g001</object-id><label>Figure 1</label><caption><title>An illustration of the sensory substitution device and its conversion principles.</title><p>A) The vOICe program is installed on the notebook computer in the backpack. The camera is hidden in the glasses and the earphones provide the result of the image-to-sound conversion. B) Conversion principles for The vOICe.</p></caption><graphic xlink:href="pone.0001840.g001"/></fig><p>Unlike all published studies on sensory substitution that employed structured in-session learning to show that training improves the performance of both blind and blindfolded subjects, we here compared within-session learning to a learning-by-doing approach in naturalistic conditions (see <xref ref-type="fig" rid="pone-0001840-g002">Figure 2</xref> for the conditions that defined our subjects). This naturalistic learning was investigated by providing the mobile substitution system to three subjects for use in their daily lives. Two of these subjects had the system continuously for 21 days (BtSc and BcSc); the third had it for the final 10 days only (BcSn/c), and therefore provided a within-subject assessment of the effects of daily practice on performance. Eight subjects used the system during the tests only (BtSt); this group essentially replicated the normal subject group in other studies of sensory substitution (e.g. <xref rid="pone.0001840-Auvray1" ref-type="bibr">[8]</xref>) who only benefit from in-session practice. Two final subjects did not use the system at all (BtSn and BcSn). The three groups allowed us to assess the effect of using the system during the tests, and to compare in-session to in-session plus naturalistic learning.</p><fig id="pone-0001840-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001840.g002</object-id><label>Figure 2</label><caption><title>Assignment of subjects to the experimental conditions.</title><p>Subject assignment to the experimental conditions was created by partially crossing blindfolding with use of the sensory substitution system. Bc&#x0200a;=&#x0200a;Blindfolding continuous (for 21 days); Bt&#x0200a;=&#x0200a;Blindfolding test-only; Sc&#x0200a;=&#x0200a;System use continuous (in daily life and for all tests); St&#x0200a;=&#x0200a;System test-only (for all test but not in daily life); Sn&#x0200a;=&#x0200a;no System used (for the tests or in daily life). Note that subject BcSn/c was a cross between BcSc and BcSn because he was blindfolded continuously for 21 days, did not have the system for the first 11 days, but did use it in daily life and the tests for the final 10 days. The colors represent the extent of system use in <xref ref-type="fig" rid="pone-0001840-g003">Figures 3</xref> to <xref ref-type="fig" rid="pone-0001840-g004"/>
<xref ref-type="fig" rid="pone-0001840-g005">5</xref>, the shapes the extent of blindfolding in <xref ref-type="fig" rid="pone-0001840-g003">Figures 3</xref> and <xref ref-type="fig" rid="pone-0001840-g005">5</xref>.</p></caption><graphic xlink:href="pone.0001840.g002"/></fig><p>Finally, we studied the impact of sensory deprivation on the learning of sensory substitution by blindfolding three of the thirteen subjects (BcSn, BcSn/c, and BcSc) for the entirety of 21 days (24 hours per day), the longest, non-clinical period of visual deprivation in the literature (for a previously long duration of 5 days, see <xref rid="pone.0001840-PascualLeone1" ref-type="bibr">[12]</xref>). The ten others were only blindfolded during the laboratory tasks, similar to previous research (BtSn, BtSc, and the eight BtSt subjects). Long-term rather than test-only blindfolding was used in three participants because it may enhance perceptual learning both for the remaining modalities that have to compensate for the visual deprivation, and by rendering subjects more dependent on the system (e.g., <xref rid="pone.0001840-Kauffman1" ref-type="bibr">[13]</xref>&#x02013;<xref rid="pone.0001840-Lessard1" ref-type="bibr">[14]</xref>).</p><p>Taken together, the study has three contributions to the literature on sensory substitution: First it focused on localization (see also <xref rid="pone.0001840-Auvray1" ref-type="bibr">[8]</xref>, <xref rid="pone.0001840-Renier1" ref-type="bibr">[9]</xref>, <xref rid="pone.0001840-Segond1" ref-type="bibr">[10]</xref>); second, it compared in-session to naturalistic learning by providing some subjects with the equipment necessary for practicing with the device in their daily lives; and third it examined the effects of long-term sensory deprivation on the learning of sensory substitution.</p></sec><sec id="s2"><title>Results</title><sec id="s2a"><title>Experiment 1: Horizontally Located Light Source</title><sec id="s2a1"><title>System use (continuous, test-only or not at all)</title><p>Sn subjects (who wore no device for the tests; green shapes in <xref ref-type="fig" rid="pone-0001840-g003">Figure 3</xref>) were much faster than those who used the system; however, accuracy was expectedly at chance level. There were 18 LEDs that could potentially be the target, and the subjects without the system had to press almost as many, on average, before hitting the target LED (mean 15 LEDs per trial). There was no change in accuracy (<italic>r</italic>&#x0200a;=&#x0200a;0.13, <italic>p</italic>&#x0200a;=&#x0200a;0.36, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.60, <italic>d</italic>&#x0200a;=&#x0200a;0.26) over the sessions. In contrast, response times improved strongly as a function of session number (<italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.70, <italic>p</italic>&#x0200a;=&#x0200a;0.012, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.95, <italic>d</italic>&#x0200a;=&#x0200a;1.96) for the Sn subjects, presumably because they learned to hit as many LEDs as they could, as fast as they could, and increasingly used both hands for the task.</p><fig id="pone-0001840-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001840.g003</object-id><label>Figure 3</label><caption><title>The horizontal LED task results from Experiment 1.</title><p>Top) Response times as a function of session. The inserted array shows the potential target LEDs (black) in the perimeter. The subjects knew that these were confined to the horizontal meridian. Bottom) Accuracy is plotted as a function of the number of LEDs pushed on a given trial until hitting the target (&#x0201c;1&#x0201d; is perfect performance). All error bars denote standard error of the means in this and all figures.</p></caption><graphic xlink:href="pone.0001840.g003"/></fig><p>The data of the BtSt subjects who only used the device for the testing session are plotted with blue triangles in <xref ref-type="fig" rid="pone-0001840-g003">Figure 3</xref>. This group took much longer to find the target LED than the Sn subjects, and their response times did not decrease across sessions (<italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.15, <italic>p</italic>&#x0200a;=&#x0200a;0.211, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.71, <italic>d</italic>&#x0200a;=&#x0200a;0.30). In further contrast to the Sn subjects, their accuracy improved from session to session (mean n trials 8.4 in session 1 versus 3.2 in session 4; <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.43, <italic>p</italic>&#x0200a;=&#x0200a;0.007, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.96, <italic>d</italic>&#x0200a;=&#x0200a;0.95). The Sc subjects who used the device during the tests as well as in daily life were faster (mean RT 38 s; red shapes in <xref ref-type="fig" rid="pone-0001840-g003">Figure 3</xref>) than the St (80.5 s), but slower than the Sn subjects s (mean 8 s). These Sc subjects showed excellent accuracy which improved significantly (mean n trials 3.3 in session 1 versus 1.5 in session 4; <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.71, <italic>p&#x0200a;=&#x0200a;</italic>0.011, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.95, <italic>d</italic>&#x0200a;=&#x0200a;2.0) along with search time (<italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.65, <italic>p&#x0200a;=&#x0200a;</italic>0.022, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.92, <italic>d</italic>&#x0200a;=&#x0200a;1.71) across sessions. Note that they became almost as fast (mean 22 s) in Session 4 as an Sn subject who systematically pressed all LEDs in Session 1 (subject BcSn/c, mean 17.5 s). That Sc subjects improved both in accuracy and speed suggests that their daily use of the system outside of the testing sessions, and the many opportunities it provided for learning to adjust their image-to-sound guided behavior to the camera's field of view, contributed to their significant improvement on both counts in this laboratory task.</p></sec><sec id="s2a2"><title>Blindfolding (continuous or test-only)</title><p>We compared the Bc and Bt subjects to investigate whether continued visual deprivation affected performance in this task. Response times did not decrease substantially for either group (Bc, <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.13, <italic>p</italic>&#x0200a;=&#x0200a;0.343, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.61, <italic>d</italic>&#x0200a;=&#x0200a;0.26; Bt, <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.12, <italic>p</italic>&#x0200a;=&#x0200a;0.226, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.70, <italic>d</italic>&#x0200a;=&#x0200a;0.24), but, as seen in <xref ref-type="fig" rid="pone-0001840-g003">Figure 3</xref>, was lower for the Bc subjects throughout. Although accuracy for both the Bt and Bc groups improved similarly from the first (9.3 trials-to-hit for Bc, 8.9 for Bt) to the last (5.4 for Bc, 4.4 for Bt) session, it improved consistently only for the Bt (<italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.30, <italic>p</italic>&#x0200a;=&#x0200a;0.03, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.91, <italic>d</italic>&#x0200a;=&#x0200a;0.63), but not the Bc subjects (<italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.29, <italic>p</italic>&#x0200a;=&#x0200a;0.18, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.74, <italic>d</italic>&#x0200a;=&#x0200a;0.61).</p></sec><sec id="s2a3"><title>Continuous system use and blindfolding</title><p>Both Sc subjects performed well and exhibited perceptual learning. The continuously blindfolded subject BcSc initially had higher accuracy than BtSc; both exhibited improvement (BcSc: <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.89, <italic>p</italic>&#x0200a;=&#x0200a;0.058, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.87, <italic>d</italic>&#x0200a;=&#x0200a;3.90), though BtSc had a higher correlation between accuracy and testing session (BtSc <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.97, <italic>p</italic>&#x0200a;=&#x0200a;0.017, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.93, <italic>d</italic>&#x0200a;=&#x0200a;7.98). Conversely, BtSc had faster RTs to begin with, and though she showed some improvement (<italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.88, <italic>p</italic>&#x0200a;=&#x0200a;0.063, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.86, <italic>d</italic>&#x0200a;=&#x0200a;3.71), BcSc had a higher correlation (<italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.95, <italic>p</italic>&#x0200a;=&#x0200a;0.024, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.92, <italic>d</italic>&#x0200a;=&#x0200a;6.08) and better accuracy throughout. Unsurprisingly, in the first two tests where he performed without the system, BcSn/c was fast and inaccurate. When he first used a system in session 3, his search time as well as his accuracy increased dramatically. In session 4, the second session he performed while using the device, his search times already decreased by half, and his localization was as precise as that of the two other Sc subjects who had performed three prior sessions with the system.</p></sec></sec><sec id="s2b"><title>Experiment 2: Hexagonally Located Light Source</title><p>In Experiment 1, all subjects knew that the target LED would be located on the horizontal row of 18 LEDs. To investigate whether the learning would transfer to a task in which subjects did not know where in the perimeter the targets might be, and had to consider all 164 LEDs as possible targets, we used a different arrangement at the end of the 4<sup>th</sup> and final session. The subjects were not informed that only six LEDs were actually used, or that each served as target twice.</p><p>
<xref ref-type="fig" rid="pone-0001840-g004">Figure 4</xref> depicts the results. The bottom panel shows the mean number of LEDs pressed up to and including the target.</p><fig id="pone-0001840-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001840.g004</object-id><label>Figure 4</label><caption><title>The hexagonal LED task results from Experiment 2.</title><p>Each of the six hexagonally arranged LEDs shown in the insert served twice as target, but as subjects were not informed about this array, they had to consider all 164 LEDs as possible targets. Mean response time (top) and number of trials to hit the target (bottom) as a function of the subject's condition (visual deprivation and system use for each subject).</p></caption><graphic xlink:href="pone.0001840.g004"/></fig><sec id="s2b1"><title>System use (continuous, test-only or not at all)</title><p>The Sn subjects needed many more trials to hit the target than in the preceding tests (mean trials to hit 73.6), and their response times were relatively fast (mean 45.8 s). As in the &#x02018;horizontal&#x02019; task, the BtSt subjects took longer to find the targets than the Sn subjects (BtSt, mean 92.3 s; Sn, mean 45.8 s; <italic>t</italic>(82)&#x0200a;=&#x0200a;3.45, <italic>p&#x0200a;=&#x0200a;</italic>0.0004, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.99, <italic>d</italic>&#x0200a;=&#x0200a;0.76; the significant <italic>p</italic> value for this and all tests is 0.01 when Bonferroni corrected for multiple comparisons), but had much better accuracy (mean 7.4 trials compared to 73.6 trials for Sn; t(23)&#x0200a;=&#x0200a;5.97, p&#x0003c;0.0001, p<sub>rep</sub>&#x0200a;=&#x0200a;1.0, d&#x0200a;=&#x0200a;&#x02212;2.5). The Sc subjects performed even better than the BtSt subjects, pressing no more than one or two LEDs adjacent to the target on almost all of the trials (mean trials to hit 2.5, median 2, mode 1; <italic>t</italic>(70)&#x0200a;=&#x0200a;4.6, <italic>p</italic>&#x0003c;0.0001, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;1.0, <italic>d</italic>&#x0200a;=&#x0200a;1.1. Furthermore, their response times were statistically similar to those of Sn subjects (with device, mean 57.5 s; without device, mean 45.8 s; <italic>t</italic>(58)&#x0200a;=&#x0200a;1.1, <italic>p&#x0200a;=&#x0200a;</italic>0.13, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.77, <italic>d</italic>&#x0200a;=&#x0200a;0.28), and considerably faster than those of the BtSt subjects (92.3 s versus 57.5 s; <italic>t</italic>(94)&#x0200a;=&#x0200a;2.5, <italic>p&#x0200a;=&#x0200a;</italic>0.008, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.96, <italic>d</italic>&#x0200a;=&#x0200a;0.51). The increased difficulty of this task confirms the superior localization performance the Sc subjects had obtained after using the system in daily life for 21 or even just 10 days.</p></sec><sec id="s2b2"><title>Blindfolding (continuous or test-only)</title><p>The effect of visual deprivation on the hexagonal LED task, independent of system-use, was mixed. The Bt subjects who were seeing in daily life found the target faster than the Bc subjects who were blindfolded continuously (32 s for Bt versus 67 s for Bc; <italic>t</italic>(55)&#x0200a;=&#x0200a;3.6, <italic>p&#x0200a;=&#x0200a;</italic>0.0003, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.99, <italic>d</italic>&#x0200a;=&#x0200a;0.98). The accuracy of the Bt and Bc subjects was not statistically different (<italic>t</italic>(41)&#x0200a;=&#x0200a;1.68, <italic>p&#x0200a;=&#x0200a;</italic>0.17, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.75, <italic>d</italic>&#x0200a;=&#x0200a;0.30). However, the numerical trend indicated that the Bc subjects had better accuracy than the Bt subjects (mean n trials to find the target was 26 for Bc versus 39 for Bt).</p></sec><sec id="s2b3"><title>Continuous system use and blindfolding</title><p>The most interesting finding of this experiment, in comparison to Experiment 1, is that the two Sc subjects that used the device daily not only had greater accuracy than the Sn subjects but also had search times that were as fast as or faster than the Sn subjects. The difference in accuracy is clear in <xref ref-type="fig" rid="pone-0001840-g004">Figure 4</xref>. Beyond the previous analyses that demonstrated that the response times were not statistically distinguishable for the Sn versus the Sc subjects, it is also interesting to note that subject BtSc was faster than the fastest Sn subject (24 s for BtSc versus 40 s for BtSn; <italic>t</italic>(12)&#x0200a;=&#x0200a;1.65, <italic>p&#x0200a;=&#x0200a;</italic>0.062, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.86, <italic>d</italic>&#x0200a;=&#x0200a;0.95).</p></sec></sec><sec id="s2c"><title>Experiment 3: Finding Objects on a Table</title><p>Whereas the first two experiments focused solely on &#x0201c;where&#x0201d; information, the final one also considered object features pertaining to &#x0201c;what&#x0201d; information, such as an object's size and shape. Different ordinary objects were used on each trial, and subjects had to localize and grasp them. This allowed us to analyze search times as well as how directly the subjects reached for the objects and how appropriate their hand grip was for the object.</p><sec id="s2c1"><title>System use (continuous, test-only or not at all) and search time</title><p>
<xref ref-type="fig" rid="pone-0001840-g005">Figure 5</xref> shows the search time data from the third experiment, where subjects had to find various everyday objects, presented one at a time on a table. They exhibited high variability and, for the Sc and Sn subjects, search times correlated weakly with session number (Sc, <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.26, <italic>p</italic>&#x0200a;=&#x0200a;0.26, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.68, <italic>d</italic>&#x0200a;=&#x0200a;0.54; Sn, <italic>r</italic>&#x0200a;=&#x0200a;&#x02212;0.29, <italic>p</italic>&#x0200a;=&#x0200a;0.21, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.71, <italic>d</italic>&#x0200a;=&#x0200a;0.61; top panel of <xref ref-type="fig" rid="pone-0001840-g005">Figure 5</xref>). The BtSt subjects showed no improvement in search time (<italic>r</italic>&#x0200a;=&#x0200a;0.026, <italic>p</italic>&#x0200a;=&#x0200a;0.44, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.54, <italic>d</italic>&#x0200a;=&#x0200a;0.05). As we had to use different tables for the BtSt subjects and the other participants, absolute search times cannot be compared. Note, however, that the search times for the BtSt and two Sc subjects are very similar on average, suggesting that the difference in table size did not have much impact on the search time results.</p><fig id="pone-0001840-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0001840.g005</object-id><label>Figure 5</label><caption><title>The table task results from Experiment 3.</title><p>Top) Mean response time as a function of session. Bottom) Rating of grasp precision as a function of session (1&#x0200a;=&#x0200a;indirect; 2&#x0200a;=&#x0200a;relatively direct; 3&#x0200a;=&#x0200a;direct). The photograph at the top right shows a directed grasp of the target object by a subject (BcSc) using the substitution device.</p></caption><graphic xlink:href="pone.0001840.g005"/></fig></sec><sec id="s2c2"><title>Blindfolding (continuous or test-only) and search time</title><p>Variability was also high when considering the Bc versus the Bt subjects, and again there was no clear improvement in search time across session numbers (<italic>p</italic>&#x0003e;0.25, <italic>p</italic>
<sub>rep</sub>&#x0003c;0.70). A comparison of the subjects who used no device (BcSn, BtSn, and BcSn/c during the first three sessions) reveals no advantage of continuous blindfolding (in the absence of using the system) in this task.</p></sec><sec id="s2c3"><title>System use (continuous, test-only or not at all) and directed grasping</title><p>We analyzed the grasping behavior of subjects to determine how much the subjects' grasping took account of the objects' position, size and orientation (see <xref ref-type="fig" rid="pone-0001840-g005">Figure 5</xref> bottom panel). The Sn subjects had ratings that corresponded with indirect grasping across all sessions. This reflects their strategy: to slide their hands across the entire table, stepping sideways to reach all edges until an object was discovered tactually. The St subjects started almost as poorly, but increased their directness of grasping up to session 3 where it reached a plateau (<italic>r</italic>&#x0200a;=&#x0200a;0.93, <italic>p</italic>&#x0200a;=&#x0200a;.034, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.90, <italic>d</italic>&#x0200a;=&#x0200a;5.06). Finally, the Sc subjects clearly improved directness in their grasping of the object (<italic>r</italic>&#x0200a;=&#x0200a;0.84, <italic>p</italic>&#x0200a;=&#x0200a;.005, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.97, <italic>d</italic>&#x0200a;=&#x0200a;3.10).</p></sec><sec id="s2c4"><title>Blindfolding (continuous or test-only) and directed grasping</title><p>There was an improvement in directed grasping across test sessions for the Bt (<italic>r</italic>&#x0200a;=&#x0200a;0.27, <italic>p</italic>&#x0200a;=&#x0200a;.048, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.88, <italic>d</italic>&#x0200a;=&#x0200a;0.56) and Bc subjects (<italic>r</italic>&#x0200a;=&#x0200a;0.45, <italic>p</italic>&#x0200a;=&#x0200a;.081, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.84, <italic>d</italic>&#x0200a;=&#x0200a;1.01). Note that there were more Bt subjects than Bc (11 versus 2) which resulted in the lower correlation having a lower, and therefore statistically significant, <italic>p</italic> value. The Bc subjects, however, had the higher correlation and effect size, suggesting that continuous blindfolding had a positive effect on grasping.</p></sec><sec id="s2c5"><title>Blindfolding and system use (single subject analyses)</title><p>BtSc performed very precisely from the first session, and BcSc grasped all objects directly in the last one (see <xref ref-type="fig" rid="pone-0001840-g005">Figure 5</xref> for an example photograph). BcSn/c performed only a single session with the system. Nevertheless, he also grasped three objects directly, and two relatively direct, suggesting that the practice he had with the system in his daily life, plus perhaps having to adapt to daily life with a blindfold in the absence of the system for the first half of the period, played a substantial role in improving his search strategy as well as his reaching (see supporting online material for video examples of subject BcSn/c in <xref ref-type="supplementary-material" rid="pone.0001840.s001">Movie S1</xref> and BcSc in <xref ref-type="supplementary-material" rid="pone.0001840.s002">Movie S2</xref> and <xref ref-type="supplementary-material" rid="pone.0001840.s003">Movie S3</xref>). BcSn/c also had search times that were faster than BcSc in the final session even though he had less experience with the system overall (30 s for BcSn/c versus 70 s for BcSc; <italic>t</italic>(4)&#x0200a;=&#x0200a;1.66, <italic>p&#x0200a;=&#x0200a;</italic>0.085, <italic>p</italic>
<sub>rep</sub>&#x0200a;=&#x0200a;0.83, <italic>d</italic>&#x0200a;=&#x0200a;1.67). Although this difference is not statistically significant due to low power and might arise because of individual differences between the subjects, there is also a possibility that one who has adapted to sensory deprivation in the absence of using a substitution device (as the blind have) may be able to learn to use such a device more quickly and with better performance.</p></sec></sec></sec><sec id="s3"><title>Discussion</title><p>Here we examined the impact of naturalistic learning and sensory deprivation on the perceptual learning of object localization via image-to-sound substitution. As noted in the Introduction, this study has three primary contributions to the literature on sensory substitution: 1) it focused primarily on the less studied localization (see also <xref rid="pone.0001840-Auvray1" ref-type="bibr">[8]</xref>, <xref rid="pone.0001840-Renier1" ref-type="bibr">[9]</xref>, <xref rid="pone.0001840-Segond1" ref-type="bibr">[10]</xref>); 2) it employed a naturalistic, learning-by-doing approach in addition to the in-session practice that is normally employed, and 3) it featured the longest non-clinical blindfolding of subjects in addition to the standard in-session-only blindfolding.</p><p>1. Localization performance, hitherto only tested with a hand-held camera <xref rid="pone.0001840-Auvray1" ref-type="bibr">[8]</xref> or a joystick <xref rid="pone.0001840-Renier1" ref-type="bibr">[9]</xref>, <xref rid="pone.0001840-Segond1" ref-type="bibr">[10]</xref>, which might also impede one's ability to make free use of one's hands, improved in both Experiments 1 and 3 in subjects who used the system. No such improvement occurred in the Sn subjects who had chance accuracy throughout. These subjects were much faster than those who used the system, at least as long as the target array was limited. The Sn subjects decreased their search times consistently over sessions in Experiment 1 due to employing more effective strategies, such as using two hands to touch the LEDs more quickly. Search times also decreased in the initially much slower St and Sc subjects who were particularly challenged by having to adjust to the smaller field of view of the camera, and to the sweep time of the conversion program, as both required appropriate adaptation of head and, especially in Experiment 3, body movements. Nevertheless, Sc and St subjects had to press fewer LEDs before hitting the target in Experiments 1 and 2, and unlike the Sn subjects, also improved the precision of their grasping of objects in Experiment 3. As our targets were clearly defined &#x02013; only the target LED was lit, and only one object was positioned on the table at a time &#x02013; we cannot conclude that a more difficult task, such as finding a particular object among distracters, will be learned as effectively. However, the improvements we observed in the hand posture during reaching gives reason for cautious optimism.</p><p>2. Previous studies only focused on laboratory practice. Our BtSt group essentially replicates this approach, and, as in other reports <xref rid="pone.0001840-Arno1" ref-type="bibr">[3]</xref>&#x02013;<xref rid="pone.0001840-Segond1" ref-type="bibr">[10]</xref>, revealed statistically significant improvements across four sessions. However, the Sc subjects who used the substitution system immersively in their daily life had superior performance to that of the St, in-laboratory, users of the device in all three experiments. Together, the results suggest that the additional daily practice and the opportunities for learning-by-doing in naturalistic conditions it afforded effectively improved performance on the localization tasks we presented. Future research will have to show whether the naturalistic-learning conditions or the additional hours of practice account for the Sc subjects' enhanced performance.</p><p>3. Continuous, rather than test-only visual deprivation might lead to greater perceptual learning of localization than just using the system alone. Although our results are not as straightforward in this respect as a previous study <xref rid="pone.0001840-Kauffman1" ref-type="bibr">[13]</xref> which found that Braille learning profited markedly from five days of continuous blindfolding, the results from our third experiment suggest that the combination of immersive use and extended sensory deprivation may be particularly effective. Subject BcSn/c, who spent the first half of the experiment blindfolded but only had the device for the second half, exhibited very rapid learning and even had superior localization performance in Experiment 3 over that of BcSc who had the system for the duration of the experiment. Although any conclusion we could draw is tempered by the small number of our Bc subjects, the blind for whom the system is designed, may thus progress faster.</p><p>The learning necessary to use the device involves not only perceptually matching the auditory input to a representation of an object or scene that is derived from vision or touch, but other types of learning as well. Subjects must learn to remap egocentric space to match the camera's viewpoint, angle, and field of view. They must adjust their head and body movements to these properties, so as not to miss a possibly vital part of the scene. In addition, they must learn to adapt their movements to the sweep rate used by the system which only provides a snapshot of the scene every one or two seconds; in fact, many subjects made fast, large head movements in the early testing sessions and noticeably more deliberate, slower, and smaller head movements later in the study. Future studies that try to determine the most effective training protocols will have to address these different types of learning. Moreover, as the adult brain that has been subject to actual, peripheral blindness is very likely different from one that has been exposed to short-term blindfolding <xref rid="pone.0001840-Amedi1" ref-type="bibr">[15]</xref>, studies with blind subjects are important for understanding the learning that accompanies sensory substitution and for improving such systems for use by the blind.</p><p>In summary, the adult auditory system can learn to localize targets based on an image-to-sound conversion system, and immersive practice holds hope for providing the perceptual learning required to localize things quickly and accurately. Most of our results speak to the question of object localization. However, the increased directness of the grasping in Experiment 3 suggests that the subjects also gained general knowledge of the objects' size and shape. By allowing blindfolded subjects to increasingly hear where silent objects are, the system provides knowledge about what is where by listening.</p></sec><sec id="s4"><title>Materials and Methods</title><sec id="s4a"><title>Participants</title><p>Having obtained approval from the University's Ethics Committee, we tested 13 sighted subjects (see <xref ref-type="fig" rid="pone-0001840-g002">Figure 2</xref>) with informed verbal consent. All were informed about the principles of image-to-sound conversion, and subjects who used the mobile substitution system only during tests (St, System-test-only) as well as those who additionally used it in daily life (Sc, System-continuously) were instructed in its use. None had prior experience with sensory substitution. As the utility of the substitution system was difficult to judge on solely its own merits, a comparison group (two additional subjects who never used the system (Sn, System-none)) was included. Of the ten subjects who were blindfolded only for the testing sessions (Bt subjects, Blindfolded-test-only, five female, age 23&#x02013;46 yrs), nine were students, and one was associated with the laboratory. The three subjects who were visually-deprived during the entire experiment (Bc subjects, Blindfolded-continuously, one female, age 25&#x02013;39 yrs) were selected from among a large number of volunteers; they had to be intrinsically motivated (for instance by having a blind relative) and had to be living with someone who agreed (in writing) to look after them during the experimental period. On day 1, one Bt and one Bc subject were equipped with a substitution device (see <xref ref-type="fig" rid="pone-0001840-g001">Figure 1</xref>); subject BcSn/c received his system on day 11 to use for the second half of the period, and thus served as an intra-subject control. All Sc subjects were asked to use the system daily for at least four hours. Compliance was very good, as established through close contact with the experimenters and the daily reports subjects provided. Sc and Sn subjects participated in further experiments during this period <xref rid="pone.0001840-Pollok1" ref-type="bibr">[16]</xref>, and received financial compensation. All subjects were blindfolded during the laboratory tests where Sc and St subjects used a substitution device.</p></sec><sec id="s4b"><title>Sensory Substitution Device</title><p>Our substitution system consisted of a small video camera hidden in sunglasses (Mace Security Products Eyeglasses Camera ST-137W) and a notebook computer (IBM ThinkPad) that received the camera's digitalized signals, and converted them into sound patterns played to the subject by means of stereo headphones (<xref ref-type="fig" rid="pone-0001840-g001">Figure 1</xref>). The camera provided a field-of-view that subtended approximately 39&#x000b0; by 31&#x000b0; of visual angle. The vOICe program uses three major conversion dimensions: 1) laterality is coded by stereo panning and the time provided by the left-to-right scanning transformation of each image (the precision of the time scanning is fixed, and users can choose the rate of scanning to occur every one or two seconds), so that the sound pertaining to an object on the right of the image will be heard late in the scan and predominantly through the right ear; 2) elevation is coded by frequency, so that down is represented by low frequencies and up by high frequencies (an exponential distribution from 500 Hz to 5000 Hz); 3) pixel brightness is coded by loudness (<xref ref-type="fig" rid="pone-0001840-g001">Figure 1b</xref>). A single bright object on an otherwise dark surface will thus generate a sound pattern whose loudness reflects its brightness, whose duration and frequency spectrum represent its size, and whose frequency modulations represent its shape (see supporting online material, Movie S4 for an example image converted into sound).</p></sec><sec id="s4c"><title>Statistical analyses</title><p>The study primarily focused on the perceptual learning of sensory substitution. We were therefore interested in how the variables impacted the performance of the subjects over time. A negative correlation (Pearson's <italic>r</italic>) of search times and errors with testing session was expected if performance improved over the three weeks of the study. For each experiment we first analyzed the data in terms of the manipulation of system use, then the manipulation of blindfolding, and finally we looked at individual subjects to consider the interaction between blindfolding and system use.</p><p>All data analyses were conducted using the <italic>p</italic>
<sub>rep</sub> statistic <xref rid="pone.0001840-Killeen1" ref-type="bibr">[17]</xref>. Note that we also provide the standard <italic>p</italic> statistic for comparison and standard interpretation. We include the <italic>p</italic>
<sub>rep</sub> statistic because it overcomes a primary problem with null hypothesis statistical tests (i.e., the inability to accept or reject the null hypothesis), and it also provides a measure of the probability of replicability that is of primary importance in all research, but especially when considering small-<italic>n</italic> research such as that presented here. Thus, data can be interpreted with the following guideline: the higher the <italic>p</italic>
<sub>rep</sub> statistic, the greater the likelihood that the results will be replicable. The values of <italic>p</italic>
<sub>rep</sub> are directly proportional to <italic>p</italic> values, however: Values of <italic>p</italic>
<sub>rep</sub> greater than 0.9 are equal to <italic>p</italic> values significant at an alpha level of 0.05. We also provide effect sizes (using Cohen's <italic>d</italic>) for an additional evaluation of our results <xref rid="pone.0001840-Cohen1" ref-type="bibr">[18]</xref>.</p><sec id="s4c1"><title>Experiment 1: Horizontally Located Light Source</title><p>A semi-cylindrical perimeter fitted with touch-sensitive red LED buttons was used for this task (see inset depiction in <xref ref-type="fig" rid="pone-0001840-g003">Figure 3</xref>). With a diameter of 90 cm and a radius of 45 cm, it formed a semicircle in the horizontal plane, with 165 LED buttons arranged in a star-like pattern. All subjects were blindfolded during testing, and first moved their hands over the perimeter's inner surface to acquaint themselves with the layout of the LEDs. Seated centrally, they started each trial by pressing a start button located on the table in front of them. This activated one of the LEDs as well as a small loudspeaker at the top-center of the perimeter that began a buzzing sound (500 Hz, adjustable volume) which continued until the subject pressed the appropriate &#x02013; illuminated &#x02013; LED button. This response extinguished both the light and the sound, informing the subjects that they had found the target. The subjects that used the vOICe device (St and Sc) could still hear the sound that announced the start and continuation of a new trial, and none reported any difficulty hearing the output of the device as a result of the external steady tone. Subjects were informed about this procedure, and also knew that only the 18 LEDs along the horizontal row would be used. Ten subjects used the audiovisual substitution system for the tests (Bc and Bt), one performed it first twice without, then twice with the system (BcSn/c), and two performed it without the device (BtSn and BcSn). All subjects using a device started with a sweep rate of one image per two seconds, but were free to accelerate the sweep rate to one image/s after one to four series. Whereas the subjects with the device were instructed to localize the LED before attempting to press it, the subjects without the device simply pressed as many LED buttons as necessary until the correct one was reached. A PC recorded each LED button pressed during the search, and measured the time from the onset of the light stimulus to the correct response.</p><p>Each LED subtended 1.9&#x000b0; at a viewing distance of approximately 45 cm. The experiment used 18 LEDs. They were distributed evenly along the horizontal meridian, with a center-to-center distance of 6.3&#x000b0;; only the distance between the two most central target LEDs was twice as large, because the centralmost LED that normally serves as continuously-lit fixation spot was covered with black felt. Each LED had a luminance of &#x0223c;8 cd/m<sup>2</sup>, and was illuminated once per series. Ambient luminance was low (0.15&#x02013;0.5 cd/m<sup>2</sup>) to increase target salience for the subjects who used the device. One or two series were given per session; only BtSc enthusiastically performed five in the second session.</p></sec><sec id="s4c2"><title>Experiment 2: Hexagonally Located Light Source</title><p>The apparatus and procedure for Experiment 2 was similar to that used for Experiment 1, except for the changes noted below.</p><sec id="s4c2a"><title>Participants</title><p>The two Sn, the three Sc, and five St subjects participated.</p></sec><sec id="s4c2b"><title>Apparatus and Procedure</title><p>As illustrated in the inset depiction in <xref ref-type="fig" rid="pone-0001840-g004">Figure 4</xref>, two active LEDs were in the upper quadrants, two on the horizontal, and two in the lower quadrants. All subjects used a sweep rate of 1 image/s.</p></sec></sec><sec id="s4c3"><title>Experiment 3: Finding Objects on a Table</title><p>For each of the five trials per session, a single object was placed on a large table completely covered with black felt-like cloth to provide enhanced contrast to aid the subjects in their search for the objects placed on it. Table size was 2.6&#x000d7;1.4 m for the BtSt subjects who were tested in D&#x000fc;sseldorf, and 2 by 1.1 m for the other five subjects who were tested at the J&#x000fc;lich Research Center where parts of the experiments were conducted. Object position was varied pseudo-randomly, and care was taken to mask any auditory cues to the object's position that could result from hearing the experimenter's footsteps or the placement itself. The subject was asked to try and find the object, and started searching while standing by the long side of the table. Five different objects that varied both in size (e.g. a pen, a CD, a trainer, a large box) and in contrast (a white shirt rolled into a ball, a gray plush mouse) to the cloth were used for each testing period, with new objects selected for each session. The subjects did not know the identity of the objects until they found and grasped them.</p><p>As in Experiment 1, all subjects were blindfolded. The Sc and St subjects used the device during the tests; the one who had the system for 10 days at the end of the experimental period (BcSn/c) performed it with the device in only the final, fourth testing session. The two Sn subjects performed blindly throughout, but, unlike those using a system, were allowed to slide their hands across the surface of the table to find the objects; as in the previous experiments, Sc and St subjects were asked to use the sound patterns for this purpose. Trials were recorded by digital video camera (Sony Digital Handycam), to time the searches by using the camera's digital clock and to assess the precision of the grasping movements. Note that the data for the second session by BcSc is missing because the camera did not record that session.</p><p>Grasping for the objects was coded as either: indirect (coded as 1), relatively direct (2), or direct (3) by two raters. Sliding the hands in a sweeping manner, rather than towards an object, was coded as 1. Reaching that was directed in the general vicinity of the object, but was followed by a tactual search, was coded as 2. Direct grasping (3) was attested when the reaching movement was directed at the object, errors were confined to those of depth (over- or under-reaching grasps), and the hand-posture was largely appropriate to the size, shape and orientation of the object. The average across the five objects tested in each session was taken, and then subjected to the analyses and figural depiction described in the Results section. A comparison between the primary and second rater resulted in a high interrater reliability (<italic>r</italic>&#x0200a;=&#x0200a;.966; full agreement on 96&#x00025; of the coded trials).</p></sec></sec></sec><sec sec-type="supplementary-material" id="s5"><title>Supporting Information</title><supplementary-material content-type="local-data" id="pone.0001840.s001"><label>Movie S1</label><caption><p>Here the subject that was continuously blindfolded and had the sensory substitution device for only the final session of Experiment 3 is shown directly grasping an object.</p><p>(2.39 MB MOV)</p></caption><media xlink:href="pone.0001840.s001.mov"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0001840.s002"><label>Movie S2</label><caption><p>Here the subject that was continuously blindfolded and had the sensory substitution device continuously is shown directly grasping an object.</p><p>(1.15 MB MPG)</p></caption><media xlink:href="pone.0001840.s002.mpg"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0001840.s003"><label>Movie S3</label><caption><p>Here the subject that was continuously blindfolded and had the sensory substitution device continuously is shown directly grasping another object.</p><p>(0.46 MB MPG)</p></caption><media xlink:href="pone.0001840.s003.mpg"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0001840.s004"><label>Movie S4</label><caption><p>Here is a video demonstrating the conversion principles in <xref ref-type="fig" rid="pone-0001840-g001">Figure 1</xref>. Here an image of three squares is transformed into sound with a sweep rate of two seconds.</p><p>(0.05 MB MPG)</p></caption><media xlink:href="pone.0001840.s004.mpg"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>We thank the subjects for participating in this challenging study, Manfred Mittelstaedt for programming, and Peter Meijer for freely providing The vOICe (<ext-link ext-link-type="uri" xlink:href="http://www.seeingwithsound.com">www.seeingwithsound.com</ext-link>) and for comments on a previous draft of this manuscript.</p></ack><fn-group><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="financial-disclosure"><p><bold>Funding: </bold>We gratefully acknowledge financial support by the Volkswagen Stiftung (I/80 742) and the Anton-Betz-Stiftung. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></fn></fn-group><ref-list><title>References</title><ref id="pone.0001840-Marr1"><label>1</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group>
<year>1982</year>
<article-title>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information.</article-title>
<publisher-loc>San Francisco</publisher-loc>
<publisher-name>WH Freeman</publisher-name>
<fpage>397</fpage>
</element-citation></ref><ref id="pone.0001840-BachyRita1"><label>2</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Bach-y-Rita</surname><given-names>P</given-names></name><name><surname>Kercel</surname><given-names>SW</given-names></name></person-group>
<year>2003</year>
<article-title>Sensory substitution and the human-machine interface.</article-title>
<source>Trends Cogn Sci</source>
<volume>7</volume>
<fpage>541</fpage>
<lpage>546</lpage>
<pub-id pub-id-type="pmid">14643370</pub-id></element-citation></ref><ref id="pone.0001840-Arno1"><label>3</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Arno</surname><given-names>P</given-names></name><name><surname>Wanet-Defalque</surname><given-names>M-C</given-names></name><name><surname>Capelle</surname><given-names>C</given-names></name><name><surname>Catalan-Ahumada</surname><given-names>M</given-names></name><name><surname>Veraart</surname><given-names>C</given-names></name></person-group>
<year>1999</year>
<article-title>Auditory coding of visual patterns for the blind.</article-title>
<source>Perception</source>
<volume>28</volume>
<fpage>1013</fpage>
<lpage>1030</lpage>
<pub-id pub-id-type="pmid">10664751</pub-id></element-citation></ref><ref id="pone.0001840-CronlyDillon1"><label>4</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Cronly-Dillon</surname><given-names>J</given-names></name><name><surname>Persaud</surname><given-names>K</given-names></name><name><surname>Gregory</surname><given-names>RPF</given-names></name></person-group>
<year>1999</year>
<article-title>The perception of visual images encoded in musical form: a study in crossmodal information transfer.</article-title>
<source>Proc R Soc Lond B Biol Sci</source>
<volume>266</volume>
<fpage>2427</fpage>
<lpage>2433</lpage>
</element-citation></ref><ref id="pone.0001840-CronlyDillon2"><label>5</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Cronly-Dillon</surname><given-names>J</given-names></name><name><surname>Persaud</surname><given-names>K</given-names></name><name><surname>Blore</surname><given-names>R</given-names></name></person-group>
<year>2000</year>
<article-title>Blind subjects construct conscious mental images of visual scenes encoded in musical form.</article-title>
<source>Proc R Soc Lond B Biol Sci</source>
<volume>267</volume>
<fpage>2231</fpage>
<lpage>2238</lpage>
</element-citation></ref><ref id="pone.0001840-Poirier1"><label>6</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Poirier</surname><given-names>C</given-names></name><name><surname>Richard</surname><given-names>M-A</given-names></name><name><surname>Tranduy</surname><given-names>D</given-names></name><name><surname>Veraart</surname><given-names>C</given-names></name></person-group>
<year>2006</year>
<article-title>Assessment of sensory substitution prosthesis potentialities in minimalist conditions of learning.</article-title>
<source>Appl Cogn Psychol</source>
<volume>20</volume>
<fpage>447</fpage>
<lpage>460</lpage>
</element-citation></ref><ref id="pone.0001840-Ptito1"><label>7</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Ptito</surname><given-names>M</given-names></name><name><surname>Moesgaard</surname><given-names>SM</given-names></name><name><surname>Gjedde</surname><given-names>A</given-names></name><name><surname>Kupers</surname><given-names>R</given-names></name></person-group>
<year>2005</year>
<article-title>Crossmodal plasticity revealed by electrotactile stimulation of the tongue in the congenitally blind.</article-title>
<source>Brain</source>
<volume>128</volume>
<fpage>609</fpage>
<lpage>614</lpage>
</element-citation></ref><ref id="pone.0001840-Auvray1"><label>8</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Auvray</surname><given-names>M</given-names></name><name><surname>Hanneton</surname><given-names>S</given-names></name><name><surname>O'Regan</surname><given-names>JK</given-names></name></person-group>
<year>2007</year>
<article-title>Learning to perceive with a visuo-auditory substitution system: Localisation and object recognition with &#x02018;The vOICe&#x02019;.</article-title>
<source>Perception</source>
<volume>36</volume>
<fpage>416</fpage>
<lpage>430</lpage>
<pub-id pub-id-type="pmid">17455756</pub-id></element-citation></ref><ref id="pone.0001840-Renier1"><label>9</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Renier</surname><given-names>L</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Poirier</surname><given-names>C</given-names></name><name><surname>Tranduy</surname><given-names>D</given-names></name><name><surname>Vanlierde</surname><given-names>A</given-names></name><etal/></person-group>
<year>2005</year>
<article-title>Cross-modal activation of visual cortex during depth perception using auditory substitution of vision.</article-title>
<source>Neuroimage</source>
<volume>26</volume>
<fpage>573</fpage>
<lpage>580</lpage>
<pub-id pub-id-type="pmid">15907314</pub-id></element-citation></ref><ref id="pone.0001840-Segond1"><label>10</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Segond</surname><given-names>H</given-names></name><name><surname>Weiss</surname><given-names>D</given-names></name><name><surname>Sampaio</surname><given-names>E</given-names></name></person-group>
<year>2005</year>
<article-title>Human spatial navigation via a visuo-tactile sensory substitution system.</article-title>
<source>Perception</source>
<volume>34</volume>
<fpage>1231</fpage>
<lpage>1249</lpage>
<pub-id pub-id-type="pmid">16309117</pub-id></element-citation></ref><ref id="pone.0001840-Meijer1"><label>11</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Meijer</surname><given-names>PBL</given-names></name></person-group>
<year>1992</year>
<article-title>An experimental system for auditory image representations.</article-title>
<source>IEEE Trans Biomed Eng</source>
<volume>39</volume>
<fpage>112</fpage>
<lpage>121</lpage>
<pub-id pub-id-type="pmid">1612614</pub-id></element-citation></ref><ref id="pone.0001840-PascualLeone1"><label>12</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Hamilton</surname><given-names>R</given-names></name></person-group>
<year>2001</year>
<article-title>The metamodal organization of the brain.</article-title>
<source>Prog Brain Res</source>
<volume>134</volume>
<fpage>1</fpage>
<lpage>19</lpage>
<pub-id pub-id-type="pmid">11702537</pub-id></element-citation></ref><ref id="pone.0001840-Kauffman1"><label>13</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kauffman</surname><given-names>T</given-names></name><name><surname>Th&#x000e9;oret</surname><given-names>H</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group>
<year>2002</year>
<article-title>Braille character discrimination in blindfolded human subjects.</article-title>
<source>NeuroReport</source>
<volume>13</volume>
<fpage>571</fpage>
<lpage>574</lpage>
<pub-id pub-id-type="pmid">11973448</pub-id></element-citation></ref><ref id="pone.0001840-Lessard1"><label>14</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lessard</surname><given-names>N</given-names></name><name><surname>Par&#x000e9;</surname><given-names>M</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name><name><surname>Lassonde</surname><given-names>M</given-names></name></person-group>
<year>1998</year>
<article-title>Early-blind human subjects localize sound sources better than sighted subjects.</article-title>
<source>Nature</source>
<volume>395</volume>
<fpage>278</fpage>
<lpage>280</lpage>
<pub-id pub-id-type="pmid">9751055</pub-id></element-citation></ref><ref id="pone.0001840-Amedi1"><label>15</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>Bermpohl</surname><given-names>F</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group>
<year>2005</year>
<article-title>The occipital cortex in the blind: Lessons about plasticity and vision.</article-title>
<source>Curr Dir Psychol Sci</source>
<volume>14</volume>
<fpage>306</fpage>
<lpage>311</lpage>
</element-citation></ref><ref id="pone.0001840-Pollok1"><label>16</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pollok</surname><given-names>B</given-names></name><name><surname>Schnitzler</surname><given-names>I</given-names></name><name><surname>Stoerig</surname><given-names>P</given-names></name><name><surname>Mierdorf</surname><given-names>T</given-names></name><name><surname>Schnitzler</surname><given-names>A</given-names></name></person-group>
<year>2005</year>
<article-title>Image-to-sound conversion: experience-induced plasticity in auditory cortex of blindfolded adults.</article-title>
<source>Exp Brain Res</source>
<volume>167</volume>
<fpage>287</fpage>
<lpage>291</lpage>
<pub-id pub-id-type="pmid">16132971</pub-id></element-citation></ref><ref id="pone.0001840-Killeen1"><label>17</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Killeen</surname><given-names>PR</given-names></name></person-group>
<year>2005</year>
<article-title>An alternative to null-hypothesis significance tests.</article-title>
<source>Psychol Sci</source>
<volume>16</volume>
<fpage>345</fpage>
<lpage>353</lpage>
<pub-id pub-id-type="pmid">15869691</pub-id></element-citation></ref><ref id="pone.0001840-Cohen1"><label>18</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name></person-group>
<year>1988</year>
<source>Statistical power analysis for the behavioral sciences (2nd ed)</source>
<publisher-loc>Hillsdale, NJ</publisher-loc>
<publisher-name>Lawrence Erlbaum Associates</publisher-name>
<fpage>567</fpage>
</element-citation></ref></ref-list></back></article>
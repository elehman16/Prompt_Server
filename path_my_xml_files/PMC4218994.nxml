<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Implement Sci</journal-id><journal-id journal-id-type="iso-abbrev">Implement Sci</journal-id><journal-title-group><journal-title>Implementation Science : IS</journal-title></journal-title-group><issn pub-type="epub">1748-5908</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">25413978</article-id><article-id pub-id-type="pmc">4218994</article-id><article-id pub-id-type="publisher-id">113</article-id><article-id pub-id-type="doi">10.1186/s13012-014-0113-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Study Protocol</subject></subj-group></article-categories><title-group><article-title>Protocol for the process evaluation of a complex intervention designed to increase the use of research in health policy and program organisations (the SPIRIT study)</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Haynes</surname><given-names>Abby</given-names></name><address><email>abby.haynes@saxinstitute.org.au</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Brennan</surname><given-names>Sue</given-names></name><address><email>sue.brennan@monash.edu</email></address><xref ref-type="aff" rid="Aff2"/></contrib><contrib contrib-type="author"><name><surname>Carter</surname><given-names>Stacy</given-names></name><address><email>stacy.carter@sydney.edu.au</email></address><xref ref-type="aff" rid="Aff3"/></contrib><contrib contrib-type="author"><name><surname>O&#x02019;Connor</surname><given-names>Denise</given-names></name><address><email>denise.oconnor@monash.edu</email></address><xref ref-type="aff" rid="Aff2"/></contrib><contrib contrib-type="author"><name><surname>Schneider</surname><given-names>Carmen Huckel</given-names></name><address><email>carmen.huckel-schneider@saxinstitute.org.au</email></address><xref ref-type="aff" rid="Aff1"/><xref ref-type="aff" rid="Aff4"/></contrib><contrib contrib-type="author"><name><surname>Turner</surname><given-names>Tari</given-names></name><address><email>tari.turner@worldvision.com.au</email></address><xref ref-type="aff" rid="Aff5"/><xref ref-type="aff" rid="Aff2"/></contrib><contrib contrib-type="author"><name><surname>Gallego</surname><given-names>Gisselle</given-names></name><address><email>g.gallego@uws.edu.au</email></address><xref ref-type="aff" rid="Aff6"/><xref ref-type="aff" rid="Aff7"/></contrib><contrib contrib-type="author"><collab>the CIPHER team</collab></contrib><aff id="Aff1"><label/>Sax Institute, 235 Jones Street, Ultimo, NSW 2007 Australia </aff><aff id="Aff2"><label/>Australasian Cochrane Centre, School of Public Health and Preventive Medicine, Monash University, The Alfred Centre, 99 Commercial Road, Melbourne, VIC 3004 Australia </aff><aff id="Aff3"><label/>Centre for Values, Ethics and the Law in Medicine (VELiM), Medical Foundation Building, Sydney School of Public Health, University of Sydney, 92-94 Parramatta Road, Camperdown, NSW 2006 Australia </aff><aff id="Aff4"><label/>Menzies Centre for Health Policy, Victor Coppleson Building, University of Sydney, Camperdown, NSW 2006 Australia </aff><aff id="Aff5"><label/>World Vision Australia, 1 Vision Drive, Burwood East, VIC 3151 Australia </aff><aff id="Aff6"><label/>Centre for Health Research, School of Medicine, Building 3, Campbelltown Campus, University of Western Sydney, Penrith, NSW 2751 Australia </aff><aff id="Aff7"><label/>Faculty of Health Science, University of Sydney, 75 East Street, Lidcombe, NSW 2141 Australia </aff></contrib-group><pub-date pub-type="epub"><day>27</day><month>9</month><year>2014</year></pub-date><pub-date pub-type="pmc-release"><day>27</day><month>9</month><year>2014</year></pub-date><pub-date pub-type="collection"><year>2014</year></pub-date><volume>9</volume><elocation-id>113</elocation-id><history><date date-type="received"><day>28</day><month>7</month><year>2014</year></date><date date-type="accepted"><day>12</day><month>8</month><year>2014</year></date></history><permissions><copyright-statement>&#x000a9; Haynes et al.; licensee BioMed Central Ltd. 2014</copyright-statement><license license-type="open-access"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p>Process evaluation is vital for understanding how interventions function in different settings, including if and why they have different effects or do not work at all. This is particularly important in trials of complex interventions in &#x02018;real world&#x02019; organisational settings where causality is difficult to determine. Complexity presents challenges for process evaluation, and process evaluations that tackle complexity are rarely reported. This paper presents the detailed protocol for a process evaluation embedded in a randomised trial of a complex intervention known as SPIRIT (Supporting Policy In health with Research: an Intervention Trial). SPIRIT aims to build capacity for using research in health policy and program agencies.</p></sec><sec><title>Methods</title><p>We describe the flexible and pragmatic methods used for capturing, managing and analysing data across three domains: (a) the intervention as it was implemented; (b) how people participated in and responded to the intervention; and (c) the contextual characteristics that mediated this relationship and may influence outcomes. Qualitative and quantitative data collection methods include purposively sampled semi-structured interviews at two time points, direct observation and coding of intervention activities, and participant feedback forms. We provide examples of the data collection and data management tools developed.</p></sec><sec><title>Discussion</title><p>This protocol provides a worked example of how to embed process evaluation in the design and evaluation of a complex intervention trial. It tackles complexity in the intervention and its implementation settings. To our knowledge, it is the only detailed example of the methods for a process evaluation of an intervention conducted as part of a randomised trial in policy organisations. We identify strengths and weaknesses, and discuss how the methods are functioning during early implementation. Using &#x02018;insider&#x02019; consultation to develop methods is enabling us to optimise data collection while minimising discomfort and burden for participants. Embedding the process evaluation within the trial design is facilitating access to data, but may impair participants&#x02019; willingness to talk openly in interviews. While it is challenging to evaluate the process of conducting a randomised trial of a complex intervention, our experience so far suggests that it is feasible and can add considerably to the knowledge generated.</p></sec><sec><title>Electronic supplementary material</title><p>The online version of this article (doi:10.1186/s13012-014-0113-0) contains supplementary material, which is available to authorized users.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Process evaluation</kwd><kwd>Complex intervention</kwd><kwd>Implementation</kwd><kwd>Knowledge exchange</kwd><kwd>Health policy</kwd><kwd>Organisational change</kwd><kwd>Capacity building</kwd><kwd>Qualitative methods</kwd><kwd>Developmental evaluation</kwd><kwd>Framework analysis</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2014</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Background</title><p>There is global interest in ensuring that health policy and program development is informed by reliable research [<xref ref-type="bibr" rid="CR1">1</xref>]. Previous studies have improved our understanding of the constraints that policymakers and program developers face in their efforts to use research [<xref ref-type="bibr" rid="CR2">2</xref>&#x02013;<xref ref-type="bibr" rid="CR4">4</xref>], and tools have been developed to support these efforts [<xref ref-type="bibr" rid="CR5">5</xref>], but there is still little evidence about what strategies are most effective in building individual or organisational capacity to use research more effectively [<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref>]. Even less is known about how and why such strategies work and what makes them effective in one context but not another [<xref ref-type="bibr" rid="CR8">8</xref>]. SPIRIT (Supporting Policy In health with Research: an Intervention Trial) was developed to address this pressing need (see Additional file <xref rid="MOESM1" ref-type="media">1</xref> for a glossary of terms used in this article) [<xref ref-type="bibr" rid="CR9">9</xref>].</p><sec id="Sec2"><title>Supporting Policy In health with Research: an Intervention Trial (SPIRIT)</title><p>SPIRIT is testing the effects of a year-long multi-component intervention designed to increase the capacity of health policy agencies to use research. SPIRIT uses a stepped wedge cluster randomised trial design. Six government agencies that develop and implement state-wide or national health policies and programs located in Sydney, Australia, will receive the intervention. Between 15 to 60 staff are expected to participate at each site. All agencies receive six intervention components: (i) audit, feedback and goal setting; (ii) a leadership program; (iii) organisational support for research; (iv) the opportunity to test systems for accessing research and reviews; (v) research exchanges; and (vi) educational symposia for staff. The development of these components was informed by change principles, as shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>
<bold>SPIRIT intervention components, subcomponents, and change principles</bold>
</p></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th>
<bold>Intervention components</bold>
</th><th>
<bold>Subcomponents</bold>
</th><th>
<bold>Change principles underpinning each subcomponent</bold>
</th></tr></thead><tbody><tr valign="top"><td rowspan="9">
<bold>1. Audit, feedback and goal setting</bold>
</td><td rowspan="3">a. Feedback forum</td><td>&#x02022; Engages agencies in owning and driving the program</td></tr><tr valign="top"><td>&#x02022; Provides feedback about current practice</td></tr><tr valign="top"><td>&#x02022; Provides a clear rationale for change</td></tr><tr valign="top"><td rowspan="2">b. Intervention selection</td><td>&#x02022; Develops agreement about concrete and specific change goals</td></tr><tr valign="top"><td>&#x02022; Is tailored to focus on the agency&#x02019;s priorities</td></tr><tr valign="top"><td>c. Identification of other strategies</td><td>&#x02022; Develops agreement about concrete and specific change goals</td></tr><tr valign="top"><td rowspan="2">d. Mid-intervention feedback</td><td>&#x02022; Engages agencies in owning and driving the program</td></tr><tr valign="top"><td>&#x02022; Monitors and provides feedback about change during the intervention</td></tr><tr valign="top"><td>e. SPIRIT newsletter</td><td>&#x02022; Monitors and provides feedback about change during the intervention</td></tr><tr valign="top"><td rowspan="7">
<bold>2. Leadership program</bold>
</td><td rowspan="3">a. Supporting organisational use of evidence</td><td>&#x02022; Addresses systems, operations, structures and relations</td></tr><tr valign="top"><td>&#x02022; Engages agencies in owning and driving the program</td></tr><tr valign="top"><td>&#x02022; Develops agreement about concrete and specific change goals</td></tr><tr valign="top"><td rowspan="4">b. Leading organisational change</td><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td>&#x02022; Recognises the expertise of participants</td></tr><tr valign="top"><td>&#x02022; Is interactive with a focus on shared reflection and problem solving</td></tr><tr valign="top"><td>&#x02022; Uses credible, dynamic experts as presenters</td></tr><tr valign="top"><td rowspan="7">
<bold>3. Organisational support for research</bold>
</td><td rowspan="3">a. Quarterly email endorsement from CEO</td><td>&#x02022; Engages agencies in owning and driving the program</td></tr><tr valign="top"><td>&#x02022; Uses champions to model and promote the use of evidence from research</td></tr><tr valign="top"><td>&#x02022; Monitors and provides feedback about change during the intervention</td></tr><tr valign="top"><td rowspan="2">b. Access to WebCIPHER</td><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td>&#x02022; Uses champions to model and promote the use of evidence from research</td></tr><tr valign="top"><td rowspan="2">c. Resources for improving agency&#x02019;s use of research</td><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td>&#x02022; Provides opportunity for rehearsal and practice</td></tr><tr valign="top"><td rowspan="4">
<bold>4. Opportunity to test systems for accessing research and reviews (brokered services)</bold>
</td><td rowspan="4">a. Brokered commission of a rapid review, evaluation plan or linked data analysis</td><td>&#x02022; Provides opportunity for rehearsal and practice</td></tr><tr valign="top"><td>&#x02022; Is tailored to focus on the agency&#x02019;s priorities</td></tr><tr valign="top"><td>&#x02022; Recognises the expertise of participants</td></tr><tr valign="top"><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td rowspan="8">
<bold>5. Research exchange</bold>
</td><td rowspan="6">a. Interactive forum</td><td>&#x02022; Is tailored to focus on the agency&#x02019;s priorities</td></tr><tr valign="top"><td>&#x02022; Recognises the expertise of participants</td></tr><tr valign="top"><td>&#x02022; Is interactive with a focus on shared reflection and problem solving</td></tr><tr valign="top"><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td>&#x02022; Uses champions to model and promote the use of evidence from research</td></tr><tr valign="top"><td>&#x02022; Uses credible, dynamic experts as presenters</td></tr><tr valign="top"><td rowspan="2">b. Summary of systematic reviews</td><td>&#x02022; Is tailored to focus on the agency&#x02019;s priorities</td></tr><tr valign="top"><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td rowspan="10">
<bold>6. Educational symposia for staff</bold>
</td><td rowspan="5">a. Valuing research symposium</td><td>&#x02022; Recognises the expertise of participants</td></tr><tr valign="top"><td>&#x02022; Is interactive with a focus on shared reflection and problem solving</td></tr><tr valign="top"><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td>&#x02022; Uses champions to model and promote the use of evidence from research</td></tr><tr valign="top"><td>&#x02022; Uses credible, dynamic experts as presenters</td></tr><tr valign="top"><td rowspan="5">b. Agencies can chose two symposia from: Access to research/Appraising research/Evaluation/Working with researchers</td><td>&#x02022; Recognises the expertise of participants</td></tr><tr valign="top"><td>&#x02022; Is interactive with a focus on shared reflection and problem solving</td></tr><tr valign="top"><td>&#x02022; Provides self-education opportunities and access to resources</td></tr><tr valign="top"><td>&#x02022; Uses champions to model and promote the use of evidence from research</td></tr><tr valign="top"><td>&#x02022; Uses credible, dynamic experts as presenters</td></tr></tbody></table></table-wrap></p><p>The components include content that is tailored to suit the interests and needs of each agency but have standardised essential elements (i.e., the hypothesised &#x02018;active ingredients&#x02019; of the intervention). We assume that, for the intervention to be optimally effective, the essential elements of each component should be delivered in each agency. The design of the SPIRIT intervention is based on a program logic model which outlines how the intervention is hypothesised to bring about change. Proximal and distal outcomes include:<list list-type="order"><list-item><p>Organisational capacity to use research (individual knowledge and skills; staff perceptions of the value of research; and organisational support for the use of research as demonstrated through leadership support, policies, tools and systems);</p></list-item><list-item><p>Research engagement actions (accessing and appraising research; generating new analyses and research including evaluation of current programs and policies; and interacting with researchers);</p></list-item><list-item><p>Research use (the different ways research informs policy or program work).</p></list-item></list></p><p>The outcome measures comprise an online survey and two structured interviews. A detailed description of SPIRIT, including the program logic model, is available online at <ext-link ext-link-type="uri" xlink:href="http://bmjopen.bmj.com/content/4/7/e005293.full#F1">http://bmjopen.bmj.com/content/4/7/e005293.full#F1</ext-link> [<xref ref-type="bibr" rid="CR9">9</xref>].</p></sec><sec id="Sec3"><title>Process evaluation of interventions to increase the use of research in health policy and program agencies</title><p>A detailed process evaluation is being conducted as part of the evaluation of SPIRIT. High quality process evaluations are critical for interpreting the outcomes of trials of complex interventions [<xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref>] where there is seldom a clear causal chain [<xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref>]. Process evaluations are increasingly used in trials of complex interventions [<xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR16">16</xref>], including those that seek to change professional behaviours in complex settings [<xref ref-type="bibr" rid="CR17">17</xref>-<xref ref-type="bibr" rid="CR19">19</xref>].</p></sec><sec id="Sec4"><title>Aims and objectives</title><p>The primary aim of the process evaluation is to describe how the SPIRIT intervention works in different settings, including if and why it has different effects or does not work at all. This will help us interpret the outcomes of the SPIRIT trial and optimise the design of future interventions. We conceptualise this work as focusing on the interaction between three Domains: 1. the intervention as it was implemented; 2. how people participated in and responded to the intervention; and 3. the contextual characteristics that mediated this relationship. Our specific objectives address each of these Domains as follows:<list list-type="order"><list-item><p>To document how the intervention was implemented, and the extent to which it was implemented as intended over time and across different intervention settings, including the degree to which essential elements were delivered (implementation fidelity). This allows us to see implementation successes or failures that may affect outcomes. (Domain 1: Implementation).</p></list-item><list-item><p>To describe how people participated in and responded to the intervention, including any variations across the settings [<xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR20">20</xref>]. This enables us to critique the program design and delivery, and helps with the interpretation of study outcomes. (Domain 2: Participation and response).</p></list-item><list-item><p>To describe the contexts in which the intervention was delivered and explore contextual factors that may influence the delivery or impact of the intervention, and the outcomes. This provides evidence about &#x02018;real world&#x02019; feasibility and may enable findings to be transferred to other contexts [<xref ref-type="bibr" rid="CR21">21</xref>-<xref ref-type="bibr" rid="CR23">23</xref>]. It may also explain intentional and unintentional differences in delivery. Reflecting on the relationship between organisational context and how each agency used the program to address local needs may have implications for future program design and delivery. (Domain 3: Context).</p></list-item></list></p><p>In addition, we address a fourth objective required for new interventions in which the program theory is untested and the process evaluation is designed from scratch rather than employing piloted methods:<list list-type="simple"><list-item><label>4.</label><p>To explore how well the theory underpinning the intervention was (a) realised in the design and (b) delivered in each participating agency:<list list-type="alpha-lower"><list-item><p>We will collect data that confirms, refutes or adds nuance to the constructs and relationships proposed in the SPIRIT Action Framework. This model was used as the basis for designing and testing SPIRIT intervention strategies (Redman S, Turner T, Davies HTO, Williamson A, Haynes A, Brennan S, Milat A, O&#x02019;Connor D, Blyth F, Jorm L: The SPIRIT Action Framework: A structured approach to selecting and testing strategies to increase the use of research in policy, submitted).</p></list-item><list-item><p>We will explore whether the essential elements captured and delivered the change principles which informed them (theoretical fidelity). This includes assessing whether the elements thought to be essential appeared to be essential in real world contexts, describing how well they delivered the program&#x02019;s change principles, and developing hypotheses about how amended or different essential elements might have better delivered the change principles. This information can inform future intervention development. Details of the development and testing of essential elements during this trial will be reported separately.</p></list-item></list></p></list-item></list></p></sec></sec><sec id="Sec5" sec-type="materials|methods"><title>Methods/Design</title><p>The process evaluation is being conducted as an integral part of the trial in each of the six participating agencies. An evaluation officer leads the work, including data collection and management. She works with a small multidisciplinary sub-team of investigators who designed the process evaluation, and who continue to monitor its implementation and contribute to the ongoing data analysis. This team is not involved in the design or implementation of the intervention.</p><p>The development of the Domains, research questions and data collection methods (including the use of interview questions, feedback form items, observation frame questions and draft indexing categories, which are discussed later) were informed by the SPIRIT study aims and change principles, and the literature on process evaluation [<xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR24">24</xref>&#x02013;<xref ref-type="bibr" rid="CR26">26</xref>], research utilisation [<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR27">27</xref>&#x02013;<xref ref-type="bibr" rid="CR34">34</xref>], adult and organisational learning [<xref ref-type="bibr" rid="CR35">35</xref>&#x02013;<xref ref-type="bibr" rid="CR40">40</xref>], and complex systems theory [<xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR41">41</xref>&#x02013;<xref ref-type="bibr" rid="CR43">43</xref>].</p><sec id="Sec6"><title>Process evaluation design</title><p>We have designed a mixed methods process evaluation: gathering quantitative measures of intervention activities (such as numbers of participants and delivered components) [<xref ref-type="bibr" rid="CR26">26</xref>], and qualitative exploration of the interaction between the intervention, how people experience it, and the contextual characteristics of the six organisations in which it is being delivered [<xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref>]. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> provides an overview of these activities, which are discussed in more detail below.<table-wrap id="Tab2"><label>Table 2</label><caption><p>
<bold>Process evaluation domains, research questions and data collection</bold>
</p></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th>
<bold>Process evaluation domains</bold>
</th><th>
<bold>Research questions</bold>
</th><th>
<bold>Core information sought</bold>
</th><th>
<bold>Data type</bold>
</th><th>
<bold>Data source</bold>
</th><th>
<bold>Record kept</bold>
</th></tr></thead><tbody><tr valign="top"><td rowspan="4">
<bold>Domain 1:</bold> Implementation</td><td>1. How was the intervention implemented in each agency? Including:</td><td>Intervention components selected and how agencies asked that they be tailored</td><td>Updates</td><td>SPIRIT staff implementing the intervention</td><td>Fieldnotes and memos - data indexed in NVivo</td></tr><tr valign="top"><td>a. What components were delivered?</td><td rowspan="3">What was delivered in each agency, including: which components; delivery format; provider recruitment and preparation; learning materials; the fidelity with which <italic>essential elements</italic> were delivered; any changes to the plan, and follow-up activities</td><td>Structured observation</td><td>Intervention sessions</td><td>Completed checklists &#x02013; data input in spreadsheet</td></tr><tr valign="top"><td rowspan="2">b. To what extent were the <italic>essential elements</italic> implemented?</td><td>Email information</td><td>SPIRIT trial coordinator&#x02019;s records</td><td>Collated spreadsheet data on email delivery</td></tr><tr valign="top"><td>Knowledge brokering records</td><td>SPIRIT staff delivering brokered services</td><td>Brokered service assessment form</td></tr><tr valign="top"><td rowspan="6">
<bold>Domain 2:</bold> Participation and response</td><td rowspan="2">2. How did people interact with the intervention? What were their levels of participation and satisfaction?</td><td rowspan="2">Session participation and responses, including: roles of attendees, proportion of invitees who attended; nature of participation (types and extent of interaction)</td><td>Pre-session sign-in and consent process</td><td>Participants attending each session</td><td>Completed sign-in/consent sheets &#x02013;no.s and roles input in spreadsheet</td></tr><tr valign="top"><td>Semi-structured and structured observation</td><td>Intervention sessions</td><td>Completed checklist, fieldnotes and memos &#x02013; data indexed in NVivo</td></tr><tr valign="top"><td rowspan="4">3. What effects that are not captured by the outcome measures did the intervention have (including unexpected effects)?</td><td rowspan="2">Participants&#x02019; evaluation of intervention sessions</td><td>Self-reported evaluation feedback</td><td>Participants attending each session</td><td>Completed feedback forms &#x02013; data input in spreadsheet</td></tr><tr valign="top"><td>Informal conversations after sessions</td><td>Liaison Person and ad hoc participants</td><td>Fieldnotes &#x02013; data indexed in NVivo</td></tr><tr valign="top"><td rowspan="2">How participants and the organisational system responded to the intervention overall (including unexpected effects)</td><td>Interviews</td><td>Purposively sampled participants, Liaison Person and CEO</td><td>Audio recordings, transcripts and memos &#x02013; data indexed in NVivo</td></tr><tr valign="top"><td>Interviews, meetings and informal conversations</td><td>SPIRIT intervention staff and providers</td><td>Fieldnotes and memos &#x02013; data indexed in NVivo</td></tr><tr valign="top"><td rowspan="4">
<bold>Domain 3:</bold> Context</td><td rowspan="4">4. What was the context of the agencies in which the intervention was implemented?</td><td>Immediate characteristics of session delivery context (site, facilities, etc.)</td><td>Structured observation</td><td>Intervention sessions</td><td>Completed checklist fieldnotes data input in spreadsheet</td></tr><tr valign="top"><td rowspan="3">Organisational context: (i) agency culture, (ii) agenda-setting &#x00026; prioritisation, (iii) leadership styles &#x00026; perceptions of leaders, (iv) how research &#x00026; other information is valued, accessed &#x00026; used, (v) barriers and enablers to using research, (vi) other contextual factors that may affect outcomes</td><td>Semi-structured observation</td><td>Intervention sessions</td><td>Audio recordings, fieldnotes and memos &#x02013; data indexed in NVivo</td></tr><tr valign="top"><td>Interviews</td><td>Purposively sampled participants</td><td>Audio recordings, transcripts and memos &#x02013; data indexed in NVivo</td></tr><tr valign="top"><td>Interviews, meetings and informal conversations</td><td>SPIRIT staff implementing the intervention</td><td>Audio recordings, fieldnotes and memos &#x02013; data indexed in NVivo</td></tr><tr valign="top"><td rowspan="2">
<bold>Across domains</bold>
</td><td colspan="4">5. How might the relationships between the program, the people and the context in each agency have shaped variations in these effects?</td><td rowspan="2">Analytic synthesis of above data</td></tr><tr valign="top"><td colspan="4">6. What lessons can we derive from this study that might be relevant for other interventions and settings?</td></tr></tbody></table></table-wrap></p><p>Our approach incorporates aspects of developmental evaluation [<xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR47">47</xref>]. Traditional process evaluation tends to align with program logic models and focus largely on documenting key aspects of these linear and predictive pathways. Developmental evaluation takes a more emergent perspective, assuming that implementation within complex organisational systems will be unpredictable and will result in local adaptation, which may be more appropriate for achieving the intended program goals in that context. This approach focuses on reflective learning at every stage of the evaluation, adapting evaluation questions and data collection methods as the program is implemented, and feeding them back into an evolving trial design. This is appropriate when trialling complex strategies that are untested, producing uncertainty about what will work, where, and with whom; and when new questions, challenges, and opportunities are likely to surface [<xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR47">47</xref>]. Core aspects of SPIRIT are intended to be standardised, so process evaluation data will not be fed back to the intervention implementation team during the trial. Nevertheless, we hypothesise SPIRIT staff and providers are likely to adapt (intentionally and unintentionally) as they interact with participants and respond to contextual opportunities and constraints. The developmental evaluation perspective helps us see this variation as more than non-adherence to the implementation plan: it is expected emergence. Thus we are exploring why expert providers make particular in-situ changes, and striving to learn from how different strategies play out. These data are collated and analysed to support post-trial critical reflection and recommendations for optimising future interventions.</p><p>Ethical approval for the trial and the process evaluation was granted by the University of Western Sydney Human Research Ethics Committee, approval number H8970. Written informed consent is obtained from all study participants. Participant interviews are transcribed and de-identified. All data is kept confidential. The methods for monitoring and documenting intervention session delivery and obtaining participant feedback were piloted in a non-participant health centre within a government department before the trial began.</p></sec><sec id="Sec7"><title>Data collection</title><sec id="Sec8"><title>Domain 1: Implementation</title><p>Documentation of the intervention delivery is informed by the work of Bellg, Borrelli and colleagues who have developed frameworks for measuring intervention fidelity of individual health behaviour change treatments [<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR25">25</xref>]. These frameworks were reviewed pragmatically for (a) applicability to the SPIRIT intervention change principles and (b) utility for our aims. Given the intervention&#x02019;s complexity, the use of external content experts to deliver the program, and the likelihood of local tailoring, some items were adapted for improved fit, some were discarded (e.g., those that assessed participants&#x02019; comprehension and ability to perform skills), and a few were added (e.g., details of how interaction and reflective learning should be facilitated). Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> shows the final items (phrased as guiding questions) and the data collection strategies used for each.<table-wrap id="Tab3"><label>Table 3</label><caption><p>
<bold>Framework for documenting intervention implementation</bold>
</p></caption><table frame="hsides" rules="groups"><thead><tr valign="top"><th rowspan="2">
<bold>Intervention implementation questions used to guide the documentation of intervention implementation</bold>
</th><th colspan="5">
<bold>Data sources</bold>
</th></tr><tr valign="top"><th>
<bold>SPIRIT team: engagement and design</bold>
</th><th>
<bold>SPIRIT team: implementation</bold>
</th><th>
<bold>Intervention session observations</bold>
</th><th>
<bold>Participant feedback</bold>
</th><th>
<bold>Participant interviews</bold>
</th></tr></thead><tbody><tr valign="top"><td>
<bold>Intervention design</bold>
</td><td/><td/><td/><td/><td/></tr><tr valign="top"><td>1. What was the planned number &#x00026; length of intervention sessions/activities, and their distribution and duration over time? Who were the targeted participants?</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>2. What theoretical model/theory-of-change were the strategies based on?</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>3. What <italic>essential elements</italic> were to be delivered in each component?</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>
<bold>Intervention providers</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>4. What selection process was used to identify providers? What were the credentials of providers?</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>5. What training, guidance or information did providers receive (what was the content and format, and were there any changes over time?)?</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>
<bold>Recruitment</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>6. How were agencies recruited as participants?</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>7. What was the nature of the relationship between these agencies and the researchers/institutions involved in the trial?*</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td></tr><tr valign="top"><td>
<bold>Tailoring</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>8. Which intervention components did agency leaders select?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>9. What reasons did they give for that selection (what goals were they targeting)?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>10. What, if any, other goals and strategies were nominated by the agency leaders for supporting research use?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td></tr><tr valign="top"><td>
<bold>Intervention delivery</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>11. What method was used to specify and direct implementation?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>12. How long was each session?</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>13. What was the type, number and distribution of intervention sessions/activities?</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>14. To what extent were the <italic>essential elements</italic> delivered? How were they monitored/measured?</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>15. Were there any planned changes made to R4P while it was in progress? Why?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/></tr><tr valign="top"><td>16. Were there any unplanned changes? What happened?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>
<bold>Context</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>17. What was the culture and overarching context of the participating agencies at the start of the intervention?*</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td></tr><tr valign="top"><td>18. Were there any changes/initiatives during the intervention that may have affected responses to the intervention?*</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td></tr><tr valign="top"><td>19. What were the immediate contextual conditions around intervention sessions?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>
<bold>Participation</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>20. Who was invited to participate? (numbers and professional roles)?</td><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>21. How many potential participants attended sessions? Who were they?</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>22. What proportion of targeted participants attended (approximately)?</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>23. Did key people (agency leaders or topic specialists) attend?</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>
<bold>Responses to intervention sessions</bold>
</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center"/></tr><tr valign="top"><td>24. How did people participate in intervention sessions?</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td></tr><tr valign="top"><td>25. How satisfied were participants with sessions and SPIRIT overall?*</td><td valign="middle" align="center"/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td></tr><tr valign="top"><td>26. Did participants identify or anticipate any changes in using research in response to intervention sessions/activities?*</td><td/><td valign="middle" align="center"/><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td><td valign="middle" align="center">
<bold>&#x02713;</bold>
</td></tr><tr valign="top"><td>
<bold>Intervention improvements</bold>
</td><td valign="middle" align="center" rowspan="3" colspan="5">Analysis of data above</td></tr><tr valign="top"><td>27. What improvements to the intervention design and/or implementation are suggested by this data?*</td></tr><tr valign="top"><td>28. What lessons might be relevant to other interventions and settings?*</td></tr></tbody></table><table-wrap-foot><p>*These items overlap with domains 2 &#x00026;/or 3. They are included here because domain 1 data collection covers aspects of this information.</p></table-wrap-foot></table-wrap></p><p>As shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, four types of data are collected under Domain 1:<list list-type="order"><list-item><p>Updates from SPIRIT intervention staff: The evaluation officer meets regularly with SPIRIT staff who are working with agencies and external providers. Updates cover on-going developments regarding the selection of intervention components and tailored content, including what choices are made and why; and variations in processes used to recruit and brief providers. The evaluation officer attends weekly SPIRIT staff meetings and so is able to monitor operational processes, including why revisions are made to implementation plans.</p></list-item><list-item><p>Structured observation for fidelity assessment: Intervention delivery checklists are used to document core information about each session (dates, duration, etc.) and the extent to which each component&#x02019;s essential elements are delivered. Checklists are scored quantitatively where possible and completed by the evaluation officer in each session using direct observation backed by digital audio recording. A scoring guide defines criteria for each essential element. A list of the essential elements and the participant feedback form for each session are sent to the session providers a week before delivery. See Additional file <xref rid="MOESM2" ref-type="media">2</xref> for an example of the delivery checklist used for the SPIRIT Leadership Program session &#x02018;Supporting organisational use of evidence.&#x02019;</p></list-item><list-item><p>Email information: The SPIRIT trial coordinator keeps records of the content and dates of emails sent by SPIRIT staff (invitations to participate in sessions and in outcome measurement surveys) and by agency staff (CEO endorsements and invitations as above).</p></list-item><list-item><p>Knowledge brokering records: We use existing monitoring and evaluation processes for the brokered intervention component (component 4 in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>). These comprise standardised quality assurance interviews with the person at the agency commissioning the product, and the lead of the research team that develops the product. Interviews take place six months after the completion of the work and ask for reflections on the brokering process (satisfaction, efficiency), level of contact between the agency and the research team, and the utility of the final product [<xref ref-type="bibr" rid="CR48">48</xref>].</p></list-item></list></p></sec><sec id="Sec9"><title>Domain 2: Participation and response</title><p>Six types of data are collected to meet the Domain 2 objectives:<list list-type="order"><list-item><p>Pre-session sign-in and consent: Participants are asked to sign in at the beginning of sessions, state their job title/position and give or decline consent for process evaluation data collection (digital recording and note-taking). Information about professional roles allows us to document different types of reach and participation in each agency.</p></list-item><list-item><p>Semi-structured and structured observation: The evaluation officer observes and digitally records intervention sessions. The delivery checklist (described above) is used to collate structured information about participation and responses to SPIRIT. Descriptive field notes are taken to supply supporting data (e.g., examples of body language or interactions that illustrate the quality of participation) and to record information the checklist does not cover, such as how participants appear to interact with session contents, providers, and with each other; plus any contributions that might help answer our research questions. Notes are marked to indicate potentially valuable comments that should be verified using the audio recording. Immediately after each session, these notes are entered into a semi-structured session memo template in order to synthesise key aspects of the data and link it to other sources, and to explore hypotheses that will inform further data collection.</p></list-item><list-item><p>Self-reported evaluation feedback: Participants are asked to complete anonymous feedback forms immediately after intervention sessions. Session-specific questions derive from the SPIRIT change principles. Additional file <xref rid="MOESM3" ref-type="media">3</xref> provides an example of the feedback form used for the Leadership Program session &#x02018;Supporting organisational use of evidence.&#x02019;</p></list-item><list-item><p>Informal conversations after sessions: When circumstances allow, the evaluation officer engages in conversation with participants about their views of the session, and wider implications of the topic/contents for their work. Providers are asked about their assessment of the session. Field notes are written immediately afterwards.</p></list-item><list-item><p>Interviews: Purposively sampled participants from each agency are interviewed at two points: early in their agency&#x02019;s intervention period, and after the intervention has concluded. Interviewees include:<list list-type="alpha-lower"><list-item><p>Each agency&#x02019;s Liaison Person (the member of staff who has been nominated to support the administration and promotion of SPIRIT in their agency).</p></list-item><list-item><p>Up to six staff identified from their responses to an online survey conducted as part of the outcome measures. They are selected for the range of variation (the highest and lowest scores) in three Domains of this tool: valuing research, confidence in using research, and research use behaviours. Interviewees are not informed of their individual survey scores.</p></list-item><list-item><p>Further staff identified by the evaluation officer during observation, or by other interviewees, as being highly informative regarding research use in their agency.</p></list-item></list></p></list-item></list></p><p>The first round of interviews focuses on agency culture and context (see section below). The second round focuses on how the interviewee, their team and the wider organisation perceived and responded to the intervention and other aspects of SPIRIT such as the outcome measures and the process evaluation. A flow chart of open-ended questions and prompts derived from the SPIRIT program logic model is used to explore interviewees&#x02019; views and accounts of how the intervention may have influenced their, and their organisation&#x02019;s, capacity to use research. A copy of the interview schedule for general participants (i.e., not the Liaison Person or CEO) is available in Additional file <xref rid="MOESM4" ref-type="media">4</xref>. All participant interviews are digitally recorded and professionally transcribed. An unstructured memo is written directly after each interview to capture initial analytic thinking and hypotheses. Memos are further developed when the transcriptions are read, corrected and de-identified.<list list-type="simple"><list-item><label>6.</label><p>Interviews, meetings and informal conversations: Throughout the trial, information is collected from the people implementing SPIRIT about participation and responses to the intervention, the outcome measures, and the process evaluation. Ad hoc conversations address issues such as how participants are responding to requests to complete outcome measures, and feedback from Liaison People about the administrative tasks they are engaged in. SPIRIT staff were interviewed after agency visits during the pre-intervention agency engagement phase, and continue to be interviewed after they provide mid-intervention feedback (these sessions are not attended by the evaluation officer). Questions focus on agency attitudes to SPIRIT and any factors that might affect engagement and outcomes.</p></list-item></list></p></sec><sec id="Sec10"><title>Domain 3: Context</title><p>We conceptualised context as incorporating the social, structural and political environment of each participating organisation, but focus on six dimensions that we identified from the bodies of literature described earlier:<list list-type="simple"><list-item><label>(i)</label><p>work practices and culture;</p></list-item><list-item><label>(ii)</label><p>agenda-setting and work prioritisation;</p></list-item><list-item><label>(iii)</label><p>leadership styles and how leaders are perceived;</p></list-item><list-item><label>(iv)</label><p>how different kinds of information, including research, is accessed, used and valued by individuals and the broader organisation;</p></list-item><list-item><label>(v)</label><p>barriers and enablers to using research;</p></list-item><list-item><label>(vi)</label><p>any other contextual factors that might affect outcomes.</p></list-item></list></p><p>Four types of data are collected under this Domain:<list list-type="order"><list-item><p>Structured observation: The delivery checklist and supplementary field notes are used to collate core information about the context of sessions (site, facilities, etc.).</p></list-item><list-item><p>Semi-structured observation: During intervention sessions, the evaluation officer takes extensive field notes in relation to the dimensions described above. This information is collated using the same methods as for the participation information (described above in Domain 2, data type 2: Semi-structured and structured observation).</p></list-item><list-item><p>Interviews: These take place with purposively sampled participants in the early phase of the intervention and focus on capturing information within the six dimensions outlined above. A copy of the interview schedule for general participants is available in Additional file <xref rid="MOESM5" ref-type="media">5</xref>. The CEO, or equivalent, for each agency will be invited to participate in an interview after the final round of outcome measures is complete. This interview will explore why they participated in the trial, what else was going on in and around the organisation that might have affected how staff engage with research, and how change does or does not occur in that organisation.</p></list-item><list-item><p>Interviews, meetings and informal conversations: The interviews, study management meetings, and informal conversations with SPIRIT staff described under Domain 2 also address contextual issues. This includes feedback from agencies about changes in funding, staff and governance; and how they are being affected by developments in external agencies, politics and the media.</p></list-item></list></p><p>Semi-structured running memos are maintained for each of the six organisations that capture information about participation, responses and contextual changes. They include additional information that is collected opportunistically from a variety of sources including ad hoc conversations with agency staff at non-SPIRIT forums (e.g., conferences) and electronic media such as Twitter and government websites. A cross-agency memo is maintained to capture overarching issues and themes, including emerging themes that require more investigation in the field.</p></sec></sec><sec id="Sec11"><title>Program improvement</title><p>Interview and observational data are collected across each Domain to inform program improvement recommendations for future studies. For Domain 1 (implementation), we focus on how delivery might be improved. This includes fidelity considerations (the congruence between intended and actual delivery) and factors that are not specified in the implementation plan, such as day-to-day communication strategies and creative variations introduced by providers. In Domain 2 (participation and response), we ask how the intervention content, structure and techniques might have better met the needs of the targeted personnel in each agency and effected change more successfully. In Domain 3 (context), we focus on how each intervention setting may have influenced proximal outcomes (including participation) and distal outcomes as measured in the trial, and how the design and delivery of the intervention could have been more appropriate for and responsive to agency culture and context.</p></sec><sec id="Sec12"><title>Data management and analysis</title><sec id="Sec13"><title>Domain 1 (implementation)</title><p>Data from the delivery checklists and participant feedback forms is entered into a database that contains fields for each item by session and by agency. This provides a comparative overview of delivery fidelity, why intentional and unintentional changes are made, participants&#x02019; evaluative feedback, and other critical information about intervention sites, program delivery and participation. See Additional file <xref rid="MOESM6" ref-type="media">6</xref> for an example of the spreadsheet used to collate data about a Leadership Program session. Analysis will focus on variation between agencies in how the intervention was implemented, particularly differences in the proportion of essential elements delivered at each site and differences in participant feedback, and any association between the two.</p></sec><sec id="Sec14"><title>Domains 2 (participation and response) and 3 (context)</title><p>All other data (early- and post-intervention interview transcripts, and session, agency and interview memos) are managed using Framework Analysis [<xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref>]. Framework allows large amounts of diverse data to be analysed systematically, it is more transparent than most qualitative data analysis methods, it simplifies and supports comparative case analysis, and it enables us to review in-progress analysis as a team [<xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref>]. All transcripts are uploaded to NVivo 10 [<xref ref-type="bibr" rid="CR51">51</xref>] and synthesised in matrices. We use three matrices: one for Domain 2 participation and response data; one for Domain 3 context data; and a third that collates data about participants&#x02019; research and information utilisation. Data are organised both by case (individuals clustered by agency) and by category. Categories were developed by the process evaluation team reviewing preliminary interview data and memos in relation to the process evaluation questions and the SPIRIT program logic model, and multiple coding interview transcripts to test and revise the categories. Categories include the range of intervention implementation strategies and the research engagement actions identified in the SPIRIT program logic model. Additional file <xref rid="MOESM7" ref-type="media">7</xref> lists the categories used for Domain 2 (participation and response) and Domain 3 (context) framework matrices. Limited qualitative information from two outcome measures is included; the person conducting and coding the outcome measures interviews collates de-identified data from transcripts in relation to the six dimensions used to guide observations (described above) and provides it in a form that allows it to be integrated into the process evaluation framework matrices for analysis. Completed matrices thereby synthesise our varied data within broad categories, in preparation for more interpretive analysis [<xref ref-type="bibr" rid="CR49">49</xref>]. Memos from interviews and intervention sessions, agency memos and the overarching cross-agency memo are re-read and coded in NVivo using Domain 2 and 3 framework categories.</p><p>All data related to each category is clustered and reviewed inductively, identifying key themes from close reading of the data &#x02014; across all sessions and participants &#x02014; to identify and distil the variation of views, experiences and behaviours within each agency. This work includes the development of schematic case studies for each agency. Interpretive memos are written that refine the theme, linking it to corroborating data sources and, when appropriate, linking it to other categories or themes in NVivo or in the implementation fidelity database so that each theme is supported by the broadest range of evidence. This data is reviewed in relation to the interactions between delivery, participation and context, and will also be reviewed in relation to outcomes when they are known. Analysis is on-going and guides continuing data collection. Early themes are revisited in the light of subsequent analytical changes [<xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref>]. Themes are explored within and across cases and will be reviewed in relation to the outcomes. The small process evaluation team, who co-designed the process evaluation and monitor its implementation, review distilled data and discuss interpretations.</p></sec></sec></sec><sec id="Sec15"><title>Trial status</title><p>SPIRIT is currently being implemented. The trial will conclude in April 2015.</p></sec><sec id="Sec16" sec-type="discussion"><title>Discussion</title><p>In this paper, we describe the design of a process evaluation embedded within a trial of a complex intervention designed to build individual and organisational capacity to use research in policy and program development. We report the methods of the process evaluation and discuss how they are functioning during implementation. In doing so, we contribute to the literature in three ways: (i) we provide a worked example of how to embed process evaluation in the design and evaluation of a complex intervention (these are rare in the literature [<xref ref-type="bibr" rid="CR15">15</xref>]); (ii) we illustrate an approach to tackling the challenges of complexity in the intervention and its implementation settings; and (iii) we provide, to our knowledge, the only detailed example of the methods for a process evaluation of an intervention conducted as part of a randomised trial in policy organisations.</p><sec id="Sec17"><title>Strengths and weaknesses</title><p>As an integral part of the SPIRIT trial, the process evaluation is well-resourced, detailed and has good access to a range of rich data sources. Early trialling and consultation with policy and program colleagues helped us identify methods that would be appropriate, feasible and effective. For example, they advised us that focus groups would have low attendance and that interviews would obtain franker responses. Also, that our original plan to use ethnographic methods to study day-to-day work practices would be regarded as unacceptably intrusive, particularly in the context of an evaluation. To date, our methods appear to be appropriate for this trial. They are sufficiently flexible to gather data responsively and, with minor exceptions, there is no indication that the process evaluation has impacted participants&#x02019; comfort or willingness to express themselves frankly in intervention sessions. Participants have given consent for intervention sessions to be observed and recorded, and the majority have completed anonymous feedback forms with no negative comments about the process evaluation. External presenters have understood the purpose of fidelity monitoring and have not appeared to be affected by it. Observations of these sessions (particularly those that are highly interactive) provide access to nuanced information about norms, values, processes, priorities and constraints that is helping us develop rich case studies of each agency&#x02019;s organisational context. The two phases of face-to-face interviews (early interviews focus on organisational culture and work processes, while post-intervention interviews focus on impact) are providing valuable insights about the relationships between the intervention, participation and context. Discussion with the multidisciplinary process evaluation team about emerging themes and interpretations strengthens the trustworthiness of findings.</p><p>However, we note some weaknesses. The role of the evaluation officer may inhibit full and frank feedback. Participants are aware that she works within the study team that includes the researchers responsible for designing and implementing the intervention, and some seem to assume that she is involved in decisions about design and implementation. This may affect the openness with which they talk about the trial. Also, the evaluation officer is a researcher asking participants about an intervention designed to increase how they value and use research. It is likely that she is not perceived as disinterested, and this may result in social desirability bias in interviews. The process evaluation itself may add to the burden of participation, and people may find it hard to raise this. To date, these factors do not appear to have had significant effects. As in previous interview-based studies with policymakers [<xref ref-type="bibr" rid="CR52">52</xref>], respondents have been generous with their time in interviews and our impression is that most have spoken openly, but people with concerns may have been deterred from participating in interviews in the first place.</p><p>Lastly, comprehensive evaluation of the action framework is outside the scope of process evaluation, so our contribution to the evolution of the SPIRIT Action Framework will be limited. We should be able to comment on applicability of the Framework within the parameters of this trial, and to flesh out some of the nuances in the relationships between its component parts. But more targeted and responsive data collection and analysis is needed to generate hypotheses that can inform further iterations of the Framework. As in other investigations of policy processes to date, our methods do not fully access the central phenomenon of policy decision-making and the role (current and potential) that research plays in it.</p></sec></sec><sec id="Sec18" sec-type="conclusion"><title>Conclusion</title><p>This paper presents a detailed protocol for the process evaluation of a unique complex intervention in health policy and program agencies. A key feature of the design is the development of flexible and pragmatic methods to capture data across three Domains: 1) How the intervention was delivered; 2) How people in each agency participated in and responded to the intervention; and 3) Contextual factors that may affect how the intervention was delivered and received. We provide examples of tools used. The data will be used to develop an understanding of how and why the intervention had the effects it did (or did not) in each setting, and to draw out implications for improving future interventions. In order to conduct a process evaluation in busy policy agencies, we had to develop methods that optimised data collection but minimised discomfort and burden for participants. We discuss some strengths and weakness associated with these methods, informed by reflections on the early implementation of the process evaluation. Given that we found little concrete guidance in the literature to help us develop these methods and tools, our account may provide a useful reference for others developing process evaluations for trials of complex interventions.</p></sec></body><back><app-group><app id="App1"><sec id="Sec19"><title>Additional files</title><p><media position="anchor" xlink:href="13012_2014_113_MOESM1_ESM.docx" id="MOESM1"><label>Additional file 1</label><caption><p>
<bold>Explanation of terms used in this article.</bold>
</p></caption></media><media position="anchor" xlink:href="13012_2014_113_MOESM2_ESM.docx" id="MOESM2"><label>Additional file 2</label><caption><p>
<bold>Example of a delivery checklist.</bold>
</p></caption></media><media position="anchor" xlink:href="13012_2014_113_MOESM3_ESM.docx" id="MOESM3"><label>Additional file 3</label><caption><p>
<bold>Example of a feedback form.</bold>
</p></caption></media><media position="anchor" xlink:href="13012_2014_113_MOESM4_ESM.docx" id="MOESM4"><label>Additional file 4</label><caption><p>
<bold>Post-intervention process evaluation interviews.</bold>
</p></caption></media><media position="anchor" xlink:href="13012_2014_113_MOESM5_ESM.docx" id="MOESM5"><label>Additional file 5</label><caption><p>
<bold>Early process evaluation interviews.</bold>
</p></caption></media><media position="anchor" xlink:href="13012_2014_113_MOESM6_ESM.docx" id="MOESM6"><label>Additional file 6</label><caption><p>
<bold>Spreadsheet for data management of Leadership program session: &#x02018;Supporting organisational use of evidence&#x02019;.</bold>
</p></caption></media><media position="anchor" xlink:href="13012_2014_113_MOESM7_ESM.docx" id="MOESM7"><label>Additional file 7</label><caption><p>
<bold>Categories used for Domain 2 and Domain 3 Framework matrices.</bold>
</p></caption></media></p></sec></app></app-group><fn-group><fn><p><bold>Competing interests</bold></p><p>SPIRIT is funded as part of the Centre for Informing Policy in Health with Evidence from Research (CIPHER), an Australian National Health and Medical Research Council Centre for Research Excellence (APP1001436) and administered by the University of Western Sydney. The Sax Institute receives a grant from the NSW Ministry of Health. The Australasian Cochrane Centre is funded by the Australian Government through the National Health and Medical Research Council (NHMRC). SC is supported by an NHMRC Career Development Fellowship (1032963). DO holds an NHMRC Public Health Fellowship (606726). DO is an Associate Editor of Implementation Science. All editorial decisions regarding this manuscript were made by another editor.</p></fn><fn><p><bold>Authors&#x02019; contributions</bold></p><p>AH leads the design and conduct of the process evaluation, and drafted the manuscript. SB, SC, DO, CHS, TT and GG were involved in the design of the process evaluation. SB, SC, DO and GG have a continuing role in monitoring the conduct of the process evaluation. The CIPHER team investigators conceived of the SPIRIT study. All named authors contributed substantially to and approved the final manuscript.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The writing team wish to thank the people and organisations participating in SPIRIT. We especially thank the staff of the participating organisations who have generously given their time and insight in interviews and other data collection activities. SPIRIT is being conducted by the CIPHER Centre for Research Excellence. CIPHER is a joint project of the Sax Institute; Australasian Cochrane Centre, Monash University; the University of Newcastle; The University of New South Wales; The University of Technology, Sydney; the Research Unit for Research Utilisation, University of St Andrews and University of Edinburgh; and the University of Western Sydney.</p><p>We also wish to thank members of the CIPHER team who provided valuable feedback on earlier versions of this manuscript: Anna Williamson, Sally Redman, Andrew Milat, Sally Green and Fiona Blyth.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanney</surname><given-names>SR</given-names></name><name><surname>Gonz&#x000e1;lez-Block</surname><given-names>MA</given-names></name></person-group><article-title>Evidence-informed health policy: are we beginning to get there at last</article-title><source>Health Res Policy Syst</source><year>2009</year><volume>22</volume><fpage>30</fpage><pub-id pub-id-type="doi">10.1186/1478-4505-7-30</pub-id><pub-id pub-id-type="pmid">20028537</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petticrew</surname><given-names>M</given-names></name><name><surname>Whitehead</surname><given-names>M</given-names></name><name><surname>Macintyre</surname><given-names>SJ</given-names></name><name><surname>Graham</surname><given-names>H</given-names></name><name><surname>Egan</surname><given-names>M</given-names></name></person-group><article-title>Evidence for public health policy on inequalities: 1: the reality according to policymakers</article-title><source>J Epidemiol Community Health</source><year>2004</year><volume>58</volume><fpage>811</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.1136/jech.2003.015289</pub-id><pub-id pub-id-type="pmid">15365104</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott</surname><given-names>H</given-names></name><name><surname>Popay</surname><given-names>J</given-names></name></person-group><article-title>How are policy makers using evidence? Models of research utilisation and local NHS policy making</article-title><source>J Epidemiol Community Health</source><year>2000</year><volume>54</volume><fpage>461</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1136/jech.54.6.461</pub-id><pub-id pub-id-type="pmid">10818123</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Campbell DM, Redman S, Jorm L, Cooke M, Zwi AB, Rychetnik L: <bold>Increasing the use of evidence in health policy: practice and views of policy makers and researchers.</bold><italic>Aust N Z Health Policy</italic> 2009, <bold>6</bold>.</mixed-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>SUPPORT Project</collab></person-group><article-title>SUPPORT Tools for evidence-informed health Policymaking (STP)</article-title><source>Health Res Policy Syst</source><year>2009</year><volume>7</volume><issue>Supply 1</issue><fpage>S1</fpage><lpage>S18</lpage><pub-id pub-id-type="pmid">20018099</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>G</given-names></name><name><surname>Redman</surname><given-names>S</given-names></name><name><surname>Haines</surname><given-names>M</given-names></name><name><surname>Todd</surname><given-names>A</given-names></name></person-group><article-title>What works to increase the use of research in population health policy and programmes: a review</article-title><source>Evid Policy</source><year>2011</year><volume>7</volume><fpage>277</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1332/174426411X579199</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobbins</surname><given-names>M</given-names></name><name><surname>Hanna</surname><given-names>SE</given-names></name><name><surname>Ciliska</surname><given-names>D</given-names></name><name><surname>Manske</surname><given-names>S</given-names></name><name><surname>Cameron</surname><given-names>R</given-names></name><name><surname>Mercer</surname><given-names>SL</given-names></name><name><surname>O&#x02019;Mara</surname><given-names>L</given-names></name><name><surname>DeCorby</surname><given-names>K</given-names></name><name><surname>Robeson</surname><given-names>P</given-names></name></person-group><article-title>A randomized controlled trial evaluating the impact of knowledge translation and exchange strategies</article-title><source>Implement Sci</source><year>2009</year><volume>4</volume><fpage>61</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-4-61</pub-id><pub-id pub-id-type="pmid">19775439</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Huckel-Schneider C, Campbell D, Milat A, Haynes A, Quinn E: <bold>What are the key organisational capabilities that facilitate research use in public health policy?</bold><italic>Public Health Res Pract</italic> 2014, <bold>1</bold>(1).</mixed-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>CIPHER Investigators</collab></person-group><article-title>Supporting Policy In health with Research: an Intervention Trial (SPIRIT)&#x02014;protocol for a stepped wedge trial</article-title><source>BMJ Open</source><year>2014</year><volume>4</volume><fpage>e005293</fpage><pub-id pub-id-type="doi">10.1136/bmjopen-2014-005293</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Century</surname><given-names>J</given-names></name><name><surname>Rudnick</surname><given-names>M</given-names></name><name><surname>Freeman</surname><given-names>C</given-names></name></person-group><article-title>A framework for measuring fidelity of implementation: a foundation for shared language and accumulation of knowledge</article-title><source>Am J Eval</source><year>2010</year><volume>31</volume><fpage>199</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1177/1098214010366173</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harachi</surname><given-names>TW</given-names></name><name><surname>Abbott</surname><given-names>RD</given-names></name><name><surname>Catalano</surname><given-names>RF</given-names></name><name><surname>Haggerty</surname><given-names>KP</given-names></name><name><surname>Fleming</surname><given-names>CB</given-names></name></person-group><article-title>Opening the Black Box: using process evaluation measures to assess implementation and theory building</article-title><source>Am J Community Psychol</source><year>1999</year><volume>27</volume><fpage>711</fpage><lpage>731</lpage><pub-id pub-id-type="doi">10.1023/A:1022194005511</pub-id><pub-id pub-id-type="pmid">10676545</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oakley</surname><given-names>A</given-names></name><name><surname>Strange</surname><given-names>V</given-names></name><name><surname>Bonell</surname><given-names>C</given-names></name><name><surname>Allen</surname><given-names>E</given-names></name><name><surname>Stephenson</surname><given-names>J</given-names></name><name><surname>Team</surname><given-names>RS</given-names></name></person-group><article-title>Health services research - process evaluation in randomised controlled trials of complex interventions</article-title><source>Br Med J</source><year>2006</year><volume>332</volume><fpage>413</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1136/bmj.332.7538.413</pub-id><pub-id pub-id-type="pmid">16484270</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ling</surname><given-names>T</given-names></name></person-group><article-title>Evaluating complex and unfolding interventions in real time</article-title><source>Evaluation</source><year>2012</year><volume>18</volume><fpage>79</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1177/1356389011429629</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanderson</surname><given-names>I</given-names></name></person-group><article-title>Evaluation in complex policy systems</article-title><source>Evaluation</source><year>2000</year><volume>6</volume><fpage>433</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1177/13563890022209415</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>G</given-names></name><name><surname>Audrey</surname><given-names>S</given-names></name><name><surname>Barker</surname><given-names>M</given-names></name><name><surname>Bond</surname><given-names>L</given-names></name><name><surname>Bonell</surname><given-names>C</given-names></name><name><surname>Cooper</surname><given-names>C</given-names></name><name><surname>Hardeman</surname><given-names>W</given-names></name><name><surname>Moore</surname><given-names>L</given-names></name><name><surname>O&#x02019;Cathain</surname><given-names>A</given-names></name><name><surname>Tinati</surname><given-names>T</given-names></name><name><surname>Wight</surname><given-names>D</given-names></name><name><surname>Baird</surname><given-names>J</given-names></name></person-group><article-title>Process evaluation in complex public health intervention studies: the need for guidance</article-title><source>J Epidemiol Community Health</source><year>2013</year><volume>68</volume><fpage>101</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1136/jech-2013-202869</pub-id><pub-id pub-id-type="pmid">24022816</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Audrey</surname><given-names>S</given-names></name><name><surname>Holliday</surname><given-names>J</given-names></name><name><surname>Parry-Langdon</surname><given-names>N</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name></person-group><article-title>Meeting the challenges of implementing process evaluation within randomized controlled trials: the example of ASSIST (A Stop Smoking in Schools Trial)</article-title><source>Health Educ Res</source><year>2006</year><volume>21</volume><fpage>366</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1093/her/cyl029</pub-id><pub-id pub-id-type="pmid">16740670</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulscher</surname><given-names>MEJL</given-names></name><name><surname>Laurant</surname><given-names>MGH</given-names></name><name><surname>Grol</surname><given-names>RPTM</given-names></name></person-group><article-title>Process evaluation on quality improvement interventions</article-title><source>Qual Saf Health Care</source><year>2003</year><volume>12</volume><fpage>40</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1136/qhc.12.1.40</pub-id><pub-id pub-id-type="pmid">12571344</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>A</given-names></name><name><surname>Treweek</surname><given-names>S</given-names></name><name><surname>Dreischulte</surname><given-names>T</given-names></name><name><surname>Foy</surname><given-names>R</given-names></name><name><surname>Guthrie</surname><given-names>B</given-names></name></person-group><article-title>Process evaluations for cluster-randomised trials of complex interventions: a proposed framework for design and reporting</article-title><source>Trials</source><year>2013</year><volume>14</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.1186/1745-6215-14-15</pub-id><pub-id pub-id-type="pmid">23311722</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grimshaw</surname><given-names>JM</given-names></name><name><surname>Zwarenstein</surname><given-names>M</given-names></name><name><surname>Tetroe</surname><given-names>JM</given-names></name><name><surname>Godin</surname><given-names>G</given-names></name><name><surname>Graham</surname><given-names>ID</given-names></name><name><surname>Lemyre</surname><given-names>L</given-names></name><name><surname>Eccles</surname><given-names>MP</given-names></name><name><surname>Johnston</surname><given-names>M</given-names></name><name><surname>Francis</surname><given-names>JJ</given-names></name><name><surname>Hux</surname><given-names>J</given-names></name></person-group><article-title>Looking inside the black box: a theory-based process evaluation alongside a randomised controlled trial of printed educational materials (the Ontario printed educational message, OPEM) to improve referral and prescribing practices in primary care in Ontario Canada</article-title><source>Implement Sci</source><year>2007</year><volume>2</volume><fpage>38</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-2-38</pub-id><pub-id pub-id-type="pmid">18039362</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H-T</given-names></name></person-group><source>Practical program evaluation: assessing and improving planning, implementation, and effectiveness</source><year>2005</year><publisher-loc>Thousand Oaks, CA</publisher-loc><publisher-name>Sage Publications</publisher-name></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>H</given-names></name></person-group><article-title>Systematic evaluation of implementation fidelity of complex interventions in health and social care</article-title><source>Implement Sci</source><year>2010</year><volume>5</volume><fpage>9</fpage><pub-id pub-id-type="doi">10.1186/1748-5908-5-67</pub-id><pub-id pub-id-type="pmid">20205873</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borrelli</surname><given-names>B</given-names></name><name><surname>Sepinwall</surname><given-names>D</given-names></name><name><surname>Ernst</surname><given-names>D</given-names></name><name><surname>Bellg</surname><given-names>AJ</given-names></name><name><surname>Czajkowski</surname><given-names>S</given-names></name><name><surname>Breger</surname><given-names>R</given-names></name><name><surname>DeFrancesco</surname><given-names>C</given-names></name><name><surname>Levesque</surname><given-names>C</given-names></name><name><surname>Sharp</surname><given-names>DL</given-names></name><name><surname>Ogedegbe</surname><given-names>G</given-names></name><name><surname>Resnick</surname><given-names>B</given-names></name><name><surname>Orwig</surname><given-names>D</given-names></name></person-group><article-title>A new tool to assess treatment fidelity and evaluation of treatment fidelity across 10&#x000a0;years of health behavior research</article-title><source>J Consult Clin Psychol</source><year>2005</year><volume>73</volume><fpage>852</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1037/0022-006X.73.5.852</pub-id><pub-id pub-id-type="pmid">16287385</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dusenbury</surname><given-names>L</given-names></name><name><surname>Brannigan</surname><given-names>R</given-names></name><name><surname>Falco</surname><given-names>M</given-names></name><name><surname>Hansen</surname><given-names>WB</given-names></name></person-group><article-title>A review of research on fidelity of implementation: implications for drug abuse prevention in school settings</article-title><source>Health Educ Res</source><year>2003</year><volume>18</volume><fpage>237</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1093/her/18.2.237</pub-id><pub-id pub-id-type="pmid">12729182</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durlak</surname><given-names>JA</given-names></name><name><surname>Dupre</surname><given-names>EP</given-names></name></person-group><article-title>Implementation matters: a review of research on the influence of implementation on program outcomes and the factors affecting implementation</article-title><source>Am J Community Psychol</source><year>2008</year><volume>41</volume><fpage>327</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1007/s10464-008-9165-0</pub-id><pub-id pub-id-type="pmid">18322790</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellg</surname><given-names>AJ</given-names></name><name><surname>Borrelli</surname><given-names>B</given-names></name><name><surname>Resnick</surname><given-names>B</given-names></name><name><surname>Hecht</surname><given-names>J</given-names></name><name><surname>Minicucci</surname><given-names>DS</given-names></name><name><surname>Ory</surname><given-names>M</given-names></name><name><surname>Ogedegbe</surname><given-names>G</given-names></name><name><surname>Orwig</surname><given-names>D</given-names></name><name><surname>Ernst</surname><given-names>D</given-names></name><name><surname>Czajkowski</surname><given-names>S</given-names></name></person-group><article-title>Enhancing treatment fidelity in health behavior change studies: best practices and recommendations from the NIH behavior change consortium</article-title><source>Health Psychol</source><year>2004</year><volume>23</volume><fpage>443</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1037/0278-6133.23.5.443</pub-id><pub-id pub-id-type="pmid">15367063</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Carroll C, Patterson M, Wood S, Booth A, Rick J, Balain S: <bold>A conceptual framework for implementation fidelity.</bold><italic>Implement Sci</italic> 2007, <bold>2</bold>(40). doi:4010.1186/1748-5908-2-40.</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>CH</given-names></name></person-group><article-title>The many meanings of research utilization</article-title><source>Public Adm Rev</source><year>1979</year><volume>39</volume><fpage>426</fpage><lpage>431</lpage><pub-id pub-id-type="doi">10.2307/3109916</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>CH</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Wagner</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>CH</given-names></name><name><surname>Wittrock</surname><given-names>B</given-names></name><name><surname>Wollman</surname><given-names>H</given-names></name></person-group><article-title>Policy Research: Data, Ideas or Argument?</article-title><source>Social Sciences and Modern States: National Experiences and Theoretical Crossroads</source><year>1991</year><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenhalgh</surname><given-names>T</given-names></name><name><surname>Robert</surname><given-names>G</given-names></name><name><surname>Macfarlane</surname><given-names>F</given-names></name><name><surname>Bate</surname><given-names>P</given-names></name><name><surname>Kyriakidou</surname><given-names>O</given-names></name></person-group><article-title>Diffusion of innovations in service organizations: systematic review and recommendations</article-title><source>Milbank Q</source><year>2004</year><volume>82</volume><fpage>581</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.1111/j.0887-378X.2004.00325.x</pub-id><pub-id pub-id-type="pmid">15595944</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>J</given-names></name><name><surname>Greenhalgh</surname><given-names>T</given-names></name><name><surname>Byrne</surname><given-names>E</given-names></name><name><surname>McDonnell</surname><given-names>J</given-names></name></person-group><article-title>Recognizing rhetoric in health care policy analysis</article-title><source>J Health Serv Res Policy</source><year>2008</year><volume>13</volume><fpage>40</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1258/jhsrp.2007.006029</pub-id><pub-id pub-id-type="pmid">18325155</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavis</surname><given-names>JN</given-names></name><name><surname>Ross</surname><given-names>SE</given-names></name><name><surname>Hurley</surname><given-names>JE</given-names></name><name><surname>Hohenadel</surname><given-names>JM</given-names></name><name><surname>Stoddart</surname><given-names>GL</given-names></name><name><surname>Woodward</surname><given-names>CA</given-names></name><name><surname>Abelson</surname><given-names>J</given-names></name></person-group><article-title>Examining the role of health services research in public policymaking</article-title><source>Milbank Q</source><year>2002</year><volume>80</volume><fpage>125</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1111/1468-0009.00005</pub-id><pub-id pub-id-type="pmid">11933791</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Hanney S, Gonzalez-Block M, Buxton M, Kogan M: <bold>The utilisation of health research in policy-making: concepts, examples and methods of assessment.</bold><italic>Health Res Policy Syst</italic> 2003, <bold>1</bold>(2).</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nutley</surname><given-names>SM</given-names></name><name><surname>Walter</surname><given-names>I</given-names></name><name><surname>Davies</surname><given-names>HTO</given-names></name></person-group><source>Using Evidence: How Research can Inform Public Services</source><year>2007</year><publisher-loc>Bristol</publisher-loc><publisher-name>The Policy Press</publisher-name></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waters</surname><given-names>E</given-names></name><name><surname>Armstrong</surname><given-names>R</given-names></name><name><surname>Swinburn</surname><given-names>B</given-names></name><name><surname>Moore</surname><given-names>L</given-names></name><name><surname>Dobbins</surname><given-names>M</given-names></name><name><surname>Anderson</surname><given-names>L</given-names></name><name><surname>Petticrew</surname><given-names>M</given-names></name><name><surname>Clark</surname><given-names>R</given-names></name><name><surname>Conning</surname><given-names>R</given-names></name><name><surname>Moodie</surname><given-names>M</given-names></name><name><surname>Carter</surname><given-names>R</given-names></name></person-group><article-title>An exploratory cluster randomised controlled trial of knowledge translation strategies to support evidence-informed decision-making in local governments (The KT4LG study)</article-title><source>BMC Public Health</source><year>2011</year><volume>11</volume><fpage>34</fpage><pub-id pub-id-type="doi">10.1186/1471-2458-11-34</pub-id><pub-id pub-id-type="pmid">21226966</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rashman</surname><given-names>L</given-names></name><name><surname>Withers</surname><given-names>E</given-names></name><name><surname>Hartley</surname><given-names>J</given-names></name></person-group><article-title>Organizational learning and knowledge in public service organizations: a systematic review of the literature</article-title><source>Int J Manag Rev</source><year>2009</year><volume>11</volume><fpage>463</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1111/j.1468-2370.2009.00257.x</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Senge</surname><given-names>PM</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Morey</surname><given-names>D</given-names></name><name><surname>Maybury</surname><given-names>MT</given-names></name><name><surname>Thuraisingham</surname><given-names>B</given-names></name></person-group><article-title>Reflections on &#x0201c;a leader&#x02019;s new Work: Building Learning Organizations&#x0201d;</article-title><source>Knowledge Management: Classic and Contemporary Works</source><year>2002</year><publisher-loc>Cambridge, Mass</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>53</fpage><lpage>60</lpage></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheetham</surname><given-names>G</given-names></name><name><surname>Chivers</surname><given-names>G</given-names></name></person-group><article-title>How professionals learn in practice: an investigation of informal learning amongst people working in professions</article-title><source>J Eur Ind Train</source><year>2001</year><volume>25</volume><fpage>247</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1108/03090590110395870</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>HA</given-names></name></person-group><article-title>Bounded rationality and organizational learning</article-title><source>Organ Sci</source><year>1991</year><volume>2</volume><fpage>125</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1287/orsc.2.1.125</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Knowles</surname><given-names>MS</given-names></name><name><surname>Holton</surname><given-names>EF</given-names><suffix>III</suffix></name><name><surname>Swanson</surname><given-names>RA</given-names></name></person-group><source>The Adult Learner</source><year>2012</year><edition>7</edition><publisher-loc>Oxford</publisher-loc><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>JS</given-names></name><name><surname>Duguid</surname><given-names>P</given-names></name></person-group><article-title>Organizational learning and communities of practice: towards a unified view of working, learning, and innovation</article-title><source>Organ Sci</source><year>1991</year><volume>2</volume><fpage>40</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1287/orsc.2.1.40</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawe</surname><given-names>P</given-names></name><name><surname>Shiell</surname><given-names>A</given-names></name><name><surname>Riley</surname><given-names>T</given-names></name></person-group><article-title>Theorising interventions as events in systems</article-title><source>Am J Community Psychol</source><year>2009</year><volume>43</volume><fpage>267</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1007/s10464-009-9229-9</pub-id><pub-id pub-id-type="pmid">19390961</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Best</surname><given-names>A</given-names></name><name><surname>Holmes</surname><given-names>B</given-names></name></person-group><article-title>Systems thinking, knowledge and action: towards better models and methods</article-title><source>Evid Policy</source><year>2010</year><volume>6</volume><fpage>145</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1332/174426410X502284</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiell</surname><given-names>A</given-names></name><name><surname>Hawe</surname><given-names>P</given-names></name><name><surname>Gold</surname><given-names>L</given-names></name></person-group><article-title>Complex interventions or complex systems? Implications for health economic evaluation</article-title><source>Br Med J</source><year>2008</year><volume>336</volume><fpage>1281</fpage><lpage>1283</lpage><pub-id pub-id-type="doi">10.1136/bmj.39569.510521.AD</pub-id><pub-id pub-id-type="pmid">18535071</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>F</given-names></name><name><surname>Wiles</surname><given-names>R</given-names></name><name><surname>Kinmonth</surname><given-names>A-L</given-names></name><name><surname>Mant</surname><given-names>D</given-names></name><name><surname>Gantley</surname><given-names>M</given-names></name></person-group><article-title>Development and evaluation of complex interventions in health services research: case study of the Southampton heart integrated care project (SHIP)</article-title><source>BMJ</source><year>1999</year><volume>318</volume><fpage>711</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1136/bmj.318.7185.711</pub-id><pub-id pub-id-type="pmid">10074018</pub-id></element-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O&#x02019;Cathain</surname><given-names>A</given-names></name><name><surname>Goode</surname><given-names>J</given-names></name><name><surname>Drabble</surname><given-names>SJ</given-names></name><name><surname>Thomas</surname><given-names>KJ</given-names></name><name><surname>Rudolph</surname><given-names>A</given-names></name><name><surname>Hewison</surname><given-names>J</given-names></name></person-group><article-title>Getting added value from using qualitative research with randomized controlled trials: a qualitative interview study</article-title><source>Trials</source><year>2014</year><volume>15</volume><fpage>215</fpage><pub-id pub-id-type="doi">10.1186/1745-6215-15-215</pub-id><pub-id pub-id-type="pmid">24913438</pub-id></element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Patton</surname><given-names>M</given-names></name></person-group><source>Developmental evaluation: applying complexity concepts to enhance innovation and use</source><year>2011</year><publisher-loc>New York</publisher-loc><publisher-name>Guilford Press</publisher-name></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>K</given-names></name></person-group><source>Developmental Evaluation: An Approach to Evaluating Complex Social Change Initiatives. Presentation at Next Generation Evaluation Conference: Embracing Complexity, Connectivity and Change</source><year>2013</year><publisher-loc>Stanford University</publisher-loc><publisher-name>FSG: Foundation Strategy Group</publisher-name></element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>D</given-names></name><name><surname>Donald</surname><given-names>B</given-names></name><name><surname>Moore</surname><given-names>G</given-names></name><name><surname>Frew</surname><given-names>D</given-names></name></person-group><article-title>Evidence Check: knowledge brokering to commission research reviews for policy</article-title><source>Evid Policy</source><year>2011</year><volume>7</volume><fpage>97</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1332/174426411X553034</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>J</given-names></name><name><surname>Spencer</surname><given-names>L</given-names></name><name><surname>O&#x02019;Connor</surname><given-names>W</given-names></name></person-group><source>Qualitative Research Practice: A Guide for Social Science Students and Researchers</source><year>2003</year><publisher-loc>London</publisher-loc><publisher-name>Sage</publisher-name><fpage>219</fpage><lpage>262</lpage></element-citation></ref><ref id="CR50"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pope</surname><given-names>C</given-names></name><name><surname>Ziebland</surname><given-names>S</given-names></name><name><surname>Mays</surname><given-names>N</given-names></name></person-group><article-title>Analysing qualitative data</article-title><source>Br Med J</source><year>2000</year><volume>320</volume><fpage>114</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1136/bmj.320.7227.114</pub-id><pub-id pub-id-type="pmid">10625273</pub-id></element-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">QSR International Pty Ltd: <italic>NVivo Qualitative Data Analysis Software: Version 10.</italic> 2012.</mixed-citation></ref><ref id="CR52"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>A</given-names></name><name><surname>Gillespie</surname><given-names>JA</given-names></name><name><surname>Derrick</surname><given-names>GE</given-names></name><name><surname>Hall</surname><given-names>WD</given-names></name><name><surname>Sally</surname><given-names>R</given-names></name><name><surname>Simon</surname><given-names>C</given-names></name><name><surname>Heidi</surname><given-names>S</given-names></name></person-group><article-title>Galvanizers guides, champions, and shields: the many ways that policymakers Use public health researchers</article-title><source>Milbank Q</source><year>2011</year><volume>89</volume><fpage>564</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1111/j.1468-0009.2011.00643.x</pub-id><pub-id pub-id-type="pmid">22188348</pub-id></element-citation></ref></ref-list></back></article>
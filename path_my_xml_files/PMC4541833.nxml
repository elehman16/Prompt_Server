<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26131678</article-id><article-id pub-id-type="pmc">4541833</article-id><article-id pub-id-type="doi">10.3390/s150715326</article-id><article-id pub-id-type="publisher-id">sensors-15-15326</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Novel Machine Vision System for the Inspection of Micro-Spray Nozzle</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Kuo-Yi</given-names></name><xref ref-type="aff" rid="af1-sensors-15-15326">1</xref><xref rid="c1-sensors-15-15326" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Ye</surname><given-names>Yu-Ting</given-names></name><xref ref-type="aff" rid="af2-sensors-15-15326">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Passaro</surname><given-names>Vittorio M.N.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-15-15326"><label>1</label>Department of Bio-Industrial Mechatronics Engineering, National Chung Hsing University, Tai-Chung 402, Taiwan</aff><aff id="af2-sensors-15-15326"><label>2</label>Department of Mechatronic Engineering, Huafan University, New Taipei City 223, Taiwan; E-Mail: <email>alexyeh1120@hotmail.com</email></aff><author-notes><corresp id="c1-sensors-15-15326"><label>*</label>Author to whom correspondence should be addressed; E-Mail: <email>kuoyi@nchu.edu.tw</email>; Tel.: +886-4-2285-2463; Fax: +886-4-2285-9351.</corresp></author-notes><pub-date pub-type="epub"><day>29</day><month>6</month><year>2015</year></pub-date><pub-date pub-type="collection"><month>7</month><year>2015</year></pub-date><volume>15</volume><issue>7</issue><fpage>15326</fpage><lpage>15338</lpage><history><date date-type="received"><day>14</day><month>5</month><year>2015</year></date><date date-type="accepted"><day>25</day><month>6</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; 2015 by the authors; licensee MDPI, Basel, Switzerland.</copyright-statement><copyright-year>2015</copyright-year><license><license-p><!--CREATIVE COMMONS-->This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>In this study, we present an application of neural network and image processing techniques for detecting the defects of an internal micro-spray nozzle. The defect regions were segmented by Canny edge detection, a randomized algorithm for detecting circles and a circle inspection (CI) algorithm. The gray level co-occurrence matrix (GLCM) was further used to evaluate the texture features of the segmented region. These texture features (contrast, entropy, energy), color features (mean and variance of gray level) and geometric features (distance variance, mean diameter and diameter ratio) were used in the classification procedures. A back-propagation neural network classifier was employed to detect the defects of micro-spray nozzles. The methodology presented herein effectively works for detecting micro-spray nozzle defects to an accuracy of 90.71%.</p></abstract><kwd-group><kwd>micro-spray nozzle</kwd><kwd>image processing</kwd><kwd>neural network</kwd></kwd-group></article-meta></front><body><sec><title>1. Introduction</title><p>Nozzles are a common mechanical component and are widely used in agriculture, manufacturing and the service industry. A micro-spray nozzle is an important component in an agricultural sprayer, which is used to spray water, pesticides and nutrients. Real-time flow uniformity depends on the internal quality of the micro-spray nozzle. Unstable flow will be created because of internal defects. The price and quality of micro-spray nozzles are down due to such defects. The internal defects of micro-spray nozzles usually result during cutting procedures with a CNC (computer numerical control) machine. Chang and Yu [<xref rid="B1-sensors-15-15326" ref-type="bibr">1</xref>] proposed a triangular-pitch shell-and-tube spray evaporator featuring an interior spray technique. The dry-out phenomenon is prevented, and the heat transfer performance is improved in their study. In addition, micro-spray nozzles have to be sorted manually, because the surface of the micro-spray nozzle is heterogeneous, which makes auto-detection very difficult.</p><p>Image processing techniques are a powerful method and are widely used to inspect industrial products. Chen <italic>et al.</italic> [<xref rid="B2-sensors-15-15326" ref-type="bibr">2</xref>] designed an automatic damage detection system of an engineering ceramic surface with image processing techniques, pattern recognition and machine vision. Paniagua <italic>et al.</italic> [<xref rid="B3-sensors-15-15326" ref-type="bibr">3</xref>] proposed a neuro-fuzzy classification system to inspect the quality of industrial cork samples by incorporating techniques based on Gabor filtering techniques and neuro-fuzzy computing with three wavelet-based texture quality features. A microscopic vision system [<xref rid="B4-sensors-15-15326" ref-type="bibr">4</xref>] has been employed to measure the surface roughness of the micro-heterogeneous texture in a deep hole, by virtue of the frequency domain features of microscopic images and a back-propagation artificial neural network optimized by the genetic algorithm. Indeed, neural networks, geometric, color and texture features were often employed in the classification of plants and crops [<xref rid="B5-sensors-15-15326" ref-type="bibr">5</xref>,<xref rid="B6-sensors-15-15326" ref-type="bibr">6</xref>,<xref rid="B7-sensors-15-15326" ref-type="bibr">7</xref>]. Shen <italic>et al.</italic> [<xref rid="B8-sensors-15-15326" ref-type="bibr">8</xref>] used the Otsu method, HSI color system and Sobel operator to extract disease spot regions and calculate leaf areas. Park <italic>et al.</italic> [<xref rid="B9-sensors-15-15326" ref-type="bibr">9</xref>] employed an unsupervised segmentation algorithm by Gaussian mixture models to segment color image regions.</p><p>However, to the best of the authors&#x02019; knowledge, there is no literature on the inspection of the internal quality of micro-spray nozzles using image processing techniques. Some research [<xref rid="B10-sensors-15-15326" ref-type="bibr">10</xref>,<xref rid="B11-sensors-15-15326" ref-type="bibr">11</xref>,<xref rid="B12-sensors-15-15326" ref-type="bibr">12</xref>] talked about the determination of drop sizes and shapes for spray nozzles. The drop size distribution of an irrigation spray nozzle was determined using image processing techniques, and the relationship between drop size distribution, operating pressure and nozzle size were discussed [<xref rid="B10-sensors-15-15326" ref-type="bibr">10</xref>]. The droplet shape and size of a diesel spray had been detected using wavelet transform, a Gaussian filter and sub-pixel contour extraction [<xref rid="B11-sensors-15-15326" ref-type="bibr">11</xref>]. The gray level gradient, boundary curvature detection, the Hough transform and the convex-hull method were employed to count and size particles in the field of sprays [<xref rid="B12-sensors-15-15326" ref-type="bibr">12</xref>].</p><p>Currently, the internal damages of micro-spray nozzles are manually inspected (sorted) using stereomicroscopes. The procedures are laborious and ineffective. The labor cost was about 80% of the total cost for micro-spray nozzle products. The detection process has to be automated to reduce the total cost. Thus, this paper aims to develop a machine vision system to correctly and automatically inspect the micro-spray nozzle. The specific goals are: (1) to develop an algorithm to segment the defect region; (2) to extract the gray level and texture features; and (3) to classify different grades using the aforementioned features by a classifier.</p><p>In summary, we develop a machine vision system to correctly and automatically inspect and classify micro-spray nozzles. The system includes an extraction algorithm of defect regions, a feature estimation algorithm for defect regions and a classifier for the quality of micro-spray nozzles.</p></sec><sec><title>2. Materials and Methods</title><sec><title>2.1. Micro-Spray Nozzle Structure and Defects</title><p>Micro-spray nozzles (shown in <xref ref-type="fig" rid="sensors-15-15326-f001">Figure 1</xref>) are provided by Natural Fog Multi-Tech Precision Industry Corporation Ltd. (Tai-Chung, Taiwan). The structure profile of the micro-spray nozzle is shown in <xref ref-type="fig" rid="sensors-15-15326-f002">Figure 2</xref>. The outlet diameter of the micro-spray nozzle is 0.1 mm. There are two inclined annular-planes, A and B (as shown in <xref ref-type="fig" rid="sensors-15-15326-f002">Figure 2</xref>), on the inside surface of the micro-spray nozzle. <xref ref-type="fig" rid="sensors-15-15326-f003">Figure 3</xref> depicts the circle textures in the inner image of micro-spray nozzles after CNC machine manufacturing. The defects of micro-spray nozzles were made by the CNC machine during the manufacturing procedures. The defects appear on the outlet and inclined planes A and B. Four possible defects, which include the outlet shape and deckle edge, pellet and stripe metal filings on inclined planes, are shown in <xref ref-type="fig" rid="sensors-15-15326-f003">Figure 3</xref>. The purpose of this study is to inspect the defects of micro-spray nozzles automatically using a machine vision system.</p><fig id="sensors-15-15326-f001" position="float"><label>Figure 1</label><caption><p>Micro-spray nozzle.</p></caption><graphic xlink:href="sensors-15-15326-g001"/></fig><fig id="sensors-15-15326-f002" position="float"><label>Figure 2</label><caption><p>The profile of the micro-spray nozzle.</p></caption><graphic xlink:href="sensors-15-15326-g002"/></fig><fig id="sensors-15-15326-f003" position="float"><label>Figure 3</label><caption><p>Four possible defects. (<bold>a</bold>) Small outlet; (<bold>b</bold>) Deckle edge of outlet; (<bold>c</bold>) Pellet metal fillings; (<bold>d</bold>) Strip metal fillings.</p></caption><graphic xlink:href="sensors-15-15326-g003"/></fig></sec><sec><title>2.2. Hardware System</title><p>The machine vision system implemented to inspect the inner images of micro-spray nozzles is illustrated in <xref ref-type="fig" rid="sensors-15-15326-f004">Figure 4</xref>. This system includes an IEEE 1394 CCD color camera (DFK-31BF03, Imaging Source Inc., Bremen, Germany), a stereomicroscope (Stemi 2000-C, Zeiss Inc., Oberkochen, Germany), a front illuminating white light LED with a diffuse filter, a fixed table and a personal computer. Open Source Computer Vision Library (OpenCV1.0, Intel Corporation) and Visual C++ 6.0 programming are linked to the programs to grab images of 1024 &#x000d7; 768 pixels.</p><fig id="sensors-15-15326-f004" position="float"><label>Figure 4</label><caption><p>Illustration of the machine vision system for micro-spray nozzle defect inspection.</p></caption><graphic xlink:href="sensors-15-15326-g004"/></fig></sec><sec><title>2.3. Region Segmentation and Features Extraction</title><p>Algorithms that include the estimation of the outlet&#x02019;s geometric feature and segmentation of the region of interest (ROI), a circle inspection (CI) algorithm and a classifier are proposed to detect micro-spray nozzle defects. Details of the proposed algorithms are described as follows.</p><sec><title>2.3.1. Outlet Extraction</title><p>Segmentation of the ROI image is an essential procedure once the features of the micro-spray nozzle have been extracted. Firstly, the outlet image (as shown in <xref ref-type="fig" rid="sensors-15-15326-f005">Figure 5</xref>) is segmented using Otsu&#x02019;s auto-thresholding method and hole-filling operations [<xref rid="B13-sensors-15-15326" ref-type="bibr">13</xref>]. By assuming that the binary image of the outlet is
<inline-formula><mml:math id="mm50"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>
(where <italic>i</italic> = 1, 2, &#x02026;, <italic>m</italic> and the total number of pixels is <italic>m</italic>), the centroid is obtained as
<inline-formula><mml:math id="mm1"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>,
<inline-formula><mml:math id="mm2"><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>. The covariance matrix is defined as
<inline-formula><mml:math id="mm3"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>U</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>M</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, in which
<inline-formula><mml:math id="mm4"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the <italic>i</italic>-th coordinate vector of the image and
<inline-formula><mml:math id="mm5"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> is the mean vector. <italic>T</italic> indicates vector transposition. A pair of orthogonal eigenvectors of the covariance matrix is then calculated. The geometric features&#x02014;principle axis length (<italic>L<sub>p</sub></italic>), secondary axis (<italic>L<sub>s</sub></italic>), centroid, area (<italic>A</italic>), perimeter (<italic>P</italic>), compactness (<italic>P</italic><sup>2</sup>/4&#x003c0;<italic>A</italic>), diameter ratio (min./max. diameter, <italic>D<sub>mm</sub></italic>) and mean diameter (<italic>D<sub>mean</sub></italic>) of the outlet&#x02014;are computed using eigenvectors. Secondly, two geometric features&#x02014;diameter ratio and mean diameter&#x02014;are employed in the micro-spray nozzle classification process.</p><fig id="sensors-15-15326-f005" position="float"><label>Figure 5</label><caption><p>Outlet image segmentation. (<bold>a</bold>) Orignal image; (<bold>b</bold>) Outlet image.</p></caption><graphic xlink:href="sensors-15-15326-g005"/></fig><p>The ROI of inclined annular-planes is the internal image of the micro-spray nozzle, as shown in <xref ref-type="fig" rid="sensors-15-15326-f006">Figure 6</xref>. Segmenting the ROI effectively is an important procedure once the defects have been detected. Firstly, Canny edge detection [<xref rid="B13-sensors-15-15326" ref-type="bibr">13</xref>], a randomized algorithm for detecting circles [<xref rid="B14-sensors-15-15326" ref-type="bibr">14</xref>], hole-filling operation and the AND logic operator are used to segment the ROI of the inclined annular-planes.</p><fig id="sensors-15-15326-f006" position="float"><label>Figure 6</label><caption><p>ROI diagram.</p></caption><graphic xlink:href="sensors-15-15326-g006"/></fig><p>The procedures of extraction are as follows.</p><list list-type="simple"><list-item><label>Step 1.</label><p>Canny edge detection for the original image (<xref ref-type="fig" rid="sensors-15-15326-f007">Figure 7</xref>a), shown as <xref ref-type="fig" rid="sensors-15-15326-f007">Figure 7</xref>b.</p></list-item><list-item><label>Step 2.</label><p>Randomized algorithm for detecting circles, shown as <xref ref-type="fig" rid="sensors-15-15326-f007">Figure 7</xref>c.</p></list-item><list-item><label>Step 3.</label><p>The ring region is filled using a hole-filling operation, shown as <xref ref-type="fig" rid="sensors-15-15326-f007">Figure 7</xref>d.</p></list-item><list-item><label>Step 4.</label><p>The ROI (shown as <xref ref-type="fig" rid="sensors-15-15326-f007">Figure 7</xref>e) is segmented using the AND logic operator for <xref ref-type="fig" rid="sensors-15-15326-f007">Figure 7</xref>a,d.</p></list-item><list-item><label>Step 5.</label><p>Geometric features&#x02019; computation.</p></list-item></list><fig id="sensors-15-15326-f007" position="float"><label>Figure 7</label><caption><p>(<bold>a</bold>) Original image; (<bold>b</bold>) after Canny edge detection; (<bold>c</bold>) randomized algorithm for detecting circles; (<bold>d</bold>) the image after hole-filling operation; (<bold>e</bold>) ROI image with the AND logic operator for (a,d).</p></caption><graphic xlink:href="sensors-15-15326-g007"/></fig></sec><sec><title>2.3.2. Possible Defect Segmentation</title><p>Segmenting defect regions is an important procedure before possible defects (including the deckle edge, pellet and stripe metal fillings) are detected and classified. A prior experiment proceeded as follows. Firstly, an inspection circle is used to find gray levels with scanning resolution in a pixel, as shown in <xref ref-type="fig" rid="sensors-15-15326-f008">Figure 8</xref>. There are different distribution forms of gray levels in different circles, as illustrated in <xref ref-type="fig" rid="sensors-15-15326-f009">Figure 9</xref>. However, the difference between defects and non-defects is difficult to distinguish using the thresholding method [<xref rid="B13-sensors-15-15326" ref-type="bibr">13</xref>]. Therefore, a novel method, the circle inspection algorithm (CI algorithm), is proposed for defect region segmentation of micro-spray nozzles in this study.</p><fig id="sensors-15-15326-f008" position="float"><label>Figure 8</label><caption><p>The scanning direction of the circle inspection for the ROI.</p></caption><graphic xlink:href="sensors-15-15326-g008"/></fig><fig id="sensors-15-15326-f009" position="float"><label>Figure 9</label><caption><p>The distribution of gray levels of the circle inspection.</p></caption><graphic xlink:href="sensors-15-15326-g009"/></fig><p>The CI algorithm is effectively executed to extract defect regions for internal images of micro-spray nozzles according to the differences of the gray level gradient. The CI algorithm is described as follows:
<list list-type="simple"><list-item><label>Step 1.</label><p>The distribution of the gray level on the image is estimated along the circle <italic>C<sub>Rj</sub></italic> (with scanning resolution in a pixel); (<italic>x<sub>c</sub></italic>, <italic>y<sub>c</sub></italic>) is the center, and <italic>R<sub>min</sub></italic> &#x0003c; <italic>R<sub>j</sub></italic>&#x0003c; <italic>R<sub>max</sub></italic>.</p></list-item><list-item><label>Step 2.</label><p>Compute the gray level gradient (<inline-formula><mml:math id="mm51"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) of the circle, as demonstrated in <xref ref-type="fig" rid="sensors-15-15326-f010">Figure 10</xref> on Circle 1.
<disp-formula id="FD1"><label>(1)</label><mml:math id="mm6"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic>g</italic>(<italic>i</italic>) is the gray level of the circle, <italic>i</italic> is the position of the circle and <italic>n</italic> is the distance.</p></list-item><list-item><label>Step 3.</label><p>Segment the possible defect regions (<italic>PDR</italic>).
<disp-formula id="FD2"><label>(2)</label><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>I</mml:mi><mml:mi>f</mml:mi><mml:mtable><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>I</mml:mi><mml:mi>f</mml:mi><mml:mtable><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>T</italic> is a threshold value.</p></list-item></list></p><fig id="sensors-15-15326-f010" position="float"><label>Figure 10</label><caption><p>Parameters of gray level gradient computation.</p></caption><graphic xlink:href="sensors-15-15326-g010"/></fig></sec></sec><sec><title>2.4. Classification: Classifier</title><p>Geometric, texture and color features analysis have been widely employed in classification. With proper feature selections, the design of a classifier can be greatly simplified. This study adopts the geometric feature (mean diameter, diameter ratio, distance variance), the color features (mean gray level, variance of gray level) and texture features (contrast, energy and entropy from the gray level co-occurrence matrix (GLCMs) [<xref rid="B15-sensors-15-15326" ref-type="bibr">15</xref>]) to classify defects and non-defects using a back propagation neural network [<xref rid="B16-sensors-15-15326" ref-type="bibr">16</xref>] (BPNN, as shown in <xref ref-type="fig" rid="sensors-15-15326-f011">Figure 11</xref>). Mathematical formulations of these features are given in <xref ref-type="table" rid="sensors-15-15326-t001">Table 1</xref>.</p><fig id="sensors-15-15326-f011" position="float"><label>Figure 11</label><caption><p>The structure of the back propagation neural network (BPNN) classifier.</p></caption><graphic xlink:href="sensors-15-15326-g011"/></fig><table-wrap id="sensors-15-15326-t001" position="float"><object-id pub-id-type="pii">sensors-15-15326-t001_Table 1</object-id><label>Table 1</label><caption><p>Mathematical formulations of the features.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Distance Variance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Variance of <inline-formula><mml:math id="mm8"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean gray level</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm9"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mtext>y</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gray level variance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm10"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Contrast (orientations 45&#x000b0;, 90&#x000b0;, 135&#x000b0; and 180&#x000b0;)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm11"><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x003b8;</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Entropy (orientations 45&#x000b0;, 90&#x000b0;, 135&#x000b0; and 180&#x000b0;)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm12"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x003b8;</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x003b8;</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Energy (orientations 45&#x000b0;, 90&#x000b0;, 135&#x000b0; and 180&#x000b0;)</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula><mml:math id="mm13"><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x003b8;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>
</td></tr></tbody></table><table-wrap-foot><fn><p>Note: <italic>D</italic>(<italic>x<sub>i</sub></italic>, <italic>y<sub>i</sub></italic>) is the distance between location (<italic>x<sub>i</sub></italic>, <italic>y<sub>i</sub></italic>) and the outlet center, <italic>p</italic>(<italic>i</italic>, <italic>j</italic>, <italic>d</italic>, &#x003b8;) represents the relative frequency of a gray level co-occurrence matrix (GLCM) of an image <italic>g</italic>(<italic>x<sub>i</sub></italic>, <italic>y<sub>i</sub></italic>), where <italic>i</italic> is the gray level at location (<italic>x</italic>, <italic>y</italic>) and <italic>j</italic> represents the gray level of a neighboring pixel at a distance <italic>d</italic> and an orientation from location (<italic>x<sub>i</sub></italic>, <italic>y<sub>i</sub></italic>). The GLCM of distance 1 pixel and orientations 45&#x000b0;, 90&#x000b0;, 135&#x000b0; and 180&#x000b0; are used for the isolated image.</p></fn></table-wrap-foot></table-wrap><p>In this study, the BPNN classifier consists of three layers: an input layer, a hidden layer and an output layer (as shown in <xref ref-type="fig" rid="sensors-15-15326-f011">Figure 11</xref>). The input layer has 17 nodes, which are related to 3 geometric features, 2 color features and 12 texture features, normalized between 0 and 1. The output layer is made of nodes, related to two categories: defect and non-defect. Initially, the number of nodes <italic>n<sub>H</sub></italic> in the hidden layer is calculated using the following formula:
<disp-formula id="FD3"><label>(3)</label><mml:math id="mm14"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>O</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></disp-formula>
where <italic>n<sub>I</sub></italic> is the number of input nodes and <italic>n<sub>o</sub></italic> is the number of output nodes. The objective of the learning process is to find a relation in a pattern that was made by the features of each segmented region. The BPNN is trained, and the weights are changed until the error convergence criterion approaches 0.1. The error signal is given by:
<disp-formula id="FD4"><label>(4)</label><mml:math id="mm15"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
where <italic>d<sub>k</sub></italic>(<italic>t</italic>) is the desired response for neuron <italic>k</italic> (<italic>k</italic> = 1~2) and <italic>y<sub>k</sub></italic>(<italic>t</italic>) is the output signal of neuron <italic>k</italic> at iteration <italic>t</italic>.</p></sec><sec><title>2.5. Overall Descriptions</title><p>The overall steps involved in the defect region segmentation and classification are shown in <xref ref-type="fig" rid="sensors-15-15326-f012">Figure 12</xref>. The algorithm for the classification of defects included image processing techniques, the CI algorithm and the BPNN classifier, are fully described in the previous two sections.</p><p>The program employed for detecting and classifying the defects is written in Microsoft Visual C++ 6.0 and OpenCV 1.0.</p><fig id="sensors-15-15326-f012" position="float"><label>Figure 12</label><caption><p>Segmentation and classification steps.</p></caption><graphic xlink:href="sensors-15-15326-g012"/></fig></sec></sec><sec><title>3. Results and Discussion</title><p>In this paper, a defect detection (DD) software for a micro-spray nozzle is developed using image processing techniques and the CI algorithm (written in Visual C++ 6.0 with OpenCV 1.0), as outlined in the first few steps shown in <xref ref-type="fig" rid="sensors-15-15326-f012">Figure 12</xref>. The algorithm is summarized as follows:
<list list-type="simple"><list-item><label>Step 1.</label><p>Two geometric features of the outlet are computed.</p></list-item><list-item><label>Step 2.</label><p>The ROIs are segmented using Canny edge detection, a randomized algorithm for detecting circles, hole-filling and the AND logic operators.</p></list-item><list-item><label>Step 3.</label><p>Possible defect regions are segmented by the CI algorithm.</p></list-item><list-item><label>Step 4.</label><p>Estimate 15 features of the segmented regions.</p></list-item><list-item><label>Step 5.</label><p>Establish and test the BPNN classifier to classify defects and non-defects.</p></list-item></list></p><p>The functions of DD include file operations (acquire, load and save images) and image analysis operations (binary operator, hole-filling, remove noises using closing, opening and Canny edge). The geometrics of the outlet, texture and color features of the segmented region are obtained, and defects can be detected by the DD computation accurately and rapidly.</p><p>The difference between defects and non-defect regions is not obvious in the original image of a micro-spray nozzle, as shown in <xref ref-type="fig" rid="sensors-15-15326-f013">Figure 13</xref>a. Possible defect regions, including defects and non-defects, are segmented using the CI algorithm (parameters <italic>n</italic> = 6, <italic>T</italic> = 11 are selected after some experimentations) and image processing techniques (such as hole-filling, erosion, dilation, opening, closing and Canny edge operators), as indicated in <xref ref-type="fig" rid="sensors-15-15326-f013">Figure 13</xref>b. <xref ref-type="fig" rid="sensors-15-15326-f013">Figure 13</xref>c results using the AND logic operator for <xref ref-type="fig" rid="sensors-15-15326-f013">Figure 13</xref>a,b. Hence, the defects of micro-spray nozzles (<xref ref-type="fig" rid="sensors-15-15326-f013">Figure 13</xref>d) are detected according to the BPNN classification.</p><p>In order to establish the BPNN classifier, 698 samples (with 565 non-defects and 133 defects) are randomly sampled. Eleven hidden nodes are obtained by Equation (3) according to 17 input features, and two output categories. The BPNN classifier is implemented using functions of MATLAB 8.0. For each configuration, the training sample set is randomly selected from all samples until the BPNN convergence is achieved. Over-fitting often occurs when the training set contains some incorrect samples in the BPNN. As the grades of micro-spray nozzles in training samples are already known before the training process, over-fitting is unlikely to occur. To ensure that the influence of over-fitting is trivial during the training process, the maximum number of iterations is set to 100,000. Further studies showed that the same inspection effect is obtained when the error rate convergence criterion is smaller than 0.1. Randomly-sampled images (1486 non-defects and 118 defects) are used to test the system. The accuracy of the classification is 90.71%. The accuracy of outlet shape and strip fillings is 100%. However, a few deckle edges and pellet fillings of the images can not be detected using the proposed algorithms, examples of classification failure and explanations are shown in <xref ref-type="table" rid="sensors-15-15326-t002">Table 2</xref>.</p><fig id="sensors-15-15326-f013" position="float"><label>Figure 13</label><caption><p>(<bold>a</bold>) Original image; (<bold>b</bold>) segmented binary image after the CI algorithm; (<bold>c</bold>) segmented image after the CI algorithm; (<bold>d</bold>) classification result.</p></caption><graphic xlink:href="sensors-15-15326-g013"/></fig><p>In this study, the proposed system can detect and classify visible defects with a CCD camera accurately and efficiently. In the future, we hope that the detection and classification system can be applied to an auto-inspection system for micro-spray nozzles.</p><table-wrap id="sensors-15-15326-t002" position="float"><object-id pub-id-type="pii">sensors-15-15326-t002_Table 2</object-id><label>Table 2</label><caption><p>Examples of classification failure and explanations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="1" colspan="1">
</th><th align="center" valign="middle" rowspan="1" colspan="1">Original Image</th><th align="center" valign="middle" rowspan="1" colspan="1">ROI</th><th align="center" valign="middle" rowspan="1" colspan="1">Explanation</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Case 1: Deckle edge</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-15-15326-i001.jpg"/></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-graphic xlink:href="sensors-15-15326-i002.jpg"/></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deckle edges are removed after ROI segmentation operator.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Case 2: Deckle edge</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-15-15326-i003.jpg"/></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-15-15326-i004.jpg"/></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Deckle edges cannot be segmented by the CI algorithm because they are not obvious.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Case 3: Pellet filings</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-15-15326-i005.jpg"/></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-15-15326-i006.jpg"/></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pellet fillings cannot be segmented by the CI algorithm because they are not obvious.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Case 4: Dark image</td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-15-15326-i007.jpg"/></td><td align="center" valign="middle" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-15-15326-i008.jpg"/></td><td align="center" valign="middle" rowspan="1" colspan="1">The image is blurred and dark.</td></tr></tbody></table></table-wrap></sec><sec><title>4. Conclusions</title><p>Real-time flow uniformity depends on the internal quality of the micro-spray nozzle, and unstable flow will result because of internal defects. A novel inspection system for internal defects of micro-spray nozzles is developed in this study. The image processing techniques, Canny edge detection, a randomized algorithm for detecting circles, the circle inspection algorithm and the BPNN classifier are used to establish the micro-spray nozzle detection system. Possible defects can be segmented efficiently using the CI algorithm and image processing techniques. Geometric, color and texture features of the defects are obtained to establish a BPNN classifier. Testing results show that the defects of the micro-spray nozzles can be detected efficiently using the proposed system. In a future study, we aim to refine the inspection algorithm in order to increase the inspection accuracy for the production line of micro-spay nozzles.</p></sec></body><back><notes><title>Author Contributions</title><p>All authors have made significant contributions to the paper. Huang K.Y. initiated the idea to develop the inspection system of micro-spray nozzles. Huang K.Y. and Ye Y.T. developed the inspection algorithms and the classifier together. Ye Y.T. wrote programs and performed the experiment. Huang, K.Y. contributed to the paper organization and technical writing to final version.</p></notes><notes><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-15-15326"><label>1.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chang</surname><given-names>T.B.</given-names></name><name><surname>Yu</surname><given-names>L.Y.</given-names></name></person-group>
<article-title>Optimal nozzle spray cone angle for triangular-pitch shell-and-tube interior spray evaporator</article-title>
<source>Int. J. Heat Mass Transf.</source>
<year>2015</year>
<volume>85</volume>
<fpage>463</fpage>
<lpage>472</lpage>
<pub-id pub-id-type="doi">10.1016/j.ijheatmasstransfer.2015.01.123</pub-id>
</element-citation></ref><ref id="B2-sensors-15-15326"><label>2.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Lin</surname><given-names>B.</given-names></name><name><surname>Han</surname><given-names>X.</given-names></name><name><surname>Liang</surname><given-names>X.</given-names></name></person-group>
<article-title>Automated inspection of engineering ceramic grinding surface damage based on image recognition</article-title>
<source>Int. J. Adv. Manuf. Technol.</source>
<year>2013</year>
<volume>66</volume>
<fpage>431</fpage>
<lpage>443</lpage>
<pub-id pub-id-type="doi">10.1007/s00170-012-4338-2</pub-id>
</element-citation></ref><ref id="B3-sensors-15-15326"><label>3.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Paniagua</surname><given-names>B.</given-names></name><name><surname>Vega-Rodr&#x000ed;guez</surname><given-names>M.A.</given-names></name><name><surname>G&#x000f3;mez-Pulido</surname><given-names>J.A.</given-names></name><name><surname>S&#x000e1;nchez-P&#x000e9;rez</surname><given-names>J.M.</given-names></name></person-group>
<article-title>Automatic texture characterization using Gabor filters and neurofuzzy computing</article-title>
<source>Int. J. Adv. Manuf. Technol.</source>
<year>2011</year>
<volume>52</volume>
<fpage>15</fpage>
<lpage>32</lpage>
<pub-id pub-id-type="doi">10.1007/s00170-010-2706-3</pub-id>
</element-citation></ref><ref id="B4-sensors-15-15326"><label>4.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Tu</surname><given-names>X.</given-names></name><name><surname>Jia</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Ma</surname><given-names>X.</given-names></name><name><surname>Bi</surname><given-names>X.</given-names></name></person-group>
<article-title>An improved surface roughness measurement method for micro-heterogeneous texture in deep hole based on gray-level co-occurrence matrix and support vector machine</article-title>
<source>Int. J. Adv. Manuf. Technol.</source>
<year>2013</year>
<volume>69</volume>
<fpage>583</fpage>
<lpage>593</lpage>
<pub-id pub-id-type="doi">10.1007/s00170-013-5048-0</pub-id>
</element-citation></ref><ref id="B5-sensors-15-15326"><label>5.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>L&#x000f3;pez-Garc&#x000ed;a</surname><given-names>F.</given-names></name><name><surname>Andreu-Garc&#x000ed;a</surname><given-names>G.</given-names></name><name><surname>Blasco</surname><given-names>J.</given-names></name><name><surname>Aleixos</surname><given-names>N.</given-names></name><name><surname>Valiente</surname><given-names>J.</given-names></name></person-group>
<article-title>Automatic detection of skin defects in citrus fruits using a multivariate image analysis approach</article-title>
<source>Comput. Electron. Agric.</source>
<year>2010</year>
<volume>71</volume>
<fpage>189</fpage>
<lpage>197</lpage>
<pub-id pub-id-type="doi">10.1016/j.compag.2010.02.001</pub-id>
</element-citation></ref><ref id="B6-sensors-15-15326"><label>6.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Kavdir</surname><given-names>I.</given-names></name></person-group>
<article-title>Discrimination of sunflower, weed and soil by artificial neural networks</article-title>
<source>Comput. Electron. Agric.</source>
<year>2004</year>
<volume>44</volume>
<fpage>153</fpage>
<lpage>160</lpage>
<pub-id pub-id-type="doi">10.1016/j.compag.2004.03.006</pub-id>
</element-citation></ref><ref id="B7-sensors-15-15326"><label>7.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Camargo</surname><given-names>A.</given-names></name><name><surname>Smith</surname><given-names>J.S.</given-names></name></person-group>
<article-title>An image-processing based algorithm to automatically identify plant disease visual symptoms</article-title>
<source>Biosyst. Eng.</source>
<year>2009</year>
<volume>102</volume>
<fpage>9</fpage>
<lpage>21</lpage>
<pub-id pub-id-type="doi">10.1016/j.biosystemseng.2008.09.030</pub-id>
</element-citation></ref><ref id="B8-sensors-15-15326"><label>8.</label><element-citation publication-type="confproc">
<person-group person-group-type="author"><name><surname>Shen</surname><given-names>W.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Wei</surname><given-names>H.</given-names></name></person-group>
<article-title>Grading method of leaf spot disease based on image processing</article-title>
<source>Proceedings of the 2008 International Conference on Computer Science and Software Engineering</source>
<conf-loc>Wuhan, China</conf-loc>
<conf-date>12&#x02013;14 December 2008</conf-date>
<fpage>491</fpage>
<lpage>494</lpage>
</element-citation></ref><ref id="B9-sensors-15-15326"><label>9.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Park</surname><given-names>J.H.</given-names></name><name><surname>Lee</surname><given-names>G.S.</given-names></name><name><surname>Park</surname><given-names>S.Y.</given-names></name></person-group>
<article-title>Color image segmentation using adaptive mean shift and statistical model-based methods</article-title>
<source>Comput. Math. Appl.</source>
<year>2009</year>
<volume>57</volume>
<fpage>970</fpage>
<lpage>980</lpage>
<pub-id pub-id-type="doi">10.1016/j.camwa.2008.10.053</pub-id>
</element-citation></ref><ref id="B10-sensors-15-15326"><label>10.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sudheera</surname><given-names>K.P.</given-names></name><name><surname>Pandab</surname><given-names>R.K.</given-names></name></person-group>
<article-title>Digital image processing for determining drop sizes from irrigation spray nozzles</article-title>
<source>Agric. Water Manag.</source>
<year>2000</year>
<volume>45</volume>
<fpage>159</fpage>
<lpage>167</lpage>
<pub-id pub-id-type="doi">10.1016/S0378-3774(99)00079-7</pub-id>
</element-citation></ref><ref id="B11-sensors-15-15326"><label>11.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Blaisot</surname><given-names>J.B.</given-names></name><name><surname>Yon</surname><given-names>J.</given-names></name></person-group>
<article-title>Droplet size and morphology characterization for dense sprays by image processing: Application to the diesel spray</article-title>
<source>Exp. Fluids</source>
<year>2005</year>
<volume>39</volume>
<fpage>997</fpage>
<lpage>994</lpage>
<pub-id pub-id-type="doi">10.1007/s00348-005-0026-4</pub-id>
</element-citation></ref><ref id="B12-sensors-15-15326"><label>12.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Lee</surname><given-names>S.Y.</given-names></name><name><surname>Kim</surname><given-names>Y.D.</given-names></name></person-group>
<article-title>Sizing of spray particles using image processing technique</article-title>
<source>KSME Int. J.</source>
<year>2004</year>
<volume>18</volume>
<fpage>879</fpage>
<lpage>894</lpage>
</element-citation></ref><ref id="B13-sensors-15-15326"><label>13.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>R.C.</given-names></name><name><surname>Woods</surname><given-names>R.E.</given-names></name></person-group>
<source>Digital Image Processing</source>
<edition>3rd ed.</edition>
<publisher-name>Prentice Hall</publisher-name>
<publisher-loc>Upper Saddle River, NJ, USA</publisher-loc>
<year>2002</year>
</element-citation></ref><ref id="B14-sensors-15-15326"><label>14.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Chen</surname><given-names>T.C.</given-names></name><name><surname>Chung</surname><given-names>K.L.</given-names></name></person-group>
<article-title>An efficient randomized algorithm for detecting circles</article-title>
<source>CVGIP Comput. Vis. Image Underst.</source>
<year>2001</year>
<volume>83</volume>
<fpage>172</fpage>
<lpage>191</lpage>
<pub-id pub-id-type="doi">10.1006/cviu.2001.0923</pub-id>
</element-citation></ref><ref id="B15-sensors-15-15326"><label>15.</label><element-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Haralick</surname><given-names>R.M.</given-names></name><name><surname>Shanmugam</surname><given-names>K.</given-names></name><name><surname>Dinstein</surname><given-names>I.</given-names></name></person-group>
<article-title>Textual features for image classification</article-title>
<source>IEEE Trans. Syst. Man Cybern.</source>
<year>1973</year>
<volume>3</volume>
<fpage>610</fpage>
<lpage>621</lpage>
<pub-id pub-id-type="doi">10.1109/TSMC.1973.4309314</pub-id>
</element-citation></ref><ref id="B16-sensors-15-15326"><label>16.</label><element-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Fausett</surname><given-names>L.</given-names></name></person-group>
<source>Fundamentals of Neural Networks: Architectures, Algorithms, and Applications</source>
<publisher-name>Prentice-Hall</publisher-name>
<publisher-loc>Upper Saddle River, NJ, USA</publisher-loc>
<year>1994</year>
</element-citation></ref></ref-list></back></article>
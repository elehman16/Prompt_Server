<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xml:lang="EN" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Exp Brain Res</journal-id><journal-title>Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale</journal-title><issn pub-type="ppub">0014-4819</issn><issn pub-type="epub">1432-1106</issn><publisher><publisher-name>Springer-Verlag</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19495733</article-id><article-id pub-id-type="pmc">2733193</article-id><article-id pub-id-type="publisher-id">1858</article-id><article-id pub-id-type="doi">10.1007/s00221-009-1858-6</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Intermodal attention affects the processing of the temporal alignment of audiovisual stimuli</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Talsma</surname><given-names>Durk</given-names></name><address><email>d.talsma@utwente.nl</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Senkowski</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Woldorff</surname><given-names>Marty G.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label>Cognitive Psychology Department, Vrije University Amsterdam, Amsterdam, The Netherlands </aff><aff id="Aff2"><label>2</label>Department of Cognitive Psychology and Ergonomics, University of Twente, PO Box 215, 7500 AE Enschede, The Netherlands </aff><aff id="Aff3"><label>3</label>Department of Neurophysiology and Pathophysiology, University Medical Center Hamburg-Eppendorf, Hamburg, Germany </aff><aff id="Aff4"><label>4</label>Center for Cognitive Neuroscience and Department of Psychiatry, Duke University, Durham, NC USA </aff></contrib-group><pub-date pub-type="epub"><day>4</day><month>6</month><year>2009</year></pub-date><pub-date pub-type="ppub"><month>9</month><year>2009</year></pub-date><volume>198</volume><issue>2-3</issue><fpage>313</fpage><lpage>328</lpage><history><date date-type="received"><day>31</day><month>10</month><year>2008</year></date><date date-type="accepted"><day>12</day><month>5</month><year>2009</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2009</copyright-statement></permissions><abstract xml:lang="EN"><p>The temporal asynchrony between inputs to different sensory modalities has been shown to be a critical factor influencing the interaction between such inputs. We used scalp-recorded event-related potentials (ERPs) to investigate the effects of attention on the processing of audiovisual multisensory stimuli as the temporal asynchrony between the auditory and visual inputs varied across the audiovisual integration window (i.e., up to 125&#x000a0;ms). Randomized streams of unisensory auditory stimuli, unisensory visual stimuli, and audiovisual stimuli (consisting of the temporally proximal presentation of the visual and auditory stimulus components) were presented centrally while participants attended to either the auditory or the visual modality to detect occasional target stimuli in that modality. ERPs elicited by each of the contributing sensory modalities were extracted by signal processing techniques from the combined ERP waveforms elicited by the multisensory stimuli. This was done for each of the five different 50-ms subranges of stimulus onset asynchrony (SOA: e.g., V precedes A by 125&#x02013;75&#x000a0;ms, by 75&#x02013;25&#x000a0;ms, etc.). The extracted ERPs for the visual inputs of the multisensory stimuli were compared among each other and with the ERPs to the unisensory visual control stimuli, separately when attention was directed to the visual or to the auditory modality. The results showed that the attention effects on the right-hemisphere visual P1 was largest when auditory and visual stimuli were temporally aligned. In contrast, the N1 attention effect was smallest at this latency, suggesting that attention may play a role in the processing of the relative temporal alignment of the constituent parts of multisensory stimuli. At longer latencies an occipital selection negativity for the attended versus unattended visual stimuli was also observed, but this effect did not vary as a function of SOA, suggesting that by that latency a stable representation of the auditory and visual stimulus components has been established.</p></abstract><kwd-group><title>Keywords</title><kwd>Electrophysiology</kwd><kwd>EEG</kwd><kwd>ERP</kwd><kwd>Multisensory</kwd><kwd>SOA</kwd></kwd-group><custom-meta-wrap><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer-Verlag 2009</meta-value></custom-meta></custom-meta-wrap></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p>To understand the processing of audiovisual stimuli, it is useful to study the conditions in which the audiovisual inputs break apart into separate auditory and visual perceptual processes. For example, in a badly mastered audio track of a movie, there can be a noticeable desynchronization between visual and auditory information streams. Such a desynchronization can also be observed in real life, such as in the case of a distant thunderstorm, a music concert in a large arena, or a fast jet aircraft that appears to fly ahead of its sound. These latter desynchronization phenomena occur because light travels at substantially faster speed than sound, thus causing the visual inputs to reach the visual receptors considerably earlier than the auditory inputs reach the auditory ones.</p><p>Behavioral studies have shown that auditory and visual stimuli coming from the same location were judged most likely to originate from a common cause when visual stimuli precede the auditory by about 50&#x000a0;ms (even as compared to simultaneous occurrence) (Lewald et&#x000a0;al. <xref ref-type="bibr" rid="CR29">2001</xref>; Lewald and Guski <xref ref-type="bibr" rid="CR27">2003</xref>). Presumably, this optimum timing difference reflects the fact that the brain has evolved to process multisensory stimuli that are located some distance away from the observer. Due to the difference between the velocities of sound and light, auditory signals from an object located at a distance of about 15&#x000a0;m (probably representing a reasonable average distance of real-world objects), reach our ears approximately 50&#x000a0;ms after light reaches the retina. To process such objects optimally, our brain has presumably evolved to compensate for this difference by means of a higher neural transmission rate for auditory signals than for visual ones. Moreover, it has been shown that the temporal window for multisensory integration processes can be adapted on the basis of viewing distance (King <xref ref-type="bibr" rid="CR26">2005</xref>; Spence and Squire <xref ref-type="bibr" rid="CR54">2003</xref>; but see Lewald and Guski <xref ref-type="bibr" rid="CR28">2004</xref>) or exposure (Navarra et&#x000a0;al. <xref ref-type="bibr" rid="CR41">2005</xref>), and that multisensory perception appears to stay relatively intact even with fairly large temporal discrepancies between visual and auditory stimuli (Shams et&#x000a0;al. <xref ref-type="bibr" rid="CR51">2001</xref>; Shams et&#x000a0;al. <xref ref-type="bibr" rid="CR52">2002</xref>). These and other results (Fendrich and Corballis <xref ref-type="bibr" rid="CR12">2001</xref>; Morein-Zamir et&#x000a0;al. <xref ref-type="bibr" rid="CR38">2003</xref>; Vroomen and De Gelder <xref ref-type="bibr" rid="CR64">2004</xref>; Talsma et&#x000a0;al. <xref ref-type="bibr" rid="CR62">2007</xref>; Busse et&#x000a0;al. <xref ref-type="bibr" rid="CR8">2005</xref>) have led to the suggestion that the processing of visual and auditory inputs may be influenced by their relative timing, as well as by endogenous factors, such as attention.</p><p>Although stimulus processing effects as a function of temporal alignment are not well understood, some evidence exists which indicates that visual inputs are perceptually realigned on the basis of an auditory stimulus (DiLuca et&#x000a0;al. <xref ref-type="bibr" rid="CR31">2009</xref>). For example, when the temporal characteristics of auditory and visual stimuli do not match, perceptual reports regarding the temporal characteristics of visual stimuli tend to be biased toward the temporal characteristics of the auditory stream (Shams et&#x000a0;al. <xref ref-type="bibr" rid="CR52">2002</xref>). More specifically, when participants were presented two flashes accompanied by three tone pips, they tended to report seeing three flashes rather than just two. We (Senkowski et&#x000a0;al. <xref ref-type="bibr" rid="CR49">2007a</xref>) and others (e.g., Andersen and Mamassian <xref ref-type="bibr" rid="CR2">2008</xref>; Spence and Squire <xref ref-type="bibr" rid="CR54">2003</xref>) have previously reported that participants begin to detect onset differences between auditory and visual stimuli when they are at least 125&#x02013;150&#x000a0;ms apart, and only able to accurately report these onset differences when the visual and auditory stimuli are 250&#x000a0;ms or more apart. These findings suggest that there is a relatively broad integration time window of as large as 250&#x000a0;ms, in which stimuli from different modalities typically tend to be integrated into a single multisensory percept. In addition, animal physiology research has revealed that a large number of neurons in the superior colliculus show enhanced responsiveness when visual and auditory stimuli are presented within a time window of less than 100&#x02013;200&#x000a0;ms (Meredith et&#x000a0;al. <xref ref-type="bibr" rid="CR36">1987</xref>; Meredith <xref ref-type="bibr" rid="CR35">2002</xref>).</p><p>The latter finding is a special case of a more general result from the animal physiological literature: a large number of neurons in superior colliculus elicit much larger responses to multisensory (i.e., audiovisual: AV) stimuli than they would do in response to the unisensory auditory (A) or unisensory visual (V) component parts presented separately (Stein and Meredith <xref ref-type="bibr" rid="CR55">1993</xref>; Wallace et&#x000a0;al. <xref ref-type="bibr" rid="CR66">1993</xref>). Supplementing these findings, human electrophysiology studies have revealed multisensory interactions in the early event-related potentials (ERPs) over sensory as well as non-sensory areas of the cortex, although how early this can occur and under what circumstances has varied (Giard and Peronn&#x000e9;t <xref ref-type="bibr" rid="CR16">1999</xref>; Molholm et&#x000a0;al. <xref ref-type="bibr" rid="CR37">2002</xref>; Teder-S&#x000e4;lej&#x000e4;rvi et&#x000a0;al. <xref ref-type="bibr" rid="CR63">2002</xref>; Talsma et&#x000a0;al. <xref ref-type="bibr" rid="CR62">2007</xref>; Senkowski et&#x000a0;al. <xref ref-type="bibr" rid="CR49">2007a</xref>).</p><p>Nevertheless, it still remains a question whether and in what way multisensory processing alters as a function of the relative timing between visual and auditory inputs, even when both visual and auditory inputs of the multisensory stimulus fall within the integration window. As noted above, behavioral and animal physiological studies have found that the window of integration is relatively broad. Yet at the same time behavioral studies have shown that there appears to be an optimum relative stimulus timing wherein visual stimuli precede auditory stimuli by about 50&#x02013;100&#x000a0;ms, such that these stimuli are subjectively most likely as being perceived as simultaneous (Lewald and Guski <xref ref-type="bibr" rid="CR27">2003</xref>). In addition, other behavioral studies have shown that, when participants are required to judge the temporal order of auditory and visual stimuli, they are less able to make an accurate judgment when these stimuli were presented at or near this optimum point of subjective simultaneity (i.e., when the visual stimulus preceded the auditory stimulus by about 50&#x02013;100&#x000a0;ms; Keetels and Vroomen <xref ref-type="bibr" rid="CR23">2005</xref>; Vroomen and Keetels <xref ref-type="bibr" rid="CR65">2006</xref>; McDonald et&#x000a0;al. <xref ref-type="bibr" rid="CR34">2005</xref>). Interestingly, these temporal order judgments gradually shift toward the direction of the actual temporal order as the auditory and visual stimulus inputs deviate more and more from the point of subjective simultaneity. This suggests that neural processes underlying these audiovisual interactions would also gradually alter with increasing temporal offsets between auditory and visual stimuli, despite still falling within the integration window.</p><p>The primary goal of the present study was to investigate to what degree these processes involved in resolving temporal asynchrony are affected by intermodal attention. Previous studies have shown that attention can interact with processes involved in multisensory perception (Talsma and Woldorff <xref ref-type="bibr" rid="CR60">2005a</xref>; Senkowski et&#x000a0;al. <xref ref-type="bibr" rid="CR47">2005</xref>). However, these effects have generally been investigated under conditions in which stimuli were fully attended or unattended; that is, attention was directed to the multisensory stimuli (i.e., at both visual and auditory modalities simultaneously), or directed elsewhere. A more recent study suggests a more complex relationship between attention and audiovisual processing (Talsma et&#x000a0;al. <xref ref-type="bibr" rid="CR62">2007</xref>). Whereas we found that it is indeed a requirement of attention to be focused on both modalities simultaneously for the relatively early latency (~50&#x000a0;ms after stimulus onset) multisensory interactions to occur, longer-latency effects suggested the spreading of attention from the visual modality to the auditory under conditions in which only the visual modality was attended (Busse et&#x000a0;al. <xref ref-type="bibr" rid="CR8">2005</xref>). Specifically, when the visual modality was attended, a slow, long-latency (&#x0003e;300&#x000a0;ms) fronto-central negativity was observed in response to multisensory stimuli, but not to either the corresponding unisensory visual or unisensory auditory stimuli occurring alone. This fronto-central negativity strongly resembled the intramodal auditory attention effect known as the late processing negativity, which is elicited by attended auditory stimuli, and which is believed to reflect the prolonged processing of relevant auditory stimuli (N&#x000e4;&#x000e4;t&#x000e4;nen <xref ref-type="bibr" rid="CR39">1992</xref>).</p><p>ERPs are ideally suited to track the temporal characteristics of psychologically related physiological processes, enabling them to be very effective in assessing how and when interactions between closely timed auditory and visual stimuli occur. Thus far, however, only a few electrophysiological studies have systematically addressed the effects of temporal asynchrony between visual and auditory inputs on the electrophysiology of multisensory processing (e.g., Senkowski et&#x000a0;al. <xref ref-type="bibr" rid="CR50">2007b</xref>; Stekelenburg and Vroomen <xref ref-type="bibr" rid="CR56">2005</xref>).</p><p>One reason why only few studies have investigated such timing effects is likely related to the problem induced by the overlap between the ERPs elicited by the visual and the auditory stimulus inputs that occur closely in time. More specifically, since both the visual and auditory inputs of a multisensory audiovisual event will each evoke an ERP, the time-locked averages to the stimuli in one modality will be contaminated by an overlapping ERP elicited by the other-modality stimulus. If the goal is to study the effects on stimulus processing of varying the relative timing by auditory and visual inputs, then the latency range and structure of this overlap distortion will differ systematically across the timing asynchrony conditions, thus confounding assessment of the actual interaction effects on multisensory processing (Woldorff <xref ref-type="bibr" rid="CR67">1993</xref>).</p><p>Stekelenburg and Vroomen (<xref ref-type="bibr" rid="CR56">2005</xref>) addressed the overlap problem by including the corresponding unisensory auditory stimuli in the sequence and using the ERPs as estimates for the overlapping activities. They reported a decrease in the posterior N1 when the auditory component preceded the visual one, compared to when visual stimuli were presented alone, and a similar, albeit smaller amplitude reduction of the N1 when auditory and visual inputs were presented simultaneously. The authors reported these results relative to the onset of the visual input using three fixed temporal asynchronies between visual and auditory inputs: &#x02212;100&#x000a0;ms (auditory preceding visual), 0&#x000a0;ms (simultaneous presentation of visual and auditory inputs), and +100&#x000a0;ms (auditory inputs lagging behind the visual).</p><p>We have previously observed multisensory audiovisual interactions in the evoked high-frequency oscillatory gamma-band responses (GBRs, 30&#x02013;80&#x000a0;Hz) (Senkowski et&#x000a0;al. <xref ref-type="bibr" rid="CR50">2007b</xref>) across this range using a fine range of temporal asynchronies (see also Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b). In the present paper we use a decomposition analysis based on the Adjar filtering technique (Woldorff <xref ref-type="bibr" rid="CR67">1993</xref>)&#x02014;a deconvolution version of the simple subtraction method of Stekelenburg and Vroomen (<xref ref-type="bibr" rid="CR56">2005</xref>)&#x02014;that takes ERPs elicited by unisensory stimuli as estimates for the overlapping ERPs (see &#x0201c;<xref rid="Sec2" ref-type="sec">Methods</xref>&#x0201d; for details). This analysis was applied to the ERP responses from the same dataset for which Senkowski et&#x000a0;al. (<xref ref-type="bibr" rid="CR50">2007b</xref>) analyzed effects on the high-frequency evoked GBRs. More specifically, visual and auditory inputs of multisensory stimuli were presented in close succession (relative asynchronies ranging between &#x02212;125&#x000a0;ms and 125&#x000a0;ms). Participants were instructed to attend to only one modality during the length of a trial block and to detect infrequent target stimuli in the designated modality. These target stimuli were slightly different from the regular non-target stimuli. In this paper, we specifically focus on reporting ERP data that were evoked by the visual stimuli as a function of attention and the audiovisual synchrony, for which we have found the most pronounced effects.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>General layout of the paradigm. <bold>a</bold> Example of a short excerpt from a possible trial sequence. Visual, auditory, multisensory, or no-stim trials were presented with random ITIs of 500&#x02013;800&#x000a0;ms. The presentation order of the various stimulus types was fully randomized. <bold>b</bold> Decomposition of the relative timing of the auditory and visual stimuli within each of the subtypes of audiovisual stimuli, when viewed as time-locked to either the visual input (left) or the auditory input (right). Notice that there were actually five different multisensory stimuli. For instance, V<sub>|A(&#x02212;100&#x000a0;ms)</sub> trials were the same as the A<sub>|V(+100&#x000a0;ms)</sub> trials), but viewed as time-locked to either the visual input (left panel) or the auditory input (right panel)</p></caption><graphic position="anchor" xlink:href="221_2009_1858_Fig1_HTML" id="MO1"/></fig></p><p>Since all stimuli were presented from central locations, we expected to extend some previous findings of non-spatial intermodal attention (De Ruiter et&#x000a0;al. <xref ref-type="bibr" rid="CR10">1998</xref>; Talsma and Kok <xref ref-type="bibr" rid="CR59">2001</xref>; Talsma et&#x000a0;al. <xref ref-type="bibr" rid="CR62">2007</xref>). Most notably, we expected that intermodal attention during multisensory stimulation would affect the early P1 and N1 visual sensory components (cf. Talsma et&#x000a0;al. <xref ref-type="bibr" rid="CR62">2007</xref>), being larger when the visual modality versus the auditory modality was attended. In addition, we expected that intermodal attention to visual stimuli would be reflected in an endogenous ERP component, known as the occipital selection negativity, which is a component that has been reported to reflect intramodal (e.g., Kenemans et&#x000a0;al. <xref ref-type="bibr" rid="CR25">1993</xref>), as well as intermodal attentional selection processes (Talsma and Kok <xref ref-type="bibr" rid="CR59">2001</xref>). Particular interest was on how these various intermodal attention effects on the processing of a visual stimulus component would vary if that component occurred as part of an audiovisual multisensory stimulus with differing degrees of asynchrony of the audio and visual parts.</p></sec><sec id="Sec2" sec-type="methods"><title>Methods</title><sec id="Sec3"><title>Participants</title><p>Twenty healthy participants volunteered for the experiment (age: 25.2; 9 males). All participants were recruited through advertisements at the campus of Duke University and were paid $10 per hour. All participants were right handed and had normal or corrected-to-normal vision and gave written informed consent to volunteer for the study. No one reported having a history of neurological or psychiatric disorder.</p></sec><sec id="Sec4"><title>Task and stimuli</title><p>Visual stimuli consisted of white horizontal gratings, presented on a black background (5.5&#x000a0;&#x000d7;&#x000a0;4.9&#x000a0;cm, subtending a visual angle of 5.4&#x000a0;&#x000d7;&#x000a0;4.8&#x000b0;) for a duration of 100&#x000a0;ms. Auditory stimuli consisted of 1600&#x000a0;Hz sinusoidal tone pips, presented at a sound pressure level of 65&#x000a0;dB(A) for a duration of 100&#x000a0;ms (including linear rise and fall times of 10&#x000a0;ms). These auditory stimuli were presented using two speakers that were placed behind the computer screen used for visual presentation, positioned such that the subjective location of the auditory stimuli matched that of the visual stimuli. Audiovisual stimulus trials were presented with stimulus onset asynchronies (SOAs) of the auditory and visual inputs ranging randomly from trial to trial between &#x000b1;125&#x000a0;ms (see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b). Importantly, for the analyses we divided the &#x000b1;125&#x000a0;ms SOA range into five different 50-ms subranges. The audiovisual SOAs were randomized at 1&#x000a0;ms resolution, but restricted so that an equal number of trials were presented in each 50-ms SOA condition. In addition to the above-described stimuli, which were designated as the standard (i.e., non-target) stimuli, a small proportion of the stimuli were characterized by an additional stimulus feature, which enabled them to serve as target stimuli in the stimulus streams. These deviant stimuli were highly similar to the standards but contained a transient dip in brightness (visual) or volume (auditory) halfway through the duration of the stimulus, inducing the subjective impression of the stimulus appearing to flicker (visual target) or to stutter (auditory target). The degree of the intensity reduction was determined for each participant individually during a training session prior to the experiment (Senkowski et&#x000a0;al. <xref ref-type="bibr" rid="CR50">2007b</xref>; Talsma and Woldorff <xref ref-type="bibr" rid="CR60">2005a</xref>), so that each deviant was just detectable for each participant. Multisensory deviants were composed of either a visual standard paired with an auditory deviant, or vice versa.</p><p>Stimuli were presented as a continuous stream of randomly mixed unisensory auditory, unisensory visual, and multisensory audiovisual stimulus events. Of the multisensory stimuli, a total number of 160 standard stimuli (consisting of a visual standard paired with an auditory standard) and 96 deviant stimuli (half consisting of the visual standard paired with the auditory deviant and the other half consisting of an auditory standard with a visual deviant) were presented with each of the five audio-visual SOA temporal-synchrony subranges. Likewise, 160 unisensory auditory standards, 160 unisensory visual standards, 96 unisensory auditory deviants, and 96 unisensory visual deviants were presented. In addition, 400 &#x0201c;no-stim&#x0201d; trials were included in each condition. These trials were used to estimate the between-trial ERP overlap (Talsma and Woldorff <xref ref-type="bibr" rid="CR60">2005a</xref>, <xref ref-type="bibr" rid="CR61">b</xref>; see Gondan and R&#x000f6;der <xref ref-type="bibr" rid="CR17">2006</xref>, for a similar suggestion). No-stims are trials on which no actual stimulus is presented, but which are presented with the same randomization characteristics as the regular trials (Burock et&#x000a0;al. <xref ref-type="bibr" rid="CR6">1998</xref>). Because the randomization of the no-stims trials satisfy the criteria delineated by Busse and Woldorff (<xref ref-type="bibr" rid="CR7">2003</xref>) to not elicit electrophysiological responses themselves, they can be used to pick up overlapping ERP activity resulting from either anticipatory processes or from adjacent-trial ERPs (see Talsma and Woldorff <xref ref-type="bibr" rid="CR60">2005a</xref>, <xref ref-type="bibr" rid="CR61">b</xref> for a full discussion) All stimulus trials were presented using relatively short inter-trial-intervals (ITIs) that ranged from 500 to 800&#x000a0;ms (mean 650&#x000a0;ms) and were equally divided across 16 blocks of trials.</p></sec><sec id="Sec5"><title>Procedure</title><p>In each block, the participants&#x02019; task was to attend to a designated modality in each run and detect the infrequent deviant target stimuli in that modality. In one condition, they were required to attend to the auditory modality and respond to the detection of any auditory deviants, and in another condition they were required to attend only to the visual modality and respond only to the visual deviants. In both conditions, participants were instructed to report their detection of the target deviants by making a button-press response as fast as possible while maintaining high accuracy. The two attention conditions (attend-auditory, attend-visual) alternated every 2 blocks, with the starting condition being randomized across participants. One practice block for each attention condition was run prior to the experiment to familiarize participants with the procedure. Participants were further instructed to maintain fixation at a centrally presented cross during the run, as well as to minimize head and body movements.</p></sec><sec id="Sec6"><title>Apparatus</title><p>The EEG and behavioral data were recorded in a dimly lit, sound attenuated, electrically shielded chamber. Stimulus presentation was controlled by a personal computer running the &#x0201c;Presentation&#x0201d; software package (Neurobehavioral Systems, Inc., Albany, CA, USA). EEGs were recorded from 64 equally spaced tin electrodes,<xref ref-type="fn" rid="Fn1">1</xref> mounted in a customized elastic cap (Electro-Cap International, Inc) and referenced to the right mastoid during recording. Electrode impedances were kept below 2&#x000a0;k&#x003a9; for the mastoids and ground, 10&#x000a0;k&#x003a9; for the eye electrodes, and 5&#x000a0;k&#x003a9; for the remaining electrodes. Horizontal eye movements were monitored by two electrodes at the outer canthi of the eyes. Vertical eye movements and eye-blinks were detected by electrodes placed below the orbital ridge of both eyes, referenced to two electrodes directly located above the eyes. During recording, eye movements were also monitored using a closed circuit video monitoring system. EEG was recorded using a Neuroscan (SynAmps) acquisition system. All EEG channels were recorded with a band-pass filter of 0.01 to 100&#x000a0;Hz and a gain setting of 1,000. Raw signals were continuously digitized with a sampling rate of 500&#x000a0;Hz and digitally stored for off-line analysis.</p></sec><sec id="Sec7"><title>Data analysis</title><sec id="Sec8"><title>Behavioral data</title><p>Reaction times (RTs) for correctly detected targets, target error rate, and false alarm rates were computed separately for the different conditions. RT and error rate measures were submitted to multivariate analysis of variance (MANOVA) with SOA (5 levels: &#x02212;100, &#x02212;50, 0, 50, and 100&#x000a0;ms) and Attention (visual or auditory) as within-subject factors. In addition, the RTs and error rates of the multisensory stimuli were contrasted against the RTs and error rates of the unisensory stimuli, using pair-wise statistical comparisons.</p></sec><sec id="Sec9"><title>ERP analyses</title><p>Artifact rejection was performed off-line by discarding epochs of the EEG that were contaminated by eye movements (&#x0003e;&#x000b1;130&#x000a0;&#x003bc;V/10&#x000a0;ms) using the detection algorithms described in Talsma and Woldorff (<xref ref-type="bibr" rid="CR61">2005b</xref>). Eye-blinks were corrected using a time-domain linear regression algorithm (e.g., Kenemans et&#x000a0;al. <xref ref-type="bibr" rid="CR24">1991</xref>). Further artifact detection was conducted using an auto-adaptive averaging procedure that finds artifactual trials on the basis of signal to noise properties of the ERP waveform (Talsma <xref ref-type="bibr" rid="CR58">2008</xref>). Approximately one-third of the trials were discarded due to artifacts. The number of discarded trials did not differ significantly between attend-visual and attend-auditory blocks (<italic>F</italic>&#x000a0;&#x0003c;&#x000a0;1). Prior to averaging, EEG data were band-pass filtered between 0.03 and 25&#x000a0;Hz, using a half-Gaussian (causal) kernel. Averages were calculated for the different stimulus types from 1,000&#x000a0;ms before to 1,200&#x000a0;ms after stimulus onset. After averaging, all channels were re-referenced to the algebraic average of the two mastoid electrodes. For the multisensory stimuli, ERPs were computed relative to the onset of the visual input as well as to the onset of the auditory input.</p></sec><sec id="Sec10"><title>Removal of overlapping adjacent stimulus activity</title><p>The ERP effects of multisensory processing were extracted using the following procedure (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). The between-trial overlap, which resulted from the relatively fast trial presentation rate, was removed by subtracting the time-locked averages to the no-stim trials from the regular ERPs (Talsma and Woldorff <xref ref-type="bibr" rid="CR60">2005a</xref>, <xref ref-type="bibr" rid="CR61">b</xref>). However, although this procedure corrects for the between-trial overlap, the ERPs for the audiovisual trials were still distorted by within-trial overlap because the visual and auditory components of these stimuli were presented in such close temporal proximity. The selective averaging time-locked to the inputs of one modality of these multisensory stimuli resulted in an ERP waveform to that stimulus type, plus distortion due to the ERPs to the temporally proximal occurrence of the other-modality input. Because each time-locked average to the sensory input of one modality had the other-modality sensory input occurring across a 50-ms SOA subrange relative to the time-lock point (e.g., 25&#x02212;75&#x000a0;ms after), the contribution of the overlapping ERP elicited by the other-modality input was partially &#x0201c;smeared out&#x0201d; and thus somewhat reduced in the time-locked average to the first modality. However, with such a relatively small temporal jitter, there would still be substantial overlap from the adjacent ERP activity (Woldorff <xref ref-type="bibr" rid="CR67">1993</xref>).<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>Steps involved in extracting the ERP data. An example of removing the overlapping auditory component from a V|<sub>A(&#x02212;100&#x000a0;ms)</sub> stimulus is shown here. A similar procedure was used for the other audiovisual stimulus types. After EEG recording (<bold>a</bold>), condition-wise averages (<bold>b</bold>) of all the multisensory ERPs (separately for each time window), unisensory stimuli, and no-stim (NS) trials were computed. <bold>c</bold> Subtraction of the no-stim ERPs from the unisensory and multisensory ERPs resulted in an ERP that eliminated the between-trial overlap, but which was still composed of contributions from an auditory input and a visual input. <bold>d</bold> The ADJAR procedure (Woldorff <xref ref-type="bibr" rid="CR67">1993</xref>) was used to convolve the unisensory ERP (as an estimate for the overlapping activity) with the distribution of all immediately preceding events (evt dist) to obtain an overlap free estimate of the contribution of one single modality (<bold>e</bold>), plus some variations due to attention and/or multisensory interaction processes (e.g., &#x0201c;ERP(V) within a specific multisensory context&#x0201d;)</p></caption><graphic position="anchor" xlink:href="221_2009_1858_Fig2_HTML" id="MO2"/></fig></p><p>To remove this within-trial overlap in the multisensory ERPs, an adapted version of the adjacent response filter (ADJAR) procedure (Woldorff <xref ref-type="bibr" rid="CR67">1993</xref>) was used. ADJAR is a post-experimental correction technique to estimate and remove from the ERP waveform the overlap that results from adjacent trials in fast-rate sequences. This is accomplished by convolving an estimate of the overlapping waveforms with the event-distribution of the preceding and succeeding events. The procedure typically starts with averages that are somewhat distorted by overlap. After the first iteration of estimating and subtracting the distorting overlap, the ADJAR algorithm iteratively uses the partially corrected ERP waveforms for better overlap estimates until the solutions converge after multiple iterations. In the present case, however, the unisensory ERP averages were used as estimates for the overlapping ERP activity from the other-modality input. Specifically, this was done by convolving the unisensory other-modality ERP with the event-distribution of the overlapping other-modality stimulus input of the multisensory stimulus. As the unisensory ERP averages were not distorted by overlap here, only a single iteration of estimating and subtracting the adjacent-response overlap was necessary.</p><p>After overlap removal, the resulting ERP waveform consisted of the basic time-locked unisensory response, plus the interaction activity resulting from its occurrence within a multisensory (audiovisual) context. The extracted waveforms from the multisensory trials were very similar to the ERP traces of the corresponding <italic>true</italic> unisensory stimuli for that modality, but with some variations due to these multisensory interactions. Multisensory processing effects can therefore be investigated by comparing the extracted ERPs with the true unisensory ERP responses. These extracted ERPs are reported relative to the onset of the visual input (V) or relative to the auditory input (A) of a multisensory AV event, depending on the time-lock point. In the remainder of this paper, temporal relation of the other-modality stimulus onset time will be reported using a subscripted notation. For instance, a response relative to a visual stimulus, with an auditory stimulus <italic>preceding</italic> this visual stimulus in the 50-ms time window centered around 100&#x000a0;ms, would be indicated as V|<sub>A(&#x02212;100&#x000a0;ms)</sub>. Accordingly, an ERP time-locked to the visual stimulus with the auditory stimulus <italic>succeeding</italic> it with its 50-ms subrange centered around 100&#x000a0;ms would be notated as V|<sub>A(100&#x000a0;ms)</sub>.</p><p>Subsequent ERP analyses focused on the overlap-corrected ERPs to the standard stimuli. This included both the estimated single-modality ERPs extracted from the multisensory standard stimulus trials and the ERPs from the unisensory standard stimuli. Scalp topography maps and plots of the ERP traces were used for the analyses of multisensory interactions between auditory and visual ERP components. Based on previous reports, the statistical analysis of multisensory processing effects was focused on a selection of fronto-central areas (for effects that were mainly driven by the auditory modality), and one over posterior areas (for effects that were mainly driven by the visual stimuli).</p><p>As the effects of Attention and SOA and their interactions were substantially more robust on the extracted visual ERPs, this paper focuses on the components of these responses. Initial inspection of the visual ERPs extracted from the multisensory responses revealed the presence of several components that were modulated by intermodal attention. Of these components, we focus here on the posterior P1, N1, and P2 components of the extracted visual ERP responses at ~110, ~150, and ~250 after stimulus onset, respectively, as well as an anterior N1 component that occurs slightly later than the visual P1. The significance of these effects was assessed by computing mean amplitudes across a short window surrounding the peaks of these components, using electrodes that were located near the observed maxima of each component. These amplitude values were then submitted to a multivariate analysis of variance (MANOVA) that contrasted the amplitudes of these conditions in each multisensory condition against the amplitudes of the unisensory control.</p><p>In addition, effects of SOA and Attention within the multisensory stimuli were tested using a MANOVA containing the factors of SOA (5 levels, corresponding to the five different SOA ranges), and Attention (two levels: evoking input was attended or unattended). Specific details regarding latency and electrode selection is given in the respective section, where each component is described.</p></sec></sec></sec><sec id="Sec11" sec-type="results"><title>Results</title><sec id="Sec12"><title>Behavioral results</title><sec id="Sec13"><title>Reaction times</title><p>Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a shows the mean response times (RTs) for the target stimuli for each of the SOA-subrange conditions, along with the mean RTs for the unisensory control stimulus targets. For the multisensory stimuli considered alone, no significant effect of the factor SOA on the RTs (<italic>F</italic>(4,16)&#x000a0;&#x0003c;&#x000a0;1) was found. Although Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> indicates that responses to auditory targets were somewhat slower than those to visual targets, no significant main effects of Attention were found (<italic>F</italic>(1,19)&#x000a0;&#x0003c;&#x000a0;1). Pair-wise comparisons confirmed that the responses to unisensory stimuli were faster than those to the multisensory stimuli (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05).<fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>Mean response times and error rates to unisensory (Uni&#x000a0;=&#x000a0;Unisensory target) and multisensory target stimuli. The other labels &#x02018;&#x02212;100&#x02019;, &#x02018;&#x02212;50&#x02019;, &#x02018;0&#x02019;, &#x02018;+50&#x02019;, &#x02018;+100&#x02019; represent the multisensory trials, in which there are targets in the relevant modality and refer to the mean SOA (&#x000b1;25&#x000a0;ms) of the accessory other-modality input of the multisensory stimulus. Thus, for the <italic>attend</italic>-<italic>auditory</italic> condition, the auditory input of the multisensory AV stimulus was the target, and the visual input the standard, occurring at the relative onset time indicated. Accordingly, for the <italic>attend-visual</italic> condition, the visual input of the multisensory AV stimulus was the target, with an auditory standard occurring at the relative onset time indicated</p></caption><graphic position="anchor" xlink:href="221_2009_1858_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec14"><title>Error rates</title><p>Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b shows the mean error rates for the target stimuli in each of the SOA conditions, along with the mean error rates for the unisensory control stimulus targets. For the multisensory targets, SOA significantly affected error rates (<italic>F</italic>(4,16)&#x000a0;=&#x000a0;4.43; <italic>p</italic>&#x000a0;=&#x000a0;0.05). This effect could be explained using a cubic polynomial fit (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;10.7; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.005), confirming the pattern observed in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b that for the multisensory stimuli the error rates were still relatively low when the unattended input was presented well before (i.e., at &#x02212;100&#x000a0;ms), or immediately after (i.e., at +50&#x000a0;ms) the attended one, but increased at other SOAs.</p></sec><sec id="Sec15"><title>False alarm rates</title><p>False alarm rates were computed for each condition separately and submitted to a MANOVA with Attention (attend-visual or attend-auditory) as within-subject factor. These tests indicated that false alarm rates did not differ significantly between the attend-visual and attend-auditory conditions (<italic>F</italic>(1,19)&#x000a0;&#x0003c;&#x000a0;1). Overall false alarm rates were about 5%.</p></sec></sec><sec id="Sec16"><title>Event-related potentials</title><p>As noted above, the focus of this paper is on the ERPs to the visual inputs of the multisensory stimuli and on the interactions of intermodal attention and audio-visual asynchrony on these responses. In these extracted ERPs, attention effects were identified on four different components. Of these four components, the attention effects on the posterior P1 and on an anterior and posterior N1 component were modulated by SOA. In contrast, the fourth effect, the occipital selection negativity, remained constant across SOA.</p><sec id="Sec17"><title>Posterior P1</title><p>As shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, the posterior P1 attention effect peaking at around 100&#x000a0;ms was characterized by a predominantly right hemispheric topographical distribution, with a strong focus around parietal&#x02013;occipital electrodes P3i and P4i. The P1 components that were elicited by the visual inputs of the multisensory stimuli were larger when they were attended, compared to when they were unattended (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;6.1; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05). Although we did not find an interaction between Attention and SOA, a significant three-way interaction between Attention, SOA, and Laterality (<italic>F</italic>(4,16)&#x000a0;=&#x000a0;3.24; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05) was observed. This interaction confirms the observation that the P1 attention effect differed across SOAs, and also more strongly over the right hemisphere. Although Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> suggests that a P1 amplitude effect was also present for the unisensory stimuli, no statistical evidence could be obtained for this observation (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;1.4; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.25).<fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>Occipital P1 attention effects on the visual ERPs. An early P1 component (~110&#x000a0;ms after onset of the visual stimulus) could be observed for the visual ERPs extracted from the multisensory trials, which was lateralized predominantly over the right hemisphere. This component was the largest when the auditory and visual stimuli were presented at the point of near optimal subjective alignment. It was also present for the unisensory visual control stimuli</p></caption><graphic position="anchor" xlink:href="221_2009_1858_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec18"><title>Anterior N1</title><p>Shortly after the latency of the posterior P1, the anterior N1 component (peaking at around 140&#x000a0;ms) could be observed over the anterior scalp areas (see Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>). This component was larger (more negative) when the visual inputs were attended than when they were unattended (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;8.70; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.01). In addition, there was also a significant interaction between Attention and SOA (<italic>F</italic>(4,16)&#x000a0;=&#x000a0;3.11; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05). In contrast, no significant attention effect was observed on this component for the unisensory visual stimuli, suggesting that the anterior N1 attention effect here was specific to the processing of multisensory stimuli. Moreover, since the magnitude of this attention effect varied with SOA, we suggest that this effect may reflect processes related to resolving the temporal misalignment of audiovisual stimuli.<fig id="Fig5"><label>Fig.&#x000a0;5</label><caption><p>Anterior N1 attention effects on the visual ERPs. Starting at around the same latency, an anterior N1 attention effect was observed, that was present only on multisensory stimuli, in particular at those SOAs, where the visual and auditory inputs were suboptimally aligned</p></caption><graphic position="anchor" xlink:href="221_2009_1858_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec19"><title>Posterior N1</title><p>The posterior N1, peaking at around 150&#x02013;180&#x000a0;ms post-visual-stimulus, was significantly larger (i.e., more negative) when the visual stimuli were attended compared to when they were unattended, as shown by a main effect of Attention on this component (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;6.24; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05; see Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>). Although overall effect of SOA was not observed (<italic>F</italic>(4,16)&#x000a0;=&#x000a0;1.77; <italic>p</italic>&#x000a0;&#x0003e;&#x000a0;0.1), we did observe a significant <italic>interaction</italic> between Attention and SOA (<italic>F</italic>(4,16)&#x000a0;=&#x000a0;3.79; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05), which appeared to result from this attention effect being largest when the auditory stimulus occurred either slightly before or synchronous with the visual stimulus (relative timing of &#x02212;50 or 0&#x000a0;ms). A significant attention effect was also observed on the N1 component elicited by unisensory visual stimuli (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;6.15; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05).<fig id="Fig6"><label>Fig.&#x000a0;6</label><caption><p>Posterior N1 attention effects on the visual ERPs. At around the latency of the posterior N1, differences between attended and unattended visual ERPs extracted from the multisensory trials were maximal when the auditory stimulus was presented just prior to, or near simultaneously with, the evoking visual stimulus. At the point of optimal temporal alignment, the N1 attention effect is attenuated</p></caption><graphic position="anchor" xlink:href="221_2009_1858_Fig6_HTML" id="MO6"/></fig></p><p>Upon closer inspection, scalp topography of this attention effect appeared to differ between the unisensory and multisensory trial types. More specifically, whereas the topography of the unisensory N1 effect was characterized by a bilateral posterior scalp distribution, that of the multisensory N1 effect was characterized by a more medioparietal, although perhaps somewhat left-sided distribution, particularly present when the auditory stimuli occurred just before or concurrent with the visual component. To test whether these topographies did indeed differ, all mean amplitudes for the attended-minus-unattended difference wave of the V|uni, V|<sub>A(&#x02212;50&#x000a0;ms)</sub>, and V|<sub>A(0&#x000a0;ms)</sub> (where these effects were observed to be the most pronounced) were subjected to a vector normalization procedure (McCarthy and Wood 1985). The resulting normalized values were compared against each other using an ANOVA containing the within-subjects factors Stimulus Type (two levels) and Channel (64 levels). The presence of a significant interaction between Stimulus Type and Channel confirmed that the scalp topography of the visual N1 attention effect for the multisensory stimulus types differed from the topography of the N1 effect on the unisensory control stimulus (<italic>F</italic>(63,1197)&#x000a0;=&#x000a0;2.00; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05; &#x003b5;&#x000a0;=&#x000a0;0.1), suggesting that either additional neural generators, or a different configuration of neural generators, are active when an auditory input is presented immediately prior to a visual input.</p></sec><sec id="Sec20"><title>Occipital selection negativity</title><p>Beginning at around 200&#x000a0;ms, ERPs elicited by attended visual inputs became significantly more negative than those elicited by unattended visual inputs (see Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>). This difference, which appeared to be similar to the slow, long-latency posterior attention effect known as the occipital selection negativity, was reflected by the presence of a significant main effect of Attention (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;10.8; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.005). For this activity, although we observed a main effect of SOA (<italic>F</italic>(4,16)&#x000a0;=&#x000a0;3.25; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05), there was no interaction between Attention and SOA, indicating that the attention-related occipital selection negativity was present for all SOAs and did <italic>not</italic> significantly differ between SOA conditions, suggesting that by this time a stable representation had been established that was beyond influence by the asynchrony between the audio and visual inputs. Finally, it should be noted that the occipital selection negativity attention effect was also present for unisensory stimuli (<italic>F</italic>(1,19)&#x000a0;=&#x000a0;4.64; <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05).<fig id="Fig7"><label>Fig.&#x000a0;7</label><caption><p>Occipital selection negativity. A relatively long-latency visual attention effect, the occipital selection negativity was found to be significant across all SOAs for the visual component of the multisensory trials, and did not vary as a function of SOA. It was also present on the unisensory stimuli</p></caption><graphic position="anchor" xlink:href="221_2009_1858_Fig7_HTML" id="MO7"/></fig></p></sec></sec></sec><sec id="Sec21" sec-type="discussion"><title>Discussion</title><p>The main goal of this study was to investigate the impact of intermodal attention on the neural processes involved in multisensory processing, especially those that interact with the degree of onset asynchrony between the auditory and visual components of a multisensory stimulus. Significant interactions between attention and stimulus asynchrony were observed. In particular, attention effects on the visually elicited P1 and N1 components differed as a function of audiovisual SOA, that is, attention effects on the right-hemisphere visual P1 were larger when the auditory and visual inputs were optimally aligned, whereas the attention effects on the visual N1 component were relatively small when this was the case. In contrast, the longer-latency occipital selection negativity attention effect was not influenced by SOA. In addition, we observed an anterior N1 attention effect component that was only present on the visual ERPs elicited by the multisensory stimuli, and which was most pronounced when the auditory and visual stimuli were <italic>suboptimally</italic> aligned.</p><sec id="Sec22"><title>Temporal asynchrony and multisensory processing</title><sec id="Sec23"><title>SOA effects in behavioral data</title><p>Overall, response times to multisensory targets were longer than those to unisensory targets. This finding stands somewhat in contrast to the behavioral advantages associated with multisensory targets that have been often reported in the literature. However, these advantages have typically been found when the multisensory targets are characterized by redundant target features (Giard and Peronn&#x000e9;t <xref ref-type="bibr" rid="CR16">1999</xref>; Molholm et&#x000a0;al. <xref ref-type="bibr" rid="CR37">2002</xref>; Senkowski et&#x000a0;al. <xref ref-type="bibr" rid="CR48">2006</xref>). In contrast, when the target features are non-redundant (Fort et&#x000a0;al. <xref ref-type="bibr" rid="CR13">2002</xref>; Gondan et&#x000a0;al. <xref ref-type="bibr" rid="CR18">2005</xref>), such behavioral advantages do not seem to occur. In the present study the target stimuli contained only the target feature in one modality, and the near-simultaneous presentation of a non-target stimulus in another modality, thus would not have been helpful in detecting the target in the attended modality and might have actually interfered with the target processing in the relevant modality. This might have in turn led to the need for invocation of an attentional selection mechanism. Thus, a response conflict between an attended target stimulus paired with an unattended non-target stimulus might have resulted in the slight increase in response times.</p><p>A similar conclusion can be drawn from the accuracy data, which show that targets that were part of a multisensory stimulus were generally processed with slightly lesser accuracy. In addition, as shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, error rates generally increased as the audiovisual SOA increased. More specifically, when the other-modality stimulus was presented well before the relevant target, only small increases in error rate were observed, until the relevant target and the irrelevant other-modality non-target were presented near simultaneously, suggesting an increase in the competition for attentional capacity up to the point of near simultaneity. One notable exception to this pattern is the +50 SOA, where error rates were lower. Although it is not clear what might be the explanation for this effect, one possibility could be that at these relatively short latencies the second stimulus in the sequence was presented late enough, such that it had relatively little impact on ongoing perceptual processes (such as those involved in determining whether multisensory stimuli should be integrated or not), but early enough not to interfere with higher-order cognitive processes, such as cross modal distraction (e.g., Escera et&#x000a0;al. <xref ref-type="bibr" rid="CR11">2000</xref>). To sum up, the results from the analysis of reaction time and error rates show that both SOA and intermodal attention influence behavioral performance.</p></sec><sec id="Sec24"><title>Interactions between SOA and attention effects in the ERPs</title><p>The ERP data also underscored the behavioral observation that the auditory and visual stimulus inputs competed for attentional capacity. The attention effect on the posterior P1 component was present on unisensory as well as on multisensory trials, and can therefore be considered as a generic effect of intermodal attention, although the latency of this effect was somewhat earlier than the typical effects of non-spatial intermodal attention that have been previously reported (Talsma and Kok <xref ref-type="bibr" rid="CR59">2001</xref>). Interestingly, this attention effect was largest when the auditory input occurred around 50&#x000a0;ms after the visual input, an asynchrony that has been proposed to be the optimum offset for the subjective perception of simultaneity (Lewald and Guski <xref ref-type="bibr" rid="CR27">2003</xref>), leading to greater multisensory integration. Thus, this intermodal attention-related P1 enhancement could reflect a low-level mechanism that boosts visual selection processing when auditory stimuli are particularly close to the point that yields subjective simultaneity.</p><p>The anterior N1 attention effect was unique to the multisensory stimuli and was not observed for the unisensory stimuli. Moreover, since this attention effect was observed predominantly when the temporal offset between visual and auditory inputs was <italic>suboptimal</italic>, we speculate that this effect could reflect a process that is involved in determining whether or not a visual input should or could be temporally aligned to the auditory inputs (Shams et&#x000a0;al <xref ref-type="bibr" rid="CR51">2001</xref>). Since the temporal resolution of the visual modality is lower than that of the auditory modality (e.g., DiLuca et&#x000a0;al. <xref ref-type="bibr" rid="CR31">2009</xref>), might be the demand on processes involved in temporal realignment of visual stimuli to the auditory stimuli is larger when the visual stimuli are attended than when they are not attended, giving rise to the pattern observed. Considering that attending to visual stimuli serves the purposes of enhancing stimulus clarity, and reducing ambiguity, it is plausible that when a visual stimulus becomes relevant, the attentional resolving mechanisms seek to enhance all aspects of the visual stimulus, including its location, visual features, and its temporal characteristics. In line with this, Correa and Nobre (<xref ref-type="bibr" rid="CR9">2008</xref>) have reported evidence that attention can enhance the temporal acuity of unisensory stimuli. Thus, the need for the realignment of a visual stimulus to an auditory one would be particularly large when the visual stimulus is attended. Therefore, in contrast to the occipital P1, which presumably reflects a boost in visual saliency, the N1 seems likely to reflect a somewhat later stage of multisensory integration.</p><p>Interestingly, we found that the early posterior unisensory N1 component was larger for attended stimuli than for unattended stimuli. Since the N1 amplitude is typically not modulated by non-spatial forms of attentional feature selection (e.g., Kenemans et&#x000a0;al. <xref ref-type="bibr" rid="CR25">1993</xref>), our findings suggest that, unlike intramodal non-spatial selective attention processes, the intermodal attention to a centrally presented stimulus may indeed exert a modulatory influence on early sensory ERPs (&#x0003c;200&#x000a0;ms). Notably, the N1 amplitude differences between attended and unattended stimuli (i.e., the attention effects) were smallest when the auditory stimulus was presented near simultaneously (effects on the anterior N1) and when the auditory stimulus followed the visual input by about 50&#x000a0;ms (effects on the anterior and posterior N1). Previous research has shown that a delay of about 50&#x000a0;ms of the auditory stimulus relative to the visual onset is optimal for multisensory integration (e.g., Lewald and Guski <xref ref-type="bibr" rid="CR27">2003</xref>). Therefore, a reduction in N1 differences between attended and unattended visual stimulus inputs of the multisensory stimuli at these latencies would be consistent with a multisensory integration account. More specifically, even though attention would not be directed to the auditory stimuli, the optimal temporal alignment may enable the auditory and visual inputs to be more effectively integrated into a multisensory representation that would be encompassed entirely by attention.</p><p>Thus, the above observations may therefore be an indicative of &#x0201c;spreading of attention&#x0201d; across modalities analogous to effects previously reported (Busse et&#x000a0;al. <xref ref-type="bibr" rid="CR8">2005</xref>; Talsma et&#x000a0;al. <xref ref-type="bibr" rid="CR62">2007</xref>). In the Busse et&#x000a0;al. (<xref ref-type="bibr" rid="CR8">2005</xref>) study, ERP responses to auditory stimuli were characterized by a prolonged late negative component when these stimuli were paired with an attended visual stimulus (and even when it was coming from a different spatial location), which was not present when the same stimulus was paired with an unattended visual stimulus. A similar effect, suggested to be the multisensory counterpart of the auditory &#x0201c;processing negativity&#x0201d;, was reported on the auditory input of a multisensory stimulus when only the visual modality was attended (Talsma et&#x000a0;al. <xref ref-type="bibr" rid="CR62">2007</xref>). In conclusion, we interpret the occipital N1 component as being related to the initial phases of a spread of attention from the visual to the auditory modality.</p><p>Note that these effects on the occipital N1 are somewhat different from those on the P1. Whereas the attention effects on the P1 were largest when the auditory and visual stimuli are optimally aligned, attention effects on the occipital N1 are diminished for optimally aligned auditory and visual inputs. This difference suggests that a possible initial boost of processing in the already attended modality is followed by a spread of attention to the unattended modality, in particular when auditory stimuli are optimally aligned.</p><p>Finally, a stable later effect of attention was found, starting at 200&#x000a0;ms and continuing beyond 300&#x000a0;ms that was identified as the occipital selection negativity. This attention effect is consistent with previously reported similar modulations due to non-spatial intermodal attention (Alho et&#x000a0;al. <xref ref-type="bibr" rid="CR1">1992</xref>; De Ruiter et&#x000a0;al. <xref ref-type="bibr" rid="CR10">1998</xref>; Talsma and Kok <xref ref-type="bibr" rid="CR59">2001</xref>). In addition, Woods et&#x000a0;al. (<xref ref-type="bibr" rid="CR69">1992</xref>) observed a similar negative displacement related to intermodal attention using lateralized stimuli. Thus, the effects of intermodal attention in the current study appear to extend these results to the processing during near-simultaneous multisensory stimuli as well. Interestingly, the occipital selection negativity was the only attention component that did <italic>not</italic> interact with SOA of the multisensory stimulus inputs. This finding is consistent with previous interpretations of the selection negativity as reflecting a cognitive evaluation process that occurs after attentional selection (and in this case multisensory integration) would have already taken place (Smid et&#x000a0;al. <xref ref-type="bibr" rid="CR53">1999</xref>). Thus, this result suggests that, by the time of occurrence of the occipital selection negativity, the temporal realignment and multisensory integration due to the adjacent auditory stimulus component has already occurred. This in turn suggests that the visual representation is stable by this time, and receives attentionally enhanced, higher-level processing that is independent of the relative timing of the auditory stimulus. Thus, the present study underscores the dissociation between an early (semi-) automatic multisensory analysis process and later stages of higher-level processing.</p><p>A similar distinction of levels of multisensory processing was reached by Magn&#x000e9;e et&#x000a0;al. (<xref ref-type="bibr" rid="CR32">2008a</xref>, <xref ref-type="bibr" rid="CR33">b</xref>), who demonstrated that in a group of individuals diagnosed with pervasive developmental disorders (DPP) the early stages of multisensory integration were still intact, whereas the later stages of multisensory integration were impaired. Since disrupted multisensory processing is considered to be a part of various clinical populations, including autism spectrum disorder (Iarocci and McDonald <xref ref-type="bibr" rid="CR22">2006</xref>) or Schizophrenia (Ross et&#x000a0;al. <xref ref-type="bibr" rid="CR43">2007</xref>), the present paradigm could be a viable tool to study the interactions between multisensory integration and cognition in these patient groups.</p></sec></sec></sec><sec id="Sec25"><title>Summary and conclusions</title><p>This study revealed behavioral and electrophysiological evidence for the influence of attention on resolving the input timing differences in a multisensory stimulus. We found that ERPs that were time-locked to the visual component of the audiovisual stimuli showed several attention effects. Of these, the amplitude of the attention effects on early latency components (posterior P1, anterior N1, and posterior N1) were modulated by SOA. Furthermore, attention effects on the posterior P1 and N1 components also occurred for unisensory stimuli. These P1 and N1 effects are therefore likely to be multisensory extensions of basic attention processes. In contrast, we observed a frontal N1 attention effect that was present only on the multisensory stimuli, and, particularly so, when the temporal alignment of the auditory and visual inputs was less than optimal. We therefore suggest that the frontal N1 effect might reflect the processing or adjusting for the temporal alignment of visual inputs to the auditory inputs. In contrast, a longer-latency attention effect, the occipital selection negativity, was not influenced by SOA. We conclude from these findings that even small temporal asynchronies, occurring within the temporal window of integration, are detected at early latencies, and that somewhat later in time frontal brain areas contribute to the processing and resolving of these temporal asynchronies. In contrast, a longer-latency attention effect, the occipital selection negativity, was observed for all SOAs and was not influenced by SOA, suggesting that by this time a stable multisensory representation had been established that was beyond influence from the within-integration-window asynchronies between the audio and visual inputs.</p></sec></body><back><ack><p>We wish to thank Tineke Grent-&#x02018;t-Jong, Ken Roberts, and Chad Hazlett for technical support. Furthermore, we would like to thank Tracy Doty and Vince Wu for their help during data collection. This study was supported by grants from NIH (R01 NS051048) and from NSF (BCS-05-24031) to Marty G. Woldorff, and by a grant from the German Research Foundation (SE 1859/1-1) to Daniel Senkowski.</p><p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Woods</surname><given-names>DL</given-names></name><name><surname>Algazi</surname><given-names>A</given-names></name><name><surname>N&#x000e4;&#x000e4;t&#x000e4;nen</surname><given-names>R</given-names></name></person-group><article-title>Intermodal selective attention II: effects of attentional load on processing auditory and visual stimuli in central space</article-title><source>Electroencephalogr Clin Neurophysiol</source><year>1992</year><volume>82</volume><fpage>356</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(92)90005-3</pub-id></citation><citation citation-type="display-unstructured">Alho K, Woods DL, Algazi A, N&#x000e4;&#x000e4;t&#x000e4;nen R (1992) Intermodal selective attention II: effects of attentional load on processing auditory and visual stimuli in central space. Electroencephalogr Clin Neurophysiol 82:356&#x02013;368 <pub-id pub-id-type="pmid">1374704</pub-id></citation></ref><ref id="CR2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>TS</given-names></name><name><surname>Mamassian</surname><given-names>P</given-names></name></person-group><article-title>Audiovisual integration of stimulus transients</article-title><source>Vis Res</source><year>2008</year><volume>48</volume><fpage>2537</fpage><lpage>2544</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.08.018</pub-id></citation><citation citation-type="display-unstructured">Andersen TS, Mamassian P (2008) Audiovisual integration of stimulus transients. Vis Res 48:2537&#x02013;2544 <pub-id pub-id-type="pmid">18801382</pub-id></citation></ref><ref id="CR6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Burock</surname><given-names>MA</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><article-title>Randomized event-related experimental designs allow for extremely rapid presentation rates using functional MRI</article-title><source>NeuroReport</source><year>1998</year><volume>9</volume><fpage>3735</fpage><lpage>3739</lpage><pub-id pub-id-type="doi">10.1097/00001756-199811160-00030</pub-id></citation><citation citation-type="display-unstructured">Burock MA, Buckner RL, Woldorff MG, Rosen BR, Dale AM (1998) Randomized event-related experimental designs allow for extremely rapid presentation rates using functional MRI. NeuroReport 9:3735&#x02013;3739 <pub-id pub-id-type="pmid">9858388</pub-id></citation></ref><ref id="CR7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><article-title>Implications of the ERP omitted stimulus response to &#x0201c;no-stim&#x0201d; events in fast-rate event-related fMRI designs</article-title><source>Neuroimage</source><year>2003</year><volume>18</volume><fpage>856</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00012-0</pub-id></citation><citation citation-type="display-unstructured">Busse L, Woldorff MG (2003) Implications of the ERP omitted stimulus response to &#x0201c;no-stim&#x0201d; events in fast-rate event-related fMRI designs. Neuroimage 18:856&#x02013;864 <pub-id pub-id-type="pmid">12725762</pub-id></citation></ref><ref id="CR8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Roberts</surname><given-names>KC</given-names></name><name><surname>Crist</surname><given-names>RE</given-names></name><name><surname>Weissman</surname><given-names>DH</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><article-title>The spread of attention across modalities and space in a multisensory object</article-title><source>Proc Natl Acad Sci USA</source><year>2005</year><volume>102</volume><fpage>18751</fpage><lpage>18756</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507704102</pub-id></citation><citation citation-type="display-unstructured">Busse L, Roberts KC, Crist RE, Weissman DH, Woldorff MG (2005) The spread of attention across modalities and space in a multisensory object. Proc Natl Acad Sci USA 102:18751&#x02013;18756 <pub-id pub-id-type="pmid">16339900</pub-id></citation></ref><ref id="CR9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Correa</surname><given-names>A</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><article-title>Spatial and temporal acuity of visual perception can be enhanced selectively by attentional set</article-title><source>Exp Brain Res</source><year>2008</year><volume>189</volume><fpage>339</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1429-2</pub-id></citation><citation citation-type="display-unstructured">Correa A, Nobre AC (2008) Spatial and temporal acuity of visual perception can be enhanced selectively by attentional set. Exp Brain Res 189:339&#x02013;344 <pub-id pub-id-type="pmid">18506432</pub-id></citation></ref><ref id="CR10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ruiter</surname><given-names>MB</given-names></name><name><surname>Kok</surname><given-names>A</given-names></name><name><surname>Schoot</surname><given-names>M</given-names></name></person-group><article-title>Effects of inter- and intramodal selective attention to non-spatial stimuli: an event-related potential analysis</article-title><source>Biol Psychol</source><year>1998</year><volume>49</volume><fpage>269</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1016/S0301-0511(98)00046-5</pub-id></citation><citation citation-type="display-unstructured">De Ruiter MB, Kok A, Van der Schoot M (1998) Effects of inter- and intramodal selective attention to non-spatial stimuli: an event-related potential analysis. Biol Psychol 49:269&#x02013;294 <pub-id pub-id-type="pmid">9858057</pub-id></citation></ref><ref id="CR31"><citation citation-type="other">DiLuca M, Machulla T-K, Ernst MO (2009) Recalibration of multisensory simultaneity: crosmodal transfer coincides with a change in perpeptual latency. J Vis (submitted)</citation></ref><ref id="CR11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Escera</surname><given-names>C</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Schr&#x000f6;ger</surname><given-names>E</given-names></name><name><surname>Winkler</surname><given-names>I</given-names></name></person-group><article-title>Involuntary attention and distractibility as evaluated with event-related brain potentials</article-title><source>Audiol Neurootol</source><year>2000</year><volume>5</volume><fpage>151</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1159/000013877</pub-id></citation><citation citation-type="display-unstructured">Escera C, Alho K, Schr&#x000f6;ger E, Winkler I (2000) Involuntary attention and distractibility as evaluated with event-related brain potentials. Audiol Neurootol 5:151&#x02013;166 <pub-id pub-id-type="pmid">10859410</pub-id></citation></ref><ref id="CR12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fendrich</surname><given-names>R</given-names></name><name><surname>Corballis</surname><given-names>PM</given-names></name></person-group><article-title>The temporal cross-capture of audition and vision</article-title><source>Percept Psychophys</source><year>2001</year><volume>63</volume><fpage>719</fpage><lpage>725</lpage></citation><citation citation-type="display-unstructured">Fendrich R, Corballis PM (2001) The temporal cross-capture of audition and vision. Percept Psychophys 63:719&#x02013;725 <pub-id pub-id-type="pmid">11436740</pub-id></citation></ref><ref id="CR13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fort</surname><given-names>A</given-names></name><name><surname>Delpuech</surname><given-names>C</given-names></name><name><surname>Pemier</surname><given-names>J</given-names></name><name><surname>Giard</surname><given-names>MH</given-names></name></person-group><article-title>Early auditory-visual interactions in human cortex during nonredundant target identification</article-title><source>Cogn Brain Res</source><year>2002</year><volume>14</volume><fpage>20</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00058-7</pub-id></citation><citation citation-type="display-unstructured">Fort A, Delpuech C, Pemier J, Giard MH (2002) Early auditory-visual interactions in human cortex during nonredundant target identification. Cogn Brain Res 14:20&#x02013;30 </citation></ref><ref id="CR16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Giard</surname><given-names>MH</given-names></name><name><surname>Peronn&#x000e9;t</surname><given-names>F</given-names></name></person-group><article-title>Auditory-visual integration during multimodal object recognition in humans: a behavioral and electrophysiological study</article-title><source>J Cogn Neurosci</source><year>1999</year><volume>11</volume><fpage>473</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1162/089892999563544</pub-id></citation><citation citation-type="display-unstructured">Giard MH, Peronn&#x000e9;t F (1999) Auditory-visual integration during multimodal object recognition in humans: a behavioral and electrophysiological study. J Cogn Neurosci 11:473&#x02013;490 <pub-id pub-id-type="pmid">10511637</pub-id></citation></ref><ref id="CR17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gondan</surname><given-names>M</given-names></name><name><surname>R&#x000f6;der</surname><given-names>B</given-names></name></person-group><article-title>A new method for detecting interactions between the senses in event-related potentials</article-title><source>Brain Res</source><year>2006</year><volume>1073</volume><fpage>389</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2005.12.050</pub-id></citation><citation citation-type="display-unstructured">Gondan M, R&#x000f6;der B (2006) A new method for detecting interactions between the senses in event-related potentials. Brain Res 1073:389&#x02013;397 <pub-id pub-id-type="pmid">16427613</pub-id></citation></ref><ref id="CR18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gondan</surname><given-names>M</given-names></name><name><surname>Niederhaus</surname><given-names>B</given-names></name><name><surname>R&#x000f6;sler</surname><given-names>F</given-names></name><name><surname>R&#x000f6;der</surname><given-names>B</given-names></name></person-group><article-title>Multisensory processing in the redundant-target effect: a behavioral and event-related potential study</article-title><source>Percept Psychophys</source><year>2005</year><volume>67</volume><fpage>713</fpage><lpage>726</lpage></citation><citation citation-type="display-unstructured">Gondan M, Niederhaus B, R&#x000f6;sler F, R&#x000f6;der B (2005) Multisensory processing in the redundant-target effect: a behavioral and event-related potential study. Percept Psychophys 67:713&#x02013;726 <pub-id pub-id-type="pmid">16134464</pub-id></citation></ref><ref id="CR22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Iarocci</surname><given-names>G</given-names></name><name><surname>McDonald</surname><given-names>JJ</given-names></name></person-group><article-title>Sensory integration and the perceptual experience of persons with autism</article-title><source>J Autism Dev Disord</source><year>2006</year><volume>36</volume><fpage>77</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1007/s10803-005-0044-3</pub-id></citation><citation citation-type="display-unstructured">Iarocci G, McDonald JJ (2006) Sensory integration and the perceptual experience of persons with autism. J Autism Dev Disord 36:77&#x02013;90 <pub-id pub-id-type="pmid">16395537</pub-id></citation></ref><ref id="CR23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Keetels</surname><given-names>M</given-names></name><name><surname>Vroomen</surname><given-names>J</given-names></name></person-group><article-title>The role of spatial disparity and hemifields in audio-visual temporal order judgments</article-title><source>Exp Brain Res</source><year>2005</year><volume>167</volume><fpage>635</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-0067-1</pub-id></citation><citation citation-type="display-unstructured">Keetels M, Vroomen J (2005) The role of spatial disparity and hemifields in audio-visual temporal order judgments. Exp Brain Res 167:635&#x02013;640 <pub-id pub-id-type="pmid">16175363</pub-id></citation></ref><ref id="CR24"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Kenemans</surname><given-names>JL</given-names></name><name><surname>Molenaar</surname><given-names>PCM</given-names></name><name><surname>Verbaten</surname><given-names>MN</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Weitkunat</surname><given-names>R</given-names></name></person-group><article-title>Models for estimation and removal of artifacts in biological signals</article-title><source>Digital biosignal processing</source><year>1991</year><publisher-loc>New York</publisher-loc><publisher-name>Elsevier</publisher-name></citation><citation citation-type="display-unstructured">Kenemans JL, Molenaar PCM, Verbaten MN (1991) Models for estimation and removal of artifacts in biological signals. In: Weitkunat R (ed) Digital biosignal processing. Elsevier, New York </citation></ref><ref id="CR25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kenemans</surname><given-names>JL</given-names></name><name><surname>Kok</surname><given-names>A</given-names></name><name><surname>Smulders</surname><given-names>FTY</given-names></name></person-group><article-title>Event-related potentials to conjunctions of spatial-frequency and orientation as a function of stimulus parameters and response requirements</article-title><source>Electroencephalogr Clin Neurophysiol</source><year>1993</year><volume>88</volume><fpage>51</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/0168-5597(93)90028-N</pub-id></citation><citation citation-type="display-unstructured">Kenemans JL, Kok A, Smulders FTY (1993) Event-related potentials to conjunctions of spatial-frequency and orientation as a function of stimulus parameters and response requirements. Electroencephalogr Clin Neurophysiol 88:51&#x02013;63 <pub-id pub-id-type="pmid">7681391</pub-id></citation></ref><ref id="CR26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>AJ</given-names></name></person-group><article-title>Multisensory integration: strategies for synchronization</article-title><source>Curr Biol</source><year>2005</year><volume>15</volume><fpage>R339</fpage><lpage>R341</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.04.022</pub-id></citation><citation citation-type="display-unstructured">King AJ (2005) Multisensory integration: strategies for synchronization. Curr Biol 15:R339&#x02013;R341 <pub-id pub-id-type="pmid">15886092</pub-id></citation></ref><ref id="CR27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname><given-names>J</given-names></name><name><surname>Guski</surname><given-names>R</given-names></name></person-group><article-title>Cross-modal perceptual integration of spatially and temporary disparate auditory and visual stimuli</article-title><source>Cogn Brain Res</source><year>2003</year><volume>16</volume><fpage>468</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(03)00074-0</pub-id></citation><citation citation-type="display-unstructured">Lewald J, Guski R (2003) Cross-modal perceptual integration of spatially and temporary disparate auditory and visual stimuli. Cogn Brain Res 16:468&#x02013;478 </citation></ref><ref id="CR28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname><given-names>J</given-names></name><name><surname>Guski</surname><given-names>R</given-names></name></person-group><article-title>Auditory-visual temporal integration as a function of distance: no compensation for sound-transmission time in human perception</article-title><source>Neurosci Lett</source><year>2004</year><volume>357</volume><fpage>119</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2003.12.045</pub-id></citation><citation citation-type="display-unstructured">Lewald J, Guski R (2004) Auditory-visual temporal integration as a function of distance: no compensation for sound-transmission time in human perception. Neurosci Lett 357:119&#x02013;122 <pub-id pub-id-type="pmid">15036589</pub-id></citation></ref><ref id="CR29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname><given-names>J</given-names></name><name><surname>Ehrenstein</surname><given-names>WH</given-names></name><name><surname>Guski</surname><given-names>R</given-names></name></person-group><article-title>Spatio-temporal constraints for auditory-visual integration</article-title><source>Behav Brain Res</source><year>2001</year><volume>121</volume><fpage>69</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/S0166-4328(00)00386-7</pub-id></citation><citation citation-type="display-unstructured">Lewald J, Ehrenstein WH, Guski R (2001) Spatio-temporal constraints for auditory-visual integration. Behav Brain Res 121:69&#x02013;79 <pub-id pub-id-type="pmid">11275285</pub-id></citation></ref><ref id="CR32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Magn&#x000e9;e</surname><given-names>MJCM</given-names></name><name><surname>Gelder</surname><given-names>B</given-names></name><name><surname>Engeland</surname><given-names>H</given-names></name><name><surname>Kemner</surname><given-names>C</given-names></name></person-group><article-title>Audiovisual speech integration in pervasive development disorder: evidence from event-related potentials</article-title><source>J Child Psychol Psychiatry</source><year>2008</year><volume>49</volume><fpage>995</fpage><lpage>1000</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7610.2008.01902.x</pub-id></citation><citation citation-type="display-unstructured">Magn&#x000e9;e MJCM, de Gelder B, van Engeland H, Kemner C (2008a) Audiovisual speech integration in pervasive development disorder: evidence from event-related potentials. J Child Psychol Psychiatry 49:995&#x02013;1000 <pub-id pub-id-type="pmid">18492039</pub-id></citation></ref><ref id="CR33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Magn&#x000e9;e</surname><given-names>MJCM</given-names></name><name><surname>Gelder</surname><given-names>B</given-names></name><name><surname>Engeland</surname><given-names>H</given-names></name><name><surname>Kemner</surname><given-names>C</given-names></name></person-group><article-title>Atypical processing of fearful face-voice pairs in pervasive developmental disorder: an ERP study</article-title><source>Clin Neurophysiol</source><year>2008</year><volume>119</volume><fpage>2004</fpage><lpage>2010</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2008.05.005</pub-id></citation><citation citation-type="display-unstructured">Magn&#x000e9;e MJCM, de Gelder B, van Engeland H, Kemner C (2008b) Atypical processing of fearful face-voice pairs in pervasive developmental disorder: an ERP study. Clin Neurophysiol 119:2004&#x02013;2010 <pub-id pub-id-type="pmid">18571467</pub-id></citation></ref><ref id="CR34"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>McDonald</surname><given-names>JJ</given-names></name><name><surname>Teder-S&#x000e4;lej&#x000e4;rvi</surname><given-names>WA</given-names></name><name><surname>Di Russo</surname><given-names>F</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>Neural basis of auditory-induced shifts in visual time-order perception</article-title><source>Nat Neurosci</source><year>2005</year><volume>8</volume><fpage>1197</fpage><lpage>1202</lpage><pub-id pub-id-type="doi">10.1038/nn1512</pub-id></citation><citation citation-type="display-unstructured">McDonald JJ, Teder-S&#x000e4;lej&#x000e4;rvi WA, Di Russo F, Hillyard SA (2005) Neural basis of auditory-induced shifts in visual time-order perception. Nat Neurosci 8:1197&#x02013;1202 <pub-id pub-id-type="pmid">16056224</pub-id></citation></ref><ref id="CR35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Meredith</surname><given-names>MA</given-names></name></person-group><article-title>On the neuronal basis for multisensory convergence: a brief overview</article-title><source>Brain Res Cogn Brain Res</source><year>2002</year><volume>14</volume><fpage>31</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00059-9</pub-id></citation><citation citation-type="display-unstructured">Meredith MA (2002) On the neuronal basis for multisensory convergence: a brief overview. Brain Res Cogn Brain Res 14:31&#x02013;40 <pub-id pub-id-type="pmid">12063128</pub-id></citation></ref><ref id="CR36"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Meredith</surname><given-names>MA</given-names></name><name><surname>Nemitz</surname><given-names>JW</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><article-title>Determinants of multisensory integration in superior colliculus neurons. 1. Temporal factors</article-title><source>J Neurosci</source><year>1987</year><volume>7</volume><fpage>3215</fpage><lpage>3229</lpage></citation><citation citation-type="display-unstructured">Meredith MA, Nemitz JW, Stein BE (1987) Determinants of multisensory integration in superior colliculus neurons. 1. Temporal factors. J Neurosci 7:3215&#x02013;3229 <pub-id pub-id-type="pmid">3668625</pub-id></citation></ref><ref id="CR37"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Molholm</surname><given-names>S</given-names></name><name><surname>Ritter</surname><given-names>W</given-names></name><name><surname>Murray</surname><given-names>MM</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><article-title>Multisensory auditory-visual interactions during early sensory processing in humans: a high-density electrical mapping study</article-title><source>Cogn Brain Res</source><year>2002</year><volume>14</volume><fpage>115</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00066-6</pub-id></citation><citation citation-type="display-unstructured">Molholm S, Ritter W, Murray MM, Javitt DC, Schroeder CE, Foxe JJ (2002) Multisensory auditory-visual interactions during early sensory processing in humans: a high-density electrical mapping study. Cogn Brain Res 14:115&#x02013;128 </citation></ref><ref id="CR38"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Morein-Zamir</surname><given-names>S</given-names></name><name><surname>Soto-Fara&#x000e7;o</surname><given-names>S</given-names></name><name><surname>Kingstone</surname><given-names>A</given-names></name></person-group><article-title>Auditory capture of vision: examining temporal ventriloquism</article-title><source>Cogn Brain Res</source><year>2003</year><volume>17</volume><fpage>154</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(03)00089-2</pub-id></citation><citation citation-type="display-unstructured">Morein-Zamir S, Soto-Fara&#x000e7;o S, Kingstone A (2003) Auditory capture of vision: examining temporal ventriloquism. Cogn Brain Res 17:154&#x02013;163 </citation></ref><ref id="CR39"><citation citation-type="book"><person-group person-group-type="author"><name><surname>N&#x000e4;&#x000e4;t&#x000e4;nen</surname><given-names>R</given-names></name></person-group><source>Attention and brain function</source><year>1992</year><publisher-loc>Hillsdale, NJ</publisher-loc><publisher-name>Lawrence Erlbaum</publisher-name></citation><citation citation-type="display-unstructured">N&#x000e4;&#x000e4;t&#x000e4;nen R (1992) Attention and brain function. Lawrence Erlbaum, Hillsdale, NJ </citation></ref><ref id="CR41"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Navarra</surname><given-names>J</given-names></name><name><surname>Vatakis</surname><given-names>A</given-names></name><name><surname>Zampini</surname><given-names>M</given-names></name><name><surname>Soto-Fara&#x000e7;o</surname><given-names>S</given-names></name><name><surname>Humphreys</surname><given-names>W</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><article-title>Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration</article-title><source>Cogn Brain Res</source><year>2005</year><volume>25</volume><fpage>499</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2005.07.009</pub-id></citation><citation citation-type="display-unstructured">Navarra J, Vatakis A, Zampini M, Soto-Fara&#x000e7;o S, Humphreys W, Spence C (2005) Exposure to asynchronous audiovisual speech extends the temporal window for audiovisual integration. Cogn Brain Res 25:499&#x02013;507 </citation></ref><ref id="CR43"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>LA</given-names></name><name><surname>Saint-Amour</surname><given-names>D</given-names></name><name><surname>Leavitt</surname><given-names>VM</given-names></name><name><surname>Molholm</surname><given-names>S</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><article-title>Impaired multisensory processing in schizophrenia: deficits in the visual enhancement of speech comprehension under noisy environmental conditions</article-title><source>Schizophr Res</source><year>2007</year><volume>97</volume><fpage>173</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1016/j.schres.2007.08.008</pub-id></citation><citation citation-type="display-unstructured">Ross LA, Saint-Amour D, Leavitt VM, Molholm S, Javitt DC, Foxe JJ (2007) Impaired multisensory processing in schizophrenia: deficits in the visual enhancement of speech comprehension under noisy environmental conditions. Schizophr Res 97:173&#x02013;183 <pub-id pub-id-type="pmid">17928202</pub-id></citation></ref><ref id="CR47"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Herrmann</surname><given-names>CS</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><article-title>Multisensory processing and oscillatory gamma responses: effects of spatial selective attention</article-title><source>Exp Brain Res</source><year>2005</year><volume>166</volume><fpage>411</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2381-z</pub-id></citation><citation citation-type="display-unstructured">Senkowski D, Talsma D, Herrmann CS, Woldorff MG (2005) Multisensory processing and oscillatory gamma responses: effects of spatial selective attention. Exp Brain Res 166:411&#x02013;426 <pub-id pub-id-type="pmid">16151775</pub-id></citation></ref><ref id="CR48"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Molholm</surname><given-names>S</given-names></name><name><surname>Gomez-Ramirez</surname><given-names>M</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><article-title>Oscillatory beta activity predicts response speed during a multisensory audiovisual reaction time task: a high-density electrical mapping study</article-title><source>Cereb Cortex</source><year>2006</year><volume>16</volume><fpage>1556</fpage><lpage>1565</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj091</pub-id></citation><citation citation-type="display-unstructured">Senkowski D, Molholm S, Gomez-Ramirez M, Foxe JJ (2006) Oscillatory beta activity predicts response speed during a multisensory audiovisual reaction time task: a high-density electrical mapping study. Cereb Cortex 16:1556&#x02013;1565 <pub-id pub-id-type="pmid">16357336</pub-id></citation></ref><ref id="CR50"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Grigutsch</surname><given-names>M</given-names></name><name><surname>Herrmann</surname><given-names>CS</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><article-title>Good times for multisensory integration: effects of the precision of temporal synchrony as revealed by gamma-band oscillations</article-title><source>Neuropsychologia</source><year>2007</year><volume>45</volume><issue>3</issue><fpage>561</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.01.013</pub-id></citation><citation citation-type="display-unstructured">Senkowski D, Talsma D, Grigutsch M, Herrmann CS, Woldorff MG (2007a) Good times for multisensory integration: effects of the precision of temporal synchrony as revealed by gamma-band oscillations. Neuropsychologia 45(3):561&#x02013;571 <pub-id pub-id-type="pmid">16542688</pub-id></citation></ref><ref id="CR49"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Saint-Amour</surname><given-names>D</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><article-title>Multisensory processing of naturalistic objects in motion: a high-density electrical mapping and source estimation study</article-title><source>Neuroimage</source><year>2007</year><volume>36</volume><fpage>877</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.01.053</pub-id></citation><citation citation-type="display-unstructured">Senkowski D, Saint-Amour D, Kelly SP, Foxe JJ (2007b) Multisensory processing of naturalistic objects in motion: a high-density electrical mapping and source estimation study. Neuroimage 36:877&#x02013;888 <pub-id pub-id-type="pmid">17481922</pub-id></citation></ref><ref id="CR51"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Thompson</surname><given-names>S</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group><article-title>Sound alters visual evoked potentials in humans</article-title><source>NeuroReport</source><year>2001</year><volume>12</volume><fpage>3849</fpage><lpage>3852</lpage><pub-id pub-id-type="doi">10.1097/00001756-200112040-00049</pub-id></citation><citation citation-type="display-unstructured">Shams L, Kamitani Y, Thompson S, Shimojo S (2001) Sound alters visual evoked potentials in humans. NeuroReport 12:3849&#x02013;3852 <pub-id pub-id-type="pmid">11726807</pub-id></citation></ref><ref id="CR52"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name></person-group><article-title>Visual illusion induced by sound</article-title><source>Cogn Brain Res</source><year>2002</year><volume>14</volume><fpage>147</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00069-1</pub-id></citation><citation citation-type="display-unstructured">Shams L, Kamitani Y, Shimojo S (2002) Visual illusion induced by sound. Cogn Brain Res 14:147&#x02013;152 </citation></ref><ref id="CR53"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Smid</surname><given-names>HGOM</given-names></name><name><surname>Jakob</surname><given-names>A</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name></person-group><article-title>An event-related brain potential study of visual selective attention to conjunctions of color and shape</article-title><source>Psychophysiology</source><year>1999</year><volume>36</volume><fpage>264</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1017/S0048577299971135</pub-id></citation><citation citation-type="display-unstructured">Smid HGOM, Jakob A, Heinze HJ (1999) An event-related brain potential study of visual selective attention to conjunctions of color and shape. Psychophysiology 36:264&#x02013;279 <pub-id pub-id-type="pmid">10194973</pub-id></citation></ref><ref id="CR54"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Squire</surname><given-names>S</given-names></name></person-group><article-title>Multisensory integration: maintaining the perception of synchrony</article-title><source>Curr Biol</source><year>2003</year><volume>12</volume><fpage>R519</fpage><lpage>R521</lpage><pub-id pub-id-type="doi">10.1016/S0960-9822(03)00445-7</pub-id></citation><citation citation-type="display-unstructured">Spence C, Squire S (2003) Multisensory integration: maintaining the perception of synchrony. Curr Biol 12:R519&#x02013;R521 <pub-id pub-id-type="pmid">12842029</pub-id></citation></ref><ref id="CR55"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Meredith</surname><given-names>MA</given-names></name></person-group><source>The merging of the senses</source><year>1993</year><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT press</publisher-name></citation><citation citation-type="display-unstructured">Stein BE, Meredith MA (1993) The merging of the senses. MIT press, Cambridge, MA </citation></ref><ref id="CR56"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Stekelenburg</surname><given-names>JJ</given-names></name><name><surname>Vroomen</surname><given-names>J</given-names></name></person-group><article-title>An event-related potential investigation of the time-course of temporal ventriloquism</article-title><source>NeuroReport</source><year>2005</year><volume>16</volume><fpage>641</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1097/00001756-200504250-00025</pub-id></citation><citation citation-type="display-unstructured">Stekelenburg JJ, Vroomen J (2005) An event-related potential investigation of the time-course of temporal ventriloquism. NeuroReport 16:641&#x02013;644 <pub-id pub-id-type="pmid">15812324</pub-id></citation></ref><ref id="CR58"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name></person-group><article-title>Auto-adaptive averaging: detecting artifacts in event-related potential data using a fully automated procedure</article-title><source>Psychophysiology</source><year>2008</year><volume>45</volume><fpage>216</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00612.x</pub-id></citation><citation citation-type="display-unstructured">Talsma D (2008) Auto-adaptive averaging: detecting artifacts in event-related potential data using a fully automated procedure. Psychophysiology 45:216&#x02013;228 <pub-id pub-id-type="pmid">17971060</pub-id></citation></ref><ref id="CR59"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Kok</surname><given-names>A</given-names></name></person-group><article-title>Nonspatial intermodal selective attention is mediated by sensory brain areas: evidence from event-related potentials</article-title><source>Psychophysiology</source><year>2001</year><volume>38</volume><fpage>736</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1017/S0048577201000798</pub-id></citation><citation citation-type="display-unstructured">Talsma D, Kok A (2001) Nonspatial intermodal selective attention is mediated by sensory brain areas: evidence from event-related potentials. Psychophysiology 38:736&#x02013;751 <pub-id pub-id-type="pmid">11577897</pub-id></citation></ref><ref id="CR60"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><article-title>Attention and multisensory integration: multiple phases of effects on the evoked brain activity</article-title><source>J Cogn Neurosci</source><year>2005</year><volume>17</volume><fpage>1098</fpage><lpage>1114</lpage><pub-id pub-id-type="doi">10.1162/0898929054475172</pub-id></citation><citation citation-type="display-unstructured">Talsma D, Woldorff MG (2005a) Attention and multisensory integration: multiple phases of effects on the evoked brain activity. J Cogn Neurosci 17:1098&#x02013;1114 <pub-id pub-id-type="pmid">16102239</pub-id></citation></ref><ref id="CR61"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Handy</surname><given-names>TC</given-names></name></person-group><article-title>Methods for the estimation and removal of artifacts and overlap in ERP waveforms</article-title><source>Event-related potentials: a methods handbook</source><year>2005</year><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>115</fpage><lpage>148</lpage></citation><citation citation-type="display-unstructured">Talsma D, Woldorff MG (2005b) Methods for the estimation and removal of artifacts and overlap in ERP waveforms. In: Handy TC (ed) Event-related potentials: a methods handbook. MIT Press, Cambridge, MA, pp 115&#x02013;148 </citation></ref><ref id="CR62"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Doty</surname><given-names>TJ</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><article-title>Selective attention and audiovisual integration: is attending to both modalities a prerequisite for early integration?</article-title><source>Cereb Cortex</source><year>2007</year><volume>17</volume><fpage>691</fpage><lpage>701</lpage></citation><citation citation-type="display-unstructured">Talsma D, Doty TJ, Woldorff MG (2007) Selective attention and audiovisual integration: is attending to both modalities a prerequisite for early integration? Cereb Cortex 17:691&#x02013;701 <pub-id pub-id-type="pmid">16627856</pub-id></citation></ref><ref id="CR63"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Teder-S&#x000e4;lej&#x000e4;rvi</surname><given-names>WA</given-names></name><name><surname>McDonald</surname><given-names>JJ</given-names></name><name><surname>Di Russo</surname><given-names>F</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>An analysis of audio-visual crossmodal integration by means of event-related potential (ERP) recordings</article-title><source>Cogn Brain Res</source><year>2002</year><volume>14</volume><fpage>106</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00065-4</pub-id></citation><citation citation-type="display-unstructured">Teder-S&#x000e4;lej&#x000e4;rvi WA, McDonald JJ, Di Russo F, Hillyard SA (2002) An analysis of audio-visual crossmodal integration by means of event-related potential (ERP) recordings. Cogn Brain Res 14:106&#x02013;114 </citation></ref><ref id="CR64"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Gelder</surname><given-names>B</given-names></name></person-group><article-title>Temporal ventriloquism: sound modulates the flash-lag effect</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2004</year><volume>30</volume><fpage>513</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.30.3.513</pub-id></citation><citation citation-type="display-unstructured">Vroomen J, De Gelder B (2004) Temporal ventriloquism: sound modulates the flash-lag effect. J Exp Psychol Hum Percept Perform 30:513&#x02013;518 <pub-id pub-id-type="pmid">15161383</pub-id></citation></ref><ref id="CR65"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Keetels</surname><given-names>M</given-names></name></person-group><article-title>The spatial constraint in intersensory pairing: no role in temporal ventriloquism</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2006</year><volume>32</volume><fpage>1063</fpage><lpage>1071</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.32.4.1063</pub-id></citation><citation citation-type="display-unstructured">Vroomen J, Keetels M (2006) The spatial constraint in intersensory pairing: no role in temporal ventriloquism. J Exp Psychol Hum Percept Perform 32:1063&#x02013;1071 <pub-id pub-id-type="pmid">16846297</pub-id></citation></ref><ref id="CR66"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Meredith</surname><given-names>MA</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><article-title>Converging influences from visual, auditory, and somatosensory cortices onto output neurons of the superior colliculus</article-title><source>J Neurophysiol</source><year>1993</year><volume>69</volume><fpage>1797</fpage><lpage>1809</lpage></citation><citation citation-type="display-unstructured">Wallace MT, Meredith MA, Stein BE (1993) Converging influences from visual, auditory, and somatosensory cortices onto output neurons of the superior colliculus. J Neurophysiol 69:1797&#x02013;1809 <pub-id pub-id-type="pmid">8350124</pub-id></citation></ref><ref id="CR67"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><article-title>Distortion of ERP averages due to overlap from temporally adjacent ERP: analysis and correction</article-title><source>Psychophysiology</source><year>1993</year><volume>30</volume><fpage>98</fpage><lpage>119</lpage></citation><citation citation-type="display-unstructured">Woldorff MG (1993) Distortion of ERP averages due to overlap from temporally adjacent ERP: analysis and correction. Psychophysiology 30:98&#x02013;119 <pub-id pub-id-type="pmid">8416067</pub-id></citation></ref><ref id="CR69"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>DL</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Algazi</surname><given-names>A</given-names></name></person-group><article-title>Intermodal selective attention I: effects of event-related potentials to lateralized auditory and visual stimuli</article-title><source>Electroencephalogr Clin Neurophysiol</source><year>1992</year><volume>82</volume><fpage>341</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(92)90004-2</pub-id></citation><citation citation-type="display-unstructured">Woods DL, Alho K, Algazi A (1992) Intermodal selective attention I: effects of event-related potentials to lateralized auditory and visual stimuli. Electroencephalogr Clin Neurophysiol 82:341&#x02013;355 <pub-id pub-id-type="pmid">1374703</pub-id></citation></ref></ref-list><fn-group><fn id="Fn1"><label>1</label><p>The EEG was recorded by the following electrodes: FPz, Fz, FCz, Cz, Fp1 m, Fp2 m, F3a, F4a, F3 s, F4 s, FC1, FC2, C1a, C2a, F7a, F8a, F3i, F4i, C3a, C4a, PA1a, PA2a, F7p, F8p, C5a, C6a, T3&#x02032;, T4&#x02032;, LC, RC, LIO, RIO, Lm, Inz, Ozi, Ozs, Pzi, Pzs, I1, I2, O1i, O2i, O1&#x02032;, O2&#x02032;, PO1, PO2, P1&#x02032;, P2&#x02032;, C1p, C2p, TI1, TI2, TO1, TO2, P3i, P4i, P3a, P4a, C3&#x02032;, C4&#x02032;, T35i, T46i, C5p, and C6p. These electrode positions are named relative to their approximate 10&#x02013;10 equivalents. A suffix of &#x02018;s&#x02019; indicates that the electrode was placed slightly (i.e., within 1&#x02013;1.5 cm) superior to the indicated standard position; &#x02018;i&#x02019; indicates it was placed inferior to the standard position. Similarly, &#x02018;a&#x02019; and &#x02018;p&#x02019; indicate the electrode in question was positioned slightly anterior or posterior to the standard locations, respectively. Electrodes position &#x0003c;1&#x000a0;cm are named by the standard location name with an added prime mark (e.g., C4&#x02032;).</p></fn></fn-group></back></article>



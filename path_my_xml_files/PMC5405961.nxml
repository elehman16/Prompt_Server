<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28445540</article-id><article-id pub-id-type="pmc">5405961</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0176534</article-id><article-id pub-id-type="publisher-id">PONE-D-16-47567</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Human Factors Engineering</subject><subj-group><subject>Man-Computer Interface</subject><subj-group><subject>Virtual Reality</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Architecture</subject><subj-group><subject>User Interfaces</subject><subj-group><subject>Virtual Reality</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Ophthalmology</subject><subj-group><subject>Visual Impairments</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Biological Locomotion</subject><subj-group><subject>Walking</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Biological Locomotion</subject><subj-group><subject>Walking</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Civil Engineering</subject><subj-group><subject>Transportation Infrastructure</subject><subj-group><subject>Roads</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Transportation</subject><subj-group><subject>Transportation Infrastructure</subject><subj-group><subject>Roads</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Individuals with severely impaired vision can learn useful orientation and mobility skills in virtual streets and can use them to improve real street safety</article-title><alt-title alt-title-type="running-head">Virtual reality street safety training for the visually impaired</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0761-2635</contrib-id><name><surname>Bowman</surname><given-names>Ellen Lambert</given-names></name><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Lei</given-names></name><xref ref-type="aff" rid="aff001"/><xref ref-type="corresp" rid="cor001">*</xref></contrib></contrib-group><aff id="aff001"><addr-line>School of Optometry, University of Alabama at Birmingham, Birmingham, Alabama, United States of America</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Lv</surname><given-names>Zhihan</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>University College London, UNITED KINGDOM</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p><list list-type="simple"><list-item><p><bold>Conceptualization:</bold> LL.</p></list-item><list-item><p><bold>Data curation:</bold> ELB LL.</p></list-item><list-item><p><bold>Formal analysis:</bold> ELB LL.</p></list-item><list-item><p><bold>Funding acquisition:</bold> LL.</p></list-item><list-item><p><bold>Investigation:</bold> ELB LL.</p></list-item><list-item><p><bold>Methodology:</bold> LL.</p></list-item><list-item><p><bold>Project administration:</bold> ELB LL.</p></list-item><list-item><p><bold>Resources:</bold> ELB LL.</p></list-item><list-item><p><bold>Software:</bold> LL.</p></list-item><list-item><p><bold>Supervision:</bold> LL.</p></list-item><list-item><p><bold>Validation:</bold> ELB LL.</p></list-item><list-item><p><bold>Visualization:</bold> ELB LL.</p></list-item><list-item><p><bold>Writing &#x02013; original draft:</bold> ELB LL.</p></list-item><list-item><p><bold>Writing &#x02013; review &#x00026; editing:</bold> ELB LL.</p></list-item></list>
</p></fn><corresp id="cor001">* E-mail: <email>liul7788@uab.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>4</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>12</volume><issue>4</issue><elocation-id>e0176534</elocation-id><history><date date-type="received"><day>1</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>12</day><month>4</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2017 Bowman, Liu</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Bowman, Liu</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0176534.pdf"/><abstract><p>Virtual reality has great potential in training road safety skills to individuals with low vision but the feasibility of such training has not been demonstrated. We tested the hypotheses that low vision individuals could learn useful skills in virtual streets and could apply them to improve real street safety. Twelve participants, whose vision was too poor to use the pedestrian signals were taught by a certified orientation and mobility specialist to determine the safest time to cross the street using the visual and auditory signals made by the start of previously stopped cars at a traffic-light controlled street intersection. Four participants were trained in real streets and eight in virtual streets presented on 3 projection screens. The crossing timing of all participants was evaluated in real streets before and after training. The participants were instructed to say &#x0201c;GO&#x0201d; at the time when they felt the safest to cross the street. A safety score was derived to quantify the GO calls based on its occurrence in the pedestrian phase (when the pedestrian sign did not show DON&#x02019;T WALK). Before training, &#x0003e; 50% of the GO calls from all participants fell in the DON&#x02019;T WALK phase of the traffic cycle and thus were totally unsafe. 20% of the GO calls fell in the latter half of the pedestrian phase. These calls were unsafe because one initiated crossing this late might not have sufficient time to walk across the street. After training, 90% of the GO calls fell in the early half of the pedestrian phase. These calls were safer because one initiated crossing in the pedestrian phase and had at least half of the pedestrian phase for walking across. Similar safety changes occurred in both virtual street and real street trained participants. An ANOVA showed a significant increase of the safety scores after training and there was no difference in this safety improvement between the virtual street and real street trained participants. This study demonstrated that virtual reality-based orientation and mobility training could be as efficient as real street training in improving street safety in individuals with severely impaired vision.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>1R21EY019549</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Lei</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>P30 EY003039</award-id></award-group><funding-statement>ELB was supported by a Fellowship from the National Leadership Consortium in Sensory Disabilities. This work was supported by grants from the National Eye Institute (1R21EY019549 to LL and P30 EY003039). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="6"/><table-count count="2"/><page-count count="20"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Our data is contained within the paper in the supporting information file.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Our data is contained within the paper in the supporting information file.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>The National Eye Institute of the United States of America estimated that 2 million Americans suffered from low vision (visual acuity between 20/40 and 20/200) and 1 million suffered blindness (acuity &#x02264; 20/200) in 2010 and the numbers will double in 2030 [<xref rid="pone.0176534.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0176534.ref002" ref-type="bibr">2</xref>]. Low vision has profound negative impacts on the individual&#x02019;s physical and psychological wellbeing, personal independence, employment and quality of life [<xref rid="pone.0176534.ref003" ref-type="bibr">3</xref>]. On the other hand, only 15% of the individuals with low vision have no usable vision (light perception or worse). The rest have various degrees of usable vision [<xref rid="pone.0176534.ref004" ref-type="bibr">4</xref>]. Rehabilitation is the primary treatment option for individuals with low vision [<xref rid="pone.0176534.ref005" ref-type="bibr">5</xref>]. It provides a wide range of professional services that enable individuals to maximize the use of their remaining vision, to supplement impaired vision with other sensory inputs and to learn alternative strategies to perform daily tasks.</p><p>One of the most sought after, high-value low vision rehabilitation items is Orientation and Mobility (O&#x00026;M) rehabilitation, which teaches special skills to individuals with low vision so that they can travel independently, safely, and swiftly through their environment [<xref rid="pone.0176534.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0176534.ref007" ref-type="bibr">7</xref>]. These skills are taught in a one-on-one manner between a certified O&#x00026;M specialist (COMS) and a visually impaired student on real streets or real buildings. While this mode of training has been the standard of care since the end of WWII, its limitations in accessibility and affordability have become more noticeable when a growing portion of the otherwise healthy population suffer from visual impairment. This is because a COMS has to accompany a low vision trainee on the street to ensure her safety throughout training, but the COMS resource is scarce and costly. In 2016, there were only 2666 current O&#x00026;M certificate holders in the US listed in the Academy for Certification of Vision Rehabilitation and Education Professionals website. Most of them concentrate in Veteran&#x02019;s Administration blind rehabilitation centers and academic university medication centers. Only 20% of non-Veteran&#x02019;s Administration low vision rehabilitation entities have at least one COMS working part-time or full-time [<xref rid="pone.0176534.ref005" ref-type="bibr">5</xref>]. Most of the O&#x00026;M rehabilitation cost comes from COMS&#x02019; time. To gain the skills to navigate unfamiliar environment safety, a couple of hundred hours of instruction is needed [<xref rid="pone.0176534.ref006" ref-type="bibr">6</xref>]. The hourly COMS cost varies from $50 to $120 in the US, which is usually not reimbursable by Medicare, Medicaid, or private insurance. Access to COMS and financial burden are main barriers to realizing a low vision individual&#x02019;s full potential for independent travel.</p><p>A large portion of the O&#x00026;M curriculum is focused on learning to gather information about the environment using the remaining vision and other sensory inputs and to make timely and safe decisions. These sensory, perceptual and cognitive skills are quite different from those used by the individual before his vision is impaired, and thus require instructions and practice. Virtual Reality (VR), a computer generated sensory environment, has great potential to improve current O&#x00026;M rehabilitation. VR has found successful applications in psychotherapy, orthopedic rehabilitation and neuropsychological rehabilitation [<xref rid="pone.0176534.ref008" ref-type="bibr">8</xref>&#x02013;<xref rid="pone.0176534.ref014" ref-type="bibr">14</xref>]. Another important area of VR applications is skill acquisition. These typically involve learning high risk skills in risk-free virtual environments, such as learning to fly a plane or to drive a truck in a VR simulator. Surgery simulators have become an integral component in surgical skill training [<xref rid="pone.0176534.ref015" ref-type="bibr">15</xref>&#x02013;<xref rid="pone.0176534.ref017" ref-type="bibr">17</xref>]. VR has been successfully applied to teaching road safety to children [<xref rid="pone.0176534.ref018" ref-type="bibr">18</xref>, <xref rid="pone.0176534.ref019" ref-type="bibr">19</xref>] and stroke patients [<xref rid="pone.0176534.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0176534.ref021" ref-type="bibr">21</xref>]. There has been a sizeable amount of literature on teaching O&#x00026;M skills to blind individuals who possess no usable vision and have to rely on alternative sensory inputs. It has been shown that these individuals could successfully explore a new virtual acoustic or haptic + acoustic space [<xref rid="pone.0176534.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0176534.ref023" ref-type="bibr">23</xref>]. Seki and Sato used blind-folded normal participants to demonstrate that training in a virtual acoustic environment could outperform training in a real environment in some O&#x00026;M skill metrics [<xref rid="pone.0176534.ref024" ref-type="bibr">24</xref>].</p><p>In comparison, there is little research on teaching visual skills to individuals with low vision in VR. Kalia, Legge and Giudice asked low vision participants to explore an indoor hallway in four presentation modes, a photo-realistic virtual hallway, a sparse virtual hallway containing only geometric cues of the hallway, a map and the real hallway [<xref rid="pone.0176534.ref025" ref-type="bibr">25</xref>]. The participants&#x02019; knowledge of the hallway was tested by map drawing and by walking to specified acoustic targets in the real hallway. Low vision participants showed better knowledge of the hallway from learning the photographic virtual hallway than from learning the sparse virtual hallway. Learning from both virtual hallways were less efficient than learning from a map or the real hallway. This study demonstrated that individuals with low vision could benefit from high quality visual information.</p><p>The key issue in VR-based O&#x00026;M skill training is whether low vision students can use VR-acquired skills to solve mobility problems in real streets. While VR provides a controllable, quantifiable, less stressful and safe environment for learning and practicing potentially dangerous road safety skills, virtual reality is not the same as physical reality. Due to technological and cost restraints, not all aspects of real world can be faithfully simulated in VR. From this point of view, VR is a compromised representation of physical reality. It is of paramount importance to determine if the students can learn anything useful from this compromised representation and, more importantly, to what degree the students can apply the VR-acquired skills to solve problems in the real world. Therefore, an important first step of any VR application development is to determine the efficacy and extent of VR to real world skill transformation. Developers and users of surgical simulators view VR to Operating Room as the gold standard for judging the impact of the simulators on surgical skill training [<xref rid="pone.0176534.ref026" ref-type="bibr">26</xref>]. The same applies to VR training of O&#x00026;M skills. Many important sensory inputs from the real environment that can potentially influence O&#x00026;M, such as stereoscopic depth, spatialized sound, vibration on the ground, heat of a car or change of air flow, are technically too complicated or too costly to simulate in a clinically deployable VR simulator. Training in a VR, one does not have to fear the consequence of making a mistake or be embarrassed by not being able to behave like a normally sighted pedestrian. Can low vision students, whose vision is already severely compromised, learn useful O&#x00026;M skills from such a compromised presentation of the real streets? If they can, will they be able to transfer the skills to the real streets? This study was designed to assess VR skill transferability in individuals with severely impaired vision. We hypothesized that these individuals can learn useful O&#x00026;M skills from an affordable VR simulator and can efficiently transfer VR-acquired skills to solving real world O&#x00026;M problems.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><sec id="sec003"><title>Research design</title><p>A two-group pre- and post-training design was used to quantitatively assess the transferability of VR-acquired O&#x00026;M skills. In this design, low vision participants were pseudo-randomly assigned to a virtual street training group and a real street training group, which received O&#x00026;M skill training in virtual streets presented by a VR simulator and in comparable real streets, respectively. Road safety of all participants were evaluated in real streets before and after training. The primary outcome measure of the research was the change of road safety in real streets before and after training.</p></sec><sec id="sec004"><title>O&#x00026;M task and skills</title><p>The task to be studied was to make a safe decision to cross a signal controlled street. It was chosen because it involved using visual and auditory skills to collect real-time information in a complex and highly dynamic environment and to make a correct decision in a timely manner. A wrong decision can potentially put the traveler in a dangerous situation.</p><p>Traffic engineers design pedestrian signal schema such that if a pedestrian leaves the curb during a prescribed period of time after the onset of the WALK sign, he is assured of sufficient time to complete his crossing. Specifically, the pedestrian signal contains two phases. The DON&#x02019;T WALK phase usually coincides with the red light on the same direction for cars. Any attempt to cross the street in this phase is unsafe. The pedestrian phase has two intervals. The WALK interval, during which the WALK sign or the white walking figure is shown. The WALK interval is typically 7 sec long. It is the time when a slow-average walker must leave the curb in order to have sufficient time to walk to the far side. The pedestrian clearance interval, or FLASHING DON&#x02019;T WALK interval, during which the pedestrian signal is flashing or counting down, indicates the time needed for a person walking at a speed of 3.5 feet/second or faster to walk across the street [<xref rid="pone.0176534.ref027" ref-type="bibr">27</xref>]. The pedestrian signal provides salient visual information to guide crossing decision making for people who can see it.</p><p>For people who cannot see the pedestrian signal in a street intersection shown in <xref ref-type="fig" rid="pone.0176534.g001">Fig 1</xref>, safe crossing is still possible but a different set of skills has to be learned to infer the traffic cycle timing. The commonly taught O&#x00026;M skill is called the near lane parallel traffic surge (NLPTS). In O&#x00026;M terminology, a perpendicular street is the street to be crossed (perpendicular to one&#x02019;s direction of travel). A parallel street is the street that is parallel to one&#x02019;s direction of travel. Parallel traffic is the traffic that runs on the parallel streets. For each crossing scenario, there is one lane of the parallel traffic that is the closest to the traveler. This is the near lane parallel traffic. Its nearness makes its cars more visible and audible than those in other lanes of parallel traffic. In the crossing scenario shown in <xref ref-type="fig" rid="pone.0176534.g001">Fig 1</xref>, the near lane parallel traffic (thick green arrow) comes from behind the traveler, over the left shoulder. If the traveler is to cross the same street in the opposite direction, the near lane parallel traffic comes from across the perpendicular street (in front). The traveler needs to learn to distinguish these two crossing scenarios. Finally, a surge in O&#x00026;M refers to the visual and auditory signals made by a previously stopped car that starts to move. Because in most circumstances, the NLPTS corresponds to the change of the traffic light to green and coincides with the onset of the WALK sign, learning to locate the near lane parallel traffic and to detect its surge enables a traveler to make correct crossing decision even if he cannot see the pedestrian signal.</p><fig id="pone.0176534.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.g001</object-id><label>Fig 1</label><caption><title>Layout of a cross street intersection and traffics.</title><p>The traveler is standing at the street corner indicated by the blue circle and intends to cross the perpendicular street in the direction indicated by the blue arrow. Red and green arrows are perpendicular and parallel traffic, respectively. The near lane parallel traffic (thick green arrow, NLPT) comes from behind, over his left shoulder. Right-turn cars (magenta dashed arrow) may share the near lane.</p></caption><graphic xlink:href="pone.0176534.g001"/></fig><p>However, in cities such as Birmingham, Alabama, cars in the parallel traffic are allowed to make right turns when the traffic light is red. This weakens the salience of the NLPTS. If the surge made by such a turning car is used to time crossing, the traveler may risk being run over by the turning car or end up in the moving perpendicular traffic. Therefore, the correct use of NLPTS is not to start crossing immediately when a surge is detected, but to wait until one of the cars in the near lane entering the center of the street intersection and thus to confirm that the surge is indeed a NLPTS. This strategy sacrifices a few precious seconds at the beginning of the pedestrian phase to improve the safety of crossing decision. The NLPTS skills were taught to almost all individuals seeking O&#x00026;M training, including totally blind individuals.</p></sec><sec id="sec005"><title>Study environments</title><sec id="sec006"><title>Real streets</title><p>Multiple real street intersections on the University of Alabama at Birmingham campus were selected for both road safety evaluations and NLPTS skill training. These are typical urban center cross intersections with different number of lanes. The traffic flow during the day was from 150 to 1300 cars/hour and the duration of the pedestrian phase was between 27 and 74 seconds, because of the differences between arterial and collector roads.</p></sec><sec id="sec007"><title>Virtual streets</title><p>The VR simulator used in this research consisted of a game computer that generate real-time street intersection scenarios, three XVGA projectors (Panasonic PT-LB90NTU), three 241 x 183 cm screens and a 5.1 surrounding sound system (Logitech Z90). For an observe standing 3 m in front of the center screen, the display presented a 168&#x000b0; x 35&#x000b0; field of view (<xref ref-type="fig" rid="pone.0176534.g002">Fig 2</xref>). The hardware of the simulator is quite affordable ($5K-6K). No head or body position sensors were used to update the display. This projection VR allowed direct communication between the COMS and the participant during training. It also minimized the risk of cybersickness that happens often in head-mount display VR. This is important because the majority of individuals with low vision are also old and it is known that this population has balance problems and a higher risk to fall [<xref rid="pone.0176534.ref028" ref-type="bibr">28</xref>&#x02013;<xref rid="pone.0176534.ref030" ref-type="bibr">30</xref>].</p><fig id="pone.0176534.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.g002</object-id><label>Fig 2</label><caption><title>The visual and auditory display components of the VR simulation used in the research.</title></caption><graphic xlink:href="pone.0176534.g002"/></fig><p>Virtual content development platform 3DVIA virTools (Dassault Systems) was used to build 3D models of the street intersections and the dynamic elements such as cars and pedestrians. Four real street intersections around the University of Alabama at Birmingham campus were chosen as the models for constructing virtual intersections. They differed in traffic patterns (one-way and two-way), street configuration (with and without central refuge) and surroundings (with and without overpass). Geographic Information System maps, Google Street images and photos taken by us provided the information to build the buildings around the intersections and the traffic control elements. Video recordings from the streets and vehicle count data were used to characterize the traffic flow.</p><p>Once started, the simulator ran continuously, randomly generating and displaying dynamic elements of the street intersections. Traffic and pedestrian control signal cycles are built to simulate the real street intersections. The movements of cars and pedestrians were synchronized with traffic control signals and obeyed traffic laws. Cars stayed in lanes, blinked turning light and gave way to pedestrians when turning. The default setting simulated mid-day traffic after the morning rush-hour. The lighting was that of clear mid-day without strong oblique light and strong shadows. Only faint shadows could be seen under the cars. The foliage was that of late spring. Each car had an engine noise source under the hood to provide the traffic noise. A white noise was used to simulate street background noise recorded from the streets. There was no interaction between the participant and the simulator display. The participant basically viewed a computer-generated wide-screen movie. <xref ref-type="fig" rid="pone.0176534.g003">Fig 3</xref> shows a virtual street intersection viewed from the location of a participant. A video clip showing two of the virtual street intersections and the real street intersections they were modeled after can be found in <xref ref-type="supplementary-material" rid="pone.0176534.s001">S1 Video</xref>.</p><fig id="pone.0176534.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.g003</object-id><label>Fig 3</label><caption><title>A virtual street intersection viewed from the location of a participant.</title></caption><graphic xlink:href="pone.0176534.g003"/></fig><p>Each virtual street intersection was a 3D model of buildings, streets, cars and pedestrians. The model was viewed by a cluster of three virtual cameras whose configuration matched the physical layout of the three projection screens that the participant viewed. Three virtual microphones at the same location as the virtual camera collected sound signals from the sound field made by the moving cars and fed them to the surrounding sound system that the participant heard. Moving the cameras and microphones to different locations in the intersection model allowed us to configure different street crossing scenarios using computer keyboard and mouse. Once a scenario was set, it was saved to a dropdown list and could be used repeatedly. A typical crossing scenario simulated a person standing at the curb side, facing the crosswalk of the perpendicular street, with one block of the traffic on both sides. The default traffic setting was 30 cars/min and 20 pedestrians/min in the intersection. The COMS could adjust the number of cars and pedestrians independently to facilitate training.</p></sec></sec><sec id="sec008" sec-type="materials|methods"><title>Procedures</title><sec id="sec009"><title>Safety evaluation protocol</title><p>The protocols for the pre- and post-training real street evaluation sessions were identical. The low vision participant was sight-guided to one of the pre-determined real street corners by a COMS and was positioned facing away from the perpendicular street. During the DON&#x02019;T WALK phase, the participant was turned around to face the crosswalk of the perpendicular street and was instructed to say &#x0201c;GO&#x0201d; at the moment he thought was the safest time to cross the street. The participant was then turned to face away from the perpendicular street until the next traffic signal cycle. No walking into the street was allowed. A trial like this was repeated three time at each crossing scenario before the participant was walked to the another evaluation scenario. The participant was evaluated at 4 crossing scenarios in each evaluation session. The experimenters offered no comments, suggestions or feedbacks on the participant&#x02019;s performance during evaluation.</p><p>During each trial, the times of three events were recorded using a stopwatch by the experimenters. 1) The time when the WALK sign was turned on (green circle in <xref ref-type="fig" rid="pone.0176534.g004">Fig 4</xref>). The sighted experimenters could see this event but the low vision participant could not. 2) The time when the rear bumper of the first straight-going car passed the outmost boundary of the crosswalk (magenta circles in <xref ref-type="fig" rid="pone.0176534.g004">Fig 4</xref>). This criterion was used by the sighted experimenters to produce consistent physical measurements of a car entering the center of the intersection. The low vision participant might be able to see a car moving into the intersection, but were not taught and might not have the vision to use this criterion. 3) The time when the participant said GO (cyan circles in <xref ref-type="fig" rid="pone.0176534.g004">Fig 4</xref>). The time interval between the first two times was designated <italic>T</italic><sub><italic>surge</italic></sub>, representing the time taken to establish a true NLPTS. The time interval between the first and the third times was designated <italic>T</italic><sub><italic>go</italic></sub>, representing the time taken for the participant to decide to start crossing the street. The duration of the pedestrian phase of the crossing scenario, <italic>T</italic><sub><italic>WALK</italic></sub>, was also recorded. <xref ref-type="fig" rid="pone.0176534.g004">Fig 4</xref> shows four possible sequences of the three events that have different safety consequences. (A) The participant said GO before the onset of the WALK sign. The time between the GO call and the onset of WALK sign in seconds was recorded as a negative number. The NLPTS event was irrelevant here. This decision was totally unsafe because the perpendicular traffic was running when the WALK sign was not on and the traveler would walk right into it. This was categorized as InRed. (B) The participant said GO after the onset of the WALK sign but before a NLPTS was established. <italic>T</italic><sub><italic>go</italic></sub> was shorter than <italic>T</italic><sub><italic>surge</italic></sub> in this case. Both were recorded as positive numbers in seconds. The safety of this decision was uncertain, depending on whether the surge was made by a turning car or a straight-going car. It was categorized as BeforeSurge. (C) The participant said GO after the onset of WALK sign and shortly after NLPTS was confirmed. The safety level of this decision was high because the traveler had a large portion of the <italic>T</italic><sub><italic>WALK</italic></sub> duration to walk across the street. It was categorized as SafeHigh. (D) The participant said GO after the onset of WALK and after NLPTS was confirmed but was near the end of the pedestrian phase. The safety level of this decision was lower because there was only a small portion of the <italic>T</italic><sub><italic>WALK</italic></sub> duration left and the traveler might not have enough time to walk across the street. It was categorized as SafeLow. Finally, the participant could have said GO after the current pedestrian phase was over and the next DON&#x02019;T WALK phase began, but this had not happened in this research.</p><fig id="pone.0176534.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.g004</object-id><label>Fig 4</label><caption><title>Events and time intervals used to evaluate safety of street crossing timing.</title><p>Four possible combinations of the participant&#x02019;s GO call (cyan circles), the onset of the WALK sign (the green circle) and the time when a near lane parallel traffic surge being confirmed (magenta circles) are shown in a WALK phase delineated by the onset of the WALK sign (WALK On) and the onset of the DON&#x02019;T WALK sign (DON&#x02019;T WALK On). (A-D) illustrate the four possible time sequences of the WALK On, Surge Confirmed and GO events.</p></caption><graphic xlink:href="pone.0176534.g004"/></fig><p>In each evaluation section, two experimenters were making independent measurement of <italic>T</italic><sub><italic>go</italic></sub> and <italic>T</italic><sub><italic>surge</italic></sub>. Because the measurements were mere recording of physical events, the inter-rater agreements were high. The Pearson r&#x02019;s for <italic>T</italic><sub><italic>go</italic></sub> and <italic>T</italic><sub><italic>surge</italic></sub> were 0.992 and 0.922, respectively (p &#x0003c; 0.0005). The mean <italic>T</italic><sub><italic>surge</italic></sub> measures from the two experimenters were 3.94 and 4.21 sec. The mean <italic>T</italic><sub><italic>go</italic></sub> measures were 7.24 and 7.34 sec. Both differences were not significant (p = 0.217 and 0.471).</p></sec><sec id="sec010"><title>Safety score of crossing decision</title><p>While <italic>T</italic><sub><italic>go</italic></sub> was a direct measure of the participant&#x02019;s crossing timing and a shorter <italic>T</italic><sub><italic>go</italic></sub> usually indicated a safer decision, there were difficulties to quantify the safety of the timing using raw <italic>T</italic><sub><italic>go</italic></sub> data. First, whether a <italic>T</italic><sub><italic>go</italic></sub> was safe or not depended on the total duration of the pedestrian phase, <italic>T</italic><sub><italic>WALK</italic></sub>. For example, a 10 seconds <italic>T</italic><sub><italic>go</italic></sub> is probably not quite safe in a crossing scenario where the <italic>T</italic><sub><italic>WALK</italic></sub> is 27 seconds because there are only 17 seconds left for the traveler to walk across the street. However, the same <italic>T</italic><sub><italic>go</italic></sub> becomes much saver in a scenario with a <italic>T</italic><sub><italic>WALK</italic></sub> of 57 seconds because the traveler now has 47 seconds to walk across the street. Second, for the reason explained above, it was difficult to compare <italic>T</italic><sub><italic>go</italic></sub> values obtained from crossing scenarios with different <italic>T</italic><sub><italic>WALK</italic></sub> durations, which varied between 27 and 74 seconds among our scenarios. Third, as explained before, a <italic>T</italic><sub><italic>go</italic></sub> shorter than <italic>T</italic><sub><italic>surge</italic></sub> suggests failing to use the NLPTS correctly and thus may not be safe. For these reasons, the <italic>T</italic><sub><italic>go</italic></sub> data was converted into a safety score (SS) using the following equation:
<disp-formula id="pone.0176534.e001"><alternatives><graphic xlink:href="pone.0176534.e001.jpg" id="pone.0176534.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula></p><p>The SS was a unit-less number between 0 and a value slightly smaller than 1.0. A large SS indicated a safer decision. If <italic>T</italic><sub><italic>go</italic></sub> was recorded as a negative number (<xref ref-type="fig" rid="pone.0176534.g004">Fig 4A</xref>), SS was zero because the participant said GO during the DON&#x02019;T WALK period and was thus totally unsafe. If <italic>T</italic><sub><italic>go</italic></sub> was shorter than <italic>T</italic><sub><italic>surge</italic></sub> (<xref ref-type="fig" rid="pone.0176534.g004">Fig 4B</xref>), the safety of the participant&#x02019;s decision was uncertain. He could be safe if the surge he detected was caused by the straight-going car but he could be unsafe if the surge was caused by a turning car. A SS of 0.5 was given to such a decision. If <italic>T</italic><sub><italic>go</italic></sub> was longer than <italic>T</italic><sub><italic>surge</italic></sub> (<xref ref-type="fig" rid="pone.0176534.g004">Fig 4C and 4D</xref>), the crossing decision was made within the pedestrian phase, and the SS value was the proportion of the <italic>T</italic><sub><italic>WALK</italic></sub> duration left after the participant said GO. Notice that SS decreased linearly with increasing length of <italic>T</italic><sub><italic>go</italic></sub> in this case. This was because the longer was the <italic>T</italic><sub><italic>go</italic></sub> delay, the shorter time left in the pedestrian phase for the participant to walk across the street. Also notice that the SS could not reach 1.0 because a low vision participant could not see the onset of the WALK sign and immediately give a correct GO call. Instead he had to wait for a NLPTS to be established. Although he could not use the entire <italic>T</italic><sub><italic>WALK</italic></sub>, he could make use of a very substantial portion of it, if he was proficient with the NLPTS skill.</p></sec><sec id="sec011"><title>NLPTS skill training protocols for virtual and real street</title><p>After the pre-training evaluation, the participant of the virtual street training group was taught the NLPTS skills in virtual streets by a COMS. The lights of the simulator room were turned off so that the projected image on the screens provided the only lighting. This greatly increased the contrast of the visual display. When the COMS needed to change the crossing scenario, the participant was asked to close his eyes to prevent him from getting sick by the fast whole field movement. The participant was allowed to use the eye wear and hearing aids that they routinely use and was allowed to hold his white cane if that made him feeling more comfortable.</p><p>The participant was asked to stand in the center of the simulator, approximately three meters in front of the center screen. A chair was placed in front of the participant, the back of which was set parallel to the perpendicular street of the training session. The participant was asked to hold the back of the chair to orient himself, to provide some support and to resume the correct orientation after taking a break. The participant was provided with another chair to sit in during a break.</p><p>Using the traffic flow in the virtual streets, the COMS explained the concepts involved in NLPTS, the actions to be taken and the cautions to be exercised. She also provided instructions on how to use visual and auditory information to identify the elements of NLPTS and to detect one correctly. The following is a sample script that the COMS used to test Joe&#x02019;s understanding of the concepts and to give feedbacks.</p><p><bold>COMS:</bold> &#x0201c;Joe, please tell me where the parallel traffic lanes are.&#x0201d;</p><p><bold>COMS:</bold> &#x0201c;Joe, please tell me where nearest lane is.&#x0201d;</p><p><bold>COMS:</bold> &#x0201c;Joe, when the cars have a red light, where will they stop in this lane?&#x0201d;</p><p><bold>COMS:</bold> &#x0201c;Joe, notice the cars going left and right have stopped.&#x0201d;</p><p><bold>COMS:</bold> &#x0201c;Joe, can you tell me where the cars are waiting for the green light?&#x0201d;</p><p><bold>COMS:</bold> &#x0201c;Joe, can you tell me when those cars drive through the intersection?&#x0201d;</p><p><bold>COMS:</bold> &#x0201c;Good, that is when you cross the street. Say &#x02018;Go&#x02019;&#x0201d;</p><p>The COMS repeated these steps until the participant made correct responses and then switched to a different virtual street for practice. The training terminated when the COMS was certain that the participant had mastered the NLPTS skills. The length of the training depended on the severity of the participant&#x02019;s sensory impairment, his ability to communicate with the COMS and his ability to follow instructions. The maximum training time in the VR for a participant was 30 minutes.</p><p>On the following visit to study site, usually the next day, the participant was given a review of the skills taught for a maximum of 15 minutes before he/she was given the post-training evaluation.</p><p>The same COMS used the same protocol to teach NLPTS skills in real streets. The participants of the RST group had to be sight-guided by the COMS to different training scenarios to ensure safety. No chairs provided for keeping orientation and rest.</p></sec></sec><sec id="sec012"><title>Participants</title><p>Low vision participants were recruited from the Center for Low Vision Rehabilitation of University of Alabama at Birmingham, Alabama Department of Rehabilitation Services and Alabama Institute for Deaf and Blind. The inclusion criteria included 19 years or older, having severe to profound vision loss that made it difficult to find or to use pedestrian signals reliably, having a demonstrated need for O&#x00026;M services but had not previously received training in using the NLPTS, physically strong enough to walk for a few street blocks and having sufficient mental and communication capability to understand and follow instructions in spoken English. After the initial screening, a potential participant was taken to real street corners to verify inability to use the pedestrian signals. Those who were unable to correctly report the status of the pedestrian signal across the street in 4 street intersections were enrolled into the research. The research was approved by the Institutional Review Board of the University of Alabama at Birmingham, and was conducted in accordance with the Declaration of Helsinki. A written informed consent was obtained from each participant before any testing and training was conducted.</p></sec><sec id="sec013"><title>Data analysis</title><p>The <italic>T</italic><sub><italic>surge</italic></sub> and <italic>T</italic><sub><italic>go</italic></sub> data collected from the streets were entered into an Excel data sheet that automatically computed SS according to the <italic>T</italic><sub><italic>WALK</italic></sub> of the scenarios. A sample data sheet is shown in <xref ref-type="table" rid="pone.0176534.t001">Table 1</xref>. A repeated ANOVA with TRAINING (pre vs. post) as the within-subject variable and GROUP (virtual vs. real street training) as the between-subjects variable was performed on the SS to assess transferability of VR training.</p><table-wrap id="pone.0176534.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.t001</object-id><label>Table 1</label><caption><title>Sample pre- and post-training real street evaluation data sheets.</title></caption><alternatives><graphic id="pone.0176534.t001g" xlink:href="pone.0176534.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" colspan="5" rowspan="1">Pre-Training Evaluation</th><th align="center" colspan="5" rowspan="1">Post-Training Evaluation</th></tr><tr><th align="center" rowspan="1" colspan="1">Scenario <italic>(T</italic><sub><italic>WALK</italic></sub><italic>)</italic></th><th align="center" rowspan="1" colspan="1">Near Lane</th><th align="center" rowspan="1" colspan="1"><italic>T</italic><sub><italic>go</italic></sub></th><th align="center" rowspan="1" colspan="1"><italic>T</italic><sub><italic>surge</italic></sub></th><th align="center" rowspan="1" colspan="1">Safety Score</th><th align="center" rowspan="1" colspan="1">Scenario <italic>(T</italic><sub><italic>WALK</italic></sub><italic>)</italic></th><th align="center" rowspan="1" colspan="1">Near Lane</th><th align="center" rowspan="1" colspan="1"><italic>T</italic><sub><italic>go</italic></sub></th><th align="center" rowspan="1" colspan="1"><italic>T</italic><sub><italic>surge</italic></sub></th><th align="center" rowspan="1" colspan="1">Safety Score</th></tr></thead><tbody><tr><td align="center" rowspan="3" colspan="1"><bold>1 (<italic>27sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>Behind</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">3.32</td><td align="center" rowspan="1" colspan="1">4.92</td><td align="center" rowspan="1" colspan="1">0.5</td><td align="center" rowspan="3" colspan="1"><bold>1 (<italic>74sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>Behind</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">3.07</td><td align="char" char="." rowspan="1" colspan="1">2.47</td><td align="char" char="." rowspan="1" colspan="1">0.946</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">-1.33</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="char" char="." rowspan="1" colspan="1">5.33</td><td align="char" char="." rowspan="1" colspan="1">3.09</td><td align="char" char="." rowspan="1" colspan="1">0.906</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">4.89</td><td align="center" rowspan="1" colspan="1">3.47</td><td align="center" rowspan="1" colspan="1">0.819</td><td align="char" char="." rowspan="1" colspan="1">8.4</td><td align="char" char="." rowspan="1" colspan="1">6.87</td><td align="char" char="." rowspan="1" colspan="1">0.853</td></tr><tr><td align="center" rowspan="3" colspan="1"><bold>2 (<italic>74sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>In Front</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">-19.29</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="3" colspan="1"><bold>2 (<italic>27sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>In Front</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">4.95</td><td align="char" char="." rowspan="1" colspan="1">4.56</td><td align="char" char="." rowspan="1" colspan="1">0.913</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">-19.87</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="char" char="." rowspan="1" colspan="1">4.23</td><td align="char" char="." rowspan="1" colspan="1">3.64</td><td align="char" char="." rowspan="1" colspan="1">0.926</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">-18.11</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="char" char="." rowspan="1" colspan="1">5.38</td><td align="char" char="." rowspan="1" colspan="1">4.53</td><td align="char" char="." rowspan="1" colspan="1">0.906</td></tr><tr><td align="center" rowspan="3" colspan="1"><bold>3 (<italic>54sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>In Front</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">-6.49</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="3" colspan="1"><bold>3 (4<italic>6sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>In Front</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">4.15</td><td align="char" char="." rowspan="1" colspan="1">3.38</td><td align="char" char="." rowspan="1" colspan="1">0.91</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">-28.38</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="char" char="." rowspan="1" colspan="1">3.71</td><td align="char" char="." rowspan="1" colspan="1">3.07</td><td align="char" char="." rowspan="1" colspan="1">0.919</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">-33.22</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="char" char="." rowspan="1" colspan="1">6.13</td><td align="char" char="." rowspan="1" colspan="1">3.65</td><td align="char" char="." rowspan="1" colspan="1">0.866</td></tr><tr><td align="center" rowspan="3" colspan="1"><bold>4 (<italic>54sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>Behind</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">-4.71</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="center" rowspan="3" colspan="1"><bold>4 (<italic>46sec)</italic></bold></td><td align="center" rowspan="3" colspan="1"><bold><italic>Behind</italic></bold></td><td align="char" char="." rowspan="1" colspan="1">3.45</td><td align="char" char="." rowspan="1" colspan="1">2.51</td><td align="char" char="." rowspan="1" colspan="1">0.925</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">4.97</td><td align="center" rowspan="1" colspan="1">2.53</td><td align="center" rowspan="1" colspan="1">0.908</td><td align="char" char="." rowspan="1" colspan="1">5.04</td><td align="char" char="." rowspan="1" colspan="1">3.13</td><td align="char" char="." rowspan="1" colspan="1">0.89</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">-19.21</td><td align="center" rowspan="1" colspan="1">NA</td><td align="center" rowspan="1" colspan="1">0</td><td align="char" char="." rowspan="1" colspan="1">4.43</td><td align="char" char="." rowspan="1" colspan="1">2.61</td><td align="char" char="." rowspan="1" colspan="1">0.904</td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>Mean</bold></td><td align="center" colspan="3" rowspan="1"/><td align="center" rowspan="1" colspan="1"><bold>0.186</bold></td><td align="center" rowspan="1" colspan="1"><bold>Mean</bold></td><td align="center" colspan="3" rowspan="1"/><td align="char" char="." rowspan="1" colspan="1"><bold>0.905</bold></td></tr><tr><td align="center" rowspan="1" colspan="1"><bold>SD</bold></td><td align="center" colspan="3" rowspan="1"/><td align="center" rowspan="1" colspan="1"><bold>0.348</bold></td><td align="center" rowspan="1" colspan="1"><bold>SD</bold></td><td align="center" colspan="3" rowspan="1"/><td align="char" char="." rowspan="1" colspan="1"><bold>0.026</bold></td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t001fn001"><p>Participant #8: 55 years old; Male; African-American; Group assignment: VST</p></fn><fn id="t001fn002"><p>Scenario: Real street crossing scenarios used for pre- and post-training safety evaluations. The durations of the pedestrian phase of the scenarios in seconds (<italic>T</italic><sub><italic>WALK</italic></sub>) are include in parentheses.</p></fn><fn id="t001fn003"><p>Near Lane: <italic>Behind</italic> means the near lane traffic is from behind (over the left shoulder). <italic>In Front</italic> means the near lane traffic is from front (across the perpendicular street).</p></fn><fn id="t001fn004"><p><italic>T</italic><sub><italic>surge</italic></sub>: NA means not available, happened when the participant said GO before the onset of the WALK sign.</p></fn></table-wrap-foot></table-wrap></sec></sec><sec sec-type="results" id="sec014"><title>Results</title><sec id="sec015"><title>Participants</title><p>Twelve qualified low vision participants were enrolled into the research (<xref ref-type="table" rid="pone.0176534.t002">Table 2</xref>). The ages of the participants ranged from 19 to 69 and the durations of visual impairment ranged from 0.5 to 36 years. The causes of visual impairment included retinitis pigmentosa, diabetic retinopathy, degenerative myopia, optic atrophy, Best disease, Usher&#x02019;s disease and non-arteritic anterior ischemic optic neuropathy. One participant had mild hearing impairment. Another participant had severe hearing aid but wore habitual hearing aid. All participants were unable to use pedestrian signals reliably and all did not know the NLPTS skill prior to training, as determined in real streets.</p><table-wrap id="pone.0176534.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.t002</object-id><label>Table 2</label><caption><title>Description of the 12 low vision participants.</title></caption><alternatives><graphic id="pone.0176534.t002g" xlink:href="pone.0176534.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">ID</th><th align="center" rowspan="1" colspan="1">Group</th><th align="center" rowspan="1" colspan="1">Age</th><th align="center" rowspan="1" colspan="1">Diagnoses</th><th align="center" rowspan="1" colspan="1">Sex</th><th align="center" rowspan="1" colspan="1">Race</th><th align="center" rowspan="1" colspan="1">Years DX</th><th align="center" rowspan="1" colspan="1">Visual Acuity</th></tr></thead><tbody><tr><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">1</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">VST</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">69</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">Diabetic Retinopathy</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">F</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">AA</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">5</td><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OD: HM</td></tr><tr><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OS: 20/607</td></tr><tr><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">VST</td><td align="center" rowspan="1" colspan="1">19</td><td align="center" rowspan="1" colspan="1">Retinitis Pigmentosa</td><td align="center" rowspan="1" colspan="1">M</td><td align="center" rowspan="1" colspan="1">W</td><td align="center" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">OU: 20/3000</td></tr><tr><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">3</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">RST</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">57</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">Best Disease</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">F</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">W</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">2</td><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OD: 20/320</td></tr><tr><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OS: 20/800</td></tr><tr><td align="center" rowspan="1" colspan="1">4</td><td align="center" rowspan="1" colspan="1">VST</td><td align="center" rowspan="1" colspan="1">51</td><td align="center" rowspan="1" colspan="1">Retinitis Pigmentosa</td><td align="center" rowspan="1" colspan="1">F</td><td align="center" rowspan="1" colspan="1">W</td><td align="center" rowspan="1" colspan="1">36</td><td align="left" rowspan="1" colspan="1">OU: 20/600</td></tr><tr><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">5</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">VST</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">26</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">Diabetic Retinopathy</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">F</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">AA</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">2</td><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OD: 20/697</td></tr><tr><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OS: LP</td></tr><tr><td align="center" rowspan="2" colspan="1">6</td><td align="center" rowspan="2" colspan="1">RST</td><td align="center" rowspan="2" colspan="1">35</td><td align="center" rowspan="2" colspan="1">Optic Atrophy</td><td align="center" rowspan="2" colspan="1">F</td><td align="center" rowspan="2" colspan="1">AA</td><td align="center" rowspan="2" colspan="1">20</td><td align="left" rowspan="1" colspan="1">OD: 20/533</td></tr><tr><td align="left" rowspan="1" colspan="1">OS: 20/1000</td></tr><tr><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">7</td><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">VST</td><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">47</td><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">Ushers &#x00026; Retinitis Pigmentosa</td><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">M</td><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">W</td><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">24</td><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OU: 20/800</td></tr><tr><td align="center" rowspan="2" colspan="1">8</td><td align="center" rowspan="2" colspan="1">VST</td><td align="center" rowspan="2" colspan="1">55</td><td align="center" rowspan="2" colspan="1">Degenerative Myopia</td><td align="center" rowspan="2" colspan="1">M</td><td align="center" rowspan="2" colspan="1">AA</td><td align="center" rowspan="2" colspan="1">35</td><td align="left" rowspan="1" colspan="1">OD: 20/1928</td></tr><tr><td align="left" rowspan="1" colspan="1">OS: 20/1162</td></tr><tr><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">9</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">RST</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">52</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">Myopic Choroidal Degeneration</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">F</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">AA</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">15</td><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OD: 20/1754</td></tr><tr><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OS: 20/400</td></tr><tr><td align="center" rowspan="2" colspan="1">10</td><td align="center" rowspan="2" colspan="1">RST</td><td align="center" rowspan="2" colspan="1">48</td><td align="center" rowspan="2" colspan="1">Retinitis Pigmentosa</td><td align="center" rowspan="2" colspan="1">F</td><td align="center" rowspan="2" colspan="1">AA</td><td align="center" rowspan="2" colspan="1">31</td><td align="left" rowspan="1" colspan="1">OD: LP</td></tr><tr><td align="left" rowspan="1" colspan="1">OS: 20/2847</td></tr><tr><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">11</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">VST</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">54</td><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">OD: NAION</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">F</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">W</td><td align="center" rowspan="2" style="background-color:#F2F2F2" colspan="1">2</td><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OD: 20/877</td></tr><tr><td align="center" style="background-color:#F2F2F2" rowspan="1" colspan="1">OS: Trauma</td><td align="left" style="background-color:#F2F2F2" rowspan="1" colspan="1">OS: LP</td></tr><tr><td align="center" rowspan="2" colspan="1">12</td><td align="center" rowspan="2" colspan="1">VST</td><td align="center" rowspan="2" colspan="1">36</td><td align="center" rowspan="2" colspan="1">Diabetic Retinopathy</td><td align="center" rowspan="2" colspan="1">F</td><td align="center" rowspan="2" colspan="1">W</td><td align="center" rowspan="2" colspan="1">5</td><td align="left" rowspan="1" colspan="1">OD: 20/439</td></tr><tr><td align="left" rowspan="1" colspan="1">OS: NLP</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="t002fn001"><p>Group: Training group assignment (VST: virtual street training; RST: real street training)</p></fn><fn id="t002fn002"><p>Years DX: Years since diagnosis</p></fn><fn id="t002fn003"><p>NAION: Non-Arteritic Anterior Ischemic Optic Neuropathy</p></fn><fn id="t002fn004"><p>AA: African-American; W: White</p></fn><fn id="t002fn005"><p>OD: Right eye; OS: Left eye</p></fn><fn id="t002fn006"><p>HM: Hand motion; LP: Light perception; NLP: No light perception</p></fn></table-wrap-foot></table-wrap><p>The mean ages of the real and virtual street training groups were 48&#x000b1;9.4 and 45&#x000b1;16.5, respectively. The age difference was not significant (t<sub>10</sub> = 0.45, p = 0.331).</p></sec><sec id="sec016"><title>Crossing decision safety</title><p><xref ref-type="fig" rid="pone.0176534.g005">Fig 5</xref> shows the proportions of the GO calls falling into the InRed, BeforeSurge, SafeLow and SafeHigh categories obtained from the pre-training (red bars) and post-training (green bars) evaluations for the virtual street training (A) and the real street training (B) groups. According to <xref ref-type="disp-formula" rid="pone.0176534.e001">Eq 1</xref>, the safety score was 0 for the InRed category and 0.5 for the BeforeSurge category. The SafeLow category was defined as the participant saying &#x0201c;GO&#x0201d; in the latter half of the WALK phase (low safety, SS &#x0003c; 0.5, might not have enough time to walk across the street) and the SafeHigh category was defined as the participant saying &#x0201c;GO&#x0201d; shortly after confirming the NLPTS, in the first half of the WALK phase (high safety, SS &#x0003e; 0.5, had more time to walk across).</p><fig id="pone.0176534.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.g005</object-id><label>Fig 5</label><caption><title>Pre- and Post-training safety categorization distributions for the virtual street and real street training groups.</title><p>(A) Proportion of the GO calls falling into the InRed, BeforeSurge, SafeLow and SafeHigh categories obtained from the pre- (red bars) and post-training (green bars) evaluations for the virtual street training (VST) group. (B) Same distribution plots for the real street training (RST) group.</p></caption><graphic xlink:href="pone.0176534.g005"/></fig><p>Prior to the NLPTS training, all participants showed highly unsafe behaviors in the pre-training real street evaluation. On average, the 8 participants of the virtual street training group said &#x0201c;GO&#x0201d; during the DON&#x02019;T WALK phase 64% of the time (red &#x0201c;InRed&#x0201d; bar in <xref ref-type="fig" rid="pone.0176534.g005">Fig 5A</xref>) and said &#x0201c;GO&#x0201d; before a NLPTS was confirmed 17.0% of the time (red &#x0201c;BeforeSurge bar). They said &#x0201c;GO&#x0201d; in the latter half of the pedestrian phase 2% of the time (red SafeLow bar). In only 17% of the time their timing offered them more than 50% of the pedestrian phase for crossing (red SafeHigh bar). The primary cause of their unsafe timing appeared to be using the &#x0201c;gaps&#x0201d; in the perpendicular traffic (temporary absence of moving cars near the intersection on the perpendicular street). The participants also tried to use other tactics, such as observing other pedestrians (not advised). Similar distribution was found in the real street training group (red bars in <xref ref-type="fig" rid="pone.0176534.g005">Fig 5B</xref>). There was no significant difference in safety scores in pre-training evaluation between the two groups (F<sub>1,10</sub> = 1.921, p = 0.196).</p><p>After the NLPTS training, the safety of all participants&#x02019; crossing decision timing was greatly improved. During post-training evaluations on real streets, participants in the virtual street training group said &#x0201c;GO&#x0201d; 3.0% of the time during the DON&#x02019;T WALK phase, 4% of the time before surge and 1% of the time too late in the pedestrian phase (green &#x0201c;InRed&#x0201d;, &#x0201c;BeforeSurge&#x0201d; and &#x0201c;SafeLow&#x0201d; bars in <xref ref-type="fig" rid="pone.0176534.g005">Fig 5A</xref>). They said &#x0201c;GO&#x0201d; a few seconds after the surge was confirmed in 92% of the time, which allowed them an average 88.9% of the pedestrian phase for crossing. A similar pattern was also observed in the RST group (green bars in <xref ref-type="fig" rid="pone.0176534.g005">Fig 5B</xref>). There was no significant difference in safety scores in post-training evaluation between the two groups of participants (F<sub>1,10</sub> = 0.006, p = 0.939). These results demonstrated a drastic shift of crossing decision safety from very unsafe to very safe after both virtual street and real street NLPTS skill training.</p></sec><sec id="sec017"><title>Transferability of VR O&#x00026;M training</title><p>Prior to training, the mean SS of the virtual and real street training groups were 0.20&#x000b1;0.11 and 0.31&#x000b1;0.15, respectively. The difference was not significant (F<sub>1,10</sub> = 1.921, p = 0.196). After training, the mean SS of virtual and real street training groups were 0.821&#x000b1;0.083 and 0.817&#x000b1;0.090, respectively. The difference was not significant (F<sub>1,10</sub> = 0.006, p = 0.939).</p><p>On average, the time needed to physically confirm a NLPTS (first straight-going car entering the center of the intersection), <italic>T</italic><sub><italic>surge</italic></sub>, was 3.65&#x000b1;1.68 seconds, which was 8.46% of the total pedestrian phase duration (<italic>T</italic><sub><italic>WALK</italic></sub>). After training, the participants said GO 5.43&#x000b1;2.95 seconds after a NLPTS was confirmed, which was 12.5% of <italic>T</italic><sub><italic>WALK</italic></sub>. These left the participants more than 80% of the <italic>T</italic><sub><italic>WALK</italic></sub> duration to cross the street.</p><p>A repeated measures ANOVA on the safety score was used assess the effectiveness of VR training (<xref ref-type="fig" rid="pone.0176534.g006">Fig 6</xref>). There was a highly significant TRAINING effect (F<sub>1,10</sub> = 288.3, p&#x0003c;0.0005). The GROUP effect was not significant (F<sub>1,10</sub> = 0.821, p = 0.386), and the TRAINING*GROUP interaction was not significant (F<sub>1,10</sub> = 2.80, p = 0.125). This analysis demonstrated that both the virtual and real street training groups made safer real street crossing decisions after the NLPTS training and that there was no difference between the groups in terms of training benefits. In other words, low vision participants could transfer their VR-acquired skills to real streets and virtual street training was as efficient as real street training.</p><fig id="pone.0176534.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0176534.g006</object-id><label>Fig 6</label><caption><title>Pre- and post-training safety scores of the virtual street training (VST) and real street training (RST) groups.</title></caption><graphic xlink:href="pone.0176534.g006"/></fig></sec></sec><sec sec-type="conclusions" id="sec018"><title>Discussion</title><p>In this research, we demonstrate a strong positive transfer of VR-trained skill to real streets. Low vision participants, whose vision was too poor to use pedestrian signs, learned a set O&#x00026;M skill in virtual streets and successfully used them to improve their street crossing timing in real streets (<xref ref-type="fig" rid="pone.0176534.g005">Fig 5</xref>). The training effect obtained from the virtual street training was comparable to that from the real street training (<xref ref-type="fig" rid="pone.0176534.g006">Fig 6</xref>). The high rate of positive transfer of VR-trained NLPTS skills to real streets also demonstrated that our low-cost simulator was adequate in training these O&#x00026;M skills. As far as we know, this is the first time a positive transfer of VR-trained O&#x00026;M skills being demonstrated in individuals with impaired but usable vision.</p><p>After training, a few participants volunteered some comments about the VR training. They appeared to like the virtual streets. &#x0201c;I liked that I was standing on a street corner and no cars would hit me.&#x0201d; &#x0201c;It kinda made me aware of my surroundings in all four directions. &#x02026; I really thought the simulator was good.&#x0201d; The participants considered the VR training they received helpful. &#x0201c;It (VR) showed me something that I was not aware of. It prepared me for the outside.&#x0201d; &#x0201c;It (VR) encouraged me to learn more travel skills. I know I can travel independently. I felt the VR simulator was helpful.&#x0201d; Our participants also pointed out some shortcomings. &#x0201c;It made me nervous to learn something new. It was a crazy intersection.&#x0201d; Future development of clinical deployable VR applications should include subjective usability as an outcome measure.</p><sec id="sec019"><title>NLPTS and traveler&#x02019;s ability to cross the street</title><p>The safety score (SS) was a measure of how promptly a traveler started crossing after the pedestrian sign turned to WALK. Does a good SS (close to 1.0) guarantee the traveler a safe crossing? After the NLPTS training, our participants made &#x0201c;GO&#x0201d; calls at an average of 5.55&#x000b1;3.02 sec after the onset of the WALK sign, indicating that they possessed good skills to start crossing early, within the 7-sec WALK period. However, whether they could make to the far side of the street depended whether they could walk 3.5 feet/sec or faster [<xref rid="pone.0176534.ref027" ref-type="bibr">27</xref>]. Three of our participants told us how fast they thought they could walk and their reasoning. &#x0201c;I take my time so I can feel the surface so I don&#x02019;t fall. About 2 feet per second.&#x0201d; (participant #4) &#x0201c;If it&#x02019;s well light I&#x02019;m a fast walker. Five feet per second, I walk fast.&#x0201d; (participant #6) &#x0201c;I&#x02019;m tall and I have really long legs, but I move slow because I don&#x02019;t want to run into a pole or fall or anything. If I know where I&#x02019;m going, maybe 3 feet per second.&#x0201d; (participant #8) Despite the large differences among their perceived walking speeds, an ANOVA showed no significant differences among their safety scores (F<sub>2,33</sub> = 0.572, p = 0.570). This should not be surprising because the NLPTS training they received concerned only about detecting the onset of the WALK sign (using NLPTS as a surrogate). If they learned the skills well, their SS should not be affected by their perceived walking speed. However, if the participants&#x02019; perceived walking speeds turned out to be equal to their actual walking speeds, then participant #4 (2 feet/sec) would not be safe to cross any streets. Participant #6 (3 feet/sec) might be able to reach the far side of some streets and participant #8 (5 feet/sec) was likely to make safe crossing on any streets. Does this large variation in participants&#x02019; ability to cross streets negate the usefulness of NLPTS training? Not at all. NLPTS only teaches the traveler how to start crossing early, and as shown by our results, this training was successful. To achieve safe crossing of signal controlled streets, the traveler also need to learn how to walk fast with impaired vision and how to judge whether a street is crossable, but these are totally different skills from the NLPTS.</p></sec><sec id="sec020"><title>VR training in O&#x00026;M rehabilitation</title><p>The role of VR training in O&#x00026;M rehabilitation is to prepare low vision trainees in a safe and efficient training environment. Like in many other fields of skill acquisition, such as pilot and surgical trainings, where errors in real world incur tremendous human and economic costs, VR is not meant to replace practicing flying a real plane, operating on a real patient or navigating real streets but it can teach many necessary knowledge and skills with minimal risk, less cost and higher efficiency so that the trainees can apply the knowledge and skills in real world tasks instead of learning everything from scratch in the real world. Our study has shown that VR-learned O&#x00026;M skills can indeed be applied to real world.</p><p>There are many skills in the O&#x00026;M curriculum that can be trained efficiently in VR, but there are others, such as white cane use and veering correction, that are impossible or difficult to train in an affordable VR, and thus have to be trained in real streets. A well-thought-out O&#x00026;M curriculum that integrate VR training with real street training may bring welcome changes to the O&#x00026;M profession in several ways. First, in the safe virtual street, the safeguarding role of the COMS is greatly reduced. If future development of VR-based courseware can also take up of the teaching role, the scarce and costly COMS resource can be focused more on trainings that have to be done on real street, and thus improves the accessibility and affordability of O&#x00026;M rehabilitation. Second, the man-made safe VR is a perfect platform to implement some the proven learning theories. Structured courseware with stepwise progression of task difficulty and scenario complexity, objective performance metrics, proximal feedback, repeated practice and learning from mistakes can be developed to make training more efficient. Third, the VR technology makes it possible to provide O&#x00026;M rehabilitation through networked satellite stations so that low vision trainees can learn O&#x00026;M skills at their convenient times and locations.</p></sec><sec id="sec021"><title>Limitations</title><p>Several limitations of the current research were noticed.</p><p>The research was designed to compared the new, virtual street O&#x00026;M training and the standard of care, real street training. If virtual street training could result in a comparable gain in real street safety, as compared to the standard of care, then the ground is laid for future exploration of VR technology&#x02019;s potentials in improving the accessibility, affordability and efficiency of O&#x00026;M rehabilitation. One limitation of this design is the lack of a non-treatment control arm, which can serve to control the effect of the initial evaluation. Our decision to use the limited number of available low vision participants in a standard of care control group instead of a non-treatment control was based on the following considerations. 1) The NLPTS skills were the most frequently taught skills to low vision patients, indicating that these skills might not be easily picked up spontaneously. Normally-sighted individuals use more direct and more salient signals, the traffic signal change, to guide their decision and would not routinely use a surrogate, such as the NLPTS. 2) The participants received absolutely no instruction or hint on how to determine the timing of crossing. They were given no feedbacks as to how well they did either. While the exposure to the street crossing task might affect the participants in a positive way, such as wakening some past street crossing experience, it was unlikely that the effect would result in systematic improvement of crossing behavior, because the basis for their previous street navigation experiences, good vision, was not available. 3) The fact that our participants, who had severely impaired vision for years, even decades, before enrolling into the study, did very poorly in the pre-training evaluation suggested also suggested that the specific training of crossing skills, not just exposures, was needed to make safe crossing decisions. In hindsight, the very short time intervals between the confirmation of NLPTS and the participants&#x02019; GO calls observed in the post-training evaluation suggested that it was the NLPTS skills that the participants learned during training was used in their crossing decision making. Nevertheless, a non-treatment control would have provided unequivocal evidence about the effect of pre-training evaluation, and future studies should take this into consideration.</p><p>As discussed above, our simulator simulated only a subset of the sensory cues of the real world and some of our experimental conditions, for example, turning off the lights in the VR room and providing a chair for assistance, did not match those of the real streets. These need to be taken into consideration when evaluating the study results. For example, we don&#x02019;t know whether an ambient light level that matches that of the real street has an effect on training outcome. One important issue in the future development of VR-based O&#x00026;M training protocols is the trade-off between high simulation fidelity (higher cost) and training efficiency.</p><p>This study included only adult participants. There are greater physical, psychological and socioeconomic benefits to conduct O&#x00026;M training in children and young adults with early-onset visual impairments. However, the lack of prior knowledge of road safety rules, immature cognitive development and the short attention span of this population can pose special challenges to O&#x00026;M training. The VR training protocol we used may not be adequate for this population.</p><p>The NLPTS skills were taught by a COMS in both the virtual and real streets. There was no interaction between the simulator and student. The COMS had to collect student responses to questions and instructions to gauge the student&#x02019;s proficiency in using the skills. This was because the purpose of the research was to determine the feasibility of VR as a sensory platform for O&#x00026;M training. Now that the feasibility has been established, the future research and development will be focus on adding sensors to collect the student&#x02019;s responses and develop interactive, game-like training routines to automate the instruction and practice process and to minimize the intervention of the COMS.</p><p>Participants in this study had severe to profound visual impairment. Although our simulation appeared to be adequate for training this sample of participants, it may be inadequate for others. Individuals with better vision may require higher visual display quality from the simulator. Individuals with poorer vision may require better auditory display than the 5.1 surround sound used in this study. Matching display quality, skills taught and patient&#x02019;s visual capacity will be the first issues to consider to design future VR training protocols.</p><p>Our post-training evaluation was conducted a few days after VR training. It thus reflected short-term training gain. Due to the logistical difficulties in low vision participant retention, the long-term retainment of the VR-trained skills was not evaluated. Although real street trained O&#x00026;M skills usually retain well unless there is a significant decay of vision, long-term retainment of VR-trained skills is unknown. This aspect of VR training should be considered in future studies.</p></sec></sec><sec sec-type="conclusions" id="sec022"><title>Conclusions</title><p>This research study demonstrated that O&#x00026;M skills learned in virtual streets can be used to improve real streets safety in individuals with low vision. Moreover, the training benefit from virtual streets can be as large as that from real street. Strong evidence thus exists that a computer generated virtual environment can be a viable platform for low vision O&#x00026;M training.</p></sec><sec sec-type="supplementary-material" id="sec023"><title>Supporting information</title><supplementary-material content-type="local-data" id="pone.0176534.s001"><label>S1 Video</label><caption><title>Video of virtual and real street intersections.</title><p>(MP4)</p></caption><media xlink:href="pone.0176534.s001.mp4"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0176534.s002"><label>S1 Table</label><caption><title>Data from all participants.</title><p>(DOCX)</p></caption><media xlink:href="pone.0176534.s002.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>We thank Jean-Marc Gauthier of tinkering.net for building the virtual reality simulator and developing the virtual street intersections, Abidin Yildirim of the Vision Science Research Center of the University of Alabama at Birmingham for building the projection system, Dawn DeCarlo of the Center for Low Vision Rehabilitation the University of Alabama at Birmingham, Jennifer Stephens and Donna Scott of Alabama Institute for the Deaf and Blind, and the Vocational Rehabilitation Counselors of Alabama Department of Rehabilitation service for referring low vision participants. We thank Eugene Bourquin of the Helen Keller National Center for his insight in traffic control and O&#x00026;M and Patti Fuhr of VA Southeastern Blind Rehabilitation for her advice on low vision rehabilitation.</p></ack><glossary><title>Abbreviations</title><def-list><def-item><term>COMS</term><def><p>certified O&#x00026;M specialist</p></def></def-item><def-item><term>NLPTS</term><def><p>near lane parallel traffic surge</p></def></def-item><def-item><term>O&#x00026;M</term><def><p>Orientation and Mobility</p></def></def-item><def-item><term>SS</term><def><p>safety score</p></def></def-item><def-item><term>VR</term><def><p>virtual reality</p></def></def-item></def-list></glossary><ref-list><title>References</title><ref id="pone.0176534.ref001"><label>1</label><mixed-citation publication-type="other">National Eye Institute. Blindness Bethesda, MD, 2010 [cited 2016 11/20]. <ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/eyedata/blind">https://nei.nih.gov/eyedata/blind</ext-link>.</mixed-citation></ref><ref id="pone.0176534.ref002"><label>2</label><mixed-citation publication-type="other">National Eye Institute. Low Vision Bethesda, MD, 2010 [cited 2016 11/20]. <ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/eyedata/lowvision">https://nei.nih.gov/eyedata/lowvision</ext-link>.</mixed-citation></ref><ref id="pone.0176534.ref003"><label>3</label><mixed-citation publication-type="book"><collab>Alliance for Aging Research</collab>. <source>The Silver Book: Vision Loss Volume II</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Alliance for Aging Research</publisher-name>; <year>2012</year>.</mixed-citation></ref><ref id="pone.0176534.ref004"><label>4</label><mixed-citation publication-type="book"><name><surname>Maureen</surname><given-names>A</given-names></name>, <name><surname>Duffy</surname><given-names>MS</given-names></name>. <source>Low Vision and Legal Blindness Terms and Descriptions</source>
<publisher-loc>New York</publisher-loc>: <publisher-name>American Foundation for the Blind</publisher-name>
<year>2016</year> [cited 2016 11/21]. <ext-link ext-link-type="uri" xlink:href="http://www.visionaware.org/info/your-eye-condition/eye-health/low-vision/low-vision-terms-and-descriptions/1235#LightPerception_and_LightProjection">http://www.visionaware.org/info/your-eye-condition/eye-health/low-vision/low-vision-terms-and-descriptions/1235#LightPerception_and_LightProjection</ext-link>.</mixed-citation></ref><ref id="pone.0176534.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Owsley</surname><given-names>C</given-names></name>, <name><surname>McGwin</surname><given-names>G</given-names><suffix>Jr.</suffix></name>, <name><surname>Lee</surname><given-names>PP</given-names></name>, <name><surname>Wasserman</surname><given-names>N</given-names></name>, <name><surname>Searcey</surname><given-names>K</given-names></name>. <article-title>Characteristics of low-vision rehabilitation services in the United States</article-title>. <source>Archives of ophthalmology</source>. <year>2009</year>;<volume>127</volume>(<issue>5</issue>):<fpage>681</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1001/archophthalmol.2009.55">10.1001/archophthalmol.2009.55</ext-link></comment>
<pub-id pub-id-type="pmid">19433720</pub-id></mixed-citation></ref><ref id="pone.0176534.ref006"><label>6</label><mixed-citation publication-type="book"><name><surname>Jacobson</surname><given-names>WH</given-names></name>. <source>The art and science of teaching orientation and mobility to persons with visual impairments</source>. <edition>2nd ed</edition>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>American Foundation for the Blind</publisher-name>; <year>2013</year>.</mixed-citation></ref><ref id="pone.0176534.ref007"><label>7</label><mixed-citation publication-type="book"><name><surname>La Grow</surname><given-names>SJ</given-names></name>, <name><surname>Weessies</surname><given-names>MJ</given-names></name>. <source>Orientation and mobility: Techniques for independence</source>. <publisher-loc>Wellington, New Zealand</publisher-loc>: <publisher-name>Dunmore Press</publisher-name>; <year>1994</year>.</mixed-citation></ref><ref id="pone.0176534.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Sveistrup</surname><given-names>H</given-names></name>, <name><surname>McComas</surname><given-names>J</given-names></name>, <name><surname>Thornton</surname><given-names>M</given-names></name>, <name><surname>Marshall</surname><given-names>S</given-names></name>, <name><surname>Finestone</surname><given-names>H</given-names></name>, <name><surname>McCormick</surname><given-names>A</given-names></name>, <etal>et al</etal>
<article-title>Experimental studies of virtual reality-delivered compared to conventional exercise programs for rehabilitation</article-title>. <source>Cyberpsychology &#x00026; behavior: the impact of the Internet, multimedia and virtual reality on behavior and society</source>. <year>2003</year>;<volume>6</volume>(<issue>3</issue>):<fpage>245</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0176534.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Burdea</surname><given-names>GC</given-names></name>. <article-title>Virtual rehabilitation&#x02014;benefits and challenges</article-title>. <source>Methods of information in medicine</source>. <year>2003</year>;<volume>42</volume>(<issue>5</issue>):<fpage>519</fpage>&#x02013;<lpage>23</lpage>. <pub-id pub-id-type="pmid">14654886</pub-id></mixed-citation></ref><ref id="pone.0176534.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Botella</surname><given-names>C</given-names></name>, <name><surname>Quero</surname><given-names>S</given-names></name>, <name><surname>Banos</surname><given-names>RM</given-names></name>, <name><surname>Perpina</surname><given-names>C</given-names></name>, <name><surname>Garcia Palacios</surname><given-names>A</given-names></name>, <name><surname>Riva</surname><given-names>G</given-names></name>. <article-title>Virtual reality and psychotherapy</article-title>. <source>Studies in health technology and informatics</source>. <year>2004</year>;<volume>99</volume>:<fpage>37</fpage>&#x02013;<lpage>54</lpage>. <pub-id pub-id-type="pmid">15295145</pub-id></mixed-citation></ref><ref id="pone.0176534.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Klinger</surname><given-names>E</given-names></name>, <name><surname>Legeron</surname><given-names>P</given-names></name>, <name><surname>Roy</surname><given-names>S</given-names></name>, <name><surname>Chemin</surname><given-names>I</given-names></name>, <name><surname>Lauer</surname><given-names>F</given-names></name>, <name><surname>Nugues</surname><given-names>P</given-names></name>. <article-title>Virtual reality exposure in the treatment of social phobia</article-title>. <source>Studies in health technology and informatics</source>. <year>2004</year>;<volume>99</volume>:<fpage>91</fpage>&#x02013;<lpage>119</lpage>. Epub 2004/08/06. <pub-id pub-id-type="pmid">15295148</pub-id></mixed-citation></ref><ref id="pone.0176534.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Morganti</surname><given-names>F</given-names></name>. <article-title>Virtual interaction in cognitive neuropsychology</article-title>. <source>Studies in health technology and informatics</source>. <year>2004</year>;<volume>99</volume>:<fpage>55</fpage>&#x02013;<lpage>70</lpage>. Epub 2004/08/06. <pub-id pub-id-type="pmid">15295146</pub-id></mixed-citation></ref><ref id="pone.0176534.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>McLay</surname><given-names>RN</given-names></name>, <name><surname>Wood</surname><given-names>DP</given-names></name>, <name><surname>Webb-Murphy</surname><given-names>JA</given-names></name>, <name><surname>Spira</surname><given-names>JL</given-names></name>, <name><surname>Wiederhold</surname><given-names>MD</given-names></name>, <name><surname>Pyne</surname><given-names>JM</given-names></name>, <etal>et al</etal>
<article-title>A randomized, controlled trial of virtual reality-graded exposure therapy for post-traumatic stress disorder in active duty service members with combat-related post-traumatic stress disorder</article-title>. <source>Cyberpsychology, behavior and social networking</source>. <year>2011</year>;<volume>14</volume>(<issue>4</issue>):<fpage>223</fpage>&#x02013;<lpage>9</lpage>. Epub 2011/02/22. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1089/cyber.2011.0003">10.1089/cyber.2011.0003</ext-link></comment>
<pub-id pub-id-type="pmid">21332375</pub-id></mixed-citation></ref><ref id="pone.0176534.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Saposnik</surname><given-names>G</given-names></name>, <name><surname>Teasell</surname><given-names>R</given-names></name>, <name><surname>Mamdani</surname><given-names>M</given-names></name>, <name><surname>Hall</surname><given-names>J</given-names></name>, <name><surname>McIlroy</surname><given-names>W</given-names></name>, <name><surname>Cheung</surname><given-names>D</given-names></name>, <etal>et al</etal>
<article-title>Effectiveness of virtual reality using Wii gaming technology in stroke rehabilitation: a pilot randomized clinical trial and proof of principle</article-title>. <source>Stroke</source>. <year>2010</year>;<volume>41</volume>(<issue>7</issue>):<fpage>1477</fpage>&#x02013;<lpage>84</lpage>. Epub 2010/05/29. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1161/STROKEAHA.110.584979">10.1161/STROKEAHA.110.584979</ext-link></comment>
<pub-id pub-id-type="pmid">20508185</pub-id></mixed-citation></ref><ref id="pone.0176534.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Gallagher</surname><given-names>AG</given-names></name>, <name><surname>Ritter</surname><given-names>EM</given-names></name>, <name><surname>Champion</surname><given-names>H</given-names></name>, <name><surname>Higgins</surname><given-names>G</given-names></name>, <name><surname>Fried</surname><given-names>MP</given-names></name>, <name><surname>Moses</surname><given-names>G</given-names></name>, <etal>et al</etal>
<article-title>Virtual reality simulation for the operating room: proficiency-based training as a paradigm shift in surgical skills training</article-title>. <source>Annals of surgery</source>. <year>2005</year>;<volume>241</volume>(<issue>2</issue>):<fpage>364</fpage>&#x02013;<lpage>72</lpage>. Epub 2005/01/15. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1097/01.sla.0000151982.85062.80">10.1097/01.sla.0000151982.85062.80</ext-link></comment>
<pub-id pub-id-type="pmid">15650649</pub-id></mixed-citation></ref><ref id="pone.0176534.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Grantcharov</surname><given-names>TP</given-names></name>. <article-title>Is virtual reality simulation an effective training method in surgery?</article-title>
<source>Nature clinical practice Gastroenterology &#x00026; hepatology</source>. <year>2008</year>;<volume>5</volume>(<issue>5</issue>):<fpage>232</fpage>&#x02013;<lpage>3</lpage>. Epub 2008/04/03.</mixed-citation></ref><ref id="pone.0176534.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Schout</surname><given-names>BM</given-names></name>, <name><surname>Hendrikx</surname><given-names>AJ</given-names></name>, <name><surname>Scheele</surname><given-names>F</given-names></name>, <name><surname>Bemelmans</surname><given-names>BL</given-names></name>, <name><surname>Scherpbier</surname><given-names>AJ</given-names></name>. <article-title>Validation and implementation of surgical simulators: a critical review of present, past, and future</article-title>. <source>Surgical endoscopy</source>. <year>2010</year>;<volume>24</volume>(<issue>3</issue>):<fpage>536</fpage>&#x02013;<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00464-009-0634-9">10.1007/s00464-009-0634-9</ext-link></comment>
<pub-id pub-id-type="pmid">19633886</pub-id></mixed-citation></ref><ref id="pone.0176534.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>McComas</surname><given-names>J</given-names></name>, <name><surname>MacKay</surname><given-names>M</given-names></name>, <name><surname>Pivik</surname><given-names>J</given-names></name>. <article-title>Effectiveness of virtual reality for teaching pedestrian safety</article-title>. <source>Cyberpsychology &#x00026; behavior: the impact of the Internet, multimedia and virtual reality on behavior and society</source>. <year>2002</year>;<volume>5</volume>(<issue>3</issue>):<fpage>185</fpage>&#x02013;<lpage>90</lpage>. Epub 2002/07/19.</mixed-citation></ref><ref id="pone.0176534.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Schwebel</surname><given-names>DC</given-names></name>, <name><surname>McClure</surname><given-names>LA</given-names></name>. <article-title>Using virtual reality to train children in safe street-crossing skills</article-title>. <source>Injury prevention: journal of the International Society for Child and Adolescent Injury Prevention</source>. <year>2010</year>;<volume>16</volume>(<issue>1</issue>):<fpage>e1</fpage>&#x02013;<lpage>5</lpage>. Epub 2010/02/25.</mixed-citation></ref><ref id="pone.0176534.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Katz</surname><given-names>N</given-names></name>, <name><surname>Ring</surname><given-names>H</given-names></name>, <name><surname>Naveh</surname><given-names>Y</given-names></name>, <name><surname>Kizony</surname><given-names>R</given-names></name>, <name><surname>Feintuch</surname><given-names>U</given-names></name>, <name><surname>Weiss</surname><given-names>PL</given-names></name>. <article-title>Interactive virtual environment training for safe street crossing of right hemisphere stroke patients with Unilateral Spatial Neglect</article-title>. <source>Disability &#x00026; Rehabilitation</source>. <year>2005</year>;<volume>27</volume>(<issue>20</issue>):<fpage>1235</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="pmid">16298925</pub-id></mixed-citation></ref><ref id="pone.0176534.ref021"><label>21</label><mixed-citation publication-type="other">Lam YS, Tam SF, Man DK, Weiss PL. Evaluation of a computer-assisted 2D interactive virtual reality system in training street survival skills of people with stroke. Proceedings of the 5th International Conference on Disability, Virtual Reality &#x00026; Associated Technology; Oxford, UK, 2004. p. 27&#x02013;32.</mixed-citation></ref><ref id="pone.0176534.ref022"><label>22</label><mixed-citation publication-type="other">Lahav O. Improving orientation and mobility skills through virtual environment for people who are blind: past research and future potential. Proceedings of the 9th International Conference of Disability, Virtual Reality &#x00026; Associated Technologies; Laval, France, 2012. p. 393&#x02013;8.</mixed-citation></ref><ref id="pone.0176534.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Connors</surname><given-names>EC</given-names></name>, <name><surname>Chrastil</surname><given-names>ER</given-names></name>, <name><surname>Sanchez</surname><given-names>J</given-names></name>, <name><surname>Merabet</surname><given-names>LB</given-names></name>. <article-title>Virtual environments for the transfer of navigation skills in the blind: a comparison of directed instruction vs. video game based learning approaches</article-title>. <source>Front Hum Neurosci</source>. <year>2014</year>;<volume>8</volume>:<fpage>223</fpage> Epub 2014/05/14. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnhum.2014.00223">10.3389/fnhum.2014.00223</ext-link></comment>
<pub-id pub-id-type="pmid">24822044</pub-id></mixed-citation></ref><ref id="pone.0176534.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Seki</surname><given-names>Y</given-names></name>, <name><surname>Sato</surname><given-names>T</given-names></name>. <article-title>A training system of orientation and mobility for blind people using acoustic virtual reality</article-title>. <source>IEEE transactions on neural systems and rehabilitation engineering: a publication of the IEEE Engineering in Medicine and Biology Society</source>. <year>2011</year>;<volume>19</volume>(<issue>1</issue>):<fpage>95</fpage>&#x02013;<lpage>104</lpage>. Epub 2010/09/02.</mixed-citation></ref><ref id="pone.0176534.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Kalia</surname><given-names>AA</given-names></name>, <name><surname>Legge</surname><given-names>GE</given-names></name>, <name><surname>Giudice</surname><given-names>NA</given-names></name>. <article-title>Learning building layouts with non-geometric visual information: the effects of visual impairment and age</article-title>. <source>Perception</source>. <year>2008</year>;<volume>37</volume>(<issue>11</issue>):<fpage>1677</fpage>&#x02013;<lpage>99</lpage>. Epub 2009/02/05. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1068/p5915">10.1068/p5915</ext-link></comment>
<pub-id pub-id-type="pmid">19189732</pub-id></mixed-citation></ref><ref id="pone.0176534.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Khalifa</surname><given-names>YM</given-names></name>, <name><surname>Bogorad</surname><given-names>D</given-names></name>, <name><surname>Gibson</surname><given-names>V</given-names></name>, <name><surname>Peifer</surname><given-names>J</given-names></name>, <name><surname>Nussbaum</surname><given-names>J</given-names></name>. <article-title>Virtual reality in ophthalmology training</article-title>. <source>Survey of ophthalmology</source>. <year>2006</year>;<volume>51</volume>(<issue>3</issue>):<fpage>259</fpage>&#x02013;<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.survophthal.2006.02.005">10.1016/j.survophthal.2006.02.005</ext-link></comment>
<pub-id pub-id-type="pmid">16644366</pub-id></mixed-citation></ref><ref id="pone.0176534.ref027"><label>27</label><mixed-citation publication-type="other">US Department of Transportation. Manual on Uniform Traffic Control Devices for Streets and Highways. In: Transportation UDo, editor. 2009 edition with revisions 1 &#x00026; 2 dated May 2012 ed: US Department of Transportation; 2012.</mixed-citation></ref><ref id="pone.0176534.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Willis</surname><given-names>JR</given-names></name>, <name><surname>Vitale</surname><given-names>SE</given-names></name>, <name><surname>Agrawal</surname><given-names>Y</given-names></name>, <name><surname>Ramulu</surname><given-names>PY</given-names></name>. <article-title>Visual impairment, uncorrected refractive error, and objectively measured balance in the United States</article-title>. <source>JAMA ophthalmology</source>. <year>2013</year>;<volume>131</volume>(<issue>8</issue>):<fpage>1049</fpage>&#x02013;<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1001/jamaophthalmol.2013.316">10.1001/jamaophthalmol.2013.316</ext-link></comment>
<pub-id pub-id-type="pmid">23744090</pub-id></mixed-citation></ref><ref id="pone.0176534.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Wada</surname><given-names>T</given-names></name>, <name><surname>Ishimoto</surname><given-names>Y</given-names></name>, <name><surname>Hirosaki</surname><given-names>M</given-names></name>, <name><surname>Konno</surname><given-names>A</given-names></name>, <name><surname>Kasahara</surname><given-names>Y</given-names></name>, <name><surname>Kimura</surname><given-names>Y</given-names></name>, <etal>et al</etal>
<article-title>Twenty-one-item fall risk index predicts falls in elderly community-dwelling Japanese</article-title>. <source>Journal of the American Geriatrics Society</source>. <year>2009</year>;<volume>57</volume>(<issue>12</issue>):<fpage>2369</fpage>&#x02013;<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1532-5415.2009.02591.x">10.1111/j.1532-5415.2009.02591.x</ext-link></comment>
<pub-id pub-id-type="pmid">20121998</pub-id></mixed-citation></ref><ref id="pone.0176534.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>HK</given-names></name>, <name><surname>Scudds</surname><given-names>RJ</given-names></name>. <article-title>Comparison of balance in older people with and without visual impairment</article-title>. <source>Age and ageing</source>. <year>2003</year>;<volume>32</volume>(<issue>6</issue>):<fpage>643</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="pmid">14600006</pub-id></mixed-citation></ref></ref-list></back></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27281341</article-id><article-id pub-id-type="pmc">4900607</article-id><article-id pub-id-type="publisher-id">PONE-D-15-51592</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0156874</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal System</subject><subj-group><subject>Limbs (Anatomy)</subject><subj-group><subject>Arms</subject><subj-group><subject>Hands</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal System</subject><subj-group><subject>Limbs (Anatomy)</subject><subj-group><subject>Arms</subject><subj-group><subject>Hands</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Neuroscience</subject><subj-group><subject>Reaction Time</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Neuroscience</subject><subj-group><subject>Reaction Time</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Mechanical Engineering</subject><subj-group><subject>Robotics</subject><subj-group><subject>Robots</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular System</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Ocular System</subject><subj-group><subject>Eyes</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Mechanical Engineering</subject><subj-group><subject>Robotics</subject><subj-group><subject>Robotic Behavior</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Mechanical Engineering</subject><subj-group><subject>Robotics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Research Design</subject><subj-group><subject>Survey Research</subject><subj-group><subject>Questionnaires</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Role of Gaze Cues in Interpersonal Motor Coordination: Towards Higher Affiliation in Human-Robot Interaction</article-title><alt-title alt-title-type="running-head">Avatar Gaze and Motor Coordination</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Khoramshahi</surname><given-names>Mahdi</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Shukla</surname><given-names>Ashwini</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Raffard</surname><given-names>St&#x000e9;phane</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Bardy</surname><given-names>Beno&#x000ee;t G.</given-names></name><xref ref-type="aff" rid="aff003"><sup>3</sup></xref><xref ref-type="aff" rid="aff004"><sup>4</sup></xref></contrib><contrib contrib-type="author"><name><surname>Billard</surname><given-names>Aude</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib></contrib-group><aff id="aff001"><label>1</label><addr-line>Learning Algorithms and Systems Laboratory, School of Engineering, EPFL, Lausanne, Switzerland</addr-line></aff><aff id="aff002"><label>2</label><addr-line>University Department of Adult Psychiatry, CHRU, &#x00026; Laboratory Epsylon, EA 4556, Montpellier, France</addr-line></aff><aff id="aff003"><label>3</label><addr-line>Movement to Health Laboratory, EuroMov, Montpellier-1 University, Montpelier, France</addr-line></aff><aff id="aff004"><label>4</label><addr-line>Institut Universitaire de France, Paris, France</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Aidman</surname><given-names>Eugene V</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>Defence Science and Technology Group, AUSTRALIA</addr-line></aff><author-notes><fn fn-type="conflict" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con" id="contrib001"><p>Conceived and designed the experiments: MK AS AB. Performed the experiments: MK AS. Analyzed the data: MK AB BB. Contributed reagents/materials/analysis tools: MK AB. Wrote the paper: MK SR BB AB.</p></fn><corresp id="cor001">* E-mail: <email>mahdi.khoramshahi@epfl.ch</email></corresp></author-notes><pub-date pub-type="collection"><year>2016</year></pub-date><pub-date pub-type="epub"><day>9</day><month>6</month><year>2016</year></pub-date><volume>11</volume><issue>6</issue><elocation-id>e0156874</elocation-id><history><date date-type="received"><day>8</day><month>12</month><year>2015</year></date><date date-type="accepted"><day>22</day><month>5</month><year>2016</year></date></history><permissions><copyright-statement>&#x000a9; 2016 Khoramshahi et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Khoramshahi et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0156874.pdf"/><abstract><sec id="sec001"><title>Background</title><p>The ability to follow one another&#x02019;s gaze plays an important role in our social cognition; especially when we <italic>synchronously</italic> perform tasks together. We investigate how gaze cues can improve performance in a simple coordination task (i.e., the <italic>mirror game</italic>), whereby two players mirror each other&#x02019;s hand motions. In this game, each player is either a leader or follower. To study the effect of gaze in a systematic manner, the leader&#x02019;s role is played by a robotic avatar. We contrast two conditions, in which the avatar provides or not explicit gaze cues that indicate the next location of its hand. Specifically, we investigated (a) whether participants are able to exploit these gaze cues to improve their coordination, (b) how gaze cues affect action prediction and temporal coordination, and (c) whether introducing active gaze behavior for avatars makes them more realistic and human-like (from the user point of view).</p></sec><sec id="sec002"><title>Methodology/Principal Findings</title><p>43 subjects participated in 8 trials of the mirror game. Each subject performed the game in the two conditions (with and without gaze cues). In this within-subject study, the order of the conditions was randomized across participants, and subjective assessment of the avatar&#x02019;s realism was assessed by administering a post-hoc questionnaire. When gaze cues were provided, a quantitative assessment of synchrony between participants and the avatar revealed a significant improvement in subject reaction-time (RT). This confirms our hypothesis that gaze cues improve the follower&#x02019;s ability to predict the avatar&#x02019;s action. An analysis of the pattern of frequency across the two players&#x02019; hand movements reveals that the gaze cues improve the overall temporal coordination across the two players. Finally, analysis of the subjective evaluations from the questionnaires reveals that, in the presence of gaze cues, participants found it not only more human-like/realistic, but also easier to interact with the avatar.</p></sec><sec id="sec003"><title>Conclusion/Significance</title><p>This work confirms that people can exploit gaze cues to predict another person&#x02019;s movements and to better coordinate their motions with their partners, even when the partner is a computer-animated avatar. Moreover, this study contributes further evidence that implementing biological features, here task-relevant gaze cues, enable the humanoid robotic avatar to appear more human-like, and thus increase the user&#x02019;s sense of affiliation.</p></sec></abstract><funding-group><award-group id="award001"><funding-source><institution>EU project AlterEgo</institution></funding-source><award-id>600010</award-id></award-group><funding-statement>This research was supported by EU project AlterEgo under grant agreement number 600010. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="12"/><table-count count="0"/><page-count count="21"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>The minimal data set underlying the findings in this study can be found in the following stable public repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/khoramshahi/Human-Avatar-interaction-dataset">https://github.com/khoramshahi/Human-Avatar-interaction-dataset</ext-link>. This dataset is also in the Supporting Information files. More details about the dataset (e.g., data collection and motion capturing) are available upon contact via <email>mahdi.khoramshahi@epfl.ch</email>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>The minimal data set underlying the findings in this study can be found in the following stable public repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/khoramshahi/Human-Avatar-interaction-dataset">https://github.com/khoramshahi/Human-Avatar-interaction-dataset</ext-link>. This dataset is also in the Supporting Information files. More details about the dataset (e.g., data collection and motion capturing) are available upon contact via <email>mahdi.khoramshahi@epfl.ch</email>.</p></notes></front><body><sec sec-type="intro" id="sec004"><title>Introduction</title><p>The cooperative eye hypothesis [<xref rid="pone.0156874.ref001" ref-type="bibr">1</xref>] suggests that the visual characteristics of human eyes, such as scelra, iris, and pupil, evolved to make it easier to follow others&#x02019; gaze directions. According to this hypothesis, evolution enhances cooperative social interactions by providing a new social function; i.e., using gaze as a means to share one&#x02019;s intention. A growing number of studies have investigated the use of gaze as a form of non-verbal communication in a variety of social interactions; e.g., to complement speech [<xref rid="pone.0156874.ref002" ref-type="bibr">2</xref>], and as a mechanism to orient others&#x02019; attention [<xref rid="pone.0156874.ref003" ref-type="bibr">3</xref>]. Gaze as a mean to orient other&#x02019;s attention is possible if we can follow the gaze of others. The ability to follow other&#x02019;s gaze-direction enables joint attention [<xref rid="pone.0156874.ref004" ref-type="bibr">4</xref>] that plays an important role in our social cognition [<xref rid="pone.0156874.ref005" ref-type="bibr">5</xref>]. Recent neurological studies have revealed visual cells sensitive to gaze direction [<xref rid="pone.0156874.ref006" ref-type="bibr">6</xref>]; these cells overlaps with neural mechanisms representing facial expression [<xref rid="pone.0156874.ref007" ref-type="bibr">7</xref>]. Moreover, eye contact modulates the activation of the social brain [<xref rid="pone.0156874.ref008" ref-type="bibr">8</xref>]. This suggests that the ability to generate and respond to gaze as a means of conveying intentions recruits common neural substrates [<xref rid="pone.0156874.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0156874.ref010" ref-type="bibr">10</xref>]. It has also been reported that gaze behavior is crucial for joint action [<xref rid="pone.0156874.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0156874.ref012" ref-type="bibr">12</xref>]. Orienting the gaze at the right location at the right time improves coordination with other individuals. It has been reported that gaze direction is also necessary in establishing a closed-loop dyadic interaction, which enables a better coordination in joint actions [<xref rid="pone.0156874.ref013" ref-type="bibr">13</xref>].</p><p>Social motor coordination as one aspect of social interaction, has received much interest in recent years; see [<xref rid="pone.0156874.ref014" ref-type="bibr">14</xref>] as a review. It refers to our ability to coordinate our movements with other individuals (i.e., interpersonal synchrony) to perform a task. The cognitive and socio-psychological aspects of joint action have been studied throughly; see [<xref rid="pone.0156874.ref011" ref-type="bibr">11</xref>] and [<xref rid="pone.0156874.ref015" ref-type="bibr">15</xref>]. Interpersonal synchrony provides an important foundation for social interaction, as it has been shown that the degree of interactional synchrony of bodily movements of co-actors during social interaction is a significant predictor of subsequent affiliation ratings and cooperation between individuals [<xref rid="pone.0156874.ref016" ref-type="bibr">16</xref>]. To better understand the mechanisms at the basis of joint action, cognitive and neural scientists have studied the underlying processes separately, including those responsible for joint attention [<xref rid="pone.0156874.ref005" ref-type="bibr">5</xref>], action observation/prediction [<xref rid="pone.0156874.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0156874.ref017" ref-type="bibr">17</xref>], action coordination [<xref rid="pone.0156874.ref018" ref-type="bibr">18</xref>], synchrony [<xref rid="pone.0156874.ref019" ref-type="bibr">19</xref>], and task sharing [<xref rid="pone.0156874.ref020" ref-type="bibr">20</xref>]. Moreover, the ability to follow another&#x02019;s gaze is central to the joint action [<xref rid="pone.0156874.ref013" ref-type="bibr">13</xref>] via its roles in joint attention [<xref rid="pone.0156874.ref021" ref-type="bibr">21</xref>] and action observation [<xref rid="pone.0156874.ref022" ref-type="bibr">22</xref>].</p><p>In this work, we complement this body of literature and study the effect that gaze cues can have on dyadic interaction between a human and non-human partner, a computer generated avatar. Our main contribution is two-fold: First, using avatars&#x02019; systematic and structured behavior in a joint action, we provide a better understanding of human performance in joint action; second, we show that gaze behavior enable avatars to be effective partners in joint action. Specifically, we hypothesize that the avatars&#x02019; gaze can re-orient the attention of their human partners during the joint action for better coordination. We investigate how the avatar&#x02019;s gaze cues might affect underlying cognitive processes in humans, such as action prediction and synchrony that can potentially lead to higher sense of realism.</p><p>To elaborate on the effects of gaze cues on dyadic interaction with avatars, we employed a simple framework that enables an in-depth investigation of synchronous coordination. The mirror game [<xref rid="pone.0156874.ref023" ref-type="bibr">23</xref>] is used to study motor coordination in dyadic interactions. In this game, individuals mirror one another&#x02019;s hand movements with or without a designated leader. By measuring temporal coordination across hand trajectories, this game provides a framework for studying social coordination. Early results of the mirror game have provided a better understanding of the human ability for joint improvisation [<xref rid="pone.0156874.ref023" ref-type="bibr">23</xref>]. It has been shown that experts can create novel, synchronous, and confident (jitter-less) motions. Moreover, it helps to identify individual-specific signatures of motion that shape the behavior of the dyad [<xref rid="pone.0156874.ref024" ref-type="bibr">24</xref>]. Nonetheless, studying the behavior of the dyad makes it difficult to separate the individual contributions. In this study, we replace one player by an avatar, whose motion is structured and controlled explicitly. This enables us to attribute precisely the human&#x02019;s contribution to the joint action and to have comparable experimental conditions. In addition, the human-avatar setting enables us to investigate the socio-psychological effects of avatars&#x02019; behaviors on human partners.</p><p>We are currently witnessing a growing number of applications for humanoid robots, androids, and computer simulated avatars in context of social interaction [<xref rid="pone.0156874.ref025" ref-type="bibr">25</xref>&#x02013;<xref rid="pone.0156874.ref027" ref-type="bibr">27</xref>]. For instance, in telecommunication, androids can elicit a strong feeling of presence in the operator [<xref rid="pone.0156874.ref027" ref-type="bibr">27</xref>]. However, to enhance the human affiliation toward a robot or an avatar, researchers have tried to improve both the visual and behavioral aspects of android and avatars [<xref rid="pone.0156874.ref028" ref-type="bibr">28</xref>]. Among others, gaze behavior has been considered an effective element to enhance social interactions [<xref rid="pone.0156874.ref029" ref-type="bibr">29</xref>, <xref rid="pone.0156874.ref030" ref-type="bibr">30</xref>]. It has been shown that by using gaze behavior, a robot can establish the participants&#x02019; roles in a conversational setting and increase the sense of affiliation among the individuals [<xref rid="pone.0156874.ref031" ref-type="bibr">31</xref>, <xref rid="pone.0156874.ref032" ref-type="bibr">32</xref>]. Robotic gaze aversion (i.e., the intentional redirection away from the face of the partner in the interaction) is also perceived by humans as intentional and thoughtful, which can effectively shape the interaction [<xref rid="pone.0156874.ref033" ref-type="bibr">33</xref>]. Researchers have also investigated different gaze behaviors in avatars [<xref rid="pone.0156874.ref034" ref-type="bibr">34</xref>, <xref rid="pone.0156874.ref035" ref-type="bibr">35</xref>] where inferred (from voice) gaze behavior enhanced the behavioral realism. It has also been shown that the duration of a gaze cue, in a social interaction setting, plays a significant role on the level of co-presence [<xref rid="pone.0156874.ref036" ref-type="bibr">36</xref>]. Previous studies have shown that, during verbal communication, active gaze behavior improves avatar liveliness and human-similarity [<xref rid="pone.0156874.ref035" ref-type="bibr">35</xref>&#x02013;<xref rid="pone.0156874.ref037" ref-type="bibr">37</xref>]. For example, gaze dynamics (shifts, aversion, and fixation) can influence the sense of affiliation [<xref rid="pone.0156874.ref038" ref-type="bibr">38</xref>]. In another study, human gaze has been tracked to orient the avatar gaze in order to create eye-contact leading to the sense of awareness of others&#x02019; gazes in virtual interaction settings [<xref rid="pone.0156874.ref039" ref-type="bibr">39</xref>]. Moreover, responsive gaze behavior from an avatar can elicit in a human partner the feeling of being looked at [<xref rid="pone.0156874.ref040" ref-type="bibr">40</xref>]. Despite numerous studies on the realism of avatars [<xref rid="pone.0156874.ref041" ref-type="bibr">41</xref>, <xref rid="pone.0156874.ref042" ref-type="bibr">42</xref>], and the realism of simulated gazes in virtual environments [<xref rid="pone.0156874.ref035" ref-type="bibr">35</xref>], little is known about the effects of avatar gazes in social motor coordination. In particular, it is unclear whether in joint action settings, avatars can effectively simulate natural gaze behavior, and whether human partners can benefit from it.</p><p>Similarity is believed to be an important factor for affiliation/attraction [<xref rid="pone.0156874.ref043" ref-type="bibr">43</xref>, <xref rid="pone.0156874.ref044" ref-type="bibr">44</xref>]. Thus, it would be interesting to see if the same principle can be applied to the avatar-robot (or human-robot) interaction, where a different aspect of similarity&#x02014;gaze cues in our case&#x02014;can boost affiliation. To increase realism in animated avatars, several models of gaze have been proposed; see [<xref rid="pone.0156874.ref045" ref-type="bibr">45</xref>] as an example where the avatar head moves between poses according to the desired gaze behavior. To create human-inspired interactions, the avatar gaze has been programmed to be reactive to the human gaze that is tracked with wearable devices [<xref rid="pone.0156874.ref046" ref-type="bibr">46</xref>] or cameras [<xref rid="pone.0156874.ref047" ref-type="bibr">47</xref>]. Moreover, as the avatar&#x02019;s hand was used for the mirror game, models suggested for human eye-hand coordination can be helpful in increasing behavioral similarity between avatars and humans. However, such proposed models in the literature are highly task-dependent; see [<xref rid="pone.0156874.ref048" ref-type="bibr">48</xref>] for search, [<xref rid="pone.0156874.ref049" ref-type="bibr">49</xref>] for sequential target contact, [<xref rid="pone.0156874.ref050" ref-type="bibr">50</xref>] for drawing, and [<xref rid="pone.0156874.ref051" ref-type="bibr">51</xref>] for rhythmical pointing tasks. Therefore, to keep the analysis simple, robust, and interpretable, we limited our gaze-hand model to a simple delay of 500<italic>ms</italic>, which is in line with previous findings in [<xref rid="pone.0156874.ref013" ref-type="bibr">13</xref>] and [<xref rid="pone.0156874.ref052" ref-type="bibr">52</xref>]. In order to check if similarity-affiliation effect persists in the case of motor coordination, we accompanied our experiment with a short questionnaire where participants&#x02019; opinions on human-similarity and on cooperation of the avatar are queried. We hypothesize that preceding movements of gaze helps the human partner with the action-prediction process which consequently improves the coordination and perception of human-likeness. Cross-checking the questionnaire results with the actual recorded performances enabled us to elaborate on these effects.</p><p>In this study, we investigate using an avatar, the role that gaze plays in socio-motor coordination. Producing structured and repetitive yet random motions, the avatar acts as the leader in the interaction and the participants are the followers. Based on the aforementioned evidence for the role of gaze direction in social interactions, we consider a human-avatar mirror game where the avatar provides the human follower with gaze cues indicating the direction-of-hand motion (i.e., the gaze precedes the hand motion). To have a control condition that can act as a baseline in our analysis, we use the case where the avatar does not provide the follower with a gaze cue; i.e., the gaze and hand moves synchronously, see <xref ref-type="fig" rid="pone.0156874.g001">Fig 1</xref>. A total of 344 trajectories (30<italic>s</italic> long each) were recorded and used for the analysis. To assess whether the participants exploited the gaze cues, the following metrics are used to quantify temporal coordination: (a) <italic>reaction times</italic>, using temporal errors at sharp changes in motion direction, and (b) <italic>phase-frequency response</italic>, using a decomposition of the dyad&#x02019;s motion in frequencies. Frequency domain techniques provide more transparent analysis, as leader-follower coordination can be expressed by a set of phase relations in this domain. These techniques provide us with a better understanding of where and when in the motion the gaze cues improve the synchrony. We hypothesize that (1) participants would exploit gaze cues, marked by improvements in their coordinations and (2) the active gaze behavior for avatars/robots makes them seem more human-like to the human partners. In the next section, we present our methodology for investigating these hypotheses.</p><fig id="pone.0156874.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g001</object-id><label>Fig 1</label><caption><title>The simulated iCub robot.</title><p>The robot is acting as the leader in the mirror game, generating random sinusoidal trajectories. (Left) the gaze is fixated on the hand. (Right) the gaze precede the hand. The blue arrows shows the next hand movement and the green arrows show the current gaze fixation point.</p></caption><graphic xlink:href="pone.0156874.g001"/></fig></sec><sec id="sec005"><title>Materials and Method</title><sec id="sec006"><title>Participants</title><p>We recruited 37 participants (26 male and 11 female) from the EPFL campus (Bachelor, Master&#x02019;s, and PhD students). Their average age was 23.1 (4.7) [<xref rid="pone.0156874.ref018" ref-type="bibr">18</xref>&#x02013;<xref rid="pone.0156874.ref039" ref-type="bibr">39</xref>] (values are presented in the form mean (standard deviation) [min-max]). Each participant took part in one session that lasted a maximum of 10 minutes. No inclusion/exclusion criteria were used for the recruitment and all participants successfully completed the session. As a consequence, no data had to be removed from the experiment. They also provided written informed consent to take part in this experiment.</p></sec><sec id="sec007"><title>Apparatus</title><p>In this study, we used a computer-generated avatar that simulates the humanoid robot iCub [<xref rid="pone.0156874.ref053" ref-type="bibr">53</xref>], a 53-DOF humanoid robot as shown in <xref ref-type="fig" rid="pone.0156874.g001">Fig 1</xref>. In the experiment, the avatar is the leader and is programmed to produce a series of sinusoidal hand motions (different in terms of amplitude, frequency, and offset), following a virtual horizontal line orthogonal-to-sagittal plane. The parameters of the trajectories [offsets, amplitudes, frequencies, and random transitions] were hand-tuned based on human trajectories (studied in our previous work [<xref rid="pone.0156874.ref052" ref-type="bibr">52</xref>]), hence they display dynamics that are qualitatively close to human natural-dynamics. Randomness was added (to offset, amplitude, and frequency) to avoid that the human player learns the pattern of the motions and use this as a predictor. The head and eyes of the robot are controlled so as to generate the desired gaze behavior. The gaze direction is generated mostly by the eye movement, and the head movement was used to create a more natural and human-like behavior. In the <italic>gaze cue</italic> condition, the eyes precede the hand motion; the hand&#x02019;s trajectory was used for the gaze, but with 500<italic>ms</italic> lag. In the <italic>no-gaze cue</italic> condition, the eyes are locked on the hand and move in synchrony with the hand, see <xref ref-type="fig" rid="pone.0156874.g001">Fig 1</xref>. In our analysis, this condition serves as the baseline for participants&#x02019; performances.</p><p>To play the mirror game as the leader, we controlled the right arm of this robot. We used a standard inverse kinematics solver to control the motion of the 6 degrees of freedom of the right arm of the robot, so as to accurately follow the desired hand trajectory. In our inverse kinematics solver, we also considered human-like postures (motion of the shoulder and elbow). To use the robot as the leader in the mirror game, we controlled the position of the hand with a sinusoidal reference trajectory with stochastic parameters (random amplitude, offset, and frequency). We used random patterns in the motion to avoid that the human player learns the pattern of motions and uses this as a predictor; this keeps the gaze cue as a useful predictor during the interaction. In order to have this randomness in the avatar&#x02019;s hand motions, we first scaled the hands reachable range to [&#x02212;1, +1]. This reachable range, with respect to the body sagittal plane, is asymmetric. Then, we considered four modes of oscillation as depicted in <xref ref-type="fig" rid="pone.0156874.g002">Fig 2</xref>. Each mode has a different combination of offsets and amplitudes as follows:
<disp-formula id="pone.0156874.e001"><alternatives><graphic xlink:href="pone.0156874.e001.jpg" id="pone.0156874.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mfenced close="]" open="["><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>&#x02208;</mml:mo><mml:mfenced close="}" open="{"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfenced close="]" open="["><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>.</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced close="]" open="["><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>.</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced close="]" open="["><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>.</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>.</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced close="]" open="["><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>.</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula></p><fig id="pone.0156874.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g002</object-id><label>Fig 2</label><caption><title>Patterns of movements.</title><p>Modes of oscillations comprise random motions of the avatar&#x02019;s hand. Three small oscillations (one to the left, center, right of the torso with amplitude of 0.3) and one large oscillation (amplitude of 0.7). Number of oscillations in each mode and transition to the next mode are random. The symmetric reachable range of the hand is scaled to [-1, +1], and it into the avatar&#x02019;s coordinates.</p></caption><graphic xlink:href="pone.0156874.g002"/></fig><p>The number of oscillations in each mode is a random number between 2 and 5 (inclusive and uniform) except for the large oscillation where the number of oscillations is fewer (one or twice). Starting a mode, velocity of the oscillation is also selected randomly (1 or 1.3m/s) increasing the difficulty of the game. Moreover, upon completion, the next mode is randomly (and uniformly) chosen. This results in a random trajectory in each trial as shown in <xref ref-type="fig" rid="pone.0156874.g003">Fig 3</xref>.</p><fig id="pone.0156874.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g003</object-id><label>Fig 3</label><caption><title>A sample of generated motion for the avatar&#x02019;s hand.</title><p>Tracking performance of the PD controller in this simulator is considered satisfactory. It is visible that the generated motion is composed of different modes (combination of offset and amplitude).</p></caption><graphic xlink:href="pone.0156874.g003"/></fig><p>The choice of parameters affects the level of difficulty of the game; switching quickly between different modes of oscillation results in fast and highly transitory motions which are harder to follow. By varying the parameters (speed and complexity of the motion) prior to the experiment, we adjusted the difficulty of the game to amplify the effects of gaze cues; at a higher level of difficulty, only relying on the hand motions does not result in a satisfactory tracking performance. Thus, we expected participants to pay attention to gaze cues and exploit this information throughout the game and, in particular, during the phases where the difficulty was the highest, specifically when the avatar changes direction of motion very rapidly. To avoid compounds due to unnatural dynamics of motion, we provided the avatar with motions that follows closely the typical dynamics of human hand motions in terms of range and frequency (studied in our previous work [<xref rid="pone.0156874.ref052" ref-type="bibr">52</xref>]). <xref ref-type="fig" rid="pone.0156874.g003">Fig 3</xref> illustrates an example of such generated hand motions and the tracking performance of the controller.</p><p>To control the gaze, we used the default gaze inverse-kinematic solver provided by the iCub simulator [<xref rid="pone.0156874.ref054" ref-type="bibr">54</xref>]. In this solver, both head and eye movements are used to generate the gaze fixation point; 3 degrees of freedom for the eyes (azimuth, elevation, and vergence angles) and 3 degrees for the head (pitch roll and yaw angles). Parameters used to generate smooth and human-like gaze behavior are reported in <xref ref-type="supplementary-material" rid="pone.0156874.s002">S1 Table</xref>.</p><p>The head and eyes of the robot are controlled so as to generate the desired gaze behavior. The gaze direction is generated mostly by the eye movement, and the head movement was used to create a more natural and human-like behavior. In the <italic>gaze cue</italic> condition, the eyes precede the hand motion; the hand&#x02019;s trajectory was used for the gaze but with 500<italic>ms</italic>. In the <italic>no-gaze cue</italic> condition, the eyes are locked on the hand and move in synchrony with the hand, see <xref ref-type="fig" rid="pone.0156874.g001">Fig 1</xref>. In our analysis, this condition serves as the baseline for participants performance.</p><p>As mentioned before, our experiment has two conditions. In the <italic>no-gaze cue</italic> condition, the eyes are locked on the hand and move in synchrony with the hand. This is illustrated in the first row of <xref ref-type="fig" rid="pone.0156874.g001">Fig 1</xref>, where the hand gaze receives the same desired trajectory. In the <italic>gaze cue</italic> condition, the gaze precedes the hand motion by 500<italic>ms</italic>, but only with respect to the offset of the oscillation as plotted in the second row of <xref ref-type="supplementary-material" rid="pone.0156874.s001">S1 Fig</xref>. It can be seen that the real gaze-trajectory differs from the desired one. This is due to the gaze controller being affected/perturbed by the hand motion. However, the leading behavior, which provides gaze cues, is preserved; the gaze moves sooner to the new offset and oscillates synchronously with the hand, and has a smaller amplitude.</p><p>In our experiment, participants were asked to follow the motion of the avatar, see <xref ref-type="fig" rid="pone.0156874.g004">Fig 4</xref>. To track the motion of the human&#x02019;s hand, we asked the subject to hold a marker, which enabled us to track their motion using OptiTrack system [<xref rid="pone.0156874.ref055" ref-type="bibr">55</xref>] (120Hz for sampling rate, and accuracy of 0.1mm).</p><fig id="pone.0156874.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g004</object-id><label>Fig 4</label><caption><title>The experimental setup.</title><p>The avatar is displayed on a big screen (46 inches). The avatar led the mirror game and the participant followed the avatar&#x02019;s hand motions. The participant held a marker for motion tracking purposes.</p></caption><graphic xlink:href="pone.0156874.g004"/></fig></sec><sec id="sec008"><title>Procedure</title><p>Each participant participated in both conditions. In order to remove the order effects, we divided the participants into two groups: one group was exposed to the &#x0201c;<italic>no-gaze cue</italic>&#x0201d; condition first, and the other was exposed to the &#x0201c;<italic>gaze cues</italic>&#x0201d; condition first. See <xref ref-type="fig" rid="pone.0156874.g005">Fig 5</xref> for our experimental protocol. In each condition, subjects played four consecutive trials, each 30 seconds long. Having played in both conditions, the participants were asked to answer a short questionnaire. This led to a total of 344 recorded trajectories (30<italic>s</italic> long each) for the analysis. Upon completion of all the trials, we asked the participants five short questions about their impressions of the difficulty and realism (similarity to human behavior) of the avatar; see <xref ref-type="supplementary-material" rid="pone.0156874.s003">S2 Fig</xref>.</p><fig id="pone.0156874.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g005</object-id><label>Fig 5</label><caption><title>The protocol used for the experiment.</title><p>Subjects were divided into two groups and participated in the experiment with a different ordering of conditions followed by a short questionnaire.</p></caption><graphic xlink:href="pone.0156874.g005"/></fig></sec><sec id="sec009"><title>Data Analysis</title><p>In our previous studies [<xref rid="pone.0156874.ref052" ref-type="bibr">52</xref>], we found that the human tracking performance can be captured by the temporal differences between the leader and the follower trajectories. Here we use the same measure; see <xref ref-type="fig" rid="pone.0156874.g006">Fig 6</xref>. For each set of leader-follower trajectories obtained from a trial, we calculate the temporal differences between the leader and the follower only across the peaks (i.e., zero-velocity points). The sign of the temporal difference shows whether the follower is leading or lagging. For each subject in a condition, we obtain a distribution for such temporal differences. We chose the average to compare the tracking performance across the two conditions, i.e., average reaction-time (RT). We refer to the within-subject RT contrast across the condition as RT improvement defined as
<disp-formula id="pone.0156874.e002"><alternatives><graphic xlink:href="pone.0156874.e002.jpg" id="pone.0156874.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic>RT</italic><sub><italic>n</italic></sub> and <italic>RT</italic><sub><italic>g</italic></sub> represent the participants&#x02019; reaction times in &#x0201c;<italic>no-gaze cue</italic>&#x0201d; and &#x0201c;<italic>gaze cues</italic>&#x0201d; conditions respectively. A positive value for this variable shows that the participant had a better performance in the presence of the gaze cues.</p><fig id="pone.0156874.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g006</object-id><label>Fig 6</label><caption><title>Reaction time analysis.</title><p>Extraction of reaction time based on zero-velocity points in the leader and follower trajectories. In this conceptual example, we have positive reaction times (the leader/follower is leading/lagging) in the first two cases, and a negative reaction time (the leader/follower is lagging/leading) in the last case.</p></caption><graphic xlink:href="pone.0156874.g006"/></fig><p>To check the effect of gaze in more detail, we applied frequency-domain techniques. This allows for a more refined analysis where the leader-follower interaction is presented as a frequency-phase relation. This helps us to understand how gaze cues improve the coordination. A cross-wavelet transform was applied to the leader-follower trajectories by using a Matlab toolbox provided by [<xref rid="pone.0156874.ref056" ref-type="bibr">56</xref>]. In this transform, the Morelet wavelet with conventional temporal resolution (<italic>&#x003c3;</italic> = 6) was used.</p><p>To pinpoint significant within-subject contrasts across the conditions, repeated measures ANOVA was performed. The reaction time, the perception of difficulty, and the perception of similarity are the three dependent variables which we measured in the two conditions; i.e., &#x0201c;<italic>no-gaze cue</italic>&#x0201d; and &#x0201c;<italic>gaze cues</italic>&#x0201d;. The condition and the order of the conditions are used as within-subject factors; i.e. independent variables. Moreover, a separate analysis included further the effect of age and gender were age was split into tree balanced groups as described in <xref ref-type="supplementary-material" rid="pone.0156874.s004">S2 Table</xref>.</p></sec></sec><sec sec-type="results" id="sec010"><title>Results</title><p>We first present the results of our questionnaire. Then, we investigate the results obtained from the motion capture systems. Afterward, we crosscheck the subjects&#x02019; performances with their impressions reported in the questionnaire. Finally, we present the results acquired from the frequency-domain analysis of the recorded participants&#x02019; motions.</p><sec id="sec011"><title>Questionnaire Results</title><sec id="sec012"><title>Cooperative and Natural Interaction by Using Gaze</title><p>
<xref ref-type="fig" rid="pone.0156874.g007">Fig 7</xref> summarizes the response distribution for the first four questions of the questionnaire. <xref ref-type="fig" rid="pone.0156874.g007">Fig 7A</xref> shows that in the absence of gaze, most of the subjects found it slightly difficult to follow the avatar. whereas, <xref ref-type="fig" rid="pone.0156874.g007">Fig 7B</xref> shows that, in the presence of gaze, following the avatar is perceived as rather easy. <xref ref-type="fig" rid="pone.0156874.g007">Fig 7C</xref> shows how presence of gaze cues affected participants&#x02019; opinion on the level of difficulty. The majority of subjects (60%) perceived the mirror game as easy (by either 1 or 2 steps) in the <italic>gaze cues</italic> condition; see <xref ref-type="fig" rid="pone.0156874.g007">Fig 7C</xref>. The analysis of variance shows that opinions are significantly shifted toward low difficulty [<italic>F</italic>(1, 35) = 5.478, <italic>p</italic> = 0.025]. No significant effects were detected due to age, gender, and the order of the conditions; see <xref ref-type="supplementary-material" rid="pone.0156874.s006">S3 Table</xref> for more details. The second row of <xref ref-type="fig" rid="pone.0156874.g007">Fig 7</xref> shows subjects&#x02019; responses to the question about how similar they found the robot&#x02019;s behavior compared to human behavior. <xref ref-type="fig" rid="pone.0156874.g007">Fig 7D</xref> shows a bell-shaped distribution for similarity index in the absence of gaze whereas <xref ref-type="fig" rid="pone.0156874.g007">Fig 7E</xref> shows a skewed distribution in the presence of gaze implying a high similarity to human behavior when the avatar uses its gaze actively. <xref ref-type="fig" rid="pone.0156874.g007">Fig 7F</xref> illustrates how presence of gaze cues affected participants&#x02019; opinions on the level of realism. A majority of subjects (71%) perceived the avatar as more human-like (by either 1, 2, or 3 steps) in the <italic>gaze cues</italic> condition; see <xref ref-type="fig" rid="pone.0156874.g007">Fig 7C</xref>. The analysis of variance shows that opinions significantly shift toward high realism [<italic>F</italic>(1, 35) = 17.897, <italic>p</italic> = 0.000]. No significant effects were detected due to age, gender, and the order of the conditions; see <xref ref-type="supplementary-material" rid="pone.0156874.s006">S3 Table</xref> for more details. In summary, <xref ref-type="fig" rid="pone.0156874.g007">Fig 7</xref> shows that use of gaze cues made the interaction easier, and elicited the avatar to be perceived as more human-like and realistic.</p><fig id="pone.0156874.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g007</object-id><label>Fig 7</label><caption><title>Distributions obtained from the answers to the questionnaire.</title><p>(A) Difficulty in the &#x0201c;no gaze&#x0201d; condition. (B) Difficulty in the &#x0201c;gaze&#x0201d; condition. (C) Changes in the subjects&#x02019; opinion from the &#x0201c;no gaze&#x0201d; to the &#x0201c;gaze&#x0201d; condition. (D) Similarity to human behavior in the &#x0201c;no gaze&#x0201d; condition. (E) Similarity to human behavior in the &#x0201c;gaze&#x0201d; condition. (F) Changes in the subjects&#x02019; opinion form the &#x0201c;no gaze&#x0201d; to the &#x0201c;gaze&#x0201d; condition. In these plots, ratio is calculated by the number of participants in each level divided by the total number of participants.</p></caption><graphic xlink:href="pone.0156874.g007"/></fig></sec><sec id="sec013"><title>Correlation Analysis Between Cooperation and Realism</title><p>To determine if perception of difficulty (cooperative behavior) and human-likeness (realism) are correlated, we computed a contingency table, see <xref ref-type="supplementary-material" rid="pone.0156874.s008">S4 Table</xref>. This table is computed based the participants&#x02019; opinions about their performances in the <italic>gaze cues</italic> condition compared to the <italic>no-gaze cue</italic> condition. <xref ref-type="supplementary-material" rid="pone.0156874.s008">S4 Table</xref> shows that a majority of participants (sum of diagonal elements: 53%), who found the avatar more realistic in the presence of gaze cues, also found the interaction easier. However, no significant dependency between difficulty and realism was detected using Spearman&#x02019;s correlation test in this table.</p></sec></sec><sec id="sec014"><title>Motion Capture Results</title><sec id="sec015"><title>Reaction Time</title><p>Now, we turn to the objective and quantifiable results on the effect of gaze on the subjects&#x02019; tracking performances. To this end, we analyzed the data on the relative velocity of participants and the avatar&#x02019;s hand motions. As mentioned before, the tracking performance of each participant is measured by the average of absolute temporal error (so-called reaction time, or in short RT). Therefore, for each participant, we compute the RT for both <italic>no-gaze cue</italic> and <italic>gaze cues</italic> conditions. To contrast the two conditions, we take the difference between the RT in each case (<xref ref-type="disp-formula" rid="pone.0156874.e002">Eq 2</xref>), which we name &#x0201c;Improvement in RT&#x0201d;. <xref ref-type="fig" rid="pone.0156874.g008">Fig 8</xref> shows the overall results of this analysis.</p><fig id="pone.0156874.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g008</object-id><label>Fig 8</label><caption><title>Overall analysis of the recorded motions.</title><p>(Left) Boxplots of subjects&#x02019; reaction times in each condition. (Center) histogram of &#x00394;<italic>RT</italic>. (Right) RT in the <italic>gaze cues</italic> condition vs. RT in the <italic>no-gaze cue</italic> condition. Each dot represents a participant. Black line is the unity line and the blue line in the result of the linear regression.</p></caption><graphic xlink:href="pone.0156874.g008"/></fig><p>
<xref ref-type="fig" rid="pone.0156874.g008">Fig 8</xref>(Left) shows the boxplots for reaction times in each condition where participants, on average, showed faster reactions with gaze cues than without. The analysis of variance shows a significant improvement in reaction times due to the gaze cues [<italic>F</italic>(1, 35) = 9.445, <italic>p</italic> = 0.004]; see <xref ref-type="supplementary-material" rid="pone.0156874.s006">S3 Table</xref> for more details. Moreover, a marginally significant effects due to age was detected [<italic>F</italic>(2, 32) = 2.996, <italic>p</italic> = 0.064]. The post-hoc analysis showed that the old participants, compared to the young ones, have a significantly higher RT improvement; see <xref ref-type="supplementary-material" rid="pone.0156874.s010">S5 Table</xref> and <xref ref-type="supplementary-material" rid="pone.0156874.s005">S3 Fig</xref> for more details. <xref ref-type="fig" rid="pone.0156874.g008">Fig 8</xref>(Center) shows the distribution of &#x00394;<italic>RT</italic>. The results of the Wilcoxon test suggests that the average of this distribution (13ms) is significantly greater than zero. The last subplot, <xref ref-type="fig" rid="pone.0156874.g008">Fig 8</xref>(Right), shows the performance of each individual change in the presence of the gaze cue. The black line indicates the unity line (the null hypothesis). As can be seen, the data is skewed to the favorable side of this line (alternative hypothesis). The blue line illustrates the linear regression of the data. The slope of this regression implies that individuals with lower performances (higher RT in the &#x0201c;no gaze&#x0201d; condition) can benefit more from gaze cues.</p></sec><sec id="sec016"><title>Frequency-Phase Profile</title><p>Thus far, for our analysis, we used a metric based on the computation of zero-velocity points only. Although this metric provides a good estimation of the reaction time and enables us to put forward significant differences across the conditions. However, it does not provide an assessment for the different aspects of joint action; i.e., action prediction, temporal coordination, and joint planning. A decomposition of the avatar and human motions in the frequency domain, using wavelet analysis, offers powerful tools for attaining such quantitative assessments. By using wavelet analysis [<xref rid="pone.0156874.ref057" ref-type="bibr">57</xref>], the leader-follower interaction can be transformed into time-frequency space where the temporal correspondence is easier to detect compared to the reaction time analysis. For this purpose, we use the Matlab Wavelet Coherence toolbox provided by [<xref rid="pone.0156874.ref056" ref-type="bibr">56</xref>]. The results of cross-wavelet coherence for one of the trials are illustrated in <xref ref-type="fig" rid="pone.0156874.g009">Fig 9</xref>.</p><fig id="pone.0156874.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g009</object-id><label>Fig 9</label><caption><title>Cross-wavelet analysis.</title><p>Right: Cross-wavelet coherence between the leader and the follower in one of the trials. Power of frequency components at each time is color coded; i.e., blue/yellow for weak/strong components, respectively. Moreover, the arrows indicate the leader-follower phase relation for each frequency over time. Left: Average phase-lag for each frequency extracted from the main plot.</p></caption><graphic xlink:href="pone.0156874.g009"/></fig><p>In cross-wavelet coherence, each point at a certain time and frequency has two components: power and angle. The power, which is color-coded in the figure, shows the strength of that frequency at that moment. The angle, however, shows the lag between the leader and the follower. The arrows, pointing to the right, indicate a perfect synchrony, whereas arrows tilting upward/downward show a leading/lagging behavior in the follower; upward/downward arrows signify 90 degree phase lead/lag. To quantify the temporal correspondence, we extracted the average phase-lag at each frequency; see <xref ref-type="fig" rid="pone.0156874.g009">Fig 9</xref>(Right). We observe that, in low frequencies, there is a satisfactory synchronization that deteriorates as frequency increases. There is an interesting point when the graph passes 90 degree, i.e., an asynchronous interaction. Similar to linear filters, this frequency (2Hz in this example) can be considered as the <italic>bandwidth</italic> of interaction; i.e., a frequency beyond which the synchronous interaction cannot be maintained. Moreover, after a certain frequency, the estimation of phase lag is not reliable as the power of that frequency drops in the cross-wavelet coherence plot.</p><p>The average phase-lag can be extracted for each subject for the two conditions, i.e., with and without gaze. Such graphs, for one of the subjects, are plotted <xref ref-type="fig" rid="pone.0156874.g010">Fig 10</xref>. It can be seen that, for both cases, synchrony reduces as frequency increases. However, the interaction has a lower lag for each frequency in the presence of the gaze. This can be assessed easier by looking at the difference of two graphs in the lower plot in <xref ref-type="fig" rid="pone.0156874.g010">Fig 10</xref>. This plot clearly shows that, for this participant, the presence of the gaze improved the interaction over all frequencies.</p><fig id="pone.0156874.g010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g010</object-id><label>Fig 10</label><caption><title>Frequency-phase profile.</title><p>Top: Average phase-lag vs. frequency of one of the participants in both conditions; with and without gaze. Bottom: Phase improvement vs. frequency of one of the participants due to the presence of the gaze cues.</p></caption><graphic xlink:href="pone.0156874.g010"/></fig><p>We applied this procedure to all participants and studied the average behavior that is plotted in <xref ref-type="fig" rid="pone.0156874.g011">Fig 11</xref>. Investigating the 95% confidence interval does not show a significant improvement (with zero improvement as the null hypothesis). However, scaled standard deviations are plotted for comparison across the frequency spectrum. As mentioned before, the average phase for high frequencies is not reliable, which, in this figure, results in wide intervals. It can be seen that improvements take place in three different regions. Interestingly, each region accounts for a different underlying process in joint actions. These processes are as follows:</p><list list-type="simple"><list-item><p><bold>Action prediction</bold>: low-frequency region (1/8 &#x02212; 1/4<italic>Hz</italic>) accounts for the variation of the offset in the motion; see <xref ref-type="fig" rid="pone.0156874.g003">Fig 3</xref>. By providing a gaze cue to the next location of the oscillations, the avatar improves the synchrony in the interaction in this region. Therefore, gaze affects the joint action by improving the action predication process.</p></list-item><list-item><p><bold>Action coordination</bold>: mid-frequency region (1/2 &#x02212; 1<italic>Hz</italic>) accounts for the oscillatory motions. The improvement in this range supports the hypothesis that, in the <italic>gaze cues</italic> condition, the follower can synchronously follow one mode of oscillation, which has a random number of repetitions, until the next gaze cue. Therefore, gaze affects the joint action by improving action coordination.</p></list-item><list-item><p><bold>Task sharing/Joint planning</bold>: high-frequency region (around 2Hz) accounts for fast and transitory motions. The improvement in this region shows that faster synchronous interactions can be sustained in the presence of the gaze. The human-follower has more confidence in initiating these fast motions, as if the task/leadership is shared between the human subject and the avatar. Therefore, gaze affects the joint action by introducing joint planning and task sharing. However, compared to the previous regions, this result is not reliable due to the wider confidence intervals.</p></list-item></list><fig id="pone.0156874.g011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g011</object-id><label>Fig 11</label><caption><title>Effect of gaze on the synchrony of the interaction across frequency (averaged over all subjects).</title><p>The red graph indicates the average improvement due to the gaze cues. Gray area indicates the scaled 95% confidence intervals.</p></caption><graphic xlink:href="pone.0156874.g011"/></fig></sec></sec><sec id="sec017"><title>Consistency Between Participants&#x02019; Perceived and Actual Performance</title><p>To determine whether the participants&#x02019; actual performances are consistent with their impressions, we analyzed their reaction times with respect to their responses in the questionnaire. <xref ref-type="fig" rid="pone.0156874.g012">Fig 12</xref>(Left) compares RT improvements (due to the gaze) for the two groups: (1) the participants who found it harder to follow the avatar with gaze cue, (2) the rest of participants. The ANOVA reveals that these two groups are significantly different [<italic>F</italic>(1, 34) = 5.495, <italic>p</italic> = 0.025]; see <italic>Model I</italic> of <xref ref-type="supplementary-material" rid="pone.0156874.s011">S6 Table</xref> for more details. This means that participants who stated that it is harder to follow the avatar in the presence of the gaze cues, actually had a slower reaction time in the <italic>gaze cues</italic> condition.</p><fig id="pone.0156874.g012" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0156874.g012</object-id><label>Fig 12</label><caption><title>Participants&#x02019; actual performance vs. their perception.</title><p>Boxplots of &#x00394;<italic>RT</italic> for the participants who found it (Left) harder to follow with gaze cues compared to the rest of the participants, and (Right) less human-like with gaze cues compared to the rest of the participants.</p></caption><graphic xlink:href="pone.0156874.g012"/></fig><p>Crosschecking the &#x00394;<italic>RT</italic> with the results for realism from the questions reveals interesting facts: The participants who found the presence of gaze cues less human-like have significantly [<italic>F</italic>(1, 34) = 6.084, <italic>p</italic> = 0.019] lower performances in the <italic>gaze cue</italic> condition; see <xref ref-type="fig" rid="pone.0156874.g012">Fig 12</xref>(Right) and <italic>Model II</italic> of <xref ref-type="supplementary-material" rid="pone.0156874.s011">S6 Table</xref> in the Appendix for more details. Based on this analysis, we can infer that the sense of realism and cooperation (level of difficulty) are related; i.e., cooperation contributes to affiliation and vice versa.</p><p>In this work, we did not monitor explicitly the gaze of the participants. Incorporating eye trackers [<xref rid="pone.0156874.ref058" ref-type="bibr">58</xref>] and monitoring the subjects&#x02019; shifts of visual attention could contribute to a finer analysis of the pattern of attention. In our study, such monitoring could provide information on when the human partners pays attention to the robot&#x02019;s face versus to the robot&#x02019;s hand. However, we used a questionnaire to assess how participants managed to divide their attention between tracking the robot&#x02019;s hand and looking at the robot&#x02019;s gaze; a five steps rating system (i.e., very easy, easy, normal, hard, very hard). On average, participants found it easy to divide their attention between the hand and the gaze of the avatar; see <xref ref-type="supplementary-material" rid="pone.0156874.s007">S4 Fig</xref> for more details. No significant effect was detected for this factor on the RT contrast in the two conditions; see <italic>Model III</italic> of <xref ref-type="supplementary-material" rid="pone.0156874.s011">S6 Table</xref> in the Appendix for more details. However, participants who found it very easy, or easy to divide their attention had a faster RT in the <italic>gaze cue</italic> condition [<italic>F</italic>(1, 34) = 3.425, <italic>p</italic> = 0.073]; see <xref ref-type="supplementary-material" rid="pone.0156874.s012">S7 Table</xref> and <xref ref-type="supplementary-material" rid="pone.0156874.s009">S5 Fig</xref> for more details.</p></sec></sec><sec sec-type="conclusions" id="sec018"><title>Discussion</title><p>The embodiment of artificial agents plays an important role in their interactions with human partners. Many works in the literature on social robotics explore this feature. For example, the presence of robotic platforms has been considered a key element in evaluating therapy in the case of autism spectrum disorders [<xref rid="pone.0156874.ref059" ref-type="bibr">59</xref>]. Moreover, another recent study [<xref rid="pone.0156874.ref060" ref-type="bibr">60</xref>] has shown that a robotic referential gaze leads human partners to take the robot&#x02019;s visual perspective. We share the same belief that embodiment can enhance the sense of affiliation. However, it is interesting to see that in this study, a gaze of a simulated robot on a screen can still elicit a sense of realism in the human partner. Replicating the same experiment using the humanoid robot, the iCub, in comparison with the avatar case, is an interesting investigation where we can study the difference between simulated and real platforms in the context of social robotics.</p><p>In this study, we used a simple model for eye-hand coordination, which does not reproduce the exact dynamics of eye-arm coordination found in humans. We learned that even such simple behavior helps the human partner with the action prediction process, and consequently improves the coordination and the perception of human-likeness. However, modeling more realistic eye-hand coordination for avatars might boost the behavioral realism and increase affiliation [<xref rid="pone.0156874.ref045" ref-type="bibr">45</xref>&#x02013;<xref rid="pone.0156874.ref048" ref-type="bibr">48</xref>]. For avatars, reactive gaze behavior to the human gaze can also potentially enrich their realism [<xref rid="pone.0156874.ref040" ref-type="bibr">40</xref>]. However, reaching a robust statistical conclusion in face of such a complex behavior of the avatar requires more thorough experimental design with a larger sample size. In this preliminary work, we benefited from our simple gaze model. We reached the robust and interpretable results that enabled us to elaborate on effects of gaze on joint action and realism of computer simulated avatars.</p><p>These findings may support the design of similar games for studying deficiencies in the ability to interpret other people&#x02019;s gaze, as displayed by individuals suffering from schizophrenia and autism spectrum disorders (ASD) [<xref rid="pone.0156874.ref061" ref-type="bibr">61</xref>&#x02013;<xref rid="pone.0156874.ref064" ref-type="bibr">64</xref>]. Interpersonal synchrony provides an important foundation for social interaction, in which recent studies suggested that people suffering from schizophrenia and ASD also have deficits in motor coordination [<xref rid="pone.0156874.ref065" ref-type="bibr">65</xref>&#x02013;<xref rid="pone.0156874.ref068" ref-type="bibr">68</xref>]. A recent study in schizophrenia found a causal relationship between impaired attention toward gaze orientation and deficits in theory of mind [<xref rid="pone.0156874.ref063" ref-type="bibr">63</xref>]. The version of the mirror game offered in our study, in which gaze is used as an active cueing device, could serve to design therapeutic games whereby patients are encouraged to process gaze information in order to increase motor synchrony during interactions. Improving interactional synchrony in schizophrenic patients, when engaged in dyadic games with a healthy partner, is shown to be beneficial for the patient and partner alike, as it also increases the motivation and sense of affiliation in the healthy partner [<xref rid="pone.0156874.ref066" ref-type="bibr">66</xref>]. Previous studies have already shown that schizophrenia patients can benefit from attentional-shaping procedures displayed by a therapist, to enhance neurocognition and functioning [<xref rid="pone.0156874.ref069" ref-type="bibr">69</xref>&#x02013;<xref rid="pone.0156874.ref071" ref-type="bibr">71</xref>], or being instructed to pay more attention to facial areas that contain information about a displayed emotion to enhance emotion recognition [<xref rid="pone.0156874.ref072" ref-type="bibr">72</xref>]. However, the use of an avatar for therapy in place of a human is advantageous in that the avatar provides a consistent and reliable feedback/behavior without the presence of a therapist.</p></sec><sec sec-type="conclusions" id="sec019"><title>Conclusion</title><p>In this study, we have tested whether, in a human-avatar joint action, an avatar gaze behavior can improve coordination. We used the mirror game paradigm where the human subject imitates the hand motions of a animated avatar. To test our hypotheses, we implemented a simple gaze behavior where an avatar provides a human subject with task-relevant cues. In a within-subject study, we recorded the performance of participants in the presence and absence of gaze cues. We assessed the avatar&#x02019;s realism and cooperation by a post-hoc questionnaire. Our main result shows that gaze cues significantly improve participants&#x02019; reaction times to the avatar&#x02019;s movements. A wavelet analysis of the interactions provided us with a better understanding of different underling aspects/processes reported for joint actions. Frequency-domain techniques helped us to model the follower&#x02019;s behavior as a frequency-dependent-phase relation that, compared to time domain analyses, is easier to interpret. We learned that, in a joint action, the leader&#x02019;s gaze cues helps the follower with action prediction, action coordination, and task sharing. The results of the questionnaire showed that participants perceived the avatar&#x02019;s gaze cues behavior not only as cooperative, but also human-like and realistic. Moreover, we observed that participants perception of similarity and cooperation is correlated with their performance in the game. This suggests that human-similarity, cooperativeness, and the sense of affiliation toward avatars, are highly interlinked. The results of this study will help us design computer-assisted cognitive-remediation therapy for pathologies with abnormal gaze and motor behavior such as schizophrenia.</p></sec><sec id="sec020"><title>Limitations</title><p>To best of our knowledge, this study is the first to investigate the effect of avatars&#x02019; gaze behavior on social motor coordination. Thus, the results must be considered as exploratory where we used a straightforward gaze model in a simple interactional framework (i.e., the mirror game). For further enhancement of avatar realism, future work should explore more sophisticated gaze models; e.g., models inspired by human behavior. It is also interesting to perform the experiment using the humanoid robots to investigate if gaze effects can be generalized to other non-human agents. In this study, we used two metrics: reaction time and frequency-dependent-phase. Both metrics captured the beneficial effects of gaze cues. We believe that the second metric was introduced for the first time in this study. Due to a higher effect size in this metric (the entire frequency domain), however, a larger sample size is required to reach substantial statical power in order to draw significant conclusions. Future studies should consider eye tracking to correct for the participants&#x02019; level of attention to the avatar&#x02019;s gaze in the statistical inferences.</p></sec><sec sec-type="supplementary-material" id="sec021"><title>Supporting Information</title><supplementary-material content-type="local-data" id="pone.0156874.s001"><label>S1 Fig</label><caption><title>An example of desired trajectories for the avatar&#x02019;s hand and gaze in two conditions.</title><p>(EPS)</p></caption><media xlink:href="pone.0156874.s001.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s002"><label>S1 Table</label><caption><title>Parameters used in the iCub gaze controller.</title><p>(EPS)</p></caption><media xlink:href="pone.0156874.s002.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s003"><label>S2 Fig</label><caption><title>The questionnaire used in this study.</title><p>(EPS)</p></caption><media xlink:href="pone.0156874.s003.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s004"><label>S2 Table</label><caption><title>The split performed on age for the ANOVA analysis.</title><p>(EPS)</p></caption><media xlink:href="pone.0156874.s004.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s005"><label>S3 Fig</label><caption><title>The reaction time improvement due to the gaze cues across age.</title><p>The ANOVA analysis in <xref ref-type="supplementary-material" rid="pone.0156874.s010">S5 Table</xref> showed that the first group (<italic>Low</italic>) and the last group (<italic>High</italic>) are significantly different.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s005.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s006"><label>S3 Table</label><caption><title>The results of the Repeated Measures ANOVA.</title><p>In each condition (i.e., <italic>gaze cue</italic> and <italic>no-gaze cue</italic>), the three different measurements done are: 1) the reaction time, 2) the perception of the difficulty of the game, and 3) the perception of the human-similarity. In <italic>Model I</italic>, the effects of conditions and the order of the conditions are studied. In <italic>Model II</italic>, the effects of age and gender are also investigated.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s006.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s007"><label>S4 Fig</label><caption><title>Participants&#x02019; attentional workload.</title><p>The distribution obtained from the answers to the questionnaire concerning the division of attention between avatar&#x02019;s gaze and hand.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s007.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s008"><label>S4 Table</label><caption><title>Correlation between cooperation and realism.</title><p>Contingency table for effect of gaze cues on participants&#x02019; opinion on the difficulty of the interaction and the realism of the avatar.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s008.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s009"><label>S5 Fig</label><caption><title>The RT in <italic>gaze cue</italic> condition vs. attention.</title><p>The <italic>RT</italic><sub><italic>g</italic></sub> distribution of participants who found it hard to divide their attention between the avatar&#x02019;s gaze and hand compared to the rest of the participants. The ANOVA analysis in <xref ref-type="supplementary-material" rid="pone.0156874.s012">S7 Table</xref> showed that the difference in these distributions is significant.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s009.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s010"><label>S5 Table</label><caption><title>The post-hoc test for age.</title><p>The post-hoc test for the detected effect of age on the reaction time in <xref ref-type="supplementary-material" rid="pone.0156874.s006">S3 Table</xref>. The multiple comparisons are done based on LSD method. The corresponding distributions are plotted <xref ref-type="supplementary-material" rid="pone.0156874.s005">S3 Fig</xref>.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s010.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s011"><label>S6 Table</label><caption><title>Crosschecking the result of the motion capture (i.e., RT) with the result of the questionnaire using repeated measures ANOVA.</title><p>In <italic>Model I</italic>, the effect of perception of difficulty on RT is studied. <italic>Diff_dummy</italic> is 0 for the participants who found it harder to follow the avatar with gaze cue, and 1 for the rest of the participants. In <italic>Model II</italic>, the effect of perception of similarity on RT is studied. <italic>Sim_dummy</italic> is 0 for the participants who found the presence of gaze cues less human-like, and 1 for the rest of the participants. In <italic>Model III</italic>, the effect of attention load on RT is studied. <italic>Sim_dummy</italic> is 1 for the participants who found it very easy, or easy to divide their attention between the avatar&#x02019;s gaze and avatar&#x02019;s hand, and 0 for the rest of the participants.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s011.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s012"><label>S7 Table</label><caption><title>The effect of attention of the RT.</title><p>The results of the univariate ANOVA to study the effect of attention on the RT in the <italic>gaze cue</italic> condition. <italic>Att_dummy</italic> is 1 for the participants who found it very easy, or easy to divide their attention between the avatar&#x02019;s gaze and avatar&#x02019;s hand, and 0 for the rest of the participants; see <xref ref-type="supplementary-material" rid="pone.0156874.s009">S5 Fig</xref>. Moreover, Levene&#x02019;s test indicated equal variances [<italic>F</italic>(14, 22) = .743, <italic>p</italic> = 0.713].</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s012.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0156874.s013"><label>S8 Table</label><caption><title>Equality of variances.</title><p>The Levene&#x02019;s test of equality of error variances for <italic>Model I</italic> and <italic>Model II</italic> presented in <xref ref-type="supplementary-material" rid="pone.0156874.s006">S3 Table</xref>. For both models df1 = 28 and df2 = 8.</p><p>(EPS)</p></caption><media xlink:href="pone.0156874.s013.eps"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>This research was supported by EU project AlterEgo under grant agreement number 600010. The authors would like to thank Ajung Moon and Nili Krausz for their comments on this work.</p></ack><ref-list><title>References</title><ref id="pone.0156874.ref001"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Tomasello</surname><given-names>M</given-names></name>, <name><surname>Hare</surname><given-names>B</given-names></name>, <name><surname>Lehmann</surname><given-names>H</given-names></name>, <name><surname>Call</surname><given-names>J</given-names></name>. <article-title>Reliance on head versus eyes in the gaze following of great apes and human infants: the cooperative eye hypothesis</article-title>. <source>Journal of Human Evolution</source>. <year>2007</year>;<volume>52</volume>(<issue>3</issue>):<fpage>314</fpage>&#x02013;<lpage>320</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jhevol.2006.10.001">10.1016/j.jhevol.2006.10.001</ext-link></comment>
<pub-id pub-id-type="pmid">17140637</pub-id></mixed-citation></ref><ref id="pone.0156874.ref002"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Kendon</surname><given-names>A</given-names></name>. <article-title>Some functions of gaze-direction in social interaction</article-title>. <source>Acta psychologica</source>. <year>1967</year>;<volume>26</volume>:<fpage>22</fpage>&#x02013;<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0001-6918(67)90005-4">10.1016/0001-6918(67)90005-4</ext-link></comment>
<pub-id pub-id-type="pmid">6043092</pub-id></mixed-citation></ref><ref id="pone.0156874.ref003"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Frischen</surname><given-names>A</given-names></name>, <name><surname>Bayliss</surname><given-names>AP</given-names></name>, <name><surname>Tipper</surname><given-names>SP</given-names></name>. <article-title>Gaze cueing of attention: visual attention, social cognition, and individual differences</article-title>. <source>Psychological bulletin</source>. <year>2007</year>;<volume>133</volume>(<issue>4</issue>):<fpage>694</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-2909.133.4.694">10.1037/0033-2909.133.4.694</ext-link></comment>
<pub-id pub-id-type="pmid">17592962</pub-id></mixed-citation></ref><ref id="pone.0156874.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Emery</surname><given-names>NJ</given-names></name>, <name><surname>Lorincz</surname><given-names>EN</given-names></name>, <name><surname>Perrett</surname><given-names>DI</given-names></name>, <name><surname>Oram</surname><given-names>MW</given-names></name>, <name><surname>Baker</surname><given-names>CI</given-names></name>. <article-title>Gaze following and joint attention in rhesus monkeys (Macaca mulatta)</article-title>. <source>Journal of comparative psychology</source>. <year>1997</year>;<volume>111</volume>(<issue>3</issue>):<fpage>286</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0735-7036.111.3.286">10.1037/0735-7036.111.3.286</ext-link></comment>
<pub-id pub-id-type="pmid">9286096</pub-id></mixed-citation></ref><ref id="pone.0156874.ref005"><label>5</label><mixed-citation publication-type="book">
<name><surname>Tomasello</surname><given-names>M</given-names></name>. <chapter-title>Joint attention as social cognition</chapter-title>
<source>Joint attention: Its origins and role in development</source>. <year>1995</year>; p. <fpage>103</fpage>&#x02013;<lpage>130</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref006"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Perrett</surname><given-names>D</given-names></name>, <name><surname>Smith</surname><given-names>P</given-names></name>, <name><surname>Potter</surname><given-names>D</given-names></name>, <name><surname>Mistlin</surname><given-names>A</given-names></name>, <name><surname>Head</surname><given-names>A</given-names></name>, <name><surname>Milner</surname><given-names>A</given-names></name>, <etal>et al</etal>
<article-title>Visual cells in the temporal cortex sensitive to face view and gaze direction</article-title>. <source>Proceedings of the Royal Society of London B: Biological Sciences</source>. <year>1985</year>;<volume>223</volume>(<issue>1232</issue>):<fpage>293</fpage>&#x02013;<lpage>317</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.1985.0003">10.1098/rspb.1985.0003</ext-link></comment>
<pub-id pub-id-type="pmid">2858100</pub-id></mixed-citation></ref><ref id="pone.0156874.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Engell</surname><given-names>AD</given-names></name>, <name><surname>Haxby</surname><given-names>JV</given-names></name>. <article-title>Facial expression and gaze-direction in human superior temporal sulcus</article-title>. <source>Neuropsychologia</source>. <year>2007</year>;<volume>45</volume>(<issue>14</issue>):<fpage>3234</fpage>&#x02013;<lpage>3241</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuropsychologia.2007.06.022">10.1016/j.neuropsychologia.2007.06.022</ext-link></comment>
<pub-id pub-id-type="pmid">17707444</pub-id></mixed-citation></ref><ref id="pone.0156874.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Senju</surname><given-names>A</given-names></name>, <name><surname>Johnson</surname><given-names>MH</given-names></name>. <article-title>The eye contact effect: mechanisms and development</article-title>. <source>Trends in cognitive sciences</source>. <year>2009</year>;<volume>13</volume>(<issue>3</issue>):<fpage>127</fpage>&#x02013;<lpage>134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2008.11.009">10.1016/j.tics.2008.11.009</ext-link></comment>
<pub-id pub-id-type="pmid">19217822</pub-id></mixed-citation></ref><ref id="pone.0156874.ref009"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Itier</surname><given-names>RJ</given-names></name>, <name><surname>Batty</surname><given-names>M</given-names></name>. <article-title>Neural bases of eye and gaze processing: the core of social cognition</article-title>. <source>Neuroscience &#x00026; Biobehavioral Reviews</source>. <year>2009</year>;<volume>33</volume>(<issue>6</issue>):<fpage>843</fpage>&#x02013;<lpage>863</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neubiorev.2009.02.004">10.1016/j.neubiorev.2009.02.004</ext-link></comment><pub-id pub-id-type="pmid">19428496</pub-id></mixed-citation></ref><ref id="pone.0156874.ref010"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Bavelas</surname><given-names>JB</given-names></name>, <name><surname>Coates</surname><given-names>L</given-names></name>, <name><surname>Johnson</surname><given-names>T</given-names></name>. <article-title>Listener responses as a collaborative process: The role of gaze</article-title>. <source>Journal of Communication</source>. <year>2002</year>;<volume>52</volume>(<issue>3</issue>):<fpage>566</fpage>&#x02013;<lpage>580</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1460-2466.2002.tb02562.x">10.1111/j.1460-2466.2002.tb02562.x</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref011"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Sebanz</surname><given-names>N</given-names></name>, <name><surname>Bekkering</surname><given-names>H</given-names></name>, <name><surname>Knoblich</surname><given-names>G</given-names></name>. <article-title>Joint action: bodies and minds moving together</article-title>. <source>Trends in cognitive sciences</source>. <year>2006</year>;<volume>10</volume>(<issue>2</issue>):<fpage>70</fpage>&#x02013;<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2005.12.009">10.1016/j.tics.2005.12.009</ext-link></comment>
<pub-id pub-id-type="pmid">16406326</pub-id></mixed-citation></ref><ref id="pone.0156874.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Sebanz</surname><given-names>N</given-names></name>, <name><surname>Knoblich</surname><given-names>G</given-names></name>. <article-title>Prediction in joint action: What, when, and where</article-title>. <source>Topics in Cognitive Science</source>. <year>2009</year>;<volume>1</volume>(<issue>2</issue>):<fpage>353</fpage>&#x02013;<lpage>367</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1756-8765.2009.01024.x">10.1111/j.1756-8765.2009.01024.x</ext-link></comment>
<pub-id pub-id-type="pmid">25164938</pub-id></mixed-citation></ref><ref id="pone.0156874.ref013"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Volcic</surname><given-names>R</given-names></name>, <name><surname>Lappe</surname><given-names>M</given-names></name>. <article-title>Keeping an eye on each other: gaze behaviour in joint action</article-title>. <source>Perception ECVP abstract</source>. <year>2009</year>;<volume>38</volume>:<fpage>174</fpage>&#x02013;<lpage>174</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref014"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Schmidt</surname><given-names>R</given-names></name>, <name><surname>Fitzpatrick</surname><given-names>P</given-names></name>, <name><surname>Caron</surname><given-names>R</given-names></name>, <name><surname>Mergeche</surname><given-names>J</given-names></name>. <article-title>Understanding social motor coordination</article-title>. <source>Human movement science</source>. <year>2011</year>;<volume>30</volume>(<issue>5</issue>):<fpage>834</fpage>&#x02013;<lpage>845</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.humov.2010.05.014">10.1016/j.humov.2010.05.014</ext-link></comment>
<pub-id pub-id-type="pmid">20817320</pub-id></mixed-citation></ref><ref id="pone.0156874.ref015"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Knoblich</surname><given-names>G</given-names></name>, <name><surname>Butterfill</surname><given-names>S</given-names></name>, <name><surname>Sebanz</surname><given-names>N</given-names></name>. <article-title>3 Psychological research on joint action: theory and data</article-title>. <source>Psychology of Learning and Motivation-Advances in Research and Theory</source>. <year>2011</year>;<volume>54</volume>:<fpage>59</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/B978-0-12-385527-5.00003-6">10.1016/B978-0-12-385527-5.00003-6</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref016"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Hove</surname><given-names>MJ</given-names></name>, <name><surname>Risen</surname><given-names>JL</given-names></name>. <article-title>It&#x02019;s all in the timing: Interpersonal synchrony increases affiliation</article-title>. <source>Social Cognition</source>. <year>2009</year>;<volume>27</volume>(<issue>6</issue>):<fpage>949</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1521/soco.2009.27.6.949">10.1521/soco.2009.27.6.949</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref017"><label>17</label><mixed-citation publication-type="journal">
<name><surname>van Schie</surname><given-names>HT</given-names></name>, <name><surname>van Waterschoot</surname><given-names>BM</given-names></name>, <name><surname>Bekkering</surname><given-names>H</given-names></name>. <article-title>Understanding action beyond imitation: reversed compatibility effects of action observation in imitation and joint action</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2008</year>;<volume>34</volume>(<issue>6</issue>):<fpage>1493</fpage>
<pub-id pub-id-type="pmid">19045988</pub-id></mixed-citation></ref><ref id="pone.0156874.ref018"><label>18</label><mixed-citation publication-type="journal">
<name><surname>Marsh</surname><given-names>KL</given-names></name>, <name><surname>Richardson</surname><given-names>MJ</given-names></name>, <name><surname>Schmidt</surname><given-names>RC</given-names></name>. <article-title>Social connection through joint action and interpersonal coordination</article-title>. <source>Topics in Cognitive Science</source>. <year>2009</year>;<volume>1</volume>(<issue>2</issue>):<fpage>320</fpage>&#x02013;<lpage>339</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1756-8765.2009.01022.x">10.1111/j.1756-8765.2009.01022.x</ext-link></comment>
<pub-id pub-id-type="pmid">25164936</pub-id></mixed-citation></ref><ref id="pone.0156874.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Valdesolo</surname><given-names>P</given-names></name>, <name><surname>Ouyang</surname><given-names>J</given-names></name>, <name><surname>DeSteno</surname><given-names>D</given-names></name>. <article-title>The rhythm of joint action: Synchrony promotes cooperative ability</article-title>. <source>Journal of Experimental Social Psychology</source>. <year>2010</year>;<volume>46</volume>(<issue>4</issue>):<fpage>693</fpage>&#x02013;<lpage>695</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jesp.2010.03.004">10.1016/j.jesp.2010.03.004</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref020"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Sebanz</surname><given-names>N</given-names></name>, <name><surname>Knoblich</surname><given-names>G</given-names></name>, <name><surname>Prinz</surname><given-names>W</given-names></name>. <article-title>How two share a task: corepresenting stimulus-response mappings</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2005</year>;<volume>31</volume>(<issue>6</issue>):<fpage>1234</fpage>
<pub-id pub-id-type="pmid">16366786</pub-id></mixed-citation></ref><ref id="pone.0156874.ref021"><label>21</label><mixed-citation publication-type="journal">
<name><surname>D&#x02019;Entremont</surname><given-names>B</given-names></name>, <name><surname>Hains</surname><given-names>S</given-names></name>, <name><surname>Muir</surname><given-names>D</given-names></name>. <article-title>A demonstration of gaze following in 3-to 6-month-olds</article-title>. <source>Infant Behavior and Development</source>. <year>1997</year>;<volume>20</volume>(<issue>4</issue>):<fpage>569</fpage>&#x02013;<lpage>572</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0163-6383(97)90048-5">10.1016/S0163-6383(97)90048-5</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref022"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Flanagan</surname><given-names>JR</given-names></name>, <name><surname>Johansson</surname><given-names>RS</given-names></name>. <article-title>Action plans used in action observation</article-title>. <source>Nature</source>. <year>2003</year>;<volume>424</volume>(<issue>6950</issue>):<fpage>769</fpage>&#x02013;<lpage>771</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature01861">10.1038/nature01861</ext-link></comment>
<pub-id pub-id-type="pmid">12917683</pub-id></mixed-citation></ref><ref id="pone.0156874.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Noy</surname><given-names>L</given-names></name>, <name><surname>Dekel</surname><given-names>E</given-names></name>, <name><surname>Alon</surname><given-names>U</given-names></name>. <article-title>The mirror game as a paradigm for studying the dynamics of two people improvising motion together</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>;<volume>108</volume>(<issue>52</issue>):<fpage>20947</fpage>&#x02013;<lpage>20952</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1108155108">10.1073/pnas.1108155108</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref024"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Hart</surname><given-names>Y</given-names></name>, <name><surname>Noy</surname><given-names>L</given-names></name>, <name><surname>Feniger-Schaal</surname><given-names>R</given-names></name>, <name><surname>Mayo</surname><given-names>AE</given-names></name>, <name><surname>Alon</surname><given-names>U</given-names></name>. <article-title>Individuality and togetherness in joint improvised motion</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>2</issue>):<fpage>e87213</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0087213">10.1371/journal.pone.0087213</ext-link></comment>
<pub-id pub-id-type="pmid">24533054</pub-id></mixed-citation></ref><ref id="pone.0156874.ref025"><label>25</label><mixed-citation publication-type="book">
<name><surname>Meadows</surname><given-names>MS</given-names></name>. <source>I, avatar: The culture and consequences of having a second life</source>. <publisher-name>New Riders</publisher-name>; <year>2007</year>.</mixed-citation></ref><ref id="pone.0156874.ref026"><label>26</label><mixed-citation publication-type="book">
<name><surname>Ishiguro</surname><given-names>H</given-names></name>. <chapter-title>Android science</chapter-title> In: <source>Robotics Research</source>. <publisher-name>Springer</publisher-name>; <year>2007</year> p. <fpage>118</fpage>&#x02013;<lpage>127</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref027"><label>27</label><mixed-citation publication-type="book">
<name><surname>Sakamoto</surname><given-names>D</given-names></name>, <name><surname>Kanda</surname><given-names>T</given-names></name>, <name><surname>Ono</surname><given-names>T</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>, <name><surname>Hagita</surname><given-names>N</given-names></name>. <chapter-title>Android as a telecommunication medium with a human-like presence</chapter-title> In: <source>Human-Robot Interaction (HRI), 2007 2nd ACM/IEEE International Conference on</source>. <publisher-name>IEEE</publisher-name>; <year>2007</year> p. <fpage>193</fpage>&#x02013;<lpage>200</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref028"><label>28</label><mixed-citation publication-type="book">
<name><surname>Minato</surname><given-names>T</given-names></name>, <name><surname>Shimada</surname><given-names>M</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>, <name><surname>Itakura</surname><given-names>S</given-names></name>. <chapter-title>Development of an android robot for studying human-robot interaction</chapter-title> In: <source>Innovations in applied artificial intelligence</source>. <publisher-name>Springer</publisher-name>; <year>2004</year> p. <fpage>424</fpage>&#x02013;<lpage>434</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref029"><label>29</label><mixed-citation publication-type="book">
<name><surname>Minato</surname><given-names>T</given-names></name>, <name><surname>Shimada</surname><given-names>M</given-names></name>, <name><surname>Itakura</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>. <chapter-title>Does gaze reveal the human likeness of an android?</chapter-title> In: <source>Development and Learning, 2005. Proceedings., The 4th International Conference on</source>. <publisher-name>IEEE</publisher-name>; <year>2005</year> p. <fpage>106</fpage>&#x02013;<lpage>111</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref030"><label>30</label><mixed-citation publication-type="journal">
<name><surname>Minato</surname><given-names>T</given-names></name>, <name><surname>Shimada</surname><given-names>M</given-names></name>, <name><surname>Itakura</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>. <article-title>Evaluating the human likeness of an android by comparing gaze behaviors elicited by the android and a person</article-title>. <source>Advanced Robotics</source>. <year>2006</year>;<volume>20</volume>(<issue>10</issue>):<fpage>1147</fpage>&#x02013;<lpage>1163</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1163/156855306778522505">10.1163/156855306778522505</ext-link></comment>
<pub-id pub-id-type="pmid">18985174</pub-id></mixed-citation></ref><ref id="pone.0156874.ref031"><label>31</label><mixed-citation publication-type="book">
<name><surname>Mutlu</surname><given-names>B</given-names></name>, <name><surname>Shiwa</surname><given-names>T</given-names></name>, <name><surname>Kanda</surname><given-names>T</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>, <name><surname>Hagita</surname><given-names>N</given-names></name>. <chapter-title>Footing in human-robot conversations: how robots might shape participant roles using gaze cues</chapter-title> In: <source>Proceedings of the 4th ACM/IEEE international conference on Human robot interaction</source>. <publisher-name>ACM</publisher-name>; <year>2009</year> p. <fpage>61</fpage>&#x02013;<lpage>68</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref032"><label>32</label><mixed-citation publication-type="journal">
<name><surname>Mutlu</surname><given-names>B</given-names></name>, <name><surname>Kanda</surname><given-names>T</given-names></name>, <name><surname>Forlizzi</surname><given-names>J</given-names></name>, <name><surname>Hodgins</surname><given-names>J</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>. <article-title>Conversational gaze mechanisms for humanlike robots</article-title>. <source>ACM Transactions on Interactive Intelligent Systems (TiiS)</source>. <year>2012</year>;<volume>1</volume>(<issue>2</issue>):<fpage>12</fpage>.</mixed-citation></ref><ref id="pone.0156874.ref033"><label>33</label><mixed-citation publication-type="book">
<name><surname>Andrist</surname><given-names>S</given-names></name>, <name><surname>Tan</surname><given-names>XZ</given-names></name>, <name><surname>Gleicher</surname><given-names>M</given-names></name>, <name><surname>Mutlu</surname><given-names>B</given-names></name>. <chapter-title>Conversational gaze aversion for humanlike robots</chapter-title> In: <source>Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction</source>. <publisher-name>ACM</publisher-name>; <year>2014</year> p. <fpage>25</fpage>&#x02013;<lpage>32</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref034"><label>34</label><mixed-citation publication-type="book">
<name><surname>Garau</surname><given-names>M</given-names></name>, <name><surname>Slater</surname><given-names>M</given-names></name>, <name><surname>Bee</surname><given-names>S</given-names></name>, <name><surname>Sasse</surname><given-names>MA</given-names></name>. <chapter-title>The impact of eye gaze on communication using humanoid avatars</chapter-title> In: <source>Proceedings of the SIGCHI conference on Human factors in computing systems</source>. <publisher-name>ACM</publisher-name>; <year>2001</year> p. <fpage>309</fpage>&#x02013;<lpage>316</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref035"><label>35</label><mixed-citation publication-type="book">
<name><surname>Garau</surname><given-names>M</given-names></name>, <name><surname>Slater</surname><given-names>M</given-names></name>, <name><surname>Vinayagamoorthy</surname><given-names>V</given-names></name>, <name><surname>Brogni</surname><given-names>A</given-names></name>, <name><surname>Steed</surname><given-names>A</given-names></name>, <name><surname>Sasse</surname><given-names>MA</given-names></name>. <chapter-title>The impact of avatar realism and eye gaze control on perceived quality of communication in a shared immersive virtual environment</chapter-title> In: <source>Proceedings of the SIGCHI conference on Human factors in computing systems</source>. <publisher-name>ACM</publisher-name>; <year>2003</year> p. <fpage>529</fpage>&#x02013;<lpage>536</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref036"><label>36</label><mixed-citation publication-type="book">
<name><surname>Bente</surname><given-names>G</given-names></name>, <name><surname>Eschenburg</surname><given-names>F</given-names></name>, <name><surname>Kr&#x000e4;mer</surname><given-names>NC</given-names></name>. <chapter-title>Virtual gaze. A pilot study on the effects of computer simulated gaze in avatar-based conversations</chapter-title> In: <source>Virtual reality</source>. <publisher-name>Springer</publisher-name>; <year>2007</year> p. <fpage>185</fpage>&#x02013;<lpage>194</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref037"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Bailenson</surname><given-names>JN</given-names></name>, <name><surname>Yee</surname><given-names>N</given-names></name>, <name><surname>Merget</surname><given-names>D</given-names></name>, <name><surname>Schroeder</surname><given-names>R</given-names></name>. <article-title>The effect of behavioral realism and form realism of real-time avatar faces on verbal disclosure, nonverbal disclosure, emotion recognition, and copresence in dyadic interaction</article-title>. <source>Presence: Teleoperators and Virtual Environments</source>. <year>2006</year>;<volume>15</volume>(<issue>4</issue>):<fpage>359</fpage>&#x02013;<lpage>372</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/pres.15.4.359">10.1162/pres.15.4.359</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref038"><label>38</label><mixed-citation publication-type="journal">
<name><surname>Mason</surname><given-names>MF</given-names></name>, <name><surname>Tatkow</surname><given-names>EP</given-names></name>, <name><surname>Macrae</surname><given-names>CN</given-names></name>. <article-title>The look of love gaze shifts and person perception</article-title>. <source>Psychological Science</source>. <year>2005</year>;<volume>16</volume>(<issue>3</issue>):<fpage>236</fpage>&#x02013;<lpage>239</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.0956-7976.2005.00809.x">10.1111/j.0956-7976.2005.00809.x</ext-link></comment>
<pub-id pub-id-type="pmid">15733205</pub-id></mixed-citation></ref><ref id="pone.0156874.ref039"><label>39</label><mixed-citation publication-type="book">
<name><surname>Steptoe</surname><given-names>W</given-names></name>, <name><surname>Wolff</surname><given-names>R</given-names></name>, <name><surname>Murgia</surname><given-names>A</given-names></name>, <name><surname>Guimaraes</surname><given-names>E</given-names></name>, <name><surname>Rae</surname><given-names>J</given-names></name>, <name><surname>Sharkey</surname><given-names>P</given-names></name>, <etal>et al</etal>
<chapter-title>Eye-tracking for avatar eye-gaze and interactional analysis in immersive collaborative virtual environments</chapter-title> In: <source>Proceedings of the 2008 ACM conference on Computer supported cooperative work</source>. <publisher-name>ACM</publisher-name>; <year>2008</year> p. <fpage>197</fpage>&#x02013;<lpage>200</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref040"><label>40</label><mixed-citation publication-type="book">
<name><surname>Yoshikawa</surname><given-names>Y</given-names></name>, <name><surname>Shinozawa</surname><given-names>K</given-names></name>, <name><surname>Ishiguro</surname><given-names>H</given-names></name>, <name><surname>Hagita</surname><given-names>N</given-names></name>, <name><surname>Miyamoto</surname><given-names>T</given-names></name>. <chapter-title>Responsive Robot Gaze to Interaction Partner</chapter-title> In: <source>Robotics: Science and systems</source>; <year>2006</year>.</mixed-citation></ref><ref id="pone.0156874.ref041"><label>41</label><mixed-citation publication-type="journal">
<name><surname>MacDorman</surname><given-names>KF</given-names></name>, <name><surname>Green</surname><given-names>RD</given-names></name>, <name><surname>Ho</surname><given-names>CC</given-names></name>, <name><surname>Koch</surname><given-names>CT</given-names></name>. <article-title>Too real for comfort? Uncanny responses to computer generated faces</article-title>. <source>Computers in human behavior</source>. <year>2009</year>;<volume>25</volume>(<issue>3</issue>):<fpage>695</fpage>&#x02013;<lpage>710</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.chb.2008.12.026">10.1016/j.chb.2008.12.026</ext-link></comment>
<pub-id pub-id-type="pmid">25506126</pub-id></mixed-citation></ref><ref id="pone.0156874.ref042"><label>42</label><mixed-citation publication-type="journal">
<name><surname>Mori</surname><given-names>M</given-names></name>, <name><surname>MacDorman</surname><given-names>KF</given-names></name>, <name><surname>Kageki</surname><given-names>N</given-names></name>. <article-title>The uncanny valley [from the field]</article-title>. <source>Robotics &#x00026; Automation Magazine, IEEE</source>. <year>2012</year>;<volume>19</volume>(<issue>2</issue>):<fpage>98</fpage>&#x02013;<lpage>100</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/MRA.2012.2192811">10.1109/MRA.2012.2192811</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref043"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Byrne</surname><given-names>D</given-names></name>. <article-title>Interpersonal attraction and attitude similarity</article-title>. <source>The Journal of Abnormal and Social Psychology</source>. <year>1961</year>;<volume>62</volume>(<issue>3</issue>):<fpage>713</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0044721">10.1037/h0044721</ext-link></comment>
<pub-id pub-id-type="pmid">13875334</pub-id></mixed-citation></ref><ref id="pone.0156874.ref044"><label>44</label><mixed-citation publication-type="journal">
<name><surname>Lydon</surname><given-names>JE</given-names></name>, <name><surname>Jamieson</surname><given-names>DW</given-names></name>, <name><surname>Zanna</surname><given-names>MP</given-names></name>. <article-title>Interpersonal similarity and the social and intellectual dimensions of first impressions</article-title>. <source>Social cognition</source>. <year>1988</year>;<volume>6</volume>(<issue>4</issue>):<fpage>269</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1521/soco.1988.6.4.269">10.1521/soco.1988.6.4.269</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref045"><label>45</label><mixed-citation publication-type="book">
<name><surname>Ramaiah</surname><given-names>M</given-names></name>, <name><surname>Vijay</surname><given-names>A</given-names></name>, <name><surname>Sharma</surname><given-names>G</given-names></name>, <name><surname>Mukerjee</surname><given-names>A</given-names></name>. <chapter-title>Head motion animation using avatar gaze space</chapter-title> In: <source>Virtual Reality (VR), 2013 IEEE</source>. <publisher-name>IEEE</publisher-name>; <year>2013</year> p. <fpage>95</fpage>&#x02013;<lpage>96</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref046"><label>46</label><mixed-citation publication-type="book">
<name><surname>Kipp</surname><given-names>M</given-names></name>, <name><surname>Gebhard</surname><given-names>P</given-names></name>. <chapter-title>Igaze: Studying reactive gaze behavior in semi-immersive human-avatar interactions</chapter-title> In: <source>Intelligent Virtual Agents</source>. <publisher-name>Springer</publisher-name>; <year>2008</year> p. <fpage>191</fpage>&#x02013;<lpage>199</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref047"><label>47</label><mixed-citation publication-type="journal">
<name><surname>Fu</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>R</given-names></name>, <name><surname>Huang</surname><given-names>TS</given-names></name>, <name><surname>Danielsen</surname><given-names>M</given-names></name>. <article-title>Real-time multimodal human&#x02013;avatar interaction</article-title>. <source>Circuits and Systems for Video Technology, IEEE Transactions on</source>. <year>2008</year>;<volume>18</volume>(<issue>4</issue>):<fpage>467</fpage>&#x02013;<lpage>477</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TCSVT.2008.918441">10.1109/TCSVT.2008.918441</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref048"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Liesker</surname><given-names>H</given-names></name>, <name><surname>Brenner</surname><given-names>E</given-names></name>, <name><surname>Smeets</surname><given-names>JB</given-names></name>. <article-title>Combining eye and hand in search is suboptimal</article-title>. <source>Experimental brain research</source>. <year>2009</year>;<volume>197</volume>(<issue>4</issue>):<fpage>395</fpage>&#x02013;<lpage>401</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-009-1928-9">10.1007/s00221-009-1928-9</ext-link></comment>
<pub-id pub-id-type="pmid">19590859</pub-id></mixed-citation></ref><ref id="pone.0156874.ref049"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Bowman</surname><given-names>MC</given-names></name>, <name><surname>Johannson</surname><given-names>RS</given-names></name>, <name><surname>Flanagan</surname><given-names>JR</given-names></name>. <article-title>Eye&#x02013;hand coordination in a sequential target contact task</article-title>. <source>Experimental brain research</source>. <year>2009</year>;<volume>195</volume>(<issue>2</issue>):<fpage>273</fpage>&#x02013;<lpage>283</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-009-1781-x">10.1007/s00221-009-1781-x</ext-link></comment>
<pub-id pub-id-type="pmid">19357841</pub-id></mixed-citation></ref><ref id="pone.0156874.ref050"><label>50</label><mixed-citation publication-type="journal">
<name><surname>Coen-Cagli</surname><given-names>R</given-names></name>, <name><surname>Coraggio</surname><given-names>P</given-names></name>, <name><surname>Napoletano</surname><given-names>P</given-names></name>, <name><surname>Schwartz</surname><given-names>O</given-names></name>, <name><surname>Ferraro</surname><given-names>M</given-names></name>, <name><surname>Boccignone</surname><given-names>G</given-names></name>. <article-title>Visuomotor characterization of eye movements in a drawing task</article-title>. <source>Vision research</source>. <year>2009</year>;<volume>49</volume>(<issue>8</issue>):<fpage>810</fpage>&#x02013;<lpage>818</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.visres.2009.02.016">10.1016/j.visres.2009.02.016</ext-link></comment>
<pub-id pub-id-type="pmid">19268685</pub-id></mixed-citation></ref><ref id="pone.0156874.ref051"><label>51</label><mixed-citation publication-type="journal">
<name><surname>Lazzari</surname><given-names>S</given-names></name>, <name><surname>Mottet</surname><given-names>D</given-names></name>, <name><surname>Vercher</surname><given-names>JL</given-names></name>. <article-title>Eye-hand coordination in rhythmical pointing</article-title>. <source>Journal of motor behavior</source>. <year>2009</year>;<volume>41</volume>(<issue>4</issue>):<fpage>294</fpage>&#x02013;<lpage>304</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3200/JMBR.41.4.294-304">10.3200/JMBR.41.4.294-304</ext-link></comment>
<pub-id pub-id-type="pmid">19508956</pub-id></mixed-citation></ref><ref id="pone.0156874.ref052"><label>52</label><mixed-citation publication-type="other">Khoramshahi M, Shukla A, Billard A. Cognitive mechanism in synchronized motion: an internal predictive model for manual tracking control. In: The 2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC2014). EPFL-CONF-200474; 2014.</mixed-citation></ref><ref id="pone.0156874.ref053"><label>53</label><mixed-citation publication-type="book">
<name><surname>Tikhanoff</surname><given-names>V</given-names></name>, <name><surname>Cangelosi</surname><given-names>A</given-names></name>, <name><surname>Fitzpatrick</surname><given-names>P</given-names></name>, <name><surname>Metta</surname><given-names>G</given-names></name>, <name><surname>Natale</surname><given-names>L</given-names></name>, <name><surname>Nori</surname><given-names>F</given-names></name>. <chapter-title>An open-source simulator for cognitive robotics research: the prototype of the iCub humanoid robot simulator</chapter-title> In: <source>Proceedings of the 8th workshop on performance metrics for intelligent systems</source>. <publisher-name>ACM</publisher-name>; <year>2008</year> p. <fpage>57</fpage>&#x02013;<lpage>61</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref054"><label>54</label><mixed-citation publication-type="book">Pattacini U. Modular cartesian controllers for humanoid robots: Design and implementation on the icub. Ph. D. dissertation, RBCS, Italian Institute of Technology, Genova; 2011.</mixed-citation></ref><ref id="pone.0156874.ref055"><label>55</label><mixed-citation publication-type="other">NaturalPoint. Optitrack;. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.optitrack.com">https://www.optitrack.com</ext-link>.</mixed-citation></ref><ref id="pone.0156874.ref056"><label>56</label><mixed-citation publication-type="journal">
<name><surname>Grinsted</surname><given-names>A</given-names></name>, <name><surname>Moore</surname><given-names>JC</given-names></name>, <name><surname>Jevrejeva</surname><given-names>S</given-names></name>. <article-title>Application of the cross wavelet transform and wavelet coherence to geophysical time series</article-title>. <source>Nonlinear processes in geophysics</source>. <year>2004</year>;<volume>11</volume>(<issue>5/6</issue>):<fpage>561</fpage>&#x02013;<lpage>566</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5194/npg-11-561-2004">10.5194/npg-11-561-2004</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref057"><label>57</label><mixed-citation publication-type="journal">
<name><surname>Torrence</surname><given-names>C</given-names></name>, <name><surname>Compo</surname><given-names>GP</given-names></name>. <article-title>A practical guide to wavelet analysis</article-title>. <source>Bulletin of the American Meteorological society</source>. <year>1998</year>;<volume>79</volume>(<issue>1</issue>):<fpage>61</fpage>&#x02013;<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1175/1520-0477(1998)079%3C0061:APGTWA%3E2.0.CO;2">10.1175/1520-0477(1998)079%3C0061:APGTWA%3E2.0.CO;2</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref058"><label>58</label><mixed-citation publication-type="journal">
<name><surname>Noris</surname><given-names>B</given-names></name>, <name><surname>Keller</surname><given-names>JB</given-names></name>, <name><surname>Billard</surname><given-names>A</given-names></name>. <article-title>A wearable gaze tracking system for children in unconstrained environments</article-title>. <source>Computer Vision and Image Understanding</source>. <year>2011</year>;<volume>115</volume>(<issue>4</issue>):<fpage>476</fpage>&#x02013;<lpage>486</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cviu.2010.11.013">10.1016/j.cviu.2010.11.013</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref059"><label>59</label><mixed-citation publication-type="book">
<name><surname>Cazzato</surname><given-names>D</given-names></name>, <name><surname>Mazzeo</surname><given-names>PL</given-names></name>, <name><surname>Spagnolo</surname><given-names>P</given-names></name>, <name><surname>Distante</surname><given-names>C</given-names></name>. <chapter-title>Automatic Joint Attention Detection During Interaction with a Humanoid Robot</chapter-title> In: <source>Social Robotics</source>. <publisher-name>Springer</publisher-name>; <year>2015</year> p. <fpage>124</fpage>&#x02013;<lpage>134</lpage>.</mixed-citation></ref><ref id="pone.0156874.ref060"><label>60</label><mixed-citation publication-type="book">Zhao X, Cusimano C, Malle BF. Do People Spontaneously Take a Robot&#x02019;s Visual Perspective? In: HRI (Extended Abstracts); 2015. p. 133&#x02013;134.</mixed-citation></ref><ref id="pone.0156874.ref061"><label>61</label><mixed-citation publication-type="journal">
<name><surname>Jayasekera</surname><given-names>A</given-names></name>, <name><surname>Hellewell</surname><given-names>J</given-names></name>, <name><surname>Perrett</surname><given-names>D</given-names></name>, <name><surname>Deakin</surname><given-names>J</given-names></name>. <article-title>The perception of gaze and attention in schizophrenia</article-title>. <source>Schizophrenia Research</source>. <year>1996</year>;<volume>18</volume>(<issue>2</issue>):<fpage>218</fpage>&#x02013;<lpage>219</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0920-9964(96)85674-X">10.1016/0920-9964(96)85674-X</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref062"><label>62</label><mixed-citation publication-type="journal">
<name><surname>Langdon</surname><given-names>RA</given-names></name>, <name><surname>Ward</surname><given-names>PB</given-names></name>. <article-title>The eyes have it: evidence of heightened sensitivity to other people&#x02019;s eye-gaze in schizophrenia</article-title>. <source>Schizophrenia Research</source>. <year>2012</year>;<volume>136</volume>:<fpage>S362</fpage>&#x02013;<lpage>S363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0920-9964(12)71058-7">10.1016/S0920-9964(12)71058-7</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref063"><label>63</label><mixed-citation publication-type="journal">
<name><surname>Roux</surname><given-names>P</given-names></name>, <name><surname>d&#x02019;Arc</surname><given-names>BF</given-names></name>, <name><surname>Passerieux</surname><given-names>C</given-names></name>, <name><surname>Ramus</surname><given-names>F</given-names></name>. <source>Is the Theory of Mind deficit observed in visual paradigms in schizophrenia explained by an impaired attention toward gaze orientation? Schizophrenia research</source>. <year>2014</year>;<volume>157</volume>(<issue>1</issue>):<fpage>78</fpage>&#x02013;<lpage>83</lpage>. <pub-id pub-id-type="pmid">24857238</pub-id></mixed-citation></ref><ref id="pone.0156874.ref064"><label>64</label><mixed-citation publication-type="journal">
<name><surname>Paus</surname><given-names>T</given-names></name>. <article-title>Two modes of central gaze fixation maintenance and oculomotor distractibility in schizophrenics</article-title>. <source>Schizophrenia research</source>. <year>1991</year>;<volume>5</volume>(<issue>2</issue>):<fpage>145</fpage>&#x02013;<lpage>152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0920-9964(91)90041-O">10.1016/0920-9964(91)90041-O</ext-link></comment>
<pub-id pub-id-type="pmid">1931807</pub-id></mixed-citation></ref><ref id="pone.0156874.ref065"><label>65</label><mixed-citation publication-type="journal">
<name><surname>Fitzpatrick</surname><given-names>PD</given-names></name>, <name><surname>Richardson</surname><given-names>R</given-names></name>, <name><surname>Schmidt</surname><given-names>M</given-names></name>. <article-title>Dynamical methods for evaluating the time-dependent unfolding of social coordination in children with autism</article-title>. <source>Frontiers in Integrative Neuroscience</source>. <year>2013</year>;<volume>7</volume>(<issue>21.10</issue>):<fpage>3389</fpage>.</mixed-citation></ref><ref id="pone.0156874.ref066"><label>66</label><mixed-citation publication-type="journal">
<name><surname>Raffard</surname><given-names>S</given-names></name>, <name><surname>Salesse</surname><given-names>RN</given-names></name>, <name><surname>Marin</surname><given-names>L</given-names></name>, <name><surname>Del-Monte</surname><given-names>J</given-names></name>, <name><surname>Schmidt</surname><given-names>RC</given-names></name>, <name><surname>Varlet</surname><given-names>M</given-names></name>, <etal>et al</etal>
<article-title>Social priming enhances interpersonal synchronization and feeling of connectedness towards schizophrenia patients</article-title>. <source>Scientific reports</source>. <year>2015</year>;<volume>5</volume>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/srep08156">10.1038/srep08156</ext-link></comment>
<pub-id pub-id-type="pmid">25640605</pub-id></mixed-citation></ref><ref id="pone.0156874.ref067"><label>67</label><mixed-citation publication-type="journal">
<name><surname>Varlet</surname><given-names>M</given-names></name>, <name><surname>Marin</surname><given-names>L</given-names></name>, <name><surname>Raffard</surname><given-names>S</given-names></name>, <name><surname>Schmidt</surname><given-names>RC</given-names></name>, <name><surname>Capdevielle</surname><given-names>D</given-names></name>, <name><surname>Boulenger</surname><given-names>JP</given-names></name>, <etal>et al</etal>
<article-title>Impairments of social motor coordination in schizophrenia</article-title>. <source>PLoS One</source>. <year>2012</year>;<volume>7</volume>(<issue>1</issue>):<fpage>e29772</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0029772">10.1371/journal.pone.0029772</ext-link></comment>
<pub-id pub-id-type="pmid">22272247</pub-id></mixed-citation></ref><ref id="pone.0156874.ref068"><label>68</label><mixed-citation publication-type="journal">
<name><surname>Varlet</surname><given-names>M</given-names></name>, <name><surname>Filippeschi</surname><given-names>A</given-names></name>, <name><surname>Ben-sadoun</surname><given-names>G</given-names></name>, <name><surname>Ratto</surname><given-names>M</given-names></name>, <name><surname>Marin</surname><given-names>L</given-names></name>, <name><surname>Ruffaldi</surname><given-names>E</given-names></name>, <etal>et al</etal>
<article-title>Virtual reality as a tool to learn interpersonal coordination: Example of team rowing</article-title>. <source>PRESENCE: Teleoperators and Virtual Environments</source>. <year>2013</year>;<volume>22</volume>(<issue>3</issue>):<fpage>202</fpage>&#x02013;<lpage>215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/PRES_a_00151">10.1162/PRES_a_00151</ext-link></comment></mixed-citation></ref><ref id="pone.0156874.ref069"><label>69</label><mixed-citation publication-type="journal">
<name><surname>Silverstein</surname><given-names>SM</given-names></name>, <name><surname>Menditto</surname><given-names>AA</given-names></name>, <name><surname>Stuve</surname><given-names>P</given-names></name>. <article-title>Shaping attention span: An operant conditioning procedure to improve neurocognition and functioning in schizophrenia</article-title>. <source>Schizophrenia Bulletin</source>. <year>2001</year>;<volume>27</volume>(<issue>2</issue>):<fpage>247</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/oxfordjournals.schbul.a006871">10.1093/oxfordjournals.schbul.a006871</ext-link></comment>
<pub-id pub-id-type="pmid">11354592</pub-id></mixed-citation></ref><ref id="pone.0156874.ref070"><label>70</label><mixed-citation publication-type="journal">
<name><surname>Combs</surname><given-names>DR</given-names></name>, <name><surname>Tosheva</surname><given-names>A</given-names></name>, <name><surname>Penn</surname><given-names>DL</given-names></name>, <name><surname>Basso</surname><given-names>MR</given-names></name>, <name><surname>Wanner</surname><given-names>JL</given-names></name>, <name><surname>Laib</surname><given-names>K</given-names></name>. <article-title>Attentional-shaping as a means to improve emotion perception deficits in schizophrenia</article-title>. <source>Schizophrenia research</source>. <year>2008</year>;<volume>105</volume>(<issue>1</issue>):<fpage>68</fpage>&#x02013;<lpage>77</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.schres.2008.05.018">10.1016/j.schres.2008.05.018</ext-link></comment>
<pub-id pub-id-type="pmid">18585899</pub-id></mixed-citation></ref><ref id="pone.0156874.ref071"><label>71</label><mixed-citation publication-type="journal">
<name><surname>Hooker</surname><given-names>CI</given-names></name>, <name><surname>Bruce</surname><given-names>L</given-names></name>, <name><surname>Fisher</surname><given-names>M</given-names></name>, <name><surname>Verosky</surname><given-names>SC</given-names></name>, <name><surname>Miyakawa</surname><given-names>A</given-names></name>, <name><surname>Vinogradov</surname><given-names>S</given-names></name>. <article-title>Neural activity during emotion recognition after combined cognitive plus social cognitive training in schizophrenia</article-title>. <source>Schizophrenia research</source>. <year>2012</year>;<volume>139</volume>(<issue>1</issue>):<fpage>53</fpage>&#x02013;<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.schres.2012.05.009">10.1016/j.schres.2012.05.009</ext-link></comment>
<pub-id pub-id-type="pmid">22695257</pub-id></mixed-citation></ref><ref id="pone.0156874.ref072"><label>72</label><mixed-citation publication-type="journal">
<name><surname>Drusch</surname><given-names>K</given-names></name>, <name><surname>Stroth</surname><given-names>S</given-names></name>, <name><surname>Kamp</surname><given-names>D</given-names></name>, <name><surname>Frommann</surname><given-names>N</given-names></name>, <name><surname>W&#x000f6;lwer</surname><given-names>W</given-names></name>. <article-title>Effects of Training of Affect Recognition on the recognition and visual exploration of emotional faces in schizophrenia</article-title>. <source>Schizophrenia research</source>. <year>2014</year>;<volume>159</volume>(<issue>2</issue>):<fpage>485</fpage>&#x02013;<lpage>490</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.schres.2014.09.003">10.1016/j.schres.2014.09.003</ext-link></comment>
<pub-id pub-id-type="pmid">25248938</pub-id></mixed-citation></ref></ref-list></back></article>
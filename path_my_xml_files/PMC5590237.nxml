<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Trials</journal-id><journal-id journal-id-type="iso-abbrev">Trials</journal-id><journal-title-group><journal-title>Trials</journal-title></journal-title-group><issn pub-type="epub">1745-6215</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28882167</article-id><article-id pub-id-type="pmc">5590237</article-id><article-id pub-id-type="publisher-id">2159</article-id><article-id pub-id-type="doi">10.1186/s13063-017-2159-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Methodology</subject></subj-group></article-categories><title-group><article-title>Implementation and results of an integrated data quality assurance protocol in a randomized controlled trial in Uttar Pradesh, India</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Gass</surname><given-names>Jonathon D.</given-names><suffix>Jr.</suffix></name><address><email>jonathon.gass@gmail.com</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Misra</surname><given-names>Anamika</given-names></name><address><email>misraanamika2@gmail.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Yadav</surname><given-names>Mahendra Nath Singh</given-names></name><address><email>mahendra07mph@gmail.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Sana</surname><given-names>Fatima</given-names></name><address><email>fsana.25@gmail.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Singh</surname><given-names>Chetna</given-names></name><address><email>singhchetna09@gmail.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Mankar</surname><given-names>Anup</given-names></name><address><email>amankar@ariadnelabs.org</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Neal</surname><given-names>Brandon J.</given-names></name><address><email>bneal@ariadnelabs.org</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Fisher-Bowman</surname><given-names>Jennifer</given-names></name><address><email>jfisherbowman@ariadnelabs.org</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Maisonneuve</surname><given-names>Jenny</given-names></name><address><email>jmaisonneuve@ariadnelabs.org</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Delaney</surname><given-names>Megan Marx</given-names></name><address><email>mmarxdelaney@ariadnelabs.org</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kumar</surname><given-names>Krishan</given-names></name><address><email>krishkulluhp@gmail.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Singh</surname><given-names>Vinay Pratap</given-names></name><address><email>vpsingh@psi.org.in</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Sharma</surname><given-names>Narender</given-names></name><address><email>nar.sharma@gmail.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gawande</surname><given-names>Atul</given-names></name><address><email>agawande@ariadnelabs.org</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Semrau</surname><given-names>Katherine</given-names></name><address><email>ksemrau@ariadnelabs.org</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hirschhorn</surname><given-names>Lisa R.</given-names></name><address><email>lisa.hirschhorn@northwestern.edu</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution>Ariadne Labs of the Brigham &#x00026; Women&#x02019;s Hospital and Harvard T.H. Chan School of Public Health, </institution></institution-wrap>Boston, MA USA </aff><aff id="Aff2"><label>2</label>Population Services International, New Delhi, India </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2299 3507</institution-id><institution-id institution-id-type="GRID">grid.16753.36</institution-id><institution>Ariadne Labs, Harvard T.H. Chan School of Public Health, Brigham &#x00026; Women&#x02019;s Hospital, </institution><institution>Northwestern University Feinberg School of Medicine, </institution></institution-wrap>Arthur J. Rubloff Building 420 East Superior Street, Chicago, 60611 Illinois USA </aff></contrib-group><pub-date pub-type="epub"><day>7</day><month>9</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>7</day><month>9</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>18</volume><elocation-id>418</elocation-id><history><date date-type="received"><day>22</day><month>2</month><year>2017</year></date><date date-type="accepted"><day>19</day><month>8</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s). 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">There are few published standards or methodological guidelines for integrating Data Quality Assurance (DQA) protocols into large-scale health systems research trials, especially in resource-limited settings. The BetterBirth Trial is a matched-pair, cluster-randomized controlled trial (RCT) of the BetterBirth Program, which seeks to improve quality of facility-based deliveries and reduce 7-day maternal and neonatal mortality and maternal morbidity in Uttar Pradesh, India. In the trial, over 6300 deliveries were observed and over 153,000 mother-baby pairs across 120 study sites were followed to assess health outcomes. We designed and implemented a robust and integrated DQA system to sustain high-quality data throughout the trial.</p></sec><sec><title>Methods</title><p id="Par2">We designed the Data Quality Monitoring and Improvement System (DQMIS) to reinforce six dimensions of data quality: accuracy, reliability, timeliness, completeness, precision, and integrity. The DQMIS was comprised of five functional components: 1) a monitoring and evaluation team to support the system; 2) a DQA protocol, including data collection audits and targets, rapid data feedback, and supportive supervision; 3) training; 4) standard operating procedures for data collection; and 5) an electronic data collection and reporting system. Routine audits by supervisors included double data entry, simultaneous delivery observations, and review of recorded calls to patients. Data feedback reports identified errors automatically, facilitating supportive supervision through a continuous quality improvement model.</p></sec><sec><title>Results</title><p id="Par3">The five functional components of the DQMIS successfully reinforced data reliability, timeliness, completeness, precision, and integrity. The DQMIS also resulted in 98.33% accuracy across all data collection activities in the trial. All data collection activities demonstrated improvement in accuracy throughout implementation. Data collectors demonstrated a statistically significant (<italic>p</italic>&#x02009;=&#x02009;0.0004) increase in accuracy throughout consecutive audits. The DQMIS was successful, despite an increase from 20 to 130 data collectors.</p></sec><sec><title>Conclusions</title><p id="Par4">In the absence of widely disseminated data quality methods and standards for large RCT interventions in limited-resource settings, we developed an integrated DQA system, combining auditing, rapid data feedback, and supportive supervision, which ensured high-quality data and could serve as a model for future health systems research trials. Future efforts should focus on standardization of DQA processes for health systems research.</p></sec><sec><title>Trial Registration</title><p id="Par5">ClinicalTrials.gov identifier, <ext-link ext-link-type="uri" xlink:href="https://clinicaltrials.gov/ct2/show/NCT02148952">NCT02148952</ext-link>. Registered on 13 February 2014.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Data Quality Assurance (DQA)</kwd><kwd>Safe Childbirth Checklist (SCC)</kwd><kwd>Maternal morbidity</kwd><kwd>Maternal and perinatal mortality</kwd><kwd>Data feedback</kwd><kwd>Supportive supervision</kwd><kwd>Patient-reported outcomes</kwd><kwd>Uttar Pradesh</kwd><kwd>India</kwd><kwd>Data accuracy</kwd><kwd>Randomized control trial (RCT)</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000865</institution-id><institution>Bill and Melinda Gates Foundation</institution></institution-wrap></funding-source><award-id>OPP1017378</award-id><principal-award-recipient><name><surname>Gawande</surname><given-names>Atul</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2017</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Background</title><p id="Par13">There are no widely accepted universal standards for data quality in health systems research, despite several articles and reports emphasizing their importance [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref>]. While there are known methods for assessing data quality in patient registries and health information systems, there are few published methodological guidelines for integrating Data Quality Assurance (DQA) protocols into large-scale health systems research trials, especially in resource-limited settings [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref>]. High-quality data are crucial in health systems research as scientific recommendations based on those data have implications for policy and practice [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR8">8</xref>].</p><p id="Par14">Error rates in clinical trials have been described in the literature ranging from 2.8% to 26.9% across multiple studies [<xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref>]. There are no minimally acceptable data-quality standards included in US Federal guidelines for clinical research; therefore, researchers establish their own acceptable error rates and measurement methods [<xref ref-type="bibr" rid="CR10">10</xref>]. Onsite monitoring of clinical trial sites and database audits occur; however, published systematic approaches to field verification of data quality during trial implementation are rare, and their absence limits opportunities to remediate data-quality issues in real time [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]. Clinical trials often require multiple data collection activities, all subject to different sources of error; therefore, DQA activities often must target multiple dimensions of quality [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]. DQA methods must address all possible sources of error in an integrated, systematic, and supportive manner to promote continuous data quality improvement throughout implementation [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>].</p><p id="Par15">The BetterBirth Trial is a matched-pair, cluster-randomized controlled trial (RCT) of the BetterBirth Program, which uses coaching-based implementation of the World Health Organization (WHO) Safe Childbirth Checklist to improve quality of facility-based deliveries in Uttar Pradesh, India, and to reduce 7-day maternal and neonatal morbidity and mortality [<xref ref-type="bibr" rid="CR26">26</xref>]. This complex and large-scale trial includes three sources of data: patient registry, delivery observation, and post-delivery patient-reported outcomes data. In the trial, over 6300 deliveries were observed, and over 153,000 mother-baby pairs across 120 study sites were followed to assess health outcomes [<xref ref-type="bibr" rid="CR27">27</xref>]. We designed and implemented the Data Quality Monitoring and Improvement System (DQMIS), a robust, multi-component, and integrated DQA mechanism, to ensure high-quality data throughout study implementation. This study aimed to evaluate the DQMIS and its effectiveness for ensuring data quality. In the absence of published approaches to field verification of data quality during trials, here we report the implementation components and results of an integrated DQA system.</p></sec><sec id="Sec2" sec-type="materials|methods"><title>Methods</title><sec id="Sec3"><title>Data collection activities in the BetterBirth Trial</title><p id="Par16">The trial included five data collection activities related to the three sources of data. 1) Essential birth practices performed by birth attendants during deliveries were observed and recorded by facility-based observers. Following observations, 2) the observation data recorded on paper forms were transferred to the electronic data entry app. 3) Patient data, sourced from paper-based facility registers, were extracted by facility-based data collectors and entered into a paper-based study register. Following data extraction, 4) patient data were transferred to the electronic data entry app by facility-based data collectors. Finally, 5) call center staff contacted patients to assess maternal and neonatal mortality and seven maternal morbidities using a standardized questionnaire [<xref ref-type="bibr" rid="CR27">27</xref>] and entered these data directly into the electronic data collection app.</p></sec><sec id="Sec4"><title>Design of the DQMIS</title><p id="Par17">We designed the DQMIS to reinforce six dimensions of data quality [<xref ref-type="bibr" rid="CR28">28</xref>] (Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>). The DQMIS comprised of five complementary functional components, including: 1) a monitoring and evaluation (M&#x00026;E) team to support data management and quality; 2) a DQA protocol, including data collection audits and targets, rapid data feedback, and supportive supervision; 3) training on data quality; 4) standard operating procedures (SOPs) for data collection; and 5) an electronic data collection and reporting system (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Operational definitions for six dimensions of data quality, adapted from Brown W, et al. [<xref ref-type="bibr" rid="CR28">28</xref>]</p></caption><table frame="hsides" rules="groups"><tbody><tr><td>Accuracy</td><td>Data are correct and reflect the truth</td></tr><tr><td>Reliability</td><td>Data are consistently collected and entered in a standard way across data collectors</td></tr><tr><td>Timeliness</td><td>Data are current due to routine data entry and available for near real-time reporting</td></tr><tr><td>Completeness</td><td>There are no missing essential data elements</td></tr><tr><td>Precision</td><td>Data have necessary detail to address research questions and management requirements</td></tr><tr><td>Integrity</td><td>Data are secure and protected from bias or manipulation</td></tr></tbody></table></table-wrap>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Functional components of the DQMIS and corresponding dimensions of data quality</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="6">Dimensions of data quality</th></tr><tr><th>Functional components of the DQMIS</th><th>Accuracy</th><th>Reliability</th><th>Timeliness</th><th>Completeness</th><th>Precision</th><th>Integrity</th></tr></thead><tbody><tr><td>M&#x00026;E team to support data management and quality</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>SOPs and tools for data collection</td><td>X</td><td>X</td><td>X</td><td/><td>X</td><td>X</td></tr><tr><td>Training for data quality</td><td>X</td><td>X</td><td>X</td><td/><td/><td>X</td></tr><tr><td>Electronic data collection and reporting system</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>DQA protocol, including data collection audits, rapid data feedback, and supportive supervision</td><td>X</td><td>X</td><td/><td/><td/><td/></tr></tbody></table><table-wrap-foot><p>
<italic>DQA</italic> Data Quality Assurance, <italic>DQMIS</italic> Data Quality Monitoring and Improvement System, <italic>M&#x00026;E</italic> Monitoring and evaluation</p></table-wrap-foot></table-wrap>
</p><sec id="Sec5"><title>Functional components of the DQMIS</title><sec id="FPar1"><title>Monitoring and evaluation (M&#x00026;E) team support for data management and quality</title><p id="Par18">Two M&#x00026;E staff managed operations of the DQMIS across all data collection activities and provided technical assistance and capacity development to supervisory staff in the field. The M&#x00026;E team was responsible for oversight of all functional components of the DQMIS, including the DQA protocol, organizing trainings, developing and revising SOPs as needed, and providing technical assistance on data collection and report interpretation. The M&#x00026;E reinforced all six dimensions of data quality throughout the trial.</p></sec><sec id="FPar2"><title>Standard operating procedures (SOPs) for data collection</title><p id="Par19">Tools were designed and SOPs for each data collection activity were defined prior to study start. All data collection tools were programmed into the electronic component of the data collection system to facilitate automated and scalable data quality monitoring. SOPs included frequency, method, and technique for each data collection activity.</p></sec><sec id="FPar3"><title>Training</title><p id="Par20">All data collectors and supervisors participated in an 8-day orientation training program focused on implementation of SOPs, data-collection tools, the electronic data collection apps, and reporting system. As a part of this orientation, a 1-day training focused on the functional components of the DQA protocol. Additionally, data collectors engaged in active learning by visiting facilities to learn study implementation processes in the field. Subsequent staff-wide and staff-specific refresher trainings were delivered throughout implementation of the trial.</p></sec><sec id="FPar4"><title>Electronic data collection and reporting system</title><p id="Par21">We developed a data collection and reporting system to centralize data management for the trial. The system included front-end smartphone and tablet-based electronic data collection applications (based off Dimagi&#x02019;s open-source CommCare platform) for each data collection tool, a secure cloud-based server for data storage and integrity, and a reporting portal for study operations, including data quality. The reporting system produced data quality reports using pre-defined algorithms and data visualizations to facilitate near real-time feedback on accuracy of trial data.</p></sec><sec id="FPar5"><title>DQA protocol, including audits, real-time data feedback, supportive supervision</title><p id="Par22">We designed a standardized DQA protocol as an integrated component of the trial to continuously assess and improve data accuracy and reliability throughout implementation. Supervisors performed audits on data collectors to address quality of the five data collection activities. Audits targeted accuracy of data entry, delivery observations, and patient-reported outcomes ascertained by the call center. The auditing process, unique for each data collection activity, required perfect accuracy on a sample of data collected by each data collector in a phased approach. Following orientation, each data collector began an intensive phase of auditing lasting 6&#x000a0;weeks (or longer in case of any difficulty achieving targets). After achieving performance targets of the intensive phase, data collectors graduated into a maintenance phase, with audits repeating every 3&#x000a0;months. No a priori decisions were made regarding the proportion of data in each data collection activity that would be assessed for quality; rather, the data collector&#x02019;s ability to achieve set targets determined the proportion of data within each data collection activity that was checked for accuracy. Perfect accuracy was required for each performance target; any errors required that the audit be repeated from the beginning (Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>).<table-wrap id="Tab3"><label>Table 3</label><caption><p>Data sources and audit methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data source</th><th>Data collection process</th><th>Audit process</th><th>Intensive phase target and duration</th><th>Monitoring phase target and frequency</th></tr></thead><tbody><tr><td colspan="5">
<italic>Accuracy of observation of birth attendant practices</italic>
</td></tr><tr><td>Birth practices performed by birth attendant during deliveries</td><td>Direct observation of deliveries with data entry into paper-based checklist by facility-based observers</td><td>Simultaneous observation by supervisor</td><td>100% accuracy on three consecutive simultaneous observations of each of four observation points (OPs); first 4&#x000a0;weeks after hire</td><td>100% accuracy on three consecutive simultaneous observations at each OP (OP1, OP2, OP3, OP4); every 3&#x000a0;months</td></tr><tr><td colspan="5">
<italic>Accuracy of data entry</italic>
</td></tr><tr><td>Observation checklist of birth attendant practices</td><td>Data entry of paper-based delivery observation data into electronic data-entry app by facility-based observers</td><td>Double data entry by supervisors</td><td>100% accuracy on two sets of 10 sequentially entered forms; first 4&#x000a0;weeks after hire</td><td>100% accuracy on one set of 10 sequentially entered forms; every 3&#x000a0;months</td></tr><tr><td>Facility registers</td><td>Extraction of patient data from paper-based facility registers into paper-based study register by facility-based data collectors</td><td>Cross verification of extracted data with facility-based register data by supervisors</td><td>No intensive phase</td><td>100% accuracy on a consecutive set of 10 patients&#x02019; extracted register information; monthly</td></tr><tr><td>Study register</td><td>Data entry of patient data from paper-based study register to electronic data entry app by facility-based data collectors</td><td>Double data entry by supervisors</td><td>No intensive phase</td><td>100% accuracy on a consecutive set of 10 patients&#x02019; register information; monthly</td></tr><tr><td colspan="5">
<italic>Accuracy of patient-reported outcomes</italic>
</td></tr><tr><td>Patient-reported outcomes</td><td>Call center staff contact patients to assess maternal and neonatal mortality and seven maternal morbidities using standardized data collection tool</td><td>Recorded call review and double data entry into electronic data entry app by supervisor</td><td>100% accuracy on four sets of 10 sequentially reviewed calls; first 4&#x000a0;weeks after hire</td><td>100% accuracy on four sets of 10 sequentially reviewed calls; every 3&#x000a0;months</td></tr></tbody></table></table-wrap>
</p><p id="Par23">The DQA protocol was supported by rapid, timely, and automatic data quality feedback. Data quality reports were designed to inform supervisors and study management staff of audit results at the level of data collector, including accuracy rates, DQA phase, error trends, target achievement, and data entry delay. Additionally, reports designed for study management presented aggregated accuracy rates and error trends across data collectors. Reports were available within 24&#x000a0;h of audits and accessed via smartphone and tablet. In observance of blinding rules related to observation and outcomes data for certain staff, reports displayed accuracy in green and errors in red, rather than the actual data (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><p>Data quality accuracy report for patient-reported outcomes</p></caption><graphic xlink:href="13063_2017_2159_Fig1_HTML" id="MO1"/></fig>
</p><p id="Par24">In addition, we designed a supportive supervision model to facilitate data accuracy and reliability (quality improvement (QI)) across all data collection activities. Experienced supervisors were assigned to support specific data collectors in order to build trust and rapport. Utilizing the reporting system, supervisors reviewed audit results on a continual basis to identify target accomplishment and occurrence of errors. Thereafter, immediate onsite support was provided to data collectors. Success was celebrated, and challenges were addressed in a supportive manner. First, supervisors shared accuracy reports with staff to address challenges. Second, sources of error were discussed, whether they were related to data entry, interpretation, or technical aspects of the app. Finally, supervisors and data collectors together devised strategic plans to improve accuracy, which included refresher training, one-on-one support, and peer-to-peer mentorship. The M&#x00026;E team provided ongoing support to supervisors in this process.</p></sec></sec></sec><sec id="Sec6"><title>Data analysis</title><p id="Par25">Descriptive statistics were calculated for accuracy results, including proportion of forms evaluated for accuracy, overall accuracy, and accuracy by data collection activity. The proportion of forms evaluated for accuracy was calculated as the number of forms audited out of the total number of forms collected over the same time period (7 November 2014 to 6 September 2016). The percent accuracy was calculated for all forms audited. A form was considered accurate if all questions were consistent between both entries of the form. A form was considered inaccurate if it contained one or more errors. The percent accuracy for forms for each activity was plotted over time by month and assessed for trends. The relative risk of accuracy for each data collection activity by each consecutive form audited was calculated using relative risk regression, clustered by data collector [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]. All statistical analyses were performed using SAS 9.4&#x000ae;.</p></sec></sec><sec id="Sec7" sec-type="results"><title>Results</title><p id="Par26">Data collection staff gradually increased as the volume of data increased over the course of the trial. At their maximum, data collection staff included 32 facility-based observers (26 data collectors, six supervisors), 116 facility-based field workers (78 data collectors, 38 supervisors), and 33 call center staff (26 callers, 6 supervisors, 1 manager).</p><sec id="Sec8"><title>Completeness, precision, and integrity</title><p id="Par27">These three dimensions of data quality were primarily guaranteed through the back-end design of the data collection and reporting system. All electronic data collection apps included required fields and skip patterns to prevent missing values upon data entry, guaranteeing completeness of all datasets. Data precision was protected through data definitions and field restrictions in the electronic data collection system. The secure cloud-based server certified the integrity of data by preventing data manipulation by any staff.</p></sec><sec id="Sec9"><title>Timeliness and reliability</title><p id="Par28">Timeliness of data was reinforced by the SOPs for data collection and by routine staff trainings, which emphasized that each data collector enter data from paper-based forms to electronic apps as soon as possible after data collection. For the two data collection activities for which primary data collection was paper-based (data entry of observation checklist, and data entry of patient data into the study register), the mean duration until electronic entry was 0.46 and 2.14&#x000a0;days, respectively. Reliability of data was accomplished through all five functional components of the DQMIS, collectively ensuring consistency in data collection across data collectors.</p></sec><sec id="Sec10"><title>Proportion and accuracy of trial data audited</title><p id="Par29">Among the five data collection activities, the proportion of forms (case-level data) audited ranged from 2.17% to 39.32%. The DQA protocol resulted in a high overall rate of accuracy across all data collection activities in the trial, with accuracy of each data collection activity ranging from 91.77% to 99.51% (Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>).<table-wrap id="Tab4"><label>Table 4</label><caption><p>Proportion and accuracy of trial data audited (7 Nov 2014 to 6 Sept 2016)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data collection activity</th><th>Total forms (<italic>N</italic>)</th><th>Forms audited (<italic>n</italic>)</th><th>Proportion of total forms audited (%)</th><th>Forms audited with total accuracy (<italic>n</italic>)</th><th>Proportion of forms audited with total accuracy (%)</th></tr></thead><tbody><tr><td colspan="6">Observation of birth attendant practices</td></tr><tr><td>OP1: On admission</td><td>4886</td><td>436</td><td>8.92%</td><td>431</td><td>98.85%</td></tr><tr><td>OP2: Just before delivery</td><td>5000</td><td>479</td><td>9.58%</td><td>445</td><td>92.90%</td></tr><tr><td>OP3: Within 1&#x000a0;min after delivery</td><td>4998</td><td>461</td><td>9.22%</td><td>454</td><td>98.48%</td></tr><tr><td>OP4: Within 1&#x000a0;h after delivery</td><td>4854</td><td>465</td><td>9.58%</td><td>451</td><td>96.99%</td></tr><tr><td>Data entry of observation checklist</td><td>5933</td><td>2333</td><td>39.32%</td><td>2141</td><td>91.77%</td></tr><tr><td>Data extraction of patient data from facility register to study register</td><td>136,057</td><td>10,341</td><td>7.60%</td><td>10,290</td><td>99.51%</td></tr><tr><td>Data entry of patient data from study register to app</td><td>136,057</td><td>8221</td><td>6.04%</td><td>8155</td><td>99.20%</td></tr><tr><td>Patient-reported outcomes</td><td>110,475</td><td>2400</td><td>2.17%</td><td>2350</td><td>97.92%</td></tr><tr><td>Overall</td><td>408,260</td><td>25,136</td><td>6.16%</td><td>24,717</td><td>98.33%</td></tr></tbody></table><table-wrap-foot><p>
<italic>OP</italic> observation point</p></table-wrap-foot></table-wrap>
</p></sec><sec id="Sec11"><title>Accuracy of trial data over time</title><p id="Par30">All data collection activities demonstrated an upward trend in accuracy improvement throughout implementation. For example, monthly accuracy of observation of birth attendant practices at observation point (OP)2 increased from 73.68% to 100% (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). The accuracy of each question in all data collection activities was also analyzed. Over time, question-level accuracy never decreased. In most instances, question-level accuracy remained high throughout and, in several instances, question-level accuracy improved over time.<fig id="Fig2"><label>Fig. 2</label><caption><p>Accuracy rate and trend of each data collection activity by month (7 Nov 2014 to 6 Sept 2016). <italic>OP</italic> observation point</p></caption><graphic xlink:href="13063_2017_2159_Fig2_HTML" id="MO2"/></fig>
</p></sec><sec id="Sec12"><title>Accuracy of data collectors over time</title><p id="Par31">Data collector accuracy remained high from the first audit through all consecutive audits. A small but significant increase in accuracy was achieved throughout consecutive audits for three of the data collection activities and for three of the four OPs. For the other data collection activities, there was no significant change in data collector accuracy as it remained high throughout the trial. In no case did accuracy decrease among data collectors throughout consecutive auditing (Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>).<table-wrap id="Tab5"><label>Table 5</label><caption><p>Unadjusted trend in accuracy of data collectors over time</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data collection activity</th><th>RR (95% CI)</th><th>
<italic>p</italic> value</th></tr></thead><tbody><tr><td colspan="3">Observation of birth-attendant practices</td></tr><tr><td>OP1: On admission</td><td>1.0003 (0.9995-1.0011)</td><td>0.4140</td></tr><tr><td>OP2: Just before delivery</td><td>1.0043 (1.0006-1.0081)</td><td>0.0242</td></tr><tr><td>OP3: Within 1&#x000a0;min after delivery</td><td>1.0015 (1.0003-1.0027)</td><td>0.0119</td></tr><tr><td>OP4: Within 1&#x000a0;h after delivery</td><td>1.0019 (0.9999-1.0039)</td><td>0.0679</td></tr><tr><td>Data entry of observation checklist</td><td>1.0006 (1.0000-1.0012)</td><td>0.0366</td></tr><tr><td>Data extraction of patient data from facility register to study register</td><td>1.0000 (1.0000-1.0001)</td><td>0.7218</td></tr><tr><td>Data entry of patient data from study register to app</td><td>1.0000 (1.0000-1.0001)</td><td>0.0473</td></tr><tr><td>Patient-reported outcomes</td><td>1.0003 (1.0000-1.0005)</td><td>0.0304</td></tr><tr><td>Total combined trend in accuracy</td><td>1.0001 (1.0000-1.0002)</td><td>0.0004</td></tr></tbody></table><table-wrap-foot><p>
<italic>CI</italic> confidence interval, <italic>OP</italic> observation point, <italic>RR</italic> relative risk</p></table-wrap-foot></table-wrap>
</p></sec></sec><sec id="Sec13" sec-type="discussion"><title>Discussion</title><p id="Par32">Our integrated DQMIS resulted in exceptionally high data quality for the trial. Error rates in clinical trials have been reported as high as 26.9%, and could range even higher due to a lack of standardization of data quality measurement [<xref ref-type="bibr" rid="CR19">19</xref>]. Our overall error rate of 1.67%, as measured by accuracy auditing, provides evidence for the feasibility and effectiveness of integrating DQA into the implementation of health systems research trials. Our DQMIS was successful, despite a steady increase in staff volume, complex and multiple data sources, a vast geographic catchment area across 24 districts, and a large sample size. This success is largely attributable to a number of factors, which we describe below.</p><sec id="Sec14"><title>Well-designed technology and data collection processes</title><p id="Par33">It is essential to plan for data quality control mechanisms during the design phase of QI and health systems research trials [<xref ref-type="bibr" rid="CR31">31</xref>]. We guaranteed completeness, precision, and integrity of data throughout implementation of the trial through several layers of quality control. Stringent and deliberate front-end data entry rules prevented data collectors from entering values outside specified ranges or choosing options that contradicted previous responses. Additionally, significant time and resources were dedicated to implementing robust back-end restrictions into the data collection system to prevent data loss or corruption from occurring. The reporting system enabled the study team, based in India and the US, to monitor data collection indicators to ensure consistent data collection processes. As reported elsewhere [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR31">31</xref>], this forethought and design facilitated a high-quality dataset.</p></sec><sec id="Sec15"><title>Well-defined SOPs</title><p id="Par34">It has also been acknowledged that SOPs and indicator definitions are essential for reliable and accurate data collection in clinical trials [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. Prior to data collection in the trial, the study protocol was systematically designed with a focus on ensuring data quality through standardization of processes. Data collection tools were designed with validated questions, pre-tested, and finalized through an iterative process. As a reference for data collectors and supervisors, tool guides were developed which included instructions for how to use instruments, definitions, and interpretation guidelines for each question. Tool guides also reinforced consistency of data collection and entry to ensure reliability. Tool guides were adapted and refined throughout the trial to address definitional and other challenges that arose during data collection. Additionally, SOPs and trainings emphasized the importance of timely data entry, reducing the possibility of lost data or inaccuracy as a result of data entry delay.</p></sec><sec id="Sec16"><title>Integration into data collection workflow</title><p id="Par35">While methods for assessing data quality in patient registries and health information systems are known, little has been recently published on integrating DQA methods into clinical trial data collection workflows [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. By integrating the DQA protocol into daily workflows, supervisors had the opportunity to support quality throughout implementation of the study. Assigning challenging targets for the intensive phase and lessening these in the maintenance phase reinforced our integrated and continuous system of quality improvement. Following orientation, each data collector was held to high performance standards, fostered by our supportive supervision model. Once achieving intensive phase targets, data collectors were still held to the same targets, but on a less frequent basis to routinely check and bolster accuracy. The aim was to make data collectors accountable for their own performance quality. In addition, the integrated nature of the DQMIS ensured that the proportion of data checked for quality was adapted to the performance of the data collector. The design of the DQA protocol established that the proportion of data checked for quality should be determined by a data collector&#x02019;s ability to achieve certain performance targets. Target achievement and ongoing supportive supervision together influenced sustained quality throughout implementation. While the ratio of data collectors to supervisors ranged from 2:1 to 4:1 depending on the data collection activity, future trials should consider data collection volume, geographic scope, and minimum quality standards when determining human resource needs for DQA.</p></sec><sec id="Sec17"><title>Data feedback paired with supportive supervision</title><p id="Par36">Coaching for QI, when paired with performance monitoring and data feedback, has been shown to be effective in healthcare and other disciplines [<xref ref-type="bibr" rid="CR32">32</xref>&#x02013;<xref ref-type="bibr" rid="CR34">34</xref>]. Recognizing this, we designed a complementary supportive supervision and data feedback model for DQA. Our near real-time reporting system facilitated the continuous monitoring of data accuracy. The design of the system, to rapidly analyze and report on audit results, enabled supervisors to promptly provide support to data collectors to improve data quality. Our supportive supervision model placed an emphasis on building capacity and promoting quality instead of penalizing lower performers. Supervisors were trained in coaching and mentorship techniques in order to emphasize strengths and target areas of improvement. Achievement of accuracy targets was celebrated, and improvement strategies were mutually identified between data collectors and supervisors. The combination of timely data feedback and supportive supervision was integral to the success of the DQA protocol.</p></sec><sec id="Sec18"><title>Impact on data collection</title><p id="Par37">During trial implementation, the DQMIS had multiple impacts on data collection methods and refinement of certain questions. Data quality reports highlighted specific concerns related to facility-based observers&#x02019; definitional interpretation of key study variables. In one instance, reports demonstrated low accuracy for the observation checklist item: &#x0201c;Was the following available at the bedside: sterile scissors or blade to cut cord.&#x0201d; Supervisors informed managers and study staff of wide variability in data collectors&#x02019; interpretation and definitions of sterility. Given this, study management staff chose to revise this checklist item to: &#x0201c;Was the following available at the bedside: clean scissors or blade to cut cord,&#x0201d; along with comprehensive guidelines on how to interpret whether the items were &#x02018;clean.&#x02019; &#x02018;Clean&#x02019; was defined as sterilized (directly removed from autoclave or boiler) or having no visible marks (dirt, blood, etc.). Data collectors received training on these changes, and scenario-based role playing helped to test their understanding. Following this, subsequent monthly accuracy rates for this checklist item increased to 100% for the duration of implementation. In the absence of data quality reports, inaccurate and unreliable data collection would have persisted.</p></sec><sec id="Sec19"><title>Limitations</title><p id="Par38">There are a few limitations to the design and implementation of the DQMIS. First, it is possible that our reliance on the supervisor as the gold standard for delivery observation may have resulted in data incorrectly being considered accurate. There was no other available gold standard, however; therefore, this choice was the most reliable option in the absence of alternatives. Additionally, facility staff not employed by the study entered data in facility registers. For this reason, our DQA is unable to verify the reliability of registration data. We also lack evidence of the cost-effectiveness of the DQMIS. Finally, in order to conduct DQA auditing of facility-based field workers and provide support across the vast geographic size of the study catchment area, the nearly 2:1 ratio of these workers to supervisors was required. This may not be feasible or necessary in other settings.</p></sec></sec><sec id="Sec20" sec-type="conclusion"><title>Conclusions</title><p id="Par39">The findings of this study demonstrate that integrated methods of DQA combined with SOPs, rapid data feedback, and supportive supervision during trial implementation are feasible, effective, and necessary to ensure high-quality data. In the absence of widely disseminated data quality methods and standards for large health systems RCT interventions, we developed the DQMIS to ensure reliability and serve as a model for future trials. Future efforts should focus on standardization of DQA processes and reporting requirements for data quality in health systems research.</p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>DQA</term><def><p id="Par6">Data Quality Assurance</p></def></def-item><def-item><term>DQMIS</term><def><p id="Par7">Data Quality Monitoring and Improvement System</p></def></def-item><def-item><term>M&#x00026;E</term><def><p id="Par8">Monitoring and evaluation</p></def></def-item><def-item><term>OP</term><def><p id="Par9">Observation point</p></def></def-item><def-item><term>QI</term><def><p id="Par10">Quality improvement</p></def></def-item><def-item><term>RCT</term><def><p id="Par11">Randomized controlled trial</p></def></def-item><def-item><term>SOP</term><def><p id="Par12">Standard operating procedure</p></def></def-item></def-list></glossary><ack><title>Acknowledgements</title><p>We thank the Governments of India and Uttar Pradesh for collaboration and support to conduct this trial in public health facilities. We are grateful to the members of the trial Scientific Advisory Committee who contributed invaluable advice and crucial guidance to the development of this study protocol and its conduct: Dinesh Baswal, Himanshu Bhushan, Zulfiqar Bhutta, Waldemar Carlo, Richard Cash, Vinita Das, France Donnay, Amit Ghosh, Amod Kumar, Matthews Mathai, Packirisamy Padmanbhan, Vinod Paul, and Rajiv Tandon. We also thank the past and current members of the BetterBirth study team in Boston and the BetterBirth field team based in Uttar Pradesh for study implementation. Most importantly, we are grateful to the laboring women and facility-based staff who participated in the study.</p><sec id="FPar6"><title>Funding</title><p id="Par40">This study is funded by the Bill &#x00026; Melinda Gates Foundation (Ref no: OPP1017378). The funders did not have input on data collection, management, analysis, or interpretation of the data. Further, they did not have any authority over the writing of the reports or decision to submit findings for publication.</p></sec><sec id="FPar7"><title>Availability of data and materials</title><p id="Par41">The datasets used and/or analyzed during the current study can be obtained from the corresponding author on reasonable request.</p></sec></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>JDG and AMi are co-lead writers and JDG is the corresponding author for this manuscript. JDG, LRH, and AMa designed all components of the DQMIS. KK, CS, JM, FS, MMD, VPS, and NS comprised the management team supporting all data collection activities and contributed to writing and editing of the manuscript. AMi, MNSY, and JDG comprised the M&#x00026;E team that managed and supported implementation of the DQMIS. JFB and BJN comprised the data management and statistical programming and analysis team and contributed to the writing of the manuscript. AG and KS are co-Principal Investigators of the BetterBirth Trial and contributed to the scientific writing and editing of the manuscript. LRH was senior author and contributed scientific mentorship to the co-lead writers. All authors read and approved the final manuscript.</p></notes><notes notes-type="COI-statement"><title>Authors&#x02019; information</title><p>Not applicable.</p><sec id="FPar8"><title>Ethics approval and consent to participate</title><p id="Par42">Women presenting for childbirth at study facilities and their newborns were enrolled and provided written consent for follow-up prior to their discharge. The call center reconfirmed consent verbally prior to initiating the outcomes questionnaire. The BetterBirth Trial study protocol has been approved by all participating institutions: Community Empowerment Lab (CEL) Ethics Review Committee (Ref no: 2014006) formerly Lucknow Ethics Committee (Ref no: 13/LEC/12), Jawaharlal Nehru Medical College Ethical Review Committee (Ref no: MDC/IECHSR/2015-16/A-53), Institutional Review Board of the Harvard T.H. Chan School of Public Health (Protocol 21975-102), Population Services International Research Ethics Board (Protocol ID: 47.2012), and the Ethical Review Committee of the World Health Organization (Protocol ID: RPC 501). The Indian Council of Medical Research also approved the study (Ref no: 5/7/858/12-RHN). The trial is registered at ClinicalTrials.gov (identifier: NCT02148952).</p></sec><sec id="FPar9"><title>Consent for publication</title><p id="Par43">Not applicable.</p></sec><sec id="FPar10"><title>Competing interests</title><p id="Par44">The authors declare that they have no competing interests.</p></sec><sec id="FPar11"><title>Publisher&#x02019;s Note</title><p id="Par45">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>CNS Summit Data Quality Monitoring Workgroup Core Members</collab></person-group><article-title>Data quality monitoring in clinical trials: has it been worth it? An evaluation and prediction of the future by all stakeholders</article-title><source>Innov Clin Neurosci.</source><year>2016</year><volume>13</volume><issue>1&#x02013;2</issue><fpage>27</fpage><lpage>33</lpage></element-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Zozus MNH, Green B, Kahn M, Richesson R, Rusincovitch S, Simon G, Smerek M. Assessing data quality for healthcare systems data used in clinical research (V. 1.0). In Collaboratory phenotypes, data standards, and data quality core. NIH Collaboratory 2014. <ext-link ext-link-type="uri" xlink:href="https://www.nihcollaboratory.org/Products/Assessing-dataquality_V1%200.pdf">https://www.nihcollaboratory.org/Products/Assessing-dataquality_V1%200.pdf</ext-link>.</mixed-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>J</given-names></name><name><surname>Kahn</surname><given-names>M</given-names></name><name><surname>Toh</surname><given-names>S</given-names></name></person-group><article-title>Data quality assessment for comparative effectiveness research in distributed data networks</article-title><source>Med Care</source><year>2013</year><volume>51</volume><issue>8 0 3</issue><fpage>S22</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1097/MLR.0b013e31829b1e2c</pub-id><pub-id pub-id-type="pmid">23793049</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>JRN</given-names></name><name><surname>Vivian</surname><given-names>P</given-names></name><name><surname>Woodcock</surname><given-names>J</given-names></name><name><surname>Estabrook</surname><given-names>RW</given-names></name></person-group><source>Assuring data quality and validity in clinical trials for regulatory decision making: workshop report</source><year>1999</year><publisher-loc>Washington (DC)</publisher-loc><publisher-name>Institute of Medicine Roundtable on Research and Development of Drugs, Biologics, Medical Devices</publisher-name></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nahm</surname><given-names>ML</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Richesson</surname><given-names>RL</given-names></name></person-group><article-title>Data quality in clinical research</article-title><source>Clinical research informatics</source><year>2012</year><publisher-loc>London</publisher-loc><publisher-name>Springer-Verlag</publisher-name><fpage>175</fpage><lpage>201</lpage></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moher</surname><given-names>D</given-names></name><etal/></person-group><article-title>Does quality of reports of randomised trials affect estimates of intervention efficacy reported in meta-analyses?</article-title><source>Lancet</source><year>1998</year><volume>352</volume><issue>9128</issue><fpage>609</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(98)01085-X</pub-id><pub-id pub-id-type="pmid">9746022</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Alsumidaie MA, Andrianov A. How do we define clinical trial data quality if no guidelines exist? Applied clinical trials. 2015. <ext-link ext-link-type="uri" xlink:href="http://www.appliedclinicaltrialsonline.com/how-do-we-define-clinical-trial-data-quality-if-no-guidelines-exist">http://www.appliedclinicaltrialsonline.com/how-do-we-define-clinical-trial-data-quality-if-no-guidelines-exist</ext-link>.</mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldhill</surname><given-names>DR</given-names></name><name><surname>Sumner</surname><given-names>A</given-names></name></person-group><article-title>APACHE II, data accuracy and outcome prediction</article-title><source>Anaesthesia</source><year>1998</year><volume>53</volume><issue>10</issue><fpage>937</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1046/j.1365-2044.1998.00534.x</pub-id><pub-id pub-id-type="pmid">9893535</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richesson</surname><given-names>RL</given-names></name><etal/></person-group><article-title>Electronic health records based phenotyping in next-generation clinical trials: a perspective from the NIH Health Care Systems Collaboratory</article-title><source>J Am Med Inform Assoc</source><year>2013</year><volume>20</volume><issue>e2</issue><fpage>e226</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1136/amiajnl-2013-001926</pub-id><pub-id pub-id-type="pmid">23956018</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Society for Clinical Data Management. Good clinical data management practices. 2013.</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houston</surname><given-names>L</given-names></name><name><surname>Probst</surname><given-names>Y</given-names></name><name><surname>Humphries</surname><given-names>A</given-names></name></person-group><article-title>Measuring data quality through a source data verification audit in a clinical research setting</article-title><source>Stud Health Technol Inform.</source><year>2015</year><volume>214</volume><fpage>107</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">26210426</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiskopf</surname><given-names>NG</given-names></name><name><surname>Weng</surname><given-names>C</given-names></name></person-group><article-title>Methods and dimensions of electronic health record data quality assessment: enabling reuse for clinical research</article-title><source>J Am Med Inform Assoc</source><year>2013</year><volume>20</volume><issue>1</issue><fpage>144</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1136/amiajnl-2011-000681</pub-id><pub-id pub-id-type="pmid">22733976</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahn</surname><given-names>MG</given-names></name><etal/></person-group><article-title>A pragmatic framework for single-site and multisite data quality assessment in electronic health record-based clinical research</article-title><source>Med Care.</source><year>2012</year><volume>50</volume><fpage>S21</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1097/MLR.0b013e318257dd67</pub-id><pub-id pub-id-type="pmid">22692254</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><etal/></person-group><article-title>A review of data quality assessment methods for public health information systems</article-title><source>Int J Environ Res Public Health</source><year>2014</year><volume>11</volume><issue>5</issue><fpage>5170</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.3390/ijerph110505170</pub-id><pub-id pub-id-type="pmid">24830450</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nahm</surname><given-names>ML</given-names></name><name><surname>Pieper</surname><given-names>CF</given-names></name><name><surname>Cunningham</surname><given-names>MM</given-names></name></person-group><article-title>Quantifying data quality for clinical trials using electronic data capture</article-title><source>PLoS One</source><year>2008</year><volume>3</volume><issue>8</issue><fpage>e3049</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0003049</pub-id><pub-id pub-id-type="pmid">18725958</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Putten</surname><given-names>E</given-names></name><etal/></person-group><article-title>A pilot study on the quality of data management in a cancer clinical trial</article-title><source>Control Clin Trials</source><year>1987</year><volume>8</volume><issue>2</issue><fpage>96</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/0197-2456(87)90034-1</pub-id><pub-id pub-id-type="pmid">3608510</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horbar</surname><given-names>JD</given-names></name><name><surname>Leahy</surname><given-names>KA</given-names></name></person-group><article-title>An assessment of data quality in the Vermont-Oxford Trials Network Database</article-title><source>Control Clin Trials</source><year>1995</year><volume>16</volume><issue>1</issue><fpage>51</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/0197-2456(94)00019-Y</pub-id><pub-id pub-id-type="pmid">7743789</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shelby-James</surname><given-names>TM</given-names></name><etal/></person-group><article-title>Handheld computers for data entry: high tech has its problems too</article-title><source>Trials.</source><year>2007</year><volume>8</volume><fpage>5</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1186/1745-6215-8-5</pub-id><pub-id pub-id-type="pmid">17309807</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldberg</surname><given-names>SI</given-names></name><name><surname>Niemierko</surname><given-names>A</given-names></name><name><surname>Turchin</surname><given-names>A</given-names></name></person-group><article-title>Analysis of data errors in clinical research databases</article-title><source>AMIA Annu Symp Proc.</source><year>2008</year><volume>2008</volume><fpage>242</fpage><lpage>6</lpage></element-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Hong&#x000a0;MKH,&#x000a0;Yao&#x000a0;HHI,&#x000a0;Pedersen&#x000a0;JS, et al. Error rates in a clinical data repository: lessons from the transition to electronic data transfer&#x02014;a descriptive study. BMJ Open.&#x000a0;2013;3:e002406.&#x000a0;&#x000a0;</mixed-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macefield</surname><given-names>RC</given-names></name><etal/></person-group><article-title>A systematic review of on-site monitoring methods for health-care randomised controlled trials</article-title><source>Clin Trials</source><year>2013</year><volume>10</volume><issue>1</issue><fpage>104</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1177/1740774512467405</pub-id><pub-id pub-id-type="pmid">23345308</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arts</surname><given-names>DGT</given-names></name><name><surname>de Keizer</surname><given-names>NF</given-names></name><name><surname>Scheffer</surname><given-names>G-J</given-names></name></person-group><article-title>Defining and improving data quality in medical registries: a literature review, case study, and generic framework</article-title><source>J Am Med Inform Assoc</source><year>2002</year><volume>9</volume><issue>6</issue><fpage>600</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1197/jamia.M1087</pub-id><pub-id pub-id-type="pmid">12386111</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>RY</given-names></name><name><surname>Strong</surname><given-names>DM</given-names></name></person-group><article-title>Beyond accuracy: what data quality means to data consumers</article-title><source>J Manage Inf Syst</source><year>1996</year><volume>12</volume><issue>4</issue><fpage>5</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1080/07421222.1996.11518099</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gassman</surname><given-names>JJ</given-names></name><etal/></person-group><article-title>Data quality assurance, monitoring, and reporting</article-title><source>Control Clin Trials</source><year>1995</year><volume>16</volume><issue>2 Suppl</issue><fpage>104s</fpage><lpage>36s</lpage><pub-id pub-id-type="doi">10.1016/0197-2456(94)00095-K</pub-id><pub-id pub-id-type="pmid">7789140</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>D</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name></person-group><article-title>Data quality assurance and quality control measures in large multicenter stroke trials: the African-American Antiplatelet Stroke Prevention Study experience</article-title><source>Trials</source><year>2001</year><volume>2</volume><issue>3</issue><fpage>115</fpage><pub-id pub-id-type="doi">10.1186/CVM-2-3-115</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Harvard School of Public Health. BetterBirth: a trial of the WHO safe childbirth checklist program<italic>.</italic> ClinicalTrials.gov [NCT02148952]. National Library of Medicine (US); 2014. <ext-link ext-link-type="uri" xlink:href="https://clinicaltrials.gov/ct2/show/NCT02148952">https://clinicaltrials.gov/ct2/show/NCT02148952</ext-link>.</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semrau</surname><given-names>K</given-names></name><name><surname>Hirschhorn</surname><given-names>LR</given-names></name><name><surname>Kodkany</surname><given-names>B</given-names></name><name><surname>Spector</surname><given-names>J</given-names></name><name><surname>Tuller</surname><given-names>DE</given-names></name><name><surname>King</surname><given-names>G</given-names></name><name><surname>Lisptiz</surname><given-names>S</given-names></name><name><surname>Sharma</surname><given-names>N</given-names></name><name><surname>Singh</surname><given-names>VP</given-names></name><name><surname>Kumar</surname><given-names>B</given-names></name><name><surname>Dhingra-Kumar</surname><given-names>N</given-names></name><name><surname>Firestone</surname><given-names>R</given-names></name><name><surname>Kumar</surname><given-names>V</given-names></name><name><surname>Gawande</surname><given-names>A</given-names></name></person-group><article-title>Effectiveness of the WHO safe childbirth checklist program in reducing severe maternal, fetal, and newborn harm: study protocol for a matched-pair, cluster randomized controlled trial in Uttar Pradesh, India</article-title><source>Trials</source><year>2016</year><volume>17</volume><issue>576</issue><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmid">26725476</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Brown W. Data quality assurance tool for program level indicators. MEASURE Evaluation, 2007.</mixed-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzmaurice</surname><given-names>GM</given-names></name><etal/></person-group><article-title>Almost efficient estimation of relative risk regression</article-title><source>Biostatistics</source><year>2014</year><volume>15</volume><issue>4</issue><fpage>745</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxu012</pub-id><pub-id pub-id-type="pmid">24705141</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>RE</given-names></name><name><surname>Lipsitz</surname><given-names>SR</given-names></name><name><surname>Tilley</surname><given-names>BC</given-names></name></person-group><article-title>Quasi-likelihood estimation for relative risk regression models</article-title><source>Biostatistics</source><year>2005</year><volume>6</volume><issue>1</issue><fpage>39</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxh016</pub-id><pub-id pub-id-type="pmid">15618526</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Needham</surname><given-names>DM</given-names></name><etal/></person-group><article-title>Improving data quality control in quality improvement projects</article-title><source>International J Qual Health Care</source><year>2009</year><volume>21</volume><issue>2</issue><fpage>145</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1093/intqhc/mzp005</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shojania</surname><given-names>KG</given-names></name><name><surname>Grimshaw</surname><given-names>JM</given-names></name></person-group><article-title>Evidence-based quality improvement: the state of the science</article-title><source>Health Aff (Millwood)</source><year>2005</year><volume>24</volume><issue>1</issue><fpage>138</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1377/hlthaff.24.1.138</pub-id><pub-id pub-id-type="pmid">15647225</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>E</given-names></name><name><surname>Kalmakis</surname><given-names>KA</given-names></name></person-group><article-title>From the sidelines: coaching as a nurse practitioner strategy for improving health outcomes</article-title><source>J Am Acad Nurse Pract</source><year>2007</year><volume>19</volume><issue>11</issue><fpage>555</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1111/j.1745-7599.2007.00264.x</pub-id><pub-id pub-id-type="pmid">17970856</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ivers</surname><given-names>NM</given-names></name><etal/></person-group><article-title>Growing literature, stagnant science? Systematic review, meta-regression and cumulative analysis of audit and feedback interventions in health care</article-title><source>J Gen Intern Med</source><year>2014</year><volume>29</volume><issue>11</issue><fpage>1534</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1007/s11606-014-2913-y</pub-id><pub-id pub-id-type="pmid">24965281</pub-id></element-citation></ref></ref-list></back></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Behav Brain Funct</journal-id><journal-title-group><journal-title>Behavioral and Brain Functions : BBF</journal-title></journal-title-group><issn pub-type="epub">1744-9081</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">20180945</article-id><article-id pub-id-type="pmc">2828975</article-id><article-id pub-id-type="publisher-id">1744-9081-6-3</article-id><article-id pub-id-type="doi">10.1186/1744-9081-6-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Music listening while you learn: No influence of background music on verbal learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="A1"><name><surname>J&#x000e4;ncke</surname><given-names>Lutz</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>l.jaencke@psychologie.unizh.ch</email></contrib><contrib contrib-type="author" id="A2"><name><surname>Sandmann</surname><given-names>Pascale</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>p.sandmann@psychologie.uzh.ch</email></contrib></contrib-group><aff id="I1"><label>1</label>University of Zurich, Psychological Institute, Department of Neuropsychology, Switzerland</aff><pub-date pub-type="collection"><year>2010</year></pub-date><pub-date pub-type="epub"><day>7</day><month>1</month><year>2010</year></pub-date><volume>6</volume><fpage>3</fpage><lpage>3</lpage><history><date date-type="received"><day>13</day><month>7</month><year>2009</year></date><date date-type="accepted"><day>7</day><month>1</month><year>2010</year></date></history><permissions><copyright-statement>Copyright &#x000a9;2010 J&#x000e4;ncke and Sandmann; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2010</copyright-year><copyright-holder>J&#x000e4;ncke and Sandmann; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="http://www.behavioralandbrainfunctions.com/content/6/1/3"/><abstract><sec><title>Background</title><p>Whether listening to background music enhances verbal learning performance is still disputed. In this study we investigated the influence of listening to background music on verbal learning performance and the associated brain activations.</p></sec><sec><title>Methods</title><p>Musical excerpts were composed for this study to ensure that they were unknown to the subjects and designed to vary in tempo (fast vs. slow) and consonance (in-tune vs. out-of-tune). Noise was used as control stimulus. 75 subjects were randomly assigned to one of five groups and learned the presented verbal material (non-words with and without semantic connotation) with and without background music. Each group was exposed to one of five different background stimuli (in-tune fast, in-tune slow, out-of-tune fast, out-of-tune slow, and noise). As dependent variable, the number of learned words was used. In addition, event-related desynchronization (ERD) and event-related synchronization (ERS) of the EEG alpha-band were calculated as a measure for cortical activation.</p></sec><sec><title>Results</title><p>We did not find any substantial and consistent influence of background music on verbal learning. There was neither an enhancement nor a decrease in verbal learning performance during the background stimulation conditions. We found however a stronger event-related desynchronization around 800 - 1200 ms after word presentation for the group exposed to in-tune fast music while they learned the verbal material. There was also a stronger event-related synchronization for the group exposed to out-of-tune fast music around 1600 - 2000 ms after word presentation.</p></sec><sec><title>Conclusion</title><p>Verbal learning during the exposure to different background music varying in tempo and consonance did not influence learning of verbal material. There was neither an enhancing nor a detrimental effect on verbal learning performance. The EEG data suggest that the different acoustic background conditions evoke different cortical activations. The reason for these different cortical activations is unclear. The most plausible reason is that when background music draws more attention verbal learning performance is kept constant by the recruitment of compensatory mechanisms.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>Whether background music influences performance in various tasks is a long-standing issue that has not yet been adequately addressed. Most published studies have concentrated on typical occupational tasks like office work, labour at the conveyer belt, or while driving a car [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B13">13</xref>]. These studies mainly concluded that background music has detrimental influences on the main task (here occupational tasks). However, the influence of background music was modulated by task complexity (the more complex the task the stronger was the detrimental effect of background music) [<xref ref-type="bibr" rid="B4">4</xref>], personality traits (with extraverts being more prone to be influenced by background music) [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B6">6</xref>], and mood [<xref ref-type="bibr" rid="B14">14</xref>]. In fact, these studies mostly emphasise that mood enhancement by pleasant and alerting background music enhances performance of monotonous tasks such as those during night shifts.</p><p>Whether background music influences performance of academic and school-related skills has also been investigated. A broad range of skills have been considered, including the impact of background music on learning mathematics, reading texts, solving problems, perceiving visual or auditory information, learning verbal material (vocabulary or poems), or during decision making [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B8">8</xref>,<xref ref-type="bibr" rid="B15">15</xref>-<xref ref-type="bibr" rid="B40">40</xref>]. The findings of these studies are mixed, but most of them revealed that background music exerts a detrimental influence on the primary academic task.</p><p>The present study was designed to readdress the question of whether background music enhances verbal learning. There are a number of reasons for this renewed interest: Firstly, only a few of the preceding studies have examined the effects of background music on verbal learning in particular [<xref ref-type="bibr" rid="B31">31</xref>,<xref ref-type="bibr" rid="B41">41</xref>-<xref ref-type="bibr" rid="B45">45</xref>], reporting more or less detrimental effects of background music on verbal learning, whereas several scientifically weak contributions have been published suggesting that listening to background music (in particular classical music) should have beneficial effects on learning languages [<xref ref-type="bibr" rid="B46">46</xref>,<xref ref-type="bibr" rid="B47">47</xref>]. Since verbal learning is an important part of academic achievement, we find it important to study the influence of background music on verbal learning more thoroughly. Secondly, the published studies used music of different genres (pop, classic) and vocals or they used instrumentals including musical pieces to elicit different emotions, music with different tempi, or simple tones as background music. Not one study has as yet controlled for the effects of emotion, complexity, tempo, and associated semantic knowledge of the musical pieces. The major aim of the present study was therefore to control these variables.</p><p>1. We used musical pieces unknown to the subjects. For this, we composed new musical pieces, avoiding any resemblance to well-known and familiar tunes. In doing this, we circumvented the well-known effect that particular contents of episodic and semantic memory are associated with musical pieces [<xref ref-type="bibr" rid="B48">48</xref>-<xref ref-type="bibr" rid="B50">50</xref>]. Thus, hearing a familiar musical piece might activate the episodic and semantic memory and lead to preferential/biased processing of the learned or the to be learned verbal stimuli.</p><p>2. A further step in avoiding activation of a semantic or episodic network was to use meaningless words. In combination with using unfamiliar musical pieces, this strategy ensures that established (or easy to establish) associations between musical pieces and particular words are not activated.</p><p>3. The musical pieces were designed in order to evoke pleasantness and activation to different degrees. Based on the mood-activation hypothesis proposed by Glenn Schellenberg and colleagues, we anticipated that music that evokes more pleasant affect might influence verbal learning more positively than music that evokes negative emotions [<xref ref-type="bibr" rid="B51">51</xref>-<xref ref-type="bibr" rid="B53">53</xref>].</p><p>4. Within the framework of the theory of <italic>changing state effects </italic>[<xref ref-type="bibr" rid="B54">54</xref>,<xref ref-type="bibr" rid="B55">55</xref>], we anticipated that rapidly changing auditory information would distract verbal learning more seriously than slowly changing music. Thus, slower musical pieces would exert less detrimental effects on verbal learning than faster music.</p><p>5. Given that the potentially beneficial effects of background music are also explained by a more or less unspecific cortical activation pattern, which should be evoked by the music and would change the activation of the cortical network involved in controlling learning and memory processes, we also registered EEG measures during learning and recognition. Here, we used event-related desynchronization and event-related synchronization of the EEG alpha band as indices of cortical activation. Our interest in event-related synchronization and desynchronization in the alpha band relates to the work of roughly the last 2 decades on alpha power demonstrating the relationship between alpha band power and cortical activity [<xref ref-type="bibr" rid="B56">56</xref>]. In addition, several recent combined EEG/fMRI and EEG/PET papers strongly indicate that power in the alpha-band is inversely related to activity in lateral frontal and parietal areas [<xref ref-type="bibr" rid="B57">57</xref>-<xref ref-type="bibr" rid="B59">59</xref>], and it has been shown that the alpha-band reflects cognitive and memory performance. For example, good memory performance is related to a large phasic (event-related) power decrease in the alpha-band [<xref ref-type="bibr" rid="B56">56</xref>].</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Subjects</title><p>77 healthy volunteers took part at this experiment (38 men and 39 women). Two subjects were excluded because of data loss during the experiment. All subjects were recruited through advertisements placed at the University Zurich and ETH Zurich. All subjects underwent evaluation to screen for chronic diseases, mental disorders, medication, and drug or alcohol abuse. Normal hearing ability was confirmed for all subjects using standard audiometry. For intelligence assessment, a short test [<xref ref-type="bibr" rid="B60">60</xref>] was used that is known to correlate with standard intelligence test batteries (r = 0.7 - 0.8). In addition, the NEO-FFI [<xref ref-type="bibr" rid="B61">61</xref>] was used to measure the personality trait "extraversion" because of its strong correlation with dual task performance [<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B6">6</xref>]. All subjects were tested for basic verbal learning ability using a standard German verbal learning test (Verbaler Lern- und Merkf&#x000e4;higkeitstest) [<xref ref-type="bibr" rid="B62">62</xref>,<xref ref-type="bibr" rid="B63">63</xref>]. All subjects were consistently right-handed, as assessed with the Annett-Handedness-Questionnaire [<xref ref-type="bibr" rid="B64">64</xref>]. All subjects indicated not having received formal musical education for more than five years during their school years and that they had not played any musical instrument in the last 5 years. We also asked the subjects whether they had previously learned while listening to music. Most of them conformed having done so, and a few (n = 5) indicated having done so frequently. The sample characteristics of the tested groups are listed in Table <xref ref-type="table" rid="T1">1</xref>. There were no statistical between-group differences in these measures. Each subject gave written, informed consent and received 30 Swiss Francs for participation. The study was carried out in accordance with the Declaration of Helsinki principles and was approved by the ethics committee of the University of Zurich.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Mean sample characteristics of the five groups studied.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="center">In-tune fast ITF</th><th align="center">In-tune slow ITS</th><th align="center">Out-of-tune fast OTF</th><th align="center">Out-of-tune slow OTS</th><th align="center">Noise</th></tr></thead><tbody><tr><td align="left">Age</td><td align="center">23.8 &#x000b1; 3.4</td><td align="center">26.8 &#x000b1; 4.9</td><td align="center">27.3 &#x000b1; 5.5</td><td align="center">26.5 &#x000b1;</td><td align="center">25.3 &#x000b1; 4.7</td></tr><tr><td align="left">IQ</td><td align="center">120 &#x000b1; 9</td><td align="center">118 &#x000b1; 8</td><td align="center">116 &#x000b1; 16</td><td align="center">119 &#x000b1; 14</td><td align="center">124 &#x000b1; 5</td></tr><tr><td align="left">Extraversion/introversion</td><td align="center">2.6 &#x000b1; 0.4</td><td align="center">2.6 &#x000b1; 0.4</td><td align="center">2.4 &#x000b1; 0.4</td><td align="center">2.5 &#x000b1; 0.5</td><td align="center">2.6 &#x000b1; 0.5</td></tr><tr><td align="left">Years of education</td><td align="center">15.6 &#x000b1; 2.3</td><td align="center">17.2 &#x000b1; 3.7</td><td align="center">16.1 &#x000b1; 3.8</td><td align="center">15.2 &#x000b1; 2.0</td><td align="center">14.6 &#x000b1; 2.2</td></tr><tr><td align="left">Gender n: f/m</td><td align="center">8/7</td><td align="center">8/7</td><td align="center">8/7</td><td align="center">8/7</td><td align="center">7/8</td></tr><tr><td align="left">N of subjects used to learn with music</td><td align="center">4</td><td align="center">2</td><td align="center">1</td><td align="center">2</td><td align="center">5</td></tr><tr><td align="left">Verbal memory (immediate recall)</td><td align="center">45.5 &#x000b1; 11.9</td><td align="center">42.3 &#x000b1; 11.5</td><td align="center">41.7 &#x000b1; 6.9</td><td align="center">47.6 &#x000b1; 10.5</td><td align="center">45.4 &#x000b1; 10.7</td></tr><tr><td align="left">Verbal memory (delayed recognition)</td><td align="center">17.3 &#x000b1; 5.3</td><td align="center">16.8 &#x000b1; 4.3</td><td align="center">16.7 &#x000b1; 4.1</td><td align="center">19.8 &#x000b1; 3.9</td><td align="center">17.2 &#x000b1; 3.4</td></tr></tbody></table><table-wrap-foot><p>Indicated are means and standard deviations.</p><p>There was no statistically significant between-group difference for these measures (all p values at least &#x0003e;0.15).</p></table-wrap-foot></table-wrap></sec><sec><title>Study design</title><p>The basic principle of this study was to explore verbal memory performance under different acoustic background stimulation conditions. The subjects performed a verbal memory test (see below) while acoustic background stimuli were present (background+) or not present (background-). Four different musical pieces and a noise stimulus were used as acoustic background stimuli (in-tune fast, in-tune slow, out-of-tune fast, out-of-tune slow, noise; for a description of these acoustic stimuli see below). The 75 subjects were randomly assigned to one of these five groups, each group comprising therefore 15 subjects. These five groups did not differ in terms of age, IQ, or extraversion/introversion (tested with Kruskal-Wallis-U-test).</p><p>In the background+ condition, participants were required to learn while one of the above-mentioned background stimuli was present. Thus, the experiment comprised two factors: a grouping factor with five levels (Group: in-tune fast, in-tune slow, out-of-tune fast, out-of-tune slow, noise) and a repeated measurements factor with two levels (Background: without acoustic background [background-] and with acoustic background [background+]). We also measured the electroencephalogram (EEG) during the different verbal learning conditions to explore whether the different learning conditions are associated with particular cortical activation patterns. The order of background stimulation (verbal learning with or without background stimulation) was counterbalanced across all subjects. There was an intermittent period of 12-14 minutes between the two learning sessions during which the subjects rated the quality of the background stimuli and rested for approximately 8 minutes.</p></sec><sec><title>Background stimuli</title><p>Several studies have shown that tempo and the level of consonance/dissonance of musical excerpts strongly determine the level of arousal and emotional feelings [<xref ref-type="bibr" rid="B51">51</xref>,<xref ref-type="bibr" rid="B65">65</xref>,<xref ref-type="bibr" rid="B66">66</xref>]. We therefore designed 4 different 16 minute-long musical pieces differing in musical tempo and tuning. The musical excerpts were computerised piano sounds designed using FL Studio 4 software [<xref ref-type="bibr" rid="B67">67</xref>]. We composed a musical excerpt in C-major consisting of a melody and accompanying fundamental chords. This original musical excerpt was systematically varied in terms of tuning (in-tune, out-of-tune) and tempo (fast, slow), resulting in four different musical background stimuli (Figure <xref ref-type="fig" rid="F1">1</xref>). Two of these background stimuli were fast (in-tune fast, out-of-tune fast), and two of them were slow (in-tune slow, out-of-tune slow) (musical excerpts can be downloaded as supplementary material [<xref ref-type="bibr" rid="B68">68</xref>]). The in-tune excerpts comprised the typical semitone steps between the tones while in the out-of-tune excerpts the melody was pitch-shifted by one quarter-tone above the original pitch, resulting in the experience of out-of-tune. The tempo of the musical excerpts was varied by changing the beats per minute (160 bpm for fast and 60 bpm for slow) [<xref ref-type="bibr" rid="B51">51</xref>]. In addition, we designed a noise stimulus (also 16 minutes long; brown noise) with a temporal envelope similar to that of the other four musical excerpts. In summary, we applied five different kinds of background stimuli: in-tune fast, in-tune slow, out-of-tune fast, out-of-tune slow, and noise.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Schematic description of the used musical excerpts (left panel).</bold> On the right the experimental design is depicted.</p></caption><graphic xlink:href="1744-9081-6-3-1"/></fig><p>In a pilot study, 21 subjects (who did not take part in the main experiment) evaluated these stimuli according to the experienced arousal and valence on a 5-point Likert scale (ranging from 0 to 4). In-tune music was generally rated as more pleasant than both out-of-tune music and noise (mean valence rating: in-tune fast: 3.04, in-tune slow: 2.5, out-of-tune fast: 1.7, out-of-tune slow: 0.9, noise: 0.6; significant differences between all stimuli). In terms of arousal, the slow musical excerpts were rated as less arousing than the fast excerpts and the noise stimulus (mean arousal rating: in-tune fast: 2.4, in-tune slow: 0.8, out-of-tune fast: 2.14, out-of-tune slow: 1.05, noise: 1.95; significant differences between all stimuli). These five acoustic stimuli were used as background stimuli for the main verbal learning experiment. In this experiment, background stimuli were binaurally presented via headphones (Sennheiser HD 25.1) at approximately 60 dB.</p></sec><sec><title>Verbal memory test</title><p>Verbal learning was examined using a standard verbal learning test, which is frequently used for investigations with German-speaking subjects (Verbaler Lerntest, VLT). This test has been shown to validly measure verbal long-term memory [<xref ref-type="bibr" rid="B62">62</xref>,<xref ref-type="bibr" rid="B63">63</xref>]. The test comprises 160 items and includes neologisms, which evoke either strong (80 items) or weak (80 items) semantic associations. In the test, most of the neologisms are novel (i.e., presented for one time), while 8 of the neologisms are repeatedly presented (7 repetitions), resulting in a total of 104 novel trials and 56 repetition trials. In the procedure used here, subjects were seated in front of a PC screen in an electromagnetically shielded room, and they were asked to discriminate between novel neologisms (NEW) and those that were presented in previous trials (OLD, i.e. repetitions). Subjects were instructed to respond after the presentation of every word by pressing either the right or left button of a computer mouse (right for OLD, left for NEW). Each trial started with a fixation cross (0 - 250 ms) followed by the presentation of a particular word (1150 - 2150 ms). The inter-trial interval, that is, the time between the onsets of the words of two consecutive trials, was 6 seconds. The performance in this memory test was measured using the number of correct responses for recognition of new and old words. For this test we had two parallel versions (version A and B) which allowed for testing the same subjects in two different background conditions (i.e., [background-] and [background+]).</p></sec><sec><title>Psychometrical measures</title><p>Several psychological measures were obtained after each experimental condition. First, the participants rated their subjective mood state using the MDBF questionnaire (Multidimensionaler Befindlichkeitsfragebogen) [<xref ref-type="bibr" rid="B69">69</xref>]. The MDBF comprises 12 adjectives (content, rested, restless, bad, worn-out, composed, tired, great, uneasy, energetic, relaxed, and unwell) for which the subjects had to indicate on a 5-point scale whether the particular adjective describes their actual feeling (1 = not at all; 5 = perfect). These evaluations are entered into summary scores along the three dimensions valence, arousal, and alertness. The acoustic background stimuli were evaluated using an adapted version of the Music Evaluation Questionnaire (MEQ) [<xref ref-type="bibr" rid="B70">70</xref>]. This questionnaire comprises questions evaluating the preference for the presented musical stimuli and how relaxing they are. In this questionnaire, subjects were also asked how they feel after listening to the music (i.e. cheerful, sad, aggressive, harmonious, drowsy, activated, and excited). All items were rated on 5-point Likert scales ranging from (1) not at all to (5) very strongly. The 10 scales were reduced to 3 scales (on the basis of a factor analysis), that is the subjective feeling of pleasantness, activation (arousal), and sadness (sadness).</p></sec><sec><title>EEG recording</title><p>The electroencephalogram (EEG) was recorded from 30 scalp electrodes (Ag/AgCl) using a Brain Vision amplifier system (BrainProducts, Germany). Electrodes were placed according to the 10-20 system. Two additional channels were placed below the outer canthi of each eye to record electro-oculograms (EOG). All channels were recorded against a reference electrode located at FCz. EEG and EOG were analogue filtered (0.1-100 Hz) and recorded with a sampling rate of 500 Hz. During recording impedances on all electrodes were kept below 5 k.</p></sec><sec><title>EEG preprocessing</title><p>EEG data were preprocessed and analysed by using BrainVision Analyzer (BrainProducts, Munich, Germany) and Matlab (Mathworks, Natick, MA). EEG data were off-line filtered (1-45 Hz), and re-referenced to a common average reference. Artefacts were rejected using an amplitude threshold criterion of &#x000b1; 100 &#x003bc;V. Independent component analysis was applied to remove ocular artefacts [<xref ref-type="bibr" rid="B71">71</xref>,<xref ref-type="bibr" rid="B72">72</xref>]. EEG data were then segmented into epochs (-1000 - 4000 ms) relative to the onset of the word stimulus.</p><p>Analysis of the time course of event-related desynchronization and event-related synchronization was performed according to the classical method described elsewhere [<xref ref-type="bibr" rid="B73">73</xref>,<xref ref-type="bibr" rid="B74">74</xref>]. We included NEW (i.e., neologisms) and OLD (i.e., presented previously) trials in the event-related synchronization/desynchronization analysis, and only those trials correctly identified as NEW and OLD by the participants. In this study, we calculated event-related synchronization/desynchronization in the alpha band. Several recent combined EEG/fMRI and EEG/PET papers strongly indicate that power in the alpha-band is inversely related to activity in lateral frontal and parietal areas [<xref ref-type="bibr" rid="B57">57</xref>-<xref ref-type="bibr" rid="B59">59</xref>], and it has been shown that the alpha-band reflects cognitive and memory performance. In the procedure used here, event-related synchronization/desynchronization in the alpha band was analyzed by filtering the artefact-free segments with a digital band-pass filter (8-12 Hz). Amplitude samples were then squared and averaged across all trials, and a low-pass filter (4 Hz) was used to smooth the data. The mean alpha-band activity in latency band -1000 - 0 ms relative to word stimulus onset was defined as intra-experimental resting condition (i.e., the baseline condition). To quantify the power changes during verbal learning, event-related synchronization/desynchronization were calculated according to the following formula: event-related desynchronization (ERD)/event-related synchronization (ERS) = ((band power<sub>task </sub>- band power<sub>baseline</sub>) * 100/band power<sub>baseline</sub>). Note that negative values indicate a relative decrease in the alpha-band (event-related desynchronization) during the experimental condition compared to the baseline condition, while positive values indicate an increase of alpha-band power during the experimental condition (event-related synchronization). In order to avoid multiple comparisons, event-related synchronization/desynchronization values were averaged over 10 time windows with a duration of 400 ms, and were collapsed for the frontal (FP1, FP2, F7, F3, Fz, F4 and F8), central-temporal (T7, C3, Cz, C4 and T8), and parieto-occipital (P7, P3, Pz, P4, P8, O1, Oz and O2) electrode locations [<xref ref-type="bibr" rid="B75">75</xref>] (see also Figure <xref ref-type="fig" rid="F2">2</xref>). Taken together, we obtained a time course of event-related synchronization/desynchronization changes over three different cortical regions (frontal, central-temporal, parieto-occipital), and over a time course of 4000 ms after stimulus presentation (10 event-related synchronization/desynchronization values for the entire time course). For this paper, we restrict our analysis to the first 5 time segments after word presentation, and thus concentrate on a time interval of 0 - 2000 ms after word presentation.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Schematic description of the definition of the electrode clusters which were used for averaging</bold>.</p></caption><graphic xlink:href="1744-9081-6-3-2"/></fig><p>In order to examine possible hemispheric differences, we subsequently calculated left- and right-sided event-related synchronization/desynchronization for the frontal, central-temporal, and parieto-occipital electrodes (left frontal: FP1, F7, F3; right frontal: FP2, F4 and F8; left central-temporal: T7, C3; right-central-temporal: C4 and T8; left parieto-occipital: P7, P3, and O1; right parieto-occipital: P4, P8 and O2). Asymmetric brain activation patterns during music perception have been reported by some studies [<xref ref-type="bibr" rid="B76">76</xref>,<xref ref-type="bibr" rid="B77">77</xref>]. Different findings have been reported by other studies [<xref ref-type="bibr" rid="B78">78</xref>,<xref ref-type="bibr" rid="B79">79</xref>].</p></sec><sec><title>Statistical analysis of event-related synchronization/desynchronization</title><p>For the main analysis of the event-related synchronization/desynchronization, a four-way ANOVA with the repeated measurements on the following factors was applied: Time Course (5 epochs after word stimulus presentation), Brain Area (3 levels: frontal, central-temporal, parieto-occipital), Background (2 levels: learning with acoustic background = background+, learning without acoustic background = background-), and the grouping factor Group (5 levels: in-tune fast, in-tune-slow, out-of-tune fast, out-of-tune slow, noise). Following this, we computed a four-way repeated measurements ANOVA, including the event-related synchronization/desynchronization data obtained for the left- and right-sided electrodes of interest, to examine whether hemispheric differences might influence the overall effect. For this, we used the multivariate approach to handle with the problem of heteroscedasticity [<xref ref-type="bibr" rid="B80">80</xref>]. Results were considered as significant at the level of p &#x0003c; 0.01. We used this more conservative approach in order to guard against problems associated with multiple testing. All statistical analyses were performed using the statistical software package SPSS 17.01 (MAC version). In case of significant interaction effects post-hoc paired t-tests were computed using the Bonferroni-Holm correction [<xref ref-type="bibr" rid="B81">81</xref>].</p><p>In order to assess whether there are between hemispheric differences in the cortical activations during listening to background music, we subjected the event-related synchronization/desynchronization data of the frontal, central-temporal, and parietal-occipital ROIs separately to a four-way ANOVA with Hemisphere (left vs. right), Group, Background, and Time as factors (Hemisphere, Group, and Background are repeated measurements factors). If background music and especially background music of different valence would evoke different lateralization patterns than the interaction between Hemisphere, Group or Hemisphere, Group, and Background should become significant. Thus, we were only interested in these interactions.</p></sec></sec><sec><title>Results</title><sec><title>Learning performance</title><p>Subjecting the verbal learning data to a 2-way repeated measurements ANOVA with repeated measurements on one factor (Background: background+ and background-) and the grouping factor (Group: in-tune fast, in-tune slow, out-of-tune fast, out-of-tune slow, noise) revealed no significant main effect (Background: F(1,70) = 0.073, p = 0.788, eta<sup>2 </sup>= 0.001; Group: F(4,70) = 1.42,p = 0.235, eta<sup>2 </sup>= 0.075) nor a significant interaction (F(4,70) = 0.90, p = 0.47, eta<sup>2 </sup>= 0.049) (Figure <xref ref-type="fig" rid="F3">3</xref> shows the means and standard errors).</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Mean verbal learning performance (number of correct responses) for the five experimental groups broken down for learning with (background+ in red) and without (background- in blue) musical background music</bold>. ITF: in-tune fast, ITS: in-tune slow, OTF: out-of-tune fast, OTS: out-of-tune slow. Shown are means and standard errors (as vertical bars).</p></caption><graphic xlink:href="1744-9081-6-3-3"/></fig></sec><sec><title>Emotional evaluation of acoustic background</title><p>The valence and arousal ratings for the three background conditions were subjected to separate repeated measurements ANOVAs. The valence measures were significantly different for the five background conditions (F(4,70) = 4.75, p &#x0003c; 0.002). Subsequently performed Bonferroni corrected t-tests revealed significant differences between the in-tune and out-of-tune conditions (mean valence rating &#x000b1; standard deviation: ITF = 3.5 &#x000b1; 1.1; ITS = 3.1 &#x000b1; 0.7; OTF = 2.7 &#x000b1; 1.0; OTS = 2.3 &#x000b1; 0.9; Noise = 2.2 &#x000b1; 1.1). For the arousal rating we obtained no significant difference between the five conditions (F(4,70) = 1.07, p = 0.378).</p><p>The MEQ rating data were subjected to three 2-way ANOVAs with one repeated measurements factor (Background: background+ vs. background-) and the grouping factor (Group). There was a significant main effect for Group with respect to pleasantness, with the in-tune melodies receiving the highest pleasantness ratings and the noise stimulus the lowest (F(4,70) = 12.5, p &#x0003c; 0.001 eta<sup>2 </sup>= 0.42). There was also a trend for interaction between Background and Group (F(4,70) = 2.40, p = 0.06, eta<sup>2 </sup>= 0.12), which is qualified by reduced pleasantness ratings for the in-tune melodies for the condition in which the subjects were learning. For the sadness scale we obtained a main effect for Background (F(1,70) = 8.14, p = 0.006, eta<sup>2 </sup>= 0.10) qualified by lower sadness ratings for the music heard while the subjects were learning.</p><p>The kind of acoustic background also influenced the subjective experience of pleasantness (F(1,70) = 24.6, p &#x0003c; 0.001, eta<sup>2 </sup>= 0.26), with less pleasantness during learning while acoustic background stimulation was present. The subjective feeling of arousal and sadness did not change as a function of different acoustic background conditions.</p></sec><sec><title>EEG data</title><p>The event-related synchronization/desynchronization data of the alpha band were first subjected to a 4-way ANOVA with repeated measurements on three factors (Time = 5 levels; Brain Area = 3 levels: frontal, central-temporal, and parieto-occipital; Background = 2 levels: background+ and background-) and one grouping factor (Group; 5 levels: in-tune fast, in-tune slow, out-of-tune fast, out-of-tune slow, noise). Where possible we used the multivariate approach to test the within-subjects effects, since this test is robust against violations of heteroscedasticity [<xref ref-type="bibr" rid="B80">80</xref>]. Figure <xref ref-type="fig" rid="F4">4</xref> demonstrates the mean ERDs and ERSs as topoplots broken down for the 10 time segments and for learning with (background+) and without (background-) musical background. Figure <xref ref-type="fig" rid="F5">5</xref> depicts the grand averages of event-related synchronization and desynchronization for the frontal, central-temporal, and parieto-occipital leads.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Mean ERDs and ERSs (in %) as topoplots broken down for the 10 time segments and for learning with (background+) and without (background-) musical background music</bold>. Blue indicates ERD and red ERS. The time segments are printed under the topoplots (upper panel). In the lower panel the topoplots are shown broken down for the two most interesting time segments (3 and 5) and the different musical background conditions.</p></caption><graphic xlink:href="1744-9081-6-3-4"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Time courses of the changes in alpha-band power after word presentation broken down for the three brain regions of interest</bold>. Each time segment represents the mean ERD/ERS over a 400 ms segment (averaged across the different groups and the 2 background conditions). Indicated are the 10 time segments after word presentation. Negative values indicate a decrease in alpha-band power (event related desynchronization: ERD) while positive values indicate an increase of alpha-band power (event related synchronization: ERS) during learning compared to the baseline condition.</p></caption><graphic xlink:href="1744-9081-6-3-5"/></fig><p>This complex ANOVA revealed several main effects and interactions. The event-related synchronization/desynchronization data showed a typical time course with a strong event-related desynchronization peaking at the second time segment (400 - 800 ms after word presentation). After reaching the maximum event-related desynchronization, the alpha-band synchronises again with the strongest event-related synchronization at the 5th time segment (1600 - 2000 ms after word presentation). This time course is highly significant (F(4,67) = 106.2, p &#x0003c; 0.001, eta<sup>2 </sup>= 0.86). The time courses of event-related synchronization/desynchronization are different for the different brain areas with larger event-related desynchronization for the parieto-occipital leads at the second time segment and larger event-related synchronization also at the parieto-occipital leads for the 5th time segment after word presentation (F(8, 63) = 34.50, p &#x0003c; 0001, eta<sup>2 </sup>= 0.81) (Figure <xref ref-type="fig" rid="F5">5</xref>). There was also a significant Background &#x000d7; Time &#x000d7; Group interaction (F(16,280) = 2.27, p = 0.004, eta<sup>2 </sup>= 0.11). In order to delineate this three-way interaction we conducted two-way ANOVAs with Background and Group as factors separately for each time segment. There were only significant Background &#x000d7; Group interactions for the time segments 3 and 5 (T3: F(4,70) = 2.6, p = 0.038, eta<sup>2 </sup>= 0.13; T5: F(4,70) = 2.7, p = 0.036, eta<sup>2 </sup>= 0.13). The interaction at time segment 3 (800 - 1200 ms after word presentation) was qualified by a larger event-related desynchronization for verbal learning with background music (background+) for the in-tune-fast group (F(1,14) = 9.2, p = 0.009, eta<sup>2 </sup>= 0.397 significant after Bonferroni-Holm correction at the level of p = 0.05). The interaction at time segment 5 (1600 - 2000 ms after word presentation) was qualified by larger event-related synchronization for the out-of-tune-fast group during background+ (F(1,14) = 6.3, p = 0.025, eta<sup>2 </sup>= 0.13, marginally significant after Bonferroni-Holm correction at the level of p = 0.05). Figure <xref ref-type="fig" rid="F6">6</xref> shows the mean event-related synchronization/desynchronization for the time segments 3 and 5.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Means and standard errors for event related desynchronization (ERD): (A) and event related synchronization (ERS): (B) values at time segment 3 and 5 (800 - 1200 ms and 1600 - 2000 ms after word presentation, respectively)</bold>. <bold>A) </bold>Mean ERD for the five experimental groups broken down for learning with (background+ in red) and without (background- in blue) musical background music. <bold>B) </bold>Mean ERS for the five experimental groups broken down for learning with (background+ in red) and without (background- in blue) musical background music. ITF: in-tune fast, ITS: in-tune slow, OTF: out-of-tune fast, OTS: out-of-tune slow).</p></caption><graphic xlink:href="1744-9081-6-3-6"/></fig><p>The ANOVA analysis conducted to examine potential between-hemisphere differences in terms of event-related synchronization/desynchronization revealed that none of the interactions of interest (Hemisphere &#x000d7; Group, Hemisphere &#x000d7; Group &#x000d7; Background) turned out to be significant, even if the statistical threshold was lowered to p = 0.10.</p></sec></sec><sec><title>Discussion</title><p>The present study examined the impact of background music on verbal learning performance. For this, we presented musical excerpts that were systematically varied in consonance and tempo. According to the <italic>Arousal-Emotion hypothesis</italic>, we anticipated that consonant and arousing musical excerpts would influence verbal learning positively, while dissonant musical excerpts would have a detrimental effect on learning. Drawing on the theory of <italic>changing state effects</italic>, we anticipated that rapidly changing auditory information would distract verbal learning more seriously than slowly changing music. Thus, slower musical pieces were expected to exert less detrimental effects on verbal learning than faster music. In order to control for the global influence of familiarity with and preference for specific music styles, we designed novel in-house musical excerpts that were unknown to the subjects. Using these excerpts, we did not uncover any substantial and consistent influence of background music on verbal learning.</p><p>The effect of passive music listening on cognitive performance is a long-standing matter of research. The findings of studies on the effects of background music on cognitive tasks are highly inconsistent, reporting either no effect, declines or improvements in performance (see the relevant literature mentioned in the introduction). The difference between the studies published to date and the present study is that we used novel musical excerpts and applied experimental conditions controlling for different levels of tempo and consonance. According to the <italic>Arousal-Emotion </italic>hypothesis, we hypothesised that positive background music arouses the perceiver and evokes positive affect. However, this hypothesis was not supported by our data since there was no beneficial effect of positive background music on verbal learning. In addition, according to the theory of <italic>changing state effects </italic>we anticipated that rapidly changing auditory information would distract verbal learning more seriously than slowly changing music. This theory also was not supported by our data.</p><p>As mentioned above, the findings of previously published studies examining the influence of background music on verbal learning and other cognitive processes are inconsistent, with more studies reporting no influence on verbal learning and other cognitive tasks. But before we argue too strongly about non-existing effects of background music on verbal learning we will discuss the differences between our study and the previous studies in this research area. Firstly, it is possible that the musical excerpts used may not have induced sufficiently strong arousal and emotional feelings to exert beneficial or detrimental effects on verbal learning. Although the four different musical excerpts significantly differed in terms of valence and arousal, the differences were a little smaller in this experiment than in the pilot experiment. Had we used musical excerpts that induce stronger emotional and arousal reactions, the effect on verbal learning may have been stronger. In addition, the difference between the slow and fast music in terms of changing auditory cues might not have been strong enough to influence verbal learning. There is however no available data to date to indicate "optimal" or more "optimal" levels of arousal and/or emotion and of changes in auditory cues for facilitating verbal learning.</p><p>A further aspect that distinguishes our study from previous studies is that the musical pieces were unknown to the subjects. It has been shown that music has an important role in autobiographical memory formation such that familiar and personally enjoyable and arousing music can elicit or more easily facilitate the retrieval of autobiographical memories (and possible other memory aspects) [<xref ref-type="bibr" rid="B48">48</xref>-<xref ref-type="bibr" rid="B50">50</xref>]. It is conceivable that the entire memory system (not only the autobiographical memory) is activated (aroused) by this kind of music, which in turn improves encoding and recall of information. Recently, S&#x000e4;rk&#x000e4;mo et al. [<xref ref-type="bibr" rid="B82">82</xref>] demonstrated that listening to preferred music improved verbal memory and attentional performance in stroke patients, thus supporting the <italic>Arousal-Emotion </italic>hypothesis. However, the specific mechanisms responsible for improving memory functions while listening to music (if indeed present) are still unclear. It is conceivable that the unknown musical pieces used in our study did not activate the memory system, thus, exerting no influence on the verbal learning system.</p><p>Although verbal learning performance did not differ between the different background stimulation conditions, there were some interesting differences in the underlying cortical activations. Before discussing them, we will outline the similarities in cortical activations observed for different background conditions. During learning, there was a general increase of event-related desynchronization 400-1200 ms after word presentation followed by an event-related synchronization in a fronto-parietal network. This time course indicates that this network is cortically more activated during encoding and retrieval in the first 1200 ms. After this, the activation pattern changes to event-related synchronization, which most likely reflects top-down inhibition processes supporting the consolidation of learned material [<xref ref-type="bibr" rid="B56">56</xref>].</p><p>Although the general pattern of cortical activation was quite similar across the different background conditions, we also identified some considerable differences. In the time window between 800 - 1200 ms after word presentation, we found stronger event-related desynchronization at frontal and parietal-occipital areas for the in-tune-fast group only. Clearly, frontal and parieto-occipital areas are stronger activated during verbal learning in the ambient setting of in-tune-fast music but not in the other conditions. Frontal and parietal areas are strongly involved in different stages of learning. The frontal cortex is involved in encoding and retrieval of information, while the parieto-occipital regions are part of a network involved in storing information [<xref ref-type="bibr" rid="B83">83</xref>-<xref ref-type="bibr" rid="B87">87</xref>]. One of the reasons for this event-related desynchronization increase could be that the fronto-parietal network devotes greater cortical resources to verbal learning material in the context of in-tune fast music (which by the way is the musical piece rated as being most pleasant and arousing). But we did not find a corresponding behavioural difference in learning performance. Thus, the activation increase may have been too small to be reflected in a behaviour-enhancing effect. A further possibility is that the in-tune-fast music is the most distracting of the background music types, therefore eliciting more bottom-up driven attention compared with the other musical pieces and constraining attentional resource availability for the verbal material. The lack of a decline in learning performance suggests that attentional resource capacity was such that this distracting effect (if indeed present) was compensated for.</p><p>A further finding is the stronger event-related synchronization in the fronto-parietal network at time segment 5 (1600 - 2000 ms after word presentation) for the out-of-tune groups and especially for the out-of-tune-fast group. Event-related synchronization is considered to reflect the action of inhibitory processes after a phase of cortical activation. For example, Klimesch et al. [<xref ref-type="bibr" rid="B56">56</xref>] propose that event-related synchronization reflects a kind of top-down inhibitory influence. Presumably, the subjects exert greater top-down inhibitory control to overcome the adverse impact on learning in the context of listening to out-of-tune background music.</p><p>We did not find different lateralisation patterns for event-related synchronization/desynchronization in the context of the different background music conditions. This is in contrast to some EEG studies reporting between hemispheric differences in terms of neurophysiological activation during listening of music of different valence. Two studies identified left-sided increase of activation especially when the subjects listened to positively valenced music opposed to negative music evoking a preponderance of activation on the right hemisphere (mostly in the frontal cortex) [<xref ref-type="bibr" rid="B76">76</xref>,<xref ref-type="bibr" rid="B77">77</xref>,<xref ref-type="bibr" rid="B88">88</xref>]. These findings are in correspondence with studies demonstrating dominance of left-sided activation during approach-related behaviour and stronger activation on the right during avoidance-related behaviour [<xref ref-type="bibr" rid="B89">89</xref>]. Thus, music eliciting positive emotions should also evoke more left-sided activation (especially in the frontal cortex), but not all studies support these assumptions. For example, the studies of Baumgartner et al. [<xref ref-type="bibr" rid="B78">78</xref>] and Sammler et al. [<xref ref-type="bibr" rid="B79">79</xref>] did not uncover lateralised activations in terms of Alpha power changes during listening to positive music. Sammler et al. identified an increase in midline Theta activity and not a lateralised activation pattern. In addition, most fMRI studies measuring cortical and subcortical activation patterns during music listening did not report lateralised brain activations [<xref ref-type="bibr" rid="B90">90</xref>-<xref ref-type="bibr" rid="B95">95</xref>]. All of these studies report mainly bilateral activation in the limbic system. One study also demonstrates that the brain responses to emotional music substantially changes over time [<xref ref-type="bibr" rid="B96">96</xref>]. In fact, a differentially lateralised activation pattern due to the valence of the presented music is not a typical finding. However, we believe that the particular pattern of brain activation and possible lateralisation patterns are due to several additional factors influencing brain activation during music listening. For example, experience or preference for particular music are potential candidates as modulatory factors. Which of these factors are indeed responsible for lateralised activations can not be clarified on the basis of current knowledge.</p><p>Future experiments should use musical pieces with which subjects are familiar and which evoke strong emotional feelings. Such musical pieces may influence memory performance and the associated cortical activations entirely differently. Future experiments should also seek to examine systematically the effect of pre-experimentally present or experimentally induced personal belief in the ability of background music to enhance performance and, depending on the findings, this should be controlled for in other subsequent studies. There may also be as yet undocumented, strong inter-individual differences in the modulatory impact of music on various psychological functions. Interestingly, subjects have different attention and empathy performance styles, and this may influence the performance of different cognitive functions.</p></sec><sec><title>Limitations</title><p>A methodological limitation of this study is the use of artificial musical stimuli, which are unknown to the subjects. In general we listen to music we really like when we have the opportunity to deliberately chose music. Thus, it might be that in case of listening to music we really appreciate to listen to while we learn the results might be entirely different. However, this has to be shown in future experiments.</p></sec><sec><title>Conclusion</title><p>Using different background music varying in tempo and consonance, we found no influence of background music on verbal learning. There were only changes in cortical activation in a fronto-parietal network (as measured with event-related desynchronization) around 400 - 800 ms after word presentation for the in-tune-fast-group, most likely reflecting a larger recruitment of cortical resources devoted to the control of memory processes. For the out-of-tune groups we found stronger event-related synchronization around 1600 - 2000 ms in a fronto-parietal network after word presentation, this thought to reflect stronger top-down inhibitory influences on the memory system. We suggest that this top-down inhibitory influence is at least in part a response to the slightly more distracting out-of-tune music that enables the memory system to adequately reengage in processing the verbal material.</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>LJ and PS both designed the experimental paradigm, performed the statistical analysis and drafted the manuscript. All authors read and approved the final manuscript.</p></sec></body><back><sec><title>Acknowledgements</title><p>We would like to thank Monika B&#x000fc;hlmann, Patricia Meier, Tim Walker, and Jasmin Sturzenegger for their diligent work in collecting the data. They also used these data for finalising their Master's thesis in psychology. We thank Marcus Cheetham for helpful comments on an earlier draft of this paper.</p></sec><ref-list><ref id="B1"><mixed-citation publication-type="journal"><name><surname>Boyle</surname><given-names>R</given-names></name><name><surname>Coltheart</surname><given-names>V</given-names></name><article-title>Effects of irrelevant sounds on phonological coding in reading comprehension and short-term memory</article-title><source>Q J Exp Psychol A</source><year>1996</year><volume>49</volume><fpage>398</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1080/027249896392702</pub-id><pub-id pub-id-type="pmid">8685391</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><name><surname>Crawford</surname><given-names>HJ</given-names></name><name><surname>Strapp</surname><given-names>CM</given-names></name><article-title>Effects of vocal and instrumental music on visuospatial and verbal performance as moderated by studying preference and personality</article-title><source>Pers Indiv Differ</source><year>1994</year><volume>16</volume><fpage>237</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/0191-8869(94)90162-7</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><name><surname>Fox</surname><given-names>JG</given-names></name><name><surname>Ebrey</surname><given-names>E</given-names></name><article-title>Music: an aid to productivity</article-title><source>Appl Eergon</source><year>1972</year><volume>3</volume><fpage>202</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1016/0003-6870(72)90101-9</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><name><surname>Furnham</surname><given-names>A</given-names></name><name><surname>Allas</surname><given-names>K</given-names></name><article-title>The influence of musical distraction of varying complexity on the cognitive performance of extroverts and introverts</article-title><source>Eur J Personality</source><year>1999</year><volume>13</volume><fpage>27</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1099-0984(199901/02)13:1&#x0003c;27::AID-PER318&#x0003e;3.0.CO;2-R</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><name><surname>Furnham</surname><given-names>A</given-names></name><name><surname>Trew</surname><given-names>S</given-names></name><name><surname>Sneade</surname><given-names>I</given-names></name><article-title>The distracting effects of vocal and instrumental music on the cognitive test performance of introverts and extraverts</article-title><source>Pers Indiv Diff</source><year>1999</year><volume>27</volume><fpage>381</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1016/S0191-8869(98)00249-9</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><name><surname>Furnham</surname><given-names>A</given-names></name><name><surname>Bradley</surname><given-names>A</given-names></name><article-title>Music while you work: The differential distraction of background music on the cognitive test performance of introverts and extraverts</article-title><source>Appl Cognitive Psych</source><year>1997</year><volume>11</volume><fpage>445</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1099-0720(199710)11:5&#x0003c;445::AID-ACP472&#x0003e;3.0.CO;2-R</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><name><surname>Hirokawa</surname><given-names>E</given-names></name><article-title>Effects of music listening and relaxation instructions on arousal changes and the working memory task in older adults</article-title><source>J Music Ther</source><year>2004</year><volume>41</volume><fpage>107</fpage><lpage>127</lpage><pub-id pub-id-type="pmid">15307814</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><name><surname>Hirokawa</surname><given-names>E</given-names></name><name><surname>Ohira</surname><given-names>H</given-names></name><article-title>The effects of music listening after a stressful task on immune functions, neuroendocrine responses, and emotional states in college students</article-title><source>J Music Ther</source><year>2003</year><volume>40</volume><fpage>189</fpage><lpage>211</lpage><pub-id pub-id-type="pmid">14567734</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><name><surname>Jancke</surname><given-names>L</given-names></name><name><surname>Musial</surname><given-names>F</given-names></name><name><surname>Vogt</surname><given-names>J</given-names></name><name><surname>Kalveram</surname><given-names>KT</given-names></name><article-title>Monitoring radio programs and time of day affect simulated car-driving performance</article-title><source>Percept Motor Skill</source><year>1994</year><volume>79</volume><fpage>484</fpage><lpage>486</lpage></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><name><surname>Kellaris</surname><given-names>JJ</given-names></name><name><surname>Kent</surname><given-names>RJ</given-names></name><article-title>The influence of music on customer's temporal perception</article-title><source>J Consum Psychol</source><year>1992</year><volume>4</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1016/S1057-7408(08)80060-5</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="other"><name><surname>Miskovic</surname><given-names>D</given-names></name><name><surname>Rosenthal</surname><given-names>R</given-names></name><name><surname>Zingg</surname><given-names>U</given-names></name><name><surname>Oertli</surname><given-names>D</given-names></name><name><surname>Metzger</surname><given-names>U</given-names></name><name><surname>Jancke</surname><given-names>L</given-names></name><article-title>Randomized controlled trial investigating the effect of music on the virtual reality laparoscopic learning performance of novice surgeons</article-title><source>Surg Endosc</source><year>2008</year></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><name><surname>Nomi</surname><given-names>JS</given-names></name><name><surname>Scherfeld</surname><given-names>D</given-names></name><name><surname>Friederichs</surname><given-names>S</given-names></name><name><surname>Schafer</surname><given-names>R</given-names></name><name><surname>Franz</surname><given-names>M</given-names></name><name><surname>Wittsack</surname><given-names>HJ</given-names></name><name><surname>Azari</surname><given-names>NP</given-names></name><name><surname>Missimer</surname><given-names>J</given-names></name><name><surname>Seitz</surname><given-names>RJ</given-names></name><article-title>On the neural networks of empathy: A principal component analysis of an fMRI study</article-title><source>Behav Brain Funct</source><year>2008</year><volume>4</volume><fpage>41</fpage><pub-id pub-id-type="doi">10.1186/1744-9081-4-41</pub-id><pub-id pub-id-type="pmid">18798977</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><name><surname>Oldham</surname><given-names>G</given-names></name><name><surname>Cummings</surname><given-names>A</given-names></name><name><surname>Mischel</surname><given-names>L</given-names></name><name><surname>Schmidhe</surname><given-names>J</given-names></name><name><surname>Zhan</surname><given-names>J</given-names></name><article-title>Listen while you work? Quasi-experimental relations between personal-stereo headset use and employee work responses</article-title><source>J Appl Psychol</source><year>1995</year><volume>80</volume><fpage>547</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1037/0021-9010.80.5.547</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><name><surname>Sousou</surname><given-names>SD</given-names></name><article-title>Effects of melody and lyrics on mood and memory</article-title><source>Percept Motor Skill</source><year>1997</year><volume>85</volume><fpage>31</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.2466/PMS.85.5.31-40</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><name><surname>Hallam</surname><given-names>S</given-names></name><name><surname>Price</surname><given-names>J</given-names></name><name><surname>Katsarou</surname><given-names>G</given-names></name><article-title>The effects of background music on primary school pupils' task perfromance</article-title><source>Educ Stud</source><year>2002</year><volume>28</volume><fpage>111</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1080/03055690220124551</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><name><surname>Bailey</surname><given-names>N</given-names></name><name><surname>Areni</surname><given-names>CS</given-names></name><article-title>Background music as a quasi clock in retrospective duration judgments</article-title><source>Percept Motor Skill</source><year>2006</year><volume>102</volume><fpage>435</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.2466/PMS.102.2.435-444</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><name><surname>Balch</surname><given-names>WR</given-names></name><name><surname>Bowman</surname><given-names>K</given-names></name><name><surname>Mohler</surname><given-names>L</given-names></name><article-title>Music-dependent memory in immediate and delayed word recall</article-title><source>Mem Cognition</source><year>1992</year><volume>20</volume><fpage>21</fpage><lpage>28</lpage></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><name><surname>Belsham</surname><given-names>RL</given-names></name><name><surname>Harman</surname><given-names>DW</given-names></name><article-title>Effect of vocal vs non-vocal music on visual recall</article-title><source>Percept Motor Skills</source><year>1977</year><volume>44</volume><fpage>857</fpage><lpage>858</lpage></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><name><surname>Brochard</surname><given-names>R</given-names></name><name><surname>Dufour</surname><given-names>A</given-names></name><name><surname>Despr&#x000e9;s</surname><given-names>O</given-names></name><article-title>Effect of musical expertise on visuospatial abilities: evidence from reaction times and mental imagery</article-title><source>Brain Cognition</source><year>2004</year><volume>54</volume><fpage>103</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1016/S0278-2626(03)00264-1</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><name><surname>Clark</surname><given-names>L</given-names></name><name><surname>Iversen</surname><given-names>SD</given-names></name><name><surname>Goodwin</surname><given-names>GM</given-names></name><article-title>The influence of positive and negative mood states on risk taking, verbal fluency, and salivary cortisol</article-title><source>J Affect Disord</source><year>2001</year><volume>63</volume><fpage>179</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/S0165-0327(00)00183-X</pub-id><pub-id pub-id-type="pmid">11246094</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><name><surname>Cockerton</surname><given-names>T</given-names></name><name><surname>Moore</surname><given-names>S</given-names></name><name><surname>Norman</surname><given-names>D</given-names></name><article-title>Cognitive test performance and background music</article-title><source>Percep Motor Skill</source><year>1997</year><volume>85</volume><fpage>1435</fpage><lpage>1438</lpage></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><name><surname>Davidson</surname><given-names>CW</given-names></name><name><surname>Powell</surname><given-names>LA</given-names></name><article-title>The effects of easy listening background music on the on-task-performance of fith-grade children</article-title><source>J Educ Res</source><year>1986</year><volume>80</volume><fpage>29</fpage><lpage>33</lpage></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><name><surname>Etaugh</surname><given-names>C</given-names></name><name><surname>Michals</surname><given-names>D</given-names></name><article-title>Effects of reading comprehension on preferred music and frequency of studying music</article-title><source>Percept Motor Skill</source><year>1975</year><volume>41</volume><fpage>553</fpage><lpage>554</lpage></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><name><surname>Etaugh</surname><given-names>C</given-names></name><name><surname>Ptasnik</surname><given-names>P</given-names></name><article-title>Effects of studying to music and post-study relaxation on reading comprehension</article-title><source>Percept Motor Skill</source><year>1985</year><volume>55</volume><fpage>141</fpage><lpage>142</lpage></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><name><surname>Felix</surname><given-names>U</given-names></name><article-title>The contribution of background music to the enhancement of learning in suggestopedia: A critical review of the literature</article-title><source>Journal of the Society of Accelerated Learning and Teaching</source><year>1993</year><volume>18</volume><fpage>277</fpage><lpage>303</lpage></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><name><surname>Fogelson</surname><given-names>S</given-names></name><article-title>Music as a distractor on reading-test performance of eight grade students</article-title><source>Percept Motor Skill</source><year>1973</year><volume>36</volume><fpage>265</fpage><lpage>1266</lpage></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><name><surname>Hilliard</surname><given-names>MO</given-names></name><name><surname>Tolin</surname><given-names>P</given-names></name><article-title>Effect of familarity with background music on performance of simple and difficult reading comprehension tasks</article-title><source>Percept Motor Skill</source><year>1979</year><volume>49</volume><fpage>713</fpage><lpage>714</lpage></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><name><surname>Johnson</surname><given-names>JK</given-names></name><name><surname>Cotman</surname><given-names>CW</given-names></name><name><surname>Tasaki</surname><given-names>CS</given-names></name><name><surname>Shaw</surname><given-names>GL</given-names></name><article-title>Enhancement of spatial-temporal reasoning after a Mozart listening condition in Alzheimer's disease: a case study</article-title><source>Neurol Res</source><year>1998</year><volume>20</volume><fpage>666</fpage><lpage>672</lpage><pub-id pub-id-type="pmid">9864729</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><name><surname>Lois</surname><given-names>H</given-names></name><article-title>Listening to music enhances spatial-temporal reasoning: evidence for the "Mozart Effect</article-title><source>J Aesthet Edu</source><year>2000</year><volume>34</volume><fpage>105</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.2307/3333640</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>LK</given-names></name><name><surname>Schyb</surname><given-names>M</given-names></name><article-title>Facilitation and interference by background music</article-title><source>J Music Ther</source><year>1989</year><volume>26</volume><fpage>42</fpage><lpage>54</lpage></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><name><surname>Nittono</surname><given-names>H</given-names></name><article-title>Background instrumental music and serial recall</article-title><source>Percept Motor Skill</source><year>1997</year><volume>84</volume><fpage>1307</fpage><lpage>1313</lpage></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><name><surname>Nittono</surname><given-names>H</given-names></name><name><surname>Tsuda</surname><given-names>A</given-names></name><name><surname>Akai</surname><given-names>S</given-names></name><name><surname>Nakajima</surname><given-names>Y</given-names></name><article-title>Tempo of background sound and performance speed</article-title><source>Percept Motor Skill</source><year>2000</year><volume>90</volume><fpage>1122</fpage><pub-id pub-id-type="doi">10.2466/PMS.90.3.1122-1122</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><name><surname>Register</surname><given-names>D</given-names></name><name><surname>Darrow</surname><given-names>AA</given-names></name><name><surname>Standley</surname><given-names>J</given-names></name><name><surname>Swedberg</surname><given-names>O</given-names></name><article-title>The use of music to enhance reading skills of second grade students and students with reading disabilities</article-title><source>J Music Ther</source><year>2007</year><volume>44</volume><fpage>23</fpage><lpage>37</lpage><pub-id pub-id-type="pmid">17419662</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><name><surname>Salam&#x000e9;</surname><given-names>P</given-names></name><name><surname>Baddeley</surname><given-names>A</given-names></name><article-title>Effects of background music on phonological short-term memory</article-title><source>Q J Exp Psychol A</source><year>1989</year><volume>41</volume><fpage>107</fpage><lpage>122</lpage></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><name><surname>Schreiber</surname><given-names>EH</given-names></name><article-title>Influence on music on college students' achievements</article-title><source>Percept Motor Skill</source><year>1988</year><volume>66</volume><fpage>338</fpage></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><name><surname>Schueller</surname><given-names>M</given-names></name><name><surname>Bond</surname><given-names>ZS</given-names></name><name><surname>Fucci</surname><given-names>D</given-names></name><name><surname>Gunderson</surname><given-names>F</given-names></name><name><surname>Vaz</surname><given-names>P</given-names></name><article-title>Possible influence of linguistic musical background on perceptual pitch-matching tasks: a pilot study</article-title><source>Percept Motor Skill</source><year>2004</year><volume>99</volume><fpage>421</fpage><lpage>428</lpage></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><name><surname>Stainback</surname><given-names>SB</given-names></name><name><surname>Stainback</surname><given-names>WC</given-names></name><name><surname>Hallahan</surname><given-names>DP</given-names></name><article-title>Effect of background music on learning</article-title><source>Except Child</source><year>1973</year><volume>40</volume><fpage>109</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">4741993</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><name><surname>Stainback</surname><given-names>SB</given-names></name><name><surname>Stainback</surname><given-names>WC</given-names></name><name><surname>Hallahan</surname><given-names>DP</given-names></name><name><surname>Payne</surname><given-names>JS</given-names></name><article-title>Effects of selected background music on task-relevant and task-irrelevant learning of institutionalized educable mentally retarded students</article-title><source>Train Sch Bull (Vinel)</source><year>1974</year><volume>71</volume><fpage>188</fpage><lpage>194</lpage><pub-id pub-id-type="pmid">4446220</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><name><surname>Tucker</surname><given-names>A</given-names></name><name><surname>Bushman</surname><given-names>BJ</given-names></name><article-title>Effects of rock and roll music on mathematical, verbal, and reading comprehension performance</article-title><source>Percept Motor Skill</source><year>1991</year><volume>72</volume><fpage>942</fpage><pub-id pub-id-type="doi">10.2466/PMS.72.3.942-942</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><name><surname>Wolf</surname><given-names>RH</given-names></name><name><surname>Wejner</surname><given-names>FF</given-names></name><article-title>Effects of four noise conditions on arithmetic performance</article-title><source>Percept Motor Skill</source><year>1972</year><volume>35</volume><fpage>928</fpage><lpage>930</lpage></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><name><surname>Ellermeier</surname><given-names>W</given-names></name><name><surname>Hellbruck</surname><given-names>J</given-names></name><article-title>Is level irrelevant in "Irrelevant speech"? Effects of loudness, signal-to-noise ratio, and binaural unmasking</article-title><source>J Exp Psychol Human</source><year>1998</year><volume>24</volume><fpage>1406</fpage><lpage>1414</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.24.5.1406</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><name><surname>Iwanaga</surname><given-names>M</given-names></name><name><surname>Ito</surname><given-names>T</given-names></name><article-title>Disturbance effect of music on processing of verbal and spatial memories</article-title><source>Percept Motor Skill</source><year>2002</year><volume>94</volume><fpage>1251</fpage><lpage>1258</lpage></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><name><surname>Klatte</surname><given-names>M</given-names></name><name><surname>Kilcher</surname><given-names>H</given-names></name><name><surname>Hellbruck</surname><given-names>J</given-names></name><article-title>The effects of temporal structure of background noise on working memory: Theoretical and practical implications</article-title><source>Z Exp Psychol</source><year>1995</year><volume>42</volume><fpage>517</fpage><lpage>544</lpage></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><name><surname>Klatte</surname><given-names>M</given-names></name><name><surname>Meis</surname><given-names>M</given-names></name><name><surname>Sukowski</surname><given-names>H</given-names></name><name><surname>Schick</surname><given-names>A</given-names></name><article-title>Effects of irrelevant speech and traffic noise on speech perception and cognitive performance in elementary school children</article-title><source>Noise Health</source><year>2007</year><volume>9</volume><fpage>64</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.4103/1463-1741.36982</pub-id><pub-id pub-id-type="pmid">18025757</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="book"><name><surname>Wallace</surname><given-names>WT</given-names></name><name><surname>Rubin</surname><given-names>DC</given-names></name><person-group person-group-type="editor">Neisser U, Winograd E</person-group><article-title>Wreck of the olg 97: A real event remembered in song</article-title><source>Remembering reconsidered: Ecological and traditional approaches to the study of memory</source><year>1988</year><publisher-name>Cambridge, England: Cambridge University Press</publisher-name><fpage>283</fpage><lpage>310</lpage></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="book"><name><surname>Lozanov</surname><given-names>G</given-names></name><source>Suggestology and outlines of suggestopedy</source><year>1978</year><publisher-name>New York: Gordon &#x00026; Breach</publisher-name></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="book"><name><surname>Schiffler</surname><given-names>L</given-names></name><source>Suggestop&#x000e4;die und Superlearning - empirisch gepr&#x000fc;ft</source><year>1989</year><publisher-name>Frankfurt am Main: Moritz Diesterweg Verlag</publisher-name></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><name><surname>Eschrich</surname><given-names>S</given-names></name><name><surname>Munte</surname><given-names>TF</given-names></name><name><surname>Altenmuller</surname><given-names>EO</given-names></name><article-title>Remember Bach: an investigation in episodic memory for music</article-title><source>Ann N Y Acad Sci</source><year>2005</year><volume>1060</volume><fpage>438</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1196/annals.1360.045</pub-id><pub-id pub-id-type="pmid">16597798</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><name><surname>Eschrich</surname><given-names>S</given-names></name><name><surname>Munte</surname><given-names>TF</given-names></name><name><surname>Altenmuller</surname><given-names>EO</given-names></name><article-title>Unforgettable film music: the role of emotion in episodic long-term memory for music</article-title><source>BMC Neurosci</source><year>2008</year><volume>9</volume><fpage>48</fpage><pub-id pub-id-type="doi">10.1186/1471-2202-9-48</pub-id><pub-id pub-id-type="pmid">18505596</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><name><surname>Jancke</surname><given-names>L</given-names></name><article-title>Music, memory and emotion</article-title><source>J Biol</source><year>2008</year><volume>7</volume><fpage>21</fpage><pub-id pub-id-type="pmid">18710596</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><name><surname>Husain</surname><given-names>G</given-names></name><name><surname>Thompson</surname><given-names>W</given-names></name><name><surname>Schellenberg</surname><given-names>E</given-names></name><article-title>Effects of musical tempo and mode on arousal, mood, and spatial abilities</article-title><source>Music Percept</source><year>2002</year><volume>20</volume><fpage>151</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1525/mp.2002.20.2.151</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><name><surname>Nantais</surname><given-names>K</given-names></name><name><surname>Schellenberg</surname><given-names>E</given-names></name><article-title>The Mozart Effect: An artifact of preference</article-title><source>Psychol Sci</source><year>1999</year><volume>10</volume><fpage>370</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00170</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><name><surname>Thompson</surname><given-names>WF</given-names></name><name><surname>Schellenberg</surname><given-names>EG</given-names></name><name><surname>Husain</surname><given-names>G</given-names></name><article-title>Arousal, mood, and the Mozart effect</article-title><source>Psychol Sci</source><year>2001</year><volume>12</volume><fpage>248</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00345</pub-id><pub-id pub-id-type="pmid">11437309</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><name><surname>Banbury</surname><given-names>SP</given-names></name><name><surname>Macken</surname><given-names>WJ</given-names></name><name><surname>Tremblay</surname><given-names>S</given-names></name><name><surname>Jones</surname><given-names>DM</given-names></name><article-title>Auditory distraction and short-term memory: phenomena and practical implications</article-title><source>Hum Factors</source><year>2001</year><volume>43</volume><fpage>12</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1518/001872001775992462</pub-id><pub-id pub-id-type="pmid">11474757</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><name><surname>Jones</surname><given-names>DM</given-names></name><name><surname>Alford</surname><given-names>D</given-names></name><name><surname>Macken</surname><given-names>WJ</given-names></name><name><surname>Banbury</surname><given-names>SP</given-names></name><name><surname>Tremblay</surname><given-names>S</given-names></name><article-title>Interference from degraded auditory stimuli: linear effects of changing-state in the irrelevant sequence</article-title><source>J Acoust Soc Am</source><year>2000</year><volume>108</volume><fpage>1082</fpage><lpage>1088</lpage><pub-id pub-id-type="doi">10.1121/1.1288412</pub-id><pub-id pub-id-type="pmid">11008810</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><name><surname>Klimesch</surname><given-names>W</given-names></name><article-title>EEG alpha and theta oscillations reflect cognitive and memory performance: a review and analysis</article-title><source>Brain Res Brain Res Rev</source><year>1999</year><volume>29</volume><fpage>169</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/S0165-0173(98)00056-3</pub-id><pub-id pub-id-type="pmid">10209231</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><name><surname>Laufs</surname><given-names>H</given-names></name><name><surname>Holt</surname><given-names>JL</given-names></name><name><surname>Elfont</surname><given-names>R</given-names></name><name><surname>Krams</surname><given-names>M</given-names></name><name><surname>Paul</surname><given-names>JS</given-names></name><name><surname>Krakow</surname><given-names>K</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><article-title>Where the BOLD signal goes when alpha EEG leaves</article-title><source>Neuroimage</source><year>2006</year><volume>31</volume><fpage>1408</fpage><lpage>1418</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.02.002</pub-id><pub-id pub-id-type="pmid">16537111</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><name><surname>Laufs</surname><given-names>H</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><name><surname>Beyerle</surname><given-names>A</given-names></name><name><surname>Eger</surname><given-names>E</given-names></name><name><surname>Salek-Haddadi</surname><given-names>A</given-names></name><name><surname>Preibisch</surname><given-names>C</given-names></name><name><surname>Krakow</surname><given-names>K</given-names></name><article-title>EEG-correlated fMRI of human alpha activity</article-title><source>Neuroimage</source><year>2003</year><volume>19</volume><fpage>1463</fpage><lpage>1476</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00286-6</pub-id><pub-id pub-id-type="pmid">12948703</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><name><surname>Oakes</surname><given-names>TR</given-names></name><name><surname>Pizzagalli</surname><given-names>DA</given-names></name><name><surname>Hendrick</surname><given-names>AM</given-names></name><name><surname>Horras</surname><given-names>KA</given-names></name><name><surname>Larson</surname><given-names>CL</given-names></name><name><surname>Abercrombie</surname><given-names>HC</given-names></name><name><surname>Schaefer</surname><given-names>SM</given-names></name><name><surname>Koger</surname><given-names>JV</given-names></name><name><surname>Davidson</surname><given-names>RJ</given-names></name><article-title>Functional coupling of simultaneous electrical and metabolic activity in the human brain</article-title><source>Hum Brain Mapp</source><year>2004</year><volume>21</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1002/hbm.20004</pub-id><pub-id pub-id-type="pmid">15038007</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="book"><name><surname>Lehrl</surname><given-names>S</given-names></name><name><surname>Gallwitz</surname><given-names>A</given-names></name><name><surname>Blaha</surname><given-names>L</given-names></name><source>Kurztest f&#x000fc;r Allgemeine Intelligenz</source><year>1992</year><publisher-name>G&#x000f6;ttingen: Hogrefe Testzentrale</publisher-name></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="book"><name><surname>Borkenau</surname><given-names>P</given-names></name><name><surname>Ostendorf</surname><given-names>F</given-names></name><source>NEO-FFI NEO-F&#x000fc;nf-Faktoren Inventar</source><year>2008</year><publisher-name>G&#x000f6;ttingen: G&#x000f6;ttingen: Hogrefe Testzentrale</publisher-name></mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><name><surname>Helmstaedter</surname><given-names>C</given-names></name><name><surname>Durwen</surname><given-names>HF</given-names></name><article-title>VLMT: Verbaler Lern- und Merkf&#x000e4;higkeitstest: Ein praktikables und differenziertes Instrumentarium zur Pr&#x000fc;fung der verbalen Ged&#x000e4;chtnisleistungen</article-title><source>Schweiz Arch Neurol</source><year>1990</year><volume>141</volume><fpage>21</fpage><lpage>30</lpage></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="book"><name><surname>Helmstaedter</surname><given-names>C</given-names></name><name><surname>Lendt</surname><given-names>M</given-names></name><name><surname>Lux</surname><given-names>S</given-names></name><source>VLMT -Verbaler Lern-und Merkf&#x000e4;higkeitstest</source><year>2001</year><publisher-name>G&#x000f6;ttingen: Hogrefe Testzentrale</publisher-name></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><name><surname>Annett</surname><given-names>M</given-names></name><article-title>A classification of hand preference by association analysis</article-title><source>Br J Psychol</source><year>1970</year><volume>61</volume><fpage>303</fpage><lpage>321</lpage><pub-id pub-id-type="pmid">5457503</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><name><surname>Blood</surname><given-names>AJ</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><article-title>Intensely pleasurable responses to music correlate with activity in brain regions implicated in reward and emotion</article-title><source>P Natl Acad Sci USA</source><year>2001</year><volume>20</volume><fpage>11818</fpage><lpage>11823</lpage><pub-id pub-id-type="doi">10.1073/pnas.191355898</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><name><surname>Blood</surname><given-names>AJ</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Bermudez</surname><given-names>P</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><article-title>Emotional responses to pleasant and unpleasant music correlate with activity in paralimbic brain regions</article-title><source>Nat Neurosci</source><year>1999</year><volume>4</volume><fpage>382</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1038/7299</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="other"><article-title>FL studio 4 software</article-title><ext-link ext-link-type="uri" xlink:href="http://flstudio.image-line.com/">http://flstudio.image-line.com/</ext-link></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="other"><article-title>Soundfiles used in this experiment</article-title><ext-link ext-link-type="uri" xlink:href="http://www.neurowissenschaft.ch/mmeyer/BBF">http://www.neurowissenschaft.ch/mmeyer/BBF</ext-link></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="book"><name><surname>Steyer</surname><given-names>R</given-names></name><name><surname>Schwenkmezger</surname><given-names>PNP</given-names></name><name><surname>Eid</surname><given-names>M</given-names></name><year>1997</year><publisher-name>G&#x000f6;ttingen: Hogrefe</publisher-name></mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><name><surname>Nater</surname><given-names>UM</given-names></name><name><surname>Abbruzzese</surname><given-names>E</given-names></name><name><surname>Krebs</surname><given-names>M</given-names></name><name><surname>Ehlert</surname><given-names>U</given-names></name><article-title>Sex differences in emotional and psychophysiological responses to musical stimuli</article-title><source>Int J Psychophysiol</source><year>2006</year><volume>62</volume><fpage>300</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2006.05.011</pub-id><pub-id pub-id-type="pmid">16828911</pub-id></mixed-citation></ref><ref id="B71"><mixed-citation publication-type="journal"><name><surname>Jung</surname><given-names>TP</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name><name><surname>Humphries</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>TW</given-names></name><name><surname>McKeown</surname><given-names>MJ</given-names></name><name><surname>Iragui</surname><given-names>V</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><article-title>Removing electroencephalographic artifacts by blind source separation</article-title><source>Psychophysiology</source><year>2000</year><volume>37</volume><fpage>163</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1017/S0048577200980259</pub-id><pub-id pub-id-type="pmid">10731767</pub-id></mixed-citation></ref><ref id="B72"><mixed-citation publication-type="journal"><name><surname>Jung</surname><given-names>TP</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name><name><surname>Westerfield</surname><given-names>M</given-names></name><name><surname>Townsend</surname><given-names>J</given-names></name><name><surname>Courchesne</surname><given-names>E</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><article-title>Removal of eye activity artifacts from visual event-related potentials in normal and clinical subjects</article-title><source>Clin Neurophysiol</source><year>2000</year><volume>111</volume><fpage>1745</fpage><lpage>1758</lpage><pub-id pub-id-type="doi">10.1016/S1388-2457(00)00386-2</pub-id><pub-id pub-id-type="pmid">11018488</pub-id></mixed-citation></ref><ref id="B73"><mixed-citation publication-type="journal"><name><surname>Pfurtscheller</surname><given-names>G</given-names></name><name><surname>Lopes</surname><given-names>da Silva FH</given-names></name><article-title>Event-related EEG/MEG synchronization and desynchronization: basic principles</article-title><source>Clin Neurophysiol</source><year>1999</year><volume>110</volume><fpage>1842</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1016/S1388-2457(99)00141-8</pub-id><pub-id pub-id-type="pmid">10576479</pub-id></mixed-citation></ref><ref id="B74"><mixed-citation publication-type="journal"><name><surname>Pfurtscheller</surname><given-names>G</given-names></name><name><surname>Andrew</surname><given-names>C</given-names></name><article-title>Event-Related changes of band power and coherence: methodology and interpretation</article-title><source>J Clin Neurophysiol</source><year>1999</year><volume>16</volume><fpage>512</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1097/00004691-199911000-00003</pub-id><pub-id pub-id-type="pmid">10600019</pub-id></mixed-citation></ref><ref id="B75"><mixed-citation publication-type="journal"><name><surname>Jausovec</surname><given-names>N</given-names></name><name><surname>Habe</surname><given-names>K</given-names></name><article-title>The "Mozart effect": an electroencephalographic analysis employing the methods of induced event-related desynchronization/synchronization and event-related coherence</article-title><source>Brain Topogr</source><year>2003</year><volume>16</volume><fpage>73</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1023/B:BRAT.0000006331.10425.4b</pub-id><pub-id pub-id-type="pmid">14977200</pub-id></mixed-citation></ref><ref id="B76"><mixed-citation publication-type="journal"><name><surname>Schmidt</surname><given-names>LA</given-names></name><name><surname>Trainor</surname><given-names>LJ</given-names></name><article-title>Frontal brain electrical activity (EEG) distinguishes valence and intensity of musical emotions</article-title><source>Cognition Emotion</source><year>2001</year><volume>15</volume><fpage>487</fpage><lpage>500</lpage></mixed-citation></ref><ref id="B77"><mixed-citation publication-type="journal"><name><surname>Tsang</surname><given-names>CD</given-names></name><name><surname>Trainor</surname><given-names>LJ</given-names></name><name><surname>Santesso</surname><given-names>DL</given-names></name><name><surname>Tasker</surname><given-names>SL</given-names></name><name><surname>Schmidt</surname><given-names>LA</given-names></name><article-title>Frontal EEG responses as a function of affective musical features</article-title><source>Ann N Y Acad Sci</source><year>2001</year><volume>930</volume><fpage>439</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">11458862</pub-id></mixed-citation></ref><ref id="B78"><mixed-citation publication-type="journal"><name><surname>Baumgartner</surname><given-names>T</given-names></name><name><surname>Esslen</surname><given-names>M</given-names></name><name><surname>Jancke</surname><given-names>L</given-names></name><article-title>From emotion perception to emotion experience: emotions evoked by pictures and classical music</article-title><source>Int J Psychophysiol</source><year>2006</year><volume>60</volume><fpage>34</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2005.04.007</pub-id><pub-id pub-id-type="pmid">15993964</pub-id></mixed-citation></ref><ref id="B79"><mixed-citation publication-type="journal"><name><surname>Sammler</surname><given-names>D</given-names></name><name><surname>Grigutsch</surname><given-names>M</given-names></name><name><surname>Fritz</surname><given-names>T</given-names></name><name><surname>Koelsch</surname><given-names>S</given-names></name><article-title>Music and emotion: Electrophysiological correlates of the processing of pleasant and unpleasant music</article-title><source>Psychophysiology</source><year>2007</year><volume>44</volume><fpage>293</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00497.x</pub-id><pub-id pub-id-type="pmid">17343712</pub-id></mixed-citation></ref><ref id="B80"><mixed-citation publication-type="journal"><name><surname>O'Brien</surname><given-names>RG</given-names></name><name><surname>Kaiser</surname><given-names>MK</given-names></name><article-title>MANOVA method for analyzing repeated measures designs: An extensive primer</article-title><source>Psychol Bull</source><year>1985</year><volume>97</volume><fpage>316</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.97.2.316</pub-id><pub-id pub-id-type="pmid">3983301</pub-id></mixed-citation></ref><ref id="B81"><mixed-citation publication-type="other"><name><surname>Holm</surname><given-names>S</given-names></name><article-title>A simple sequentially rejective multiple test procedure</article-title><source>Scand J Stat</source><year>1979</year><fpage>65</fpage><lpage>70</lpage></mixed-citation></ref><ref id="B82"><mixed-citation publication-type="journal"><name><surname>S&#x000e4;rk&#x000e4;m&#x000f6;</surname><given-names>T</given-names></name><name><surname>Tervaniemi</surname><given-names>M</given-names></name><name><surname>Laitinen</surname><given-names>S</given-names></name><name><surname>Forsblom</surname><given-names>A</given-names></name><name><surname>Soinila</surname><given-names>S</given-names></name><name><surname>Mikkonen</surname><given-names>M</given-names></name><name><surname>Autti</surname><given-names>T</given-names></name><name><surname>Silvennoinen</surname><given-names>HM</given-names></name><name><surname>Erkkil&#x000e4;</surname><given-names>J</given-names></name><name><surname>Laine</surname><given-names>M</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Hietanen</surname><given-names>M</given-names></name><article-title>Music listening enhances cognitive recovery and mood after middle cerebral artery stroke</article-title><source>Brain</source><year>2008</year><volume>131</volume><fpage>866</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1093/brain/awn013</pub-id><pub-id pub-id-type="pmid">18287122</pub-id></mixed-citation></ref><ref id="B83"><mixed-citation publication-type="journal"><name><surname>Eustache</surname><given-names>F</given-names></name><name><surname>Desgranges</surname><given-names>B</given-names></name><article-title>MNESIS: towards the integration of current multisystem models of memory</article-title><source>Neuropsychol Rev</source><year>2008</year><volume>18</volume><fpage>53</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1007/s11065-008-9052-3</pub-id><pub-id pub-id-type="pmid">18311523</pub-id></mixed-citation></ref><ref id="B84"><mixed-citation publication-type="journal"><name><surname>Gabrieli</surname><given-names>JD</given-names></name><article-title>Cognitive neuroscience of human memory</article-title><source>Annu Rev Psychol</source><year>1998</year><volume>49</volume><fpage>87</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.49.1.87</pub-id><pub-id pub-id-type="pmid">9496622</pub-id></mixed-citation></ref><ref id="B85"><mixed-citation publication-type="journal"><name><surname>Gabrieli</surname><given-names>JD</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Desmond</surname><given-names>JE</given-names></name><article-title>The role of left prefrontal cortex in language and memory</article-title><source>Proc Natl Acad Sci USA</source><year>1998</year><volume>95</volume><fpage>906</fpage><lpage>913</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.3.906</pub-id><pub-id pub-id-type="pmid">9448258</pub-id></mixed-citation></ref><ref id="B86"><mixed-citation publication-type="journal"><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gabrieli</surname><given-names>JD</given-names></name><article-title>Memory and the brain: what's right and what's left?</article-title><source>Cell</source><year>1998</year><volume>93</volume><fpage>1091</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1016/S0092-8674(00)81451-8</pub-id><pub-id pub-id-type="pmid">9657140</pub-id></mixed-citation></ref><ref id="B87"><mixed-citation publication-type="journal"><name><surname>Tulving</surname><given-names>E</given-names></name><article-title>[Episodic memory: from mind to brain]</article-title><source>Rev Neurol (Paris)</source><year>2004</year><volume>160</volume><fpage>S9</fpage><lpage>23</lpage><pub-id pub-id-type="pmid">15118549</pub-id></mixed-citation></ref><ref id="B88"><mixed-citation publication-type="journal"><name><surname>Altenm&#x000fc;ller</surname><given-names>E</given-names></name><name><surname>Sch&#x000fc;rmann</surname><given-names>K</given-names></name><name><surname>Lim</surname><given-names>VK</given-names></name><name><surname>Parlitz</surname><given-names>D</given-names></name><article-title>Hits to the left, flops to the right: different emotions during listening to music are reflected in cortical lateralisation patterns</article-title><source>Neuropsychologia</source><year>2002</year><volume>13</volume><fpage>2242</fpage><lpage>2256</lpage><pub-id pub-id-type="doi">10.1016/S0028-3932(02)00107-0</pub-id></mixed-citation></ref><ref id="B89"><mixed-citation publication-type="journal"><name><surname>Davidson</surname><given-names>RJ</given-names></name><article-title>Anterior electrophysiological asymmetries, emotion, and depression: conceptual and methodological conundrums</article-title><source>Psychophysiology</source><year>1998</year><volume>35</volume><fpage>607</fpage><lpage>614</lpage><pub-id pub-id-type="doi">10.1017/S0048577298000134</pub-id><pub-id pub-id-type="pmid">9715104</pub-id></mixed-citation></ref><ref id="B90"><mixed-citation publication-type="journal"><name><surname>Gosselin</surname><given-names>N</given-names></name><name><surname>Samson</surname><given-names>S</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name><name><surname>Noulhiane</surname><given-names>M</given-names></name><name><surname>Roy</surname><given-names>M</given-names></name><name><surname>Hasboun</surname><given-names>D</given-names></name><name><surname>Baulac</surname><given-names>M</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><article-title>Emotional responses to unpleasant music correlates with damage to the parahippocampal cortex</article-title><source>Brain</source><year>2006</year><volume>129</volume><fpage>2585</fpage><lpage>2592</lpage><pub-id pub-id-type="doi">10.1093/brain/awl240</pub-id><pub-id pub-id-type="pmid">16959817</pub-id></mixed-citation></ref><ref id="B91"><mixed-citation publication-type="journal"><name><surname>Green</surname><given-names>AC</given-names></name><name><surname>Baerentsen</surname><given-names>KB</given-names></name><name><surname>St&#x000f8;dkilde-J&#x000f8;rgensen</surname><given-names>H</given-names></name><name><surname>Wallentin</surname><given-names>M</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><article-title>Music in minor activates limbic structures: a relationship with dissonance?</article-title><source>Neuroreport</source><year>2008</year><volume>19</volume><fpage>711</fpage><lpage>715</lpage><pub-id pub-id-type="doi">10.1097/WNR.0b013e3282fd0dd8</pub-id><pub-id pub-id-type="pmid">18418244</pub-id></mixed-citation></ref><ref id="B92"><mixed-citation publication-type="journal"><name><surname>Khalfa</surname><given-names>S</given-names></name><name><surname>Schon</surname><given-names>D</given-names></name><name><surname>Anton</surname><given-names>JL</given-names></name><name><surname>Li&#x000e9;geois-Chauvel</surname><given-names>C</given-names></name><article-title>Brain regions involved in the recognition of happiness and sadness in music</article-title><source>Neuroreport</source><year>2005</year><volume>16</volume><fpage>1981</fpage><lpage>1984</lpage><pub-id pub-id-type="doi">10.1097/00001756-200512190-00002</pub-id><pub-id pub-id-type="pmid">16317338</pub-id></mixed-citation></ref><ref id="B93"><mixed-citation publication-type="journal"><name><surname>Mitterschiffthaler</surname><given-names>MT</given-names></name><name><surname>Fu</surname><given-names>CH</given-names></name><name><surname>Dalton</surname><given-names>JA</given-names></name><name><surname>Andrew</surname><given-names>CM</given-names></name><name><surname>Williams</surname><given-names>SC</given-names></name><article-title>A functional MRI study of happy and sad affective states induced by classical music</article-title><source>Hum Brain Mapp</source><year>2007</year><volume>28</volume><fpage>1150</fpage><lpage>1162</lpage><pub-id pub-id-type="doi">10.1002/hbm.20337</pub-id><pub-id pub-id-type="pmid">17290372</pub-id></mixed-citation></ref><ref id="B94"><mixed-citation publication-type="journal"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Fritz</surname><given-names>T</given-names></name><name><surname>Schlaug</surname><given-names>G</given-names></name><article-title>Amygdala activity can be modulated by unexpected chord functions during music listening</article-title><source>Neuroreport</source><year>2008</year><volume>19</volume><fpage>1815</fpage><lpage>1819</lpage><pub-id pub-id-type="doi">10.1097/WNR.0b013e32831a8722</pub-id><pub-id pub-id-type="pmid">19050462</pub-id></mixed-citation></ref><ref id="B95"><mixed-citation publication-type="journal"><name><surname>Lerner</surname><given-names>Y</given-names></name><name><surname>Papo</surname><given-names>D</given-names></name><name><surname>Zhdanov</surname><given-names>A</given-names></name><name><surname>Belozersky</surname><given-names>L</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><article-title>Eyes wide shut: amygdala mediates eyes-closed effect on emotional experience with music</article-title><source>PLoS ONE</source><year>2009</year><volume>4</volume><fpage>e6230</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0006230</pub-id><pub-id pub-id-type="pmid">19603072</pub-id></mixed-citation></ref><ref id="B96"><mixed-citation publication-type="journal"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Fritz</surname><given-names>TV</given-names></name><name><surname>Cramon</surname><given-names>DY</given-names></name><name><surname>Muller</surname><given-names>K</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><article-title>Investigating emotion with music: an fMRI study</article-title><source>Hum Brain Mapp</source><year>2006</year><volume>27</volume><fpage>239</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1002/hbm.20180</pub-id><pub-id pub-id-type="pmid">16078183</pub-id></mixed-citation></ref></ref-list></back></article>
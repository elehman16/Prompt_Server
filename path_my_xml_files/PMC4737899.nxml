<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26869944</article-id><article-id pub-id-type="pmc">4737899</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2015.02066</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Auditory Perceptual Learning in Adults with and without Age-Related Hearing Loss</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Karawani</surname><given-names>Hanin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/205758/overview"/></contrib><contrib contrib-type="author"><name><surname>Bitan</surname><given-names>Tali</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/299733/overview"/></contrib><contrib contrib-type="author"><name><surname>Attias</surname><given-names>Joseph</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Banai</surname><given-names>Karen</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/15240/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>The Department of Communication Sciences and Disorders, Faculty of Social Welfare and Health Sciences, University of Haifa</institution><country>Haifa, Israel</country></aff><aff id="aff2"><sup>2</sup><institution>The Department of Psychology, Faculty of Social Sciences, University of Haifa</institution><country>Haifa, Israel</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Jerker R&#x000f6;nnberg, Link&#x000f6;ping University, Sweden</p></fn><fn fn-type="edited-by"><p>Reviewed by: Carine Signoret, Linnaeus Centre HEAD, Sweden; Larry E. Humes, Indiana University, USA</p></fn><corresp id="fn001">*Correspondence: Hanin Karawani <email xlink:type="simple">hanin7@gmail.com</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Auditory Cognitive Neuroscience, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>03</day><month>2</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>6</volume><elocation-id>2066</elocation-id><history><date date-type="received"><day>30</day><month>9</month><year>2015</year></date><date date-type="accepted"><day>31</day><month>12</month><year>2015</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016 Karawani, Bitan, Attias and Banai.</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Karawani, Bitan, Attias and Banai</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p><bold>Introduction :</bold> Speech recognition in adverse listening conditions becomes more difficult as we age, particularly for individuals with age-related hearing loss (ARHL). Whether these difficulties can be eased with training remains debated, because it is not clear whether the outcomes are sufficiently general to be of use outside of the training context. The aim of the current study was to compare training-induced learning and generalization between normal-hearing older adults and those with ARHL.</p><p><bold>Methods :</bold> Fifty-six listeners (60&#x02013;72 y/o), 35 participants with ARHL, and 21 normal hearing adults participated in the study. The study design was a cross over design with three groups (immediate-training, delayed-training, and no-training group). Trained participants received 13 sessions of home-based auditory training over the course of 4 weeks. Three adverse listening conditions were targeted: (1) Speech-in-noise, (2) time compressed speech, and (3) competing speakers, and the outcomes of training were compared between normal and ARHL groups. Pre- and post-test sessions were completed by all participants. Outcome measures included tests on all of the trained conditions as well as on a series of untrained conditions designed to assess the transfer of learning to other speech and non-speech conditions.</p><p><bold>Results :</bold> Significant improvements on all trained conditions were observed in both ARHL and normal-hearing groups over the course of training. Normal hearing participants learned more than participants with ARHL in the speech-in-noise condition, but showed similar patterns of learning in the other conditions. Greater pre- to post-test changes were observed in trained than in untrained listeners on all trained conditions. In addition, the ability of trained listeners from the ARHL group to discriminate minimally different pseudowords in noise also improved with training.</p><p><bold>Conclusions :</bold> ARHL did not preclude auditory perceptual learning but there was little generalization to untrained conditions. We suggest that most training-related changes occurred at higher level task-specific cognitive processes in both groups. However, these were enhanced by high quality perceptual representations in the normal-hearing group. In contrast, some training-related changes have also occurred at the level of phonemic representations in the ARHL group, consistent with an interaction between bottom-up and top-down processes.</p></abstract><kwd-group><kwd>presbycusis</kwd><kwd>age-related hearing loss</kwd><kwd>auditory training</kwd><kwd>speech in noise</kwd><kwd>time-compressed speech</kwd><kwd>perceptual learning</kwd></kwd-group><counts><fig-count count="5"/><table-count count="3"/><equation-count count="0"/><ref-count count="59"/><page-count count="14"/><word-count count="10795"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Speech perception and communication in noisy environments become more difficult as we age. Specifically, older adults often experience considerable difficulties when listening to speech in the presence of background noise, to competing speech signals or to rapid speech (Pichora-Fuller et al., <xref rid="B36" ref-type="bibr">1995</xref>). Because these conditions are present in everyday situations, many older-adults find it difficult to understand speech in everyday life. These difficulties are often exacerbated by age-related hearing loss (ARHL; Fitzgibbons and Gordon-Salant, <xref rid="B18" ref-type="bibr">2010</xref>) which is one of the most prevalent chronic health conditions among the elderly (Yueh et al., <xref rid="B58" ref-type="bibr">2003</xref>). ARHL is estimated to affect more than 25% of the population aged 60 or more and its incidence is expected to increase with the aging of the population (Roth et al., <xref rid="B41" ref-type="bibr">2011</xref>). While it has been shown that ARHL is the major cause of these speech perception difficulties, research has shown that cognitive functions such as memory and attention also affect these difficulties (Pichora-Fuller, <xref rid="B33" ref-type="bibr">2008</xref>; Humes and Dubno, <xref rid="B25" ref-type="bibr">2010</xref>).</p><p>Individuals with sensorineural hearing loss can regain some lost auditory function with the help of hearing aids (Gil and Iorio, <xref rid="B19" ref-type="bibr">2010</xref>; Lavie et al., <xref rid="B28" ref-type="bibr">2014</xref>, <xref rid="B29" ref-type="bibr">2015</xref>), however this is often insufficient when speech perception under non-optimal conditions is considered (Kochkin, <xref rid="B26" ref-type="bibr">2000</xref>; Gordon-Salant, <xref rid="B20" ref-type="bibr">2005</xref>). Therefore, attempts are being made to supplement the rehabilitation process with patient-centered education, counseling, and auditory training, which were hypothesized to help listeners compensate for degradation in the auditory signal and improve communication (Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>). In this vein a number of studies have suggested that auditory training may be beneficial for individuals with ARHL (Sweetow and Palmer, <xref rid="B47" ref-type="bibr">2005</xref>; Stecker et al., <xref rid="B46" ref-type="bibr">2006</xref>; Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>, <xref rid="B50" ref-type="bibr">2007</xref>; Sweetow and Henderson Sabes, <xref rid="B48" ref-type="bibr">2010</xref>; Lavie et al., <xref rid="B27" ref-type="bibr">2013</xref>). Studies with older adults have shown that even participants with normal pure-tone and speech perception thresholds often report that listening in everyday life has become effortful (Schneider et al., <xref rid="B43" ref-type="bibr">2002</xref>). Thus, the current study specifically asks whether a home-based auditory training approach that mimics the challenges of real-world listening can improve speech perception in normal-hearing and in hearing impaired older adults, and whether the patterns of learning and generalization are influenced by the presence of a hearing impairment.</p><sec><title>Speech processing in younger and older adults</title><p>Speech processing involves not only the perception and identification of individual speech sounds and words, but also the integration of successively heard words, phrases, and sentences to achieve a coherent and accurate representation of the meaning of the message being communicated. In this process distinct (but interactive) neural networks process both the acoustic structure and the meaning of speech. The end result, mapping sounds to meaning, relies on matching the output from acoustic and phonetic analyses with stored lexical representations (Davis and Johnsrude, <xref rid="B13" ref-type="bibr">2007</xref>; Hickok and Poeppel, <xref rid="B23" ref-type="bibr">2007</xref>). Thus, accurate speech processing requires the use of voice and emotions cues, the use of silent gaps and duration cues to recognize phonemes, the use of temporal envelope patterns related to the rate of speech and spectral information, and access to and retrieval of semantic information (Price et al., <xref rid="B38" ref-type="bibr">2005</xref>; Pichora-Fuller and Macdonald, <xref rid="B35" ref-type="bibr">2008</xref>). Moreover, cognitive processes such as working memory, selective attention, and the speed at which information can be processed also affect speech understanding (Pichora-Fuller and Singh, <xref rid="B37" ref-type="bibr">2006</xref>). The use of knowledge and semantic context (e.g., phonological and semantic knowledge of phonemes, words, and sentences) is known to enhance recall and comprehension in older and younger adults (Wingfield and Stine-Morrow, <xref rid="B55" ref-type="bibr">2000</xref>; Pichora-Fuller, <xref rid="B33" ref-type="bibr">2008</xref>; Tun et al., <xref rid="B52" ref-type="bibr">2012</xref>).</p><p>According to several theoretical accounts, the relative contributions of lower-level sensory and perceptual processes or representations and higher-level cognitive processes (e.g., working memory, semantic processes) to speech recognition may differ between optimal and unfavorable listening conditions [e.g., the Ease of Language Understanding Model (R&#x000f6;nnberg et al., <xref rid="B39" ref-type="bibr">2013</xref>) or the Reverse Hierarchy Theory RHT (Ahissar et al., <xref rid="B1" ref-type="bibr">2009</xref>)]. According to the Ease of Language Understanding Model (ELU), incoming speech is initially processed automatically and a phonological representation of the signal is created. Word recognition (or &#x0201c;lexical access&#x0201d;) should occur if this automatically created representation matches an existing representation in long term memory. However, when an automatically created representation does not match an existing representation in long term memory, for example when the signal is degraded or when sensory processing of the signal is less precise due to hearing loss, an explicit and effortful working memory process is engaged in an attempt to compensate for the mismatch between the phonological representation and long term memory prolonging the recognition process (R&#x000f6;nnberg et al., <xref rid="B40" ref-type="bibr">2008</xref>, <xref rid="B39" ref-type="bibr">2013</xref>). Therefore, under difficult listening conditions or when hearing is impaired listeners are more likely than otherwise to engage in top-down processes that would allow semantic or real-world knowledge to influence speech recognition through working memory or attentional processes (R&#x000f6;nnberg et al., <xref rid="B39" ref-type="bibr">2013</xref>).</p><p>Lower level processes are compromised to a greater extent in older-adults with ARHL group than in normal-hearing older adults. For example, older adults with presbycusis required more favorable signal-to-noise ratios (SNRs) to benefit from the ability to predict sentence-final words from sentence context than older adults with normal hearing, even though the magnitude of the context effect was similar in the two groups (Pichora-Fuller et al., <xref rid="B36" ref-type="bibr">1995</xref>). This example also suggests that hearing impairment does not necessarily interfere with the ability to engage top-down processes to support listening. Rather, studies have shown that as supra-threshold auditory processing gradually declines over decades, the brain reorganizes so that more frontal brain areas, including those serving semantic processing and working memory, are activated to a greater extent in older compared to younger brains in conditions in which the performance of older and younger adults is matched (Wingfield and Grossman, <xref rid="B54" ref-type="bibr">2006</xref>; Peelle et al., <xref rid="B32" ref-type="bibr">2011</xref>). As speech becomes less intelligible, processing relies more on top-down influences from frontal areas (Pichora-Fuller et al., <xref rid="B36" ref-type="bibr">1995</xref>; Zekveld et al., <xref rid="B59" ref-type="bibr">2006</xref>). A similar conclusion was reached in an MRI study that found higher correlation between the volume of frontal areas and speech in noise perception in older adults compared to normal-hearing young adults (Wong et al., <xref rid="B56" ref-type="bibr">2010</xref>). Despite this compensatory engagement of higher-level brain areas, older adults experience disproportionate difficulties in understanding speech in ecological conditions that include suboptimal noise conditions and fast talkers. Therefore, successful auditory training in this population should foster an effective balance between bottom-up, signal-based processes, and top-down knowledge-based processes (Pichora-Fuller and Levitt, <xref rid="B34" ref-type="bibr">2012</xref>).</p></sec><sec><title>Auditory training</title><p>Auditory training for the purpose of hearing rehabilitation involves active listening to auditory stimuli and aims to improve the ability of participants to comply with the demands of non-optimal listening environments (Boothroyd, <xref rid="B8" ref-type="bibr">2007</xref>; Henderson Sabes and Sweetow, <xref rid="B21" ref-type="bibr">2007</xref>). Home-based auditory training programs were developed to allow adults with hearing loss to engage in perceptual learning, which in turn may lead to better speech understanding and improved communication ability (Sweetow and Sabes, <xref rid="B50" ref-type="bibr">2007</xref>). The consequences of training specific auditory skills are often specific to the trained stimuli (e.g., Wright et al., <xref rid="B57" ref-type="bibr">1997</xref>; Cainer et al., <xref rid="B12" ref-type="bibr">2008</xref>). In addition training outcomes also depend on the trained task (Amitay et al., <xref rid="B3" ref-type="bibr">2006</xref>), suggesting that plasticity is also mediated by cognitive task-specific mechanisms rather than by only the sensory attributes of the trained stimuli. Other factors such as feedback (Amitay et al., <xref rid="B2" ref-type="bibr">2010</xref>) and motivation (Amitay et al., <xref rid="B2" ref-type="bibr">2010</xref>; Levitt et al., <xref rid="B30" ref-type="bibr">2011</xref>; Ferguson and Henshaw, <xref rid="B15" ref-type="bibr">2015a</xref>) likewise influence training outcomes.</p><p>Two aspects of learning were typically quantified to document the effects of training on listening skills in the context of hearing rehabilitation&#x02014;&#x0201c;on-task&#x0201d; learning defined as improvements on the trained tasks and &#x0201c;generalization&#x0201d; defined as improvements in tasks that are not trained directly. On-task learning following auditory training in older adults with ARHL is usually robust, however generalization of learning to untrained tasks or stimuli that were not experienced directly during training does not always occur, or is very small (see Henshaw and Ferguson, <xref rid="B22" ref-type="bibr">2013</xref> for a similar use of the terms). Robust effects of &#x0201c;on-task learning&#x0201d; were previously reported for syllables and words in older adults with hearing loss (Burk et al., <xref rid="B11" ref-type="bibr">2006</xref>; Stecker et al., <xref rid="B46" ref-type="bibr">2006</xref>; Burk and Humes, <xref rid="B10" ref-type="bibr">2008</xref>; Humes et al., <xref rid="B24" ref-type="bibr">2009</xref>; Ferguson et al., <xref rid="B17" ref-type="bibr">2014</xref>). Burk and colleagues examined the effect of word-based auditory training and focused on word-recognition abilities within a background noise with varied words and talkers. Such training on perceptual distinctions assumes that by resolving lower level sensory issues through training, listening and communication should improve in a bottom-up manner. In their studies, improvements on the trained task were maintained over an extended period of time; however, generalization to untrained words did not occur (Burk et al., <xref rid="B11" ref-type="bibr">2006</xref>). Although there is evidence to suggest that training using multiple talkers promotes greater word-in-noise learning that generalizes to unfamiliar speakers (Burk et al., <xref rid="B11" ref-type="bibr">2006</xref>), such training yields learning that is specific to the content of the trained stimuli and does not always generalize to unfamiliar words, nor familiar words embedded in unfamiliar sentences (Humes et al., <xref rid="B24" ref-type="bibr">2009</xref>).</p><p>Other studies suggest that training in ecological tasks, with whole sentences which emphasize top-down processes (such as generating semantic expectations, requiring working memory, and selective attention) might result in wider generalization than training that emphasizes specific auditory capacities (Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>; Smith et al., <xref rid="B44" ref-type="bibr">2009</xref>; Anderson et al., <xref rid="B6" ref-type="bibr">2013a</xref>,<xref rid="B5" ref-type="bibr">b</xref>). Two home-based training programs were used in previous studies (1) Brain Fitness&#x02122; (Smith et al., <xref rid="B44" ref-type="bibr">2009</xref>) that consists of modules designed to increase the speed and accuracy of auditory processing and (2) &#x0201c;listening and communication enhancement&#x0201d; LACE&#x02122; (Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>) that provides a variety of interactive and adaptive tasks in three categories: degraded speech, cognitive skills, and communication strategies. In the latter program, listeners train on speech recognition in passages on a wide variety of topics, in conditions such as competing speakers, time-compressed speech and speech-in-noise, that mimics the challenges of real-world listening. The overall goal of such ecological training approaches is to improve sensory function, and engage higher level processes that support sensory processing (Schneider and Pichora-Fuller, <xref rid="B42" ref-type="bibr">2000</xref>).</p><p>Previous studies evaluated the effects of home-based ecological training on participants with ARHL (Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>; Anderson et al., <xref rid="B6" ref-type="bibr">2013a</xref>) and normal-hearing (Anderson et al., <xref rid="B5" ref-type="bibr">2013b</xref>). They found that training changed the neural processing of speech sounds and promoted cognitive and perceptual skills. In one of these studies (Anderson et al., <xref rid="B6" ref-type="bibr">2013a</xref>), participants improved in both physiological (brainstem timing) as well as perceptual assessments (speech-in-noise perception, short-term memory and speech processing) following 40 sessions of computerized home-based auditory training. In another study Anderson et al. (<xref rid="B5" ref-type="bibr">2013b</xref>) compared learning in the ARHL group to normal-hearing adults, and found significant training-induced changes in speech-in-noise perception specific to the hearing impaired trained group, with no corresponding changes in the normal-hearing group. Sweetow and Sabes (<xref rid="B49" ref-type="bibr">2006</xref>) tested older adult hearing-aid users on trained and untrained measures of speech-in-noise. They reported significant on-task learning effects but only small effects of generalization and only in one of the two untrained tasks with sentences stimuli.</p><p>In the current study we trained listeners on speech perception tasks similar to the ecological training programs used in previous studies (e.g., Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>; Song et al., <xref rid="B45" ref-type="bibr">2012</xref>). Passages on a wide array of topics were presented in degraded form (noise or time-compression) or in parallel to a competing talker. Listeners had to answer content-related questions and the level of acoustic difficulty was adapted based on their responses. We chose this approach because evidence from normal-hearing individuals and few auditory rehabilitation studies shows that emphasizing top-down processes (selective attention, working memory, use of linguistic, and world knowledge) during training is more effective in terms of generalization than training on basic acoustic features (Borg, <xref rid="B9" ref-type="bibr">2000</xref>; Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>; Moore, <xref rid="B31" ref-type="bibr">2007</xref>). Whole sentences are expected to provide top-down lexical feedback in the perceptual learning process (Davis et al., <xref rid="B14" ref-type="bibr">2005</xref>). Thus, the listener may learn to use their stored semantic knowledge about the topic and about language, as well as visually presented verbal information, to facilitate their perception of the &#x0201c;interrupted&#x0201d; acoustic signal. Finally, training on whole sentences is expected to motivate participants and promote compliance with the training regimen.</p><p>We focused on adults with mild-to-moderate sensorineural hearing loss who were experiencing hearing difficulties, but had not yet sought intervention for their hearing loss, as well as on normal-hearing adults. To the best of our knowledge, the present study is one of the first studies to conduct home-based training research in everyday listening situations; in fact it is the first of its kind in relation to Hebrew speakers. We expect that training-induced behavioral gains will be observed. Moreover, perceptual learning studies usually ask if learning simple auditory skills can generalize to more complex ones. In the current study, generalization to untrained speech tasks was examined in normal-hearing older adults and those with ARHL. We also ask whether training on complex sounds generalizes to simple acoustic tasks, by testing participants on non-verbal auditory discrimination tasks. The aims of the current study were (1) to examine the efficacy of a home-based auditory training scheme in improving speech perception abilities among normal-hearing older adults and among hearing impaired non-aided older adults. (2) To compare the patterns of training-induced learning between normal-hearing adults and those with ARHL and (3) to assess learning on the trained tasks and transfer to other untrained (speech and non-speech) tasks to study generalization.</p></sec></sec><sec sec-type="materials and methods" id="s2"><title>Materials and methods</title><sec><title>Participants</title><p>Seventy one adults (44 females) aged 60&#x02013;71 years (mean age = 66.5 years &#x000b1; 4 months) with no history of neurological disorders, were recruited for this study. Participants were recruited from the Institute for Audiology and Clinical Neurophysiology at the Interdisciplinary Clinical Center at the University of Haifa, from the Hearing and Speech Center at the Rambam Health Care Campus and through advertisements at the University and Rambam. Recruitment criteria included age 60&#x02013;72 years, normal-hearing or hearing impairment with no neurologic disorders and Hebrew as a first language. Exclusions from the study were on the basis of audiometric results of asymmetric or conductive hearing loss (<italic>n</italic> = 4), being an existing hearing aid user (<italic>n</italic> = 5), unwillingness to participate in post-test sessions (<italic>n</italic> = 4), inability to control a computer mouse (<italic>n</italic> = 2). Participants provided informed consent and were compensated for their time. All procedures were approved by the Faculty of Social Welfare and Health Sciences, University of Haifa Review Board (approval number 197/12). Pure-tone audiometric thresholds were obtained bilaterally for air conduction at octave frequencies 250&#x02013;8000 Hz and at 3000 and 6000 Hz and for bone conduction at octave frequencies 250&#x02013;4000 Hz.</p><p>A total of 56 participants (35 females) met the inclusion criteria reported above and their data is included in the analyses reported in this manuscript. Based on audiometric thresholds participants were divided into normal-hearing (NH, mean age = 64.6 years &#x000b1; 4.3, <italic>n</italic> = 21) and ARHL (mean age = 67.6 years &#x000b1; 3.3, <italic>n</italic> = 35) groups; no significant age difference was found between the groups [<italic>t</italic><sub>(54)</sub> = 0.7, <italic>p</italic> = 0.59]. The normal-hearing participants had hearing thresholds &#x02264; 25 dBHL through 6000 Hz and &#x02264; 30 dBHL through 8000 Hz. Participants with ARHL had symmetrical mild to moderate hearing loss with hearing thresholds &#x02264;60 dBHL through 8000 Hz, and did not use hearing aids either in the past or at the time of the study. Audiograms for both groups are shown in Figure <xref ref-type="fig" rid="F1">1</xref>. No significant differences between the right and left ears were found in pure-tone average of 500, 1000, and 2000 Hz in air conduction thresholds therefore an average of both ears are shown in Figure <xref ref-type="fig" rid="F1">1</xref> [<italic>t</italic><sub>(110)</sub> = 0.6, <italic>p</italic> = 0.54]. In addition there were no significant differences in bone conduction thresholds between right and left ears [<italic>t</italic><sub>(110)</sub> = 1.03, <italic>p</italic> = 0.305]. All participants received standardized cognitive tests taken from the Wechsler Abbreviated Scale of Intelligence (WASI, Similarities, and Block Design) and the Digit span memory subtest from the Wechsler Intelligence Test (Wechsler, <xref rid="B53" ref-type="bibr">1997</xref>) and showed age normal cognitive function.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Audiogram</bold>. Mean air conduction hearing thresholds across ears and participants are plotted for all Normal-Hearing (NH) and Age-Related Hearing Loss (ARHL) participants. Error bars represent standard deviations (SDs).</p></caption><graphic xlink:href="fpsyg-06-02066-g0001"/></fig></sec><sec><title>Study design</title><p>The study used a randomized, controlled, quasi-crossover design similar in concept to Ferguson et al. (<xref rid="B17" ref-type="bibr">2014</xref>). Participants completed three test sessions (see Figure <xref ref-type="fig" rid="F2">2</xref>). Subgroups of participants underwent auditory training between different test sessions such that overall, participants served as their own untrained controls. All participants (NH and ARHL) underwent a series of tests in session 1 (t1), and then were randomly assigned to either complete the auditory-based training phase immediately (immediate-training, mean age = 65 &#x000b1; 4.3, <italic>n</italic> = 24; NH = 10, ARHL = 14) or to a waiting phase (delayed-training, mean age = 66 &#x000b1; 3.1, <italic>n</italic> = 22; NH = 11, ARHL = 11). Another group of participants with ARHL did not train at all (no-training ARHL, mean age = 67 &#x000b1; 3.4, <italic>n</italic> = 10) and participated in two testing sessions only, see Figure <xref ref-type="fig" rid="F2">2</xref>. Four weeks after t1 all participants underwent another session (t2). As shown in Figure <xref ref-type="fig" rid="F2">2</xref>, training occurred between times t1 and t2 for the immediate-training participants and between times t2 and t3 for the delayed-training participants, and the retention period occurred between times t2 and t3 for immediate-training participants. Training data was collected from both training periods (t1&#x02013;t2, t2&#x02013;t3); a total of 46 participants underwent the training phase (introduced in the Sections Materials and Methods and Results as trained NH, <italic>n</italic> = 21 and trained ARHL, <italic>n</italic> = 25). Data from the retention period will not be discussed in the current paper.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Study design</bold>. Three testing sessions were conducted for the Immediate-Training and Delayed-Training groups (t1, t2, t3) and two testing sessions for the No-Training group (t1, t2). Immediate-training group underwent training between times t1 and t2 and Delayed-training group between t2 and t3. Blue (top) circles represent testing on trained tasks, yellow (bottom) circles represent testing on untrained tasks.</p></caption><graphic xlink:href="fpsyg-06-02066-g0002"/></fig><p>Details of test sessions for each group: The three testing sessions were conducted at the University of Haifa and included tests on the trained tasks to assess the training effect (on-task learning), and on a series of untrained tasks to assess generalization. As shown in Figure <xref ref-type="fig" rid="F2">2</xref>, the Immediate-training and No-training groups were tested on the trained and untrained tasks in t1 (pre-test) and in t2 (post-test). For the Immediate-training group, t3 also included tests on the untrained tasks to assess retention (which will not be discussed in the current paper). The delayed-training group was tested only on the untrained tasks in t1, and was then tested on both trained and untrained tasks in t2 and t3.</p><p>As shown in Table <xref ref-type="table" rid="T1">1</xref>, demographic characteristics and indices of cognitive function (assessed at t1) were similar across all five NH and ARHL groups [<italic>F</italic><sub>(4, 51)</sub> &#x02264; 1.4, <italic>p</italic> &#x02265; 0.25]. Likewise, demographic and cognitive characteristics were similar across the immediate-training, delayed-training and no-training groups [<italic>F</italic><sub>(2, 53)</sub> &#x02264; 0.92, <italic>p</italic> &#x02265; 0.86].</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Means and (SDs) of demographic and cognitive measures across all groups (immediate-training, delayed-training, and no-training) divided into normal-hearing (NH) and Age-related hearing loss (ARHL) groups</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1"><bold>Normal-hearing</bold></th><th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1"><bold>ARHL</bold></th></tr><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>Immediate-training</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Delayed-training</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Immediate-training</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Delayed-training</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>No-training</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>N</italic></td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">11</td><td valign="top" align="center" rowspan="1" colspan="1">14</td><td valign="top" align="center" rowspan="1" colspan="1">11</td><td valign="top" align="center" rowspan="1" colspan="1">10</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Age</td><td valign="top" align="center" rowspan="1" colspan="1">64 (4.59)</td><td valign="top" align="center" rowspan="1" colspan="1">65 (4.5)</td><td valign="top" align="center" rowspan="1" colspan="1">66 (3.08)</td><td valign="top" align="center" rowspan="1" colspan="1">69 (2.53)</td><td valign="top" align="center" rowspan="1" colspan="1">67.6 (4.42)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Male/female</td><td valign="top" align="center" rowspan="1" colspan="1">4/6</td><td valign="top" align="center" rowspan="1" colspan="1">8/3</td><td valign="top" align="center" rowspan="1" colspan="1">8/6</td><td valign="top" align="center" rowspan="1" colspan="1">6/5</td><td valign="top" align="center" rowspan="1" colspan="1">7/3</td></tr><tr><td valign="top" align="left" colspan="6" style="background-color:#bbbdc0" rowspan="1"><bold>COGNITIVE FUNCTION</bold></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Digit span scaled scores</td><td valign="top" align="center" rowspan="1" colspan="1">9.5 (1.7)</td><td valign="top" align="center" rowspan="1" colspan="1">8.1 (2.08)</td><td valign="top" align="center" rowspan="1" colspan="1">9 (2.2)</td><td valign="top" align="center" rowspan="1" colspan="1">8 (2.4)</td><td valign="top" align="center" rowspan="1" colspan="1">8 (2.6)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Similarities</td><td valign="top" align="center" rowspan="1" colspan="1">15.1 (0.8)</td><td valign="top" align="center" rowspan="1" colspan="1">14.3 (2.5)</td><td valign="top" align="center" rowspan="1" colspan="1">14.3 (2.5)</td><td valign="top" align="center" rowspan="1" colspan="1">14.2 (2.9)</td><td valign="top" align="center" rowspan="1" colspan="1">14.4 (2.9)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Block design scaled scores</td><td valign="top" align="center" rowspan="1" colspan="1">11.1 (1.8)</td><td valign="top" align="center" rowspan="1" colspan="1">10.7 (2.3)</td><td valign="top" align="center" rowspan="1" colspan="1">10.8 (2.04)</td><td valign="top" align="center" rowspan="1" colspan="1">9.3 (2.4)</td><td valign="top" align="center" rowspan="1" colspan="1">10.3 (2.1)</td></tr></tbody></table></table-wrap></sec><sec><title>Training protocol and tasks</title><p>The trained groups completed 13 sessions of home-based auditory training, each lasting 20&#x02013;30 min spread over 4 weeks. The training program was designed to improve speech perception in three listening conditions (A) Speech-in-noise (B) Time-compressed speech and (C) Competing speaker. The training tasks were similar in principle to the training procedure introduced in Sweetow and Sabes (<xref rid="B49" ref-type="bibr">2006</xref>) and Song et al. (<xref rid="B45" ref-type="bibr">2012</xref>). Each session was devoted to one condition, which was practiced for three blocks, except for the last session which included training on all three conditions (one block of each condition). To keep listeners engaged, recordings on a wide variety of topics were used, and in each block a different topic was presented. The auditory training materials were thematic passages of 3&#x02013;6 min in Hebrew, read by five readers (four male voices and one female) from popular science articles. The passages were broken into content units of 1&#x02013;2 sentences of about 10 s each, using Audacity software (Audacity, version 1.2. 6). Each unit was followed by a multiple choice question related to the content of the sentences, which was presented visually. Feedback (correct/incorrect response with the correct answer) was also given visually.</p><p>During training an adaptive 2-down/1-up staircase procedure was used to adjust the level of difficulty to the performance of each listener based on their individual performance. Improvements with training is reflected by a reduction in the threshold, suggesting that as training progressed listeners could maintain a good level of accuracy even with a more &#x0201c;difficult&#x0201d; (lower quality) stimulus.</p><p>The starting values for each day of training were based on the end values of the previous session for each listener in each condition. The speech-in-noise condition sentences were embedded in four-talker babble noise which consisted of two female and two male talkers reading printed prose. The amplitude of each speech signal was maximized to a point just below peak clipping and the four recordings were mixed into a single channel. Various segments of the noise were used to avoid adaptation. The segments were applied pseudo-randomly (i.e., approximately equivalent total number of uses) across sentences to reduce possible effects of amplitude fluctuations that would be present in one noise segment. All noise segments were normalized to an overall root mean square (RMS) level of 70 dB via level 16 (Tice and Carrell, <xref rid="B51" ref-type="bibr">1997</xref>). The adaptive parameter was the signal to noise ratio, where the noise level changed by 1.5 dB. Time-compressed speech adaptive parameter was the compression rate and in the competing speaker's condition, two sentences were presented simultaneously by male and female voices, listeners were instructed to respond to a target speaker and the adaptive parameter was the signal to noise ratio of the two sentences. Mean SNR thresholds of each block was calculated for each participant in speech-in-noise and competing speaker conditions, and mean compression ratio threshold was calculated for each block in the time-compressed speech condition.</p><p>The training program was installed by the experimenter (first author HK) on all of the trained participant's personal computers and participants practiced in their homes. Stimuli were presented in sound field via two speakers (Logitech S-0264A, provided by the researchers) placed on either side of the computer and facing the participant (around 45&#x000b0;). The sound level was set at a comfortable listening level, as determined by the trainee, prior to the start of each training session. After the installation of the training program participants completed one practice block for each condition intended to familiarize them with the training program prior to the onset of independent training. At this time participants were also instructed to call the experimenter if they had any questions or if they encountered problems with the program. Subsequently participants were called on a weekly basis to encourage their continued compliance with the training regimen. At the end of training period, the results were uploaded by the experimenter from the personal computers.</p><p>Analysis of the training-phase data was conducted on the data of all trained participants (collected between t1 and t2 for the immediate-training group and between t2 and t3 for the delayed-training group). A series of univariate ANOVAs showed no significant differences in the pre-training and post-training results between the normal immediate- and delayed-training groups [<italic>F</italic><sub>(1, 19)</sub> &#x02264; 1.61, <italic>p</italic> &#x02265; 0.22], and the ARHL immediate- and delayed-training groups [<italic>F</italic><sub>(1, 23)</sub> &#x02264; 1.86, <italic>p</italic> &#x02265; 0.19], therefore the two groups were combined. A total of 21 NH and 25 ARHL listeners completed training and are referred to as trained listeners or trained groups throughout the Results Sections (Training-Phase Learning, Pre- to Post-Test Learning on the Trained Tasks).</p></sec><sec><title>Pre- and post-training assessments</title><p>Pre- and post-training assessments were conducted 4 weeks apart. Data from these sessions was used to assess learning (performance on the trained tasks but with different content) as well as generalization of learning to untrained tasks by comparing changes in performance over time between trained and untrained participants.</p><sec><title>Learning</title><p>Performance on the trained tasks (but with different passages) was used to document learning and determine whether training-related changes were significant in trained in comparison to untrained listeners. For this analysis, data was collected immediately before the first training (pre-test) session and immediately after the final training session (post-test) corresponding to times t1 and t2 for the immediate and no-training groups, and times t2 and t3 for the delayed-training group (see Figure <xref ref-type="fig" rid="F2">2</xref>, and Section Study Design for more details). Therefore, these analyses, reported in Results&#x02014;Section Pre- to Post-Test Learning on the Trained Tasks include data from 21 trained NH participants, 25 trained ARHL participants, and 10 untrained ARHL participants. Data on these tasks was not collected for untrained NH listeners, because our main goal was to test the existence of learning changes in the ARHL group. Participants were tested on two blocks of each trained condition in each time point (2 &#x000d7; speech-in-noise, 2 &#x000d7; time-compressed speech and 2 &#x000d7; competing speaker). Differences between pre- and post-tests on the trained tasks were compared between groups.</p></sec><sec><title>Generalization</title><p>Performance on untrained tasks was used to study the transfer of the potential training-induced gains to other speech and non-speech conditions (generalization). These tasks were completed by all subgroups and included (A) a speech-in-noise pseudoword discrimination task, (B) a speech-in-noise sentences task, (C) a duration discrimination task, and (D) a frequency discrimination task.</p><p>(A) In the speech-in-noise pseudoword discrimination task participants performed a same/different discrimination task in which 60 pairs of two-syllable pseudowords were presented aurally by a native female speaker with equal numbers of &#x0201c;same&#x0201d; and &#x0201c;different&#x0201d; trials. &#x0201c;Different&#x0201d; trials were minimal pairs (e.g., &#x0201c;same&#x0201d;: /damul/-/damul/, &#x0201c;different&#x0201d;: /malud/-/maluk/), with equal number of pairs from each phonetic contrast and vowel template. The pseudowords were embedded in background four-talker babble noise (same as used in the training paradigm). Pseudowords were used in this test to eliminate the effect of context provided by familiar words, shown to be stronger in individuals with presbycusis compared to near-normal hearing listeners (Pichora-Fuller et al., <xref rid="B36" ref-type="bibr">1995</xref>). (B) Speech-in-noise sentences task, in which listeners were required to make plausibility judgments on 45 Hebrew sentences embedded in the same four-talker babble noise used in the training paradigm. After hearing a sentence listeners had to determine whether the sentence was semantically plausible (&#x0201c;true&#x0201d;) or not (&#x0201c;false&#x0201d;). Both Speech-in-noise tests (pseudowords and sentences) were administered at the most comfortable level for each participant, with a starting SNR value of +5 which was adapted based on their responses with a 2-down/1-up adaptive staircase procedure. The adaptive parameter was the SNR, where the noise level changed by steps of 1.5 dB. All sets of stimuli were RMS-amplitude normalized to 70 dB SPL using Level 16. Just noticeable differences (JNDs) served as the outcome measure for discrimination thresholds in the speech-in-noise pseudowords test, while mean SNR thresholds were used for the speech-in-noise sentences tests. The two speech-in-noise tests (pseudowords and sentences) were used to study generalization to untrained speech-in-noise tasks. (C) Duration discrimination was tested with 1000 Hz reference tones with a standard duration of 200 ms in an oddball procedure. On each trial two identical standard tones and one target tone were presented with an 800-ms inter-stimulus-interval. The duration of the odd tones were adapted based on performance with a 3-down/1-up multiplicative staircase procedure. (D) Frequency discrimination was tested in an oddball procedure with 500 Hz as a reference tone in one task and 2000 Hz reference tone in another task with duration of 500 ms. The frequency difference between the odd and frequent tones was adapted based on performance. The non-speech tasks were administered using a listener friendly interface of 60 trials. These tests were used to determine whether generalization can be observed to untrained basic psychoacoustic non-speech tasks. Each psychoacoustic test lasted ~7&#x02013;10 min. Visual feedback was provided for both correct and incorrect responses. Stimuli were presented with an initial level of 70 dB SPL, but the tester adjusted the intensity of all speech and non-speech stimuli to a comfortable listening level using the computer's volume setting. Most stimuli were thus presented at the range of 80&#x02013;83 dB SPL. The level of presentation did not exceed 90 dB SPL.</p><p>The generalization tasks were administered to all NH and ARHL participants on times t1 and t2. Thus, these tasks were administered before and after the training period for the immediate-training participants, but before and after the control period for the delayed-training and no-training participants (see Figure <xref ref-type="fig" rid="F2">2</xref>). Therefore, data from ARHL delayed-training group and the ARHL no-training groups was combined since ANOVA showed no significant differences in the pre-test (t1) and post-test (t2) results [<italic>F</italic><sub>(1, 19)</sub> &#x02264; 4.33, <italic>p</italic> &#x02265; 0.06]. This resulted in four groups which were compared in subsequent analyses: two groups were tested before and after their training period&#x02014;the immediate-training NH (<italic>n</italic> = 10) and immediate-training ARHL (<italic>n</italic> = 14) groups and two groups were tested before and after their control period NH (delayed-training, <italic>n</italic> = 11) and ARHL (delayed training + no-training groups, <italic>n</italic> = 21). Data was analyzed using repeated measures ANOVA with two between subject factors (during-training vs. during-control period and NH vs. ARHL) and one within subject factor: time (t1 vs. t2). Shapiro-Wilk tests were used to confirm that the data was normally distributed within each group (<italic>p</italic> &#x0003e; 0.1). In addition, Levene tests confirmed that variances were homogeneous across groups within each analysis (<italic>p</italic> &#x0003e; 0.16).</p></sec></sec></sec><sec sec-type="results" id="s3"><title>Results</title><sec><title>Training-phase learning</title><p>Forty-one out of 46 trained participants, from both the NH or ARHL groups, completed all 13 sessions of the auditory training program, showing a high level of compliance with no dropouts; five additional participants completed 10&#x02013;11 sessions. Data from all 46 trained participants was therefore included in the statistical analysis.</p><p>In order to determine whether participants improved during training, and whether this depended on their hearing status, linear curve estimation was performed on the performance of the group in each training condition across sessions (Figure <xref ref-type="fig" rid="F3">3</xref>). These analyses (see Table <xref ref-type="table" rid="T2">2</xref> for details) revealed a good fit of the linear curves to the data with significant R-squared values (R-squared &#x0003e; 0.43, <italic>p</italic> &#x0003c; 0.01) that, suggests that a linear improvement across sessions accounts for a significant amount of the variance in performance.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Learning curves</bold>. Mean thresholds as a function of the trained block for trained Normal-Hearing (NH) and trained Age-Related Hearing Loss (ARHL) participants in <bold>(A)</bold> Speech-in-noise <bold>(B)</bold> Time-compressed speech and <bold>(C)</bold> Competing speaker conditions. Mean signal-to-noise ratio (SNR) thresholds of each block was used as the dependent measure in speech-in-noise and competing speaker conditions and the compression ratio was used for the time-compressed speech condition. Regression lines and slopes of the learning curves <bold>(A)</bold> for trained NH are shown in red and for trained ARHL in green. <sup>**</sup><italic>p</italic> &#x0003c; 0.01.</p></caption><graphic xlink:href="fpsyg-06-02066-g0003"/></fig><table-wrap id="T2" position="float"><label>Table 2</label><caption><p><bold>Linear curve estimation model of group data</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>R-squared</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold><italic>F</italic>(1, 11)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold><italic>p</italic></bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Speech-in-noise</td><td valign="top" align="left" rowspan="1" colspan="1">NH</td><td valign="top" align="center" rowspan="1" colspan="1">0.58</td><td valign="top" align="center" rowspan="1" colspan="1">15.41</td><td valign="top" align="center" rowspan="1" colspan="1">0.002</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">ARHL</td><td valign="top" align="center" rowspan="1" colspan="1">0.43</td><td valign="top" align="center" rowspan="1" colspan="1">8.12</td><td valign="top" align="center" rowspan="1" colspan="1">0.009</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Time-compressed speech</td><td valign="top" align="left" rowspan="1" colspan="1">NH</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">22.16</td><td valign="top" align="center" rowspan="1" colspan="1">0.000</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">ARHL</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">24.83</td><td valign="top" align="center" rowspan="1" colspan="1">0.000</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Competing speaker</td><td valign="top" align="left" rowspan="1" colspan="1">NH</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">30.48</td><td valign="top" align="center" rowspan="1" colspan="1">0.000</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">ARHL</td><td valign="top" align="center" rowspan="1" colspan="1">0.83</td><td valign="top" align="center" rowspan="1" colspan="1">57.10</td><td valign="top" align="center" rowspan="1" colspan="1">0.000</td></tr></tbody></table><table-wrap-foot><p><italic>R-squared, F-values with degrees of freedom and p-values are presented across conditions for trained normal-hearing (NH) and trained Age-related hearing loss (ARHL) groups</italic>.</p></table-wrap-foot></table-wrap><p>To compare the amount of training-induced changes between groups (NH and ARHL) the linear slopes of the individual learning curves were calculated for each participant in each training condition. As shown in Table <xref ref-type="table" rid="T3">3</xref>, mean slopes were significantly negative (<italic>p</italic> &#x0003c; 0.01) in both trained groups and across all three training conditions. In the speech-in-noise condition, learning curves were significantly steeper in the NH than in the ARHL group [<italic>t</italic><sub>(44)</sub> = &#x02212;2.05, <italic>p</italic> = 0.046]. No significant differences were found between the learning-curve slopes of NH and ARHL participants in the time-compressed speech condition [<italic>t</italic><sub>(44)</sub> = 0.65, <italic>p</italic> = 0.52] and in the competing speaker condition [<italic>t</italic><sub>(44)</sub> = &#x02212;0.76, <italic>p</italic> = 0.45].</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p><bold>Means and (SDs) of the individual linear learning slopes for trained normal-hearing (NH) and trained Age-related hearing loss (ARHL) groups</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>NH</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>ARHL</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold><italic>t</italic></bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold><italic>p</italic></bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>95% confidence interval of the difference</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Speech-in-noise</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.59 (0.4)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.32 (0.3)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;2.05</td><td valign="top" align="center" rowspan="1" colspan="1">0.04</td><td valign="top" align="center" rowspan="1" colspan="1">[&#x02212;0.527, &#x02212;0.005]</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Time-compressed speech</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.02 (0.01)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.02 (0.01)</td><td valign="top" align="center" rowspan="1" colspan="1">0.65</td><td valign="top" align="center" rowspan="1" colspan="1">0.52</td><td valign="top" align="center" rowspan="1" colspan="1">[&#x02212;0.008, 0.016]</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Competing speaker</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;1.21 (0.8)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;1.04 (0.7)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.76</td><td valign="top" align="center" rowspan="1" colspan="1">0.45</td><td valign="top" align="center" rowspan="1" colspan="1">[&#x02212;0.615, 0.277]</td></tr></tbody></table><table-wrap-foot><p><italic>t-values, p-values of the group comparison and 95% confidence interval of the difference between groups are also shown</italic>.</p></table-wrap-foot></table-wrap><p>Visual inspection of the data (see Figure <xref ref-type="fig" rid="F3">3</xref>) suggests that the rate of learning may have changed over the course of training with an initially rapid learning phase followed by a slower learning phase. Therefore, two-line linear curves were also fitted to the group data, separately for sessions 1&#x02013;6 and 7&#x02013;13 in each condition (see Supplementary Material). These models showed a good fit in some conditions and groups. Therefore, only for conditions in which both groups showed a significant fit to the model, individual slopes were calculated and the slopes were compared between groups. The results were similar to those obtained with the one-line model (see Supplementary Material for details).</p><p>Taken together, these data suggest that training-phase learning was observed in both the normal-hearing and the ARHL trained groups. Both trained groups showed a similar amount of learning over the course of training in the time-compressed speech and competing speaker conditions. However, in the speech-in-noise training condition normal-hearing group showed more improvements than ARHL group.</p><sec><title>Pre- to post-test learning on the trained tasks</title><p>To determine whether training resulted in greater pre- to post-test changes in trained than in untrained participants and as a function of hearing status, pre- and post-test performance on each of the trained conditions was compared across the three groups (see Figure <xref ref-type="fig" rid="F4">4</xref>) using a repeated measures ANOVA with group (NH, ARHL, no-training ARHL) as a between-subject factor and time (pre-test, post-test) as a within-subject factor followed by <italic>post-hoc</italic> tests. As explained in Section Learning, the trained tasks was administered immediately before the first training session(pre-test) and immediately after the final training session (post-test) corresponding to times t1 and t2 for the immediate and no-training groups, and times t2 and t3 for the delayed-training group (see Figure <xref ref-type="fig" rid="F2">2</xref>). Therefore, these analyses include data from 21 trained NH participants, 25 trained ARHL participants, and 10 untrained ARHL participants.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Pre-to-post learning effects</bold>. Pre- and post-test performance in trained normal-hearing [NH, trained ARHL (ARHL)] and no-training ARHL group for the three conditions: <bold>(A)</bold> Speech-in-noise <bold>(B)</bold> Time-compressed speech and <bold>(C)</bold> competing speaker. Mean signal-to-noise ratio (SNR) thresholds and SDs are shown for the speech-in-noise and competing speaker conditions and mean compression ratio thresholds and SDs are shown for the time-compressed speech condition. <sup>***</sup><italic>p</italic> &#x0003c; 0.001; <sup>**</sup><italic>p</italic> &#x0003c; 0.01.</p></caption><graphic xlink:href="fpsyg-06-02066-g0004"/></fig><p>The results showed a statistically significant effect of time and group. Performance on all three trained conditions was significantly influenced by both time [pre vs. post&#x02014;speech-in-noise: <italic>F</italic><sub>(1, 53)</sub> = 32.50, <italic>p</italic> &#x0003c; 0.0001, <inline-formula><mml:math id="M1"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.38; time-compressed speech: <italic>F</italic><sub>(1, 53)</sub> = 47.21, <italic>p</italic> &#x0003c; 0.0001, <inline-formula><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.47; competing speakers: <italic>F</italic><sub>(1, 53)</sub> = 109.98, <italic>p</italic> &#x0003c; 0.0001, <inline-formula><mml:math id="M3"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>&#x000a0;</mml:mtext><mml:mo>=</mml:mo></mml:math></inline-formula> 0.68] and group [speech-in-noise: <italic>F</italic><sub>(2, 53)</sub> = 7.8, <italic>p</italic> &#x0003c; 0.001, <inline-formula><mml:math id="M4"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.23; time-compressed speech: <italic>F</italic><sub>(2, 53)</sub> = 6.01, <italic>p</italic> &#x0003c; 0.001, <inline-formula><mml:math id="M5"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.27; competing speakers: <italic>F</italic><sub>(2, 53)</sub> = 9.68, <italic>p</italic> &#x0003c; 0.0001, <inline-formula><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.27]. The time &#x000d7; group interactions were also significant: speech-in-noise: <italic>F</italic><sub>(2, 53)</sub> = 9.01, <italic>p</italic> &#x0003c; 0.001, <inline-formula><mml:math id="M7"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>&#x000a0;</mml:mtext><mml:mo>=</mml:mo></mml:math></inline-formula> 0.26; time-compressed speech <italic>F</italic><sub>(2, 53)</sub> = 28.77, <italic>p</italic> &#x0003c; 0.001, <inline-formula><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.52; competing speaker <italic>F</italic><sub>(2, 53)</sub> = 14.41, <italic>p</italic> &#x0003c; 0.001, <inline-formula><mml:math id="M9"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.35 (see Figure <xref ref-type="fig" rid="F4">4</xref>). The significant differences between pre- and post-tests stem from greater changes in both trained groups than in the no-training group. <italic>Post-hoc</italic> Tukey HSD analysis showed significant (<italic>p</italic> &#x0003c; 0.001) pairwise comparisons between the no-training group with each trained group (NH and ARHL) for all three conditions [speech-in-noise: <italic>F</italic><sub>(2, 53)</sub> = 10.89, time-compressed speech: <italic>F</italic><sub>(2, 53)</sub> = 12.32 competing speaker: <italic>F</italic><sub>(2, 53)</sub> = 12.43]. Moreover, <italic>t</italic>-test analyses showed a significant effect of time for both NH and ARHL trained groups on the three conditions [ARHL: speech-in-noise: <italic>t</italic><sub>(33)</sub> = &#x02212;2.96, <italic>p</italic> &#x0003c; 0.001; time-compressed speech <italic>t</italic><sub>(33)</sub> = &#x02212;3.87, <italic>p</italic> &#x0003c; 0.001; competing speaker <italic>t</italic><sub>(33)</sub> = &#x02212;3.57, <italic>p</italic> &#x0003c; 0.001. NH: speech-in-noise: <italic>t</italic><sub>(29)</sub> = &#x02212;4.38, <italic>p</italic> &#x0003c; 0.001; time-compressed speech <italic>t</italic><sub>(29)</sub> = &#x02212;4.22, <italic>p</italic> &#x0003c; 0.001; competing speaker <italic>t</italic><sub>(29)</sub> = &#x02212;4.97, <italic>p</italic> &#x0003c; 0.001]. On the other hand, as can be seen in Figure <xref ref-type="fig" rid="F4">4</xref>, untrained listeners hardly changed between the two points and no significant differences between pre- and post-tests for the untrained group were found in any condition [speech-in-noise: <italic>t</italic><sub>(9)</sub> = 1.03, <italic>p</italic> = 0.57; time-compressed speech: <italic>t</italic><sub>(9)</sub> = &#x02212;2, <italic>p</italic> = 0.95; competing speaker: <italic>t</italic><sub>(9)</sub> = 1.8, <italic>p</italic> = 0.1]. Taken together, training induced learning was observed for trained tasks in both normal-hearing and ARHL trained groups in all conditions, untrained listeners did not show any changes between pre- and post-tests and significant differences were observed between trained and untrained listeners; all these confirm that trained listeners improved more than untrained listeners between the pre- and the post-tests. Moreover, normal-hearing trained group significantly outperformed ARHL trained group in the speech-in-noise condition in the post-test session, [Hearing group effect: <italic>F</italic><sub>(1, 44)</sub>= 7.97, <italic>p</italic> &#x0003c; 0.01, Figure <xref ref-type="fig" rid="F4">4A</xref>], consistent with the steeper learning curves observed in this group during training.</p></sec><sec><title>Generalization</title><p>To study the transfer of learning and to determine whether training resulted in greater pre- to post-test changes during-training than during-control period and as a function of hearing level, pre- (t1) and post-test performance (t2) on the untrained tasks was compared across the immediate-, delayed-, and the no-training groups between the times t1 and t2 (see Figure <xref ref-type="fig" rid="F2">2</xref>). As shown in the Materials and Methods&#x02014;Section Generalization. The participants were divided into four groups (1. NH immediate-training, 2. ARHL immediate-training, 3. NH delayed-training, 4. ARHL delayed-training + no-training, see Figure <xref ref-type="fig" rid="F5">5</xref>) using a repeated measures ANOVA with two between subject factors (training and hearing groups) and one within subject factor, time (pre vs. post). Mean group thresholds are shown in Figure <xref ref-type="fig" rid="F5">5</xref>, across all untrained tasks, as a function of hearing and training factors, for speech (speech-in-noise pseudowords and sentences, Figures <xref ref-type="fig" rid="F5">5A,B</xref>) and non-speech tasks (duration discrimination Figure <xref ref-type="fig" rid="F5">5C</xref> and frequency discrimination, Figures <xref ref-type="fig" rid="F5">5D,E</xref>).</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Generalization</bold>. Means and SDs of <bold>(A)</bold> speech-in-noise pseudowords and <bold>(B)</bold> speech-in-noise sentences thresholds in dBs <bold>(C)</bold> duration discrimination in milliseconds (ms) <bold>(D)</bold> 500 Hz frequency discrimination and <bold>(E)</bold> 2000 Hz frequency discrimination thresholds in Hz, obtained from pre- and post-tests for Normal-Hearing (NH) and Age-Related Hearing Loss (ARHL) groups. For the subgroups: NH immediate-training, ARHL immediate-training, NH delayed-training, and ARHL delayed-training + no-training. See Materials and Methods&#x02014;Section Generalization for subgroups division.</p></caption><graphic xlink:href="fpsyg-06-02066-g0005"/></fig><sec><title>Speech in noise tests</title><p>Significant effects of hearing group were found in both speech-in-noise tests (Figures <xref ref-type="fig" rid="F5">5A,B</xref>), where normal-hearing participants significantly outperformed participants with ARHL [speech-in-noise pseudowords: <italic>F</italic><sub>(1, 52)</sub> = 8.14, <italic>p</italic> = 0.006, <inline-formula><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.14; speech in-noise sentences: <italic>F</italic><sub>(1, 52)</sub> =11.13, <italic>p</italic> = 0.002, <inline-formula><mml:math id="M11"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.18]. A significant main effect of time was observed only in the speech-in-noise pseudowords task [time: <italic>F</italic><sub>(1, 52)</sub> = 23.42, <italic>p</italic> &#x0003c; 0.001, <inline-formula><mml:math id="M12"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.32]. No significant effect of time was shown in the speech-in-noise sentences task. A significant interaction of time &#x000d7; training group was observed only in the speech-in-noise pseudowords task [<italic>F</italic><sub>(1, 52)</sub> = 4.47, <italic>p</italic> = 0.036, <inline-formula><mml:math id="M13"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.08]. This interaction stems from a significant effect of time [<italic>F</italic><sub>(1, 33)</sub> = 21.01, <italic>p</italic> &#x0003c; 0.001], and a significant interaction of time &#x000d7; training group [time: <inline-formula><mml:math id="M14"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.40; time &#x000d7; train: <italic>F</italic><sub>(1, 33)</sub> = 6.24, <italic>p</italic> = 0.018, <inline-formula><mml:math id="M15"><mml:msubsup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo></mml:math></inline-formula> 0.16] only for the ARHL groups. The interaction time &#x000d7; training was not significant among normal-hearing participants. Therefore, transfer of learning was observed only for the speech-in-noise pseudowords task and only in ARHL group.</p></sec><sec><title>Duration and frequency discrimination tasks</title><p>No significant differences were observed between any of the groups on these tasks (neither hearing differences nor training vs. control period differences, <italic>p</italic> &#x0003e; 0.4). There was no main effect of hearing group (<italic>p</italic> &#x0003e; 0.10) or training group (<italic>p</italic> &#x0003e; 0.12) in any of the non-speech tasks. In frequency discrimination 500 Hz task, there was a main effect of time [<italic>F</italic><sub>(1, 52)</sub> = 5.42, <italic>p</italic> = 0.026], but without any interaction with either hearing group (<italic>p</italic> = 0.13) or training group (<italic>p</italic> = 0.89). These results indicate that there was no transfer of learning to the duration discrimination or frequency discrimination tasks in any of the groups (Figures <xref ref-type="fig" rid="F5">5C&#x02013;E</xref>).</p></sec></sec></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>The present study tested the effect of a home-based training program in everyday listening situations, specifically focused on older adults with mild-to-moderate sensorineural hearing loss who experienced hearing difficulties but did not have hearing aids as well as normal-hearing listeners in the same age range. The outcomes of training on speech perception were compared between normal-hearing adults and those with ARHL. The outcomes of training on generalization to other speech and non-speech tasks were assessed.</p><p>The major outcomes of the current study were: (i) Robust training-induced learning effects were found in both normal-hearing and individuals with ARHL, and for the trained tasks these were not limited to the trained materials. (ii) The normal-hearing group showed more learning than the ARHL in the speech-in-noise trained condition. (iii) Generalization to the perception of pseudowords in-noise was observed in the ARHL group only. (iv) The perception of sentences in-noise, duration discrimination and frequency discrimination did not improve in either of the trained groups. Together these findings suggest that although learning remains robust in older adults with normal hearing and in older adults with ARHL, generalization is limited.</p><sec><title>Learning and generalization</title><sec><title>Learning on the trained tasks</title><p>Consistent with previous studies (Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>; Humes et al., <xref rid="B24" ref-type="bibr">2009</xref>), and as expected, learning was observed in the trained groups. In the current study, significant training-phase learning was observed in both normal-hearing and ARHL. Participants performed significantly better at the end of the training period than on the initial blocks (Figure <xref ref-type="fig" rid="F3">3</xref>), indicating that participants' understanding of speech improved over the course of training, in all three conditions: speech-in-noise, time-compressed speech, and competing speaker. Furthermore, between the pre- and post-tests both ARHL and normal-hearing participants improved on the trained conditions more than untrained participants (Figure <xref ref-type="fig" rid="F4">4</xref>). The normal-hearing and the ARHL groups showed similar patterns of learning over the course of training in the time-compressed speech and competing speaker's conditions as evident by their overlapping learning curves (Figure <xref ref-type="fig" rid="F3">3</xref>). On the other hand, in the speech-in-noise condition, the learning curves were significantly steeper in the normal-hearing than in the ARHL group (Figure <xref ref-type="fig" rid="F3">3</xref>), suggesting that on this condition, training had a greater influence on normal-hearing listeners than on listeners with ARHL. The current study is the first to compare between the training outcomes of normal hearing and ARHL groups. These groups are defined by a difference in lower level sensory processes. Even though the training program was designed to emphasize higher level top-down cognitive processes the difference in learning between groups suggests that the poor quality of perceptual representations in ARHL reduced the benefit of this type of training. It is possible that the use of hearing aids might improve the quality of representations and therefore could enhance the benefits of training.</p></sec><sec><title>Generalization</title><p>Although learning on the trained conditions was not stimulus specific (Figure <xref ref-type="fig" rid="F4">4</xref>), the magnitude of training-induced transfer to other speech-in-noise tasks was small (Figure <xref ref-type="fig" rid="F5">5A</xref>). Transfer was limited to the pseudowords task and to the ARHL group. This finding is consistent with the findings of Anderson et al. (<xref rid="B5" ref-type="bibr">2013b</xref>) where significant improvements in the speech-in-noise outcome measure were specific to the hearing impaired group and generalization was not shown in the normal-hearing group. On the other hand, in the current study, training (in both ARHL and normal-hearing) did not generalize to an untrained speech-in-noise sentence task. In this task, listeners had to judge the semantic plausibility of sentences embedded in noise. This task was different from the trained task, in which listeners were asked multiple-choice questions about the content of the sentence they had heard. So despite using the same babble noise, the change in task requirements was sufficient to preclude generalization. Moreover, no transfer was found in either group to more basic psychophysical abilities such as duration or frequency discrimination (Figures <xref ref-type="fig" rid="F5">5C&#x02013;E</xref>). These findings suggest that the type of training used in the current study affected higher level task-specific cognitive processes and did not enhance low-level auditory processing of duration or frequency.</p><p>The small effect of generalization observed in the ARHL group was also reported in previous training studies using a similar training program (e.g., Sweetow and Sabes, <xref rid="B49" ref-type="bibr">2006</xref>). Sweetow and Sabes reported only small effects of generalization to speech outcomes in adults with ARHL, and only in one of the two untrained tasks with sentences stimuli. In contrast, normal-hearing young adults showed generalization to untrained speech tasks when trained with the same program, suggesting that training improved the neural representation of cues important for speech perception (Song et al., <xref rid="B45" ref-type="bibr">2012</xref>). Altogether these results suggest that the restricted generalization in the current study, in which both groups were of older age, is associated with the degenerative changes that occur due to aging or hearing loss or both.</p><p>One potential interpretation for the discrepancy between learning and generalization in the ARHL group is that during training, although listeners focused on the content of the sentences and not on the acoustic/phonetic characteristics of the stimuli, the low quality of the signal (due to both their auditory loss and noise) had driven listeners to rely on lower-level sensory representations that were not sentence specific. Although the ability to use lower-level sensory representations may have been helpful when making decisions about pseudowords, it would not have been enough when new semantic demands were imposed by semantic judgment task [see Ahissar et al., <xref rid="B1" ref-type="bibr">2009</xref> for the detailed theoretical framework and (Banai and Lavner, <xref rid="B7" ref-type="bibr">2014</xref>) for a previous discussion in the context of the perceptual learning of speech]. Consistent with this interpretation, it is plausible that mid-level sensory representations were used during training and that was shown in the pseudowords task. Learning did not reach as high as the levels of sentences representations and it did not go as low as the acoustic parameters of frequency or duration. This may be due to the type of task and feedback used during training, or may be a more general feature of auditory training as suggested by small generalization effects observed in previous studies see Henshaw and Ferguson (<xref rid="B22" ref-type="bibr">2013</xref>).</p><p>An alternative hypothesis is that generalization at the perceptual level of speech in noise could be identified with other outcome measures not used in the current study (Amitay et al., <xref rid="B4" ref-type="bibr">2014</xref>), such as identification of real words or identification of key words in a sentence. Moreover, changes in higher level processes could perhaps be identified with tests of working memory and attention (Ferguson and Henshaw, <xref rid="B16" ref-type="bibr">2015b</xref>). On the other hand, a variety of outcome measures have been used across previous studies, but only small effects of generalization have been reported. Therefore, auditory training may prove useful in hearing rehabilitation, but only if future studies converge on training regimens that yield greater generalization than observed with the regimens studied so far. A potential way forward is to combine the different types of training approaches in order to offer generalization benefits to real world listening abilities as suggested by Ferguson and Henshaw (<xref rid="B16" ref-type="bibr">2015b</xref>).</p></sec><sec><title>Comparisons between normal-hearing and ARHL groups in the generalization of learning</title><p>Differences between normal-hearing and ARHL were shown when looking at the transfer tests (Figure <xref ref-type="fig" rid="F5">5A</xref>); where a significant transfer effect, albeit small, was observed in the ARHL in the pseudowords task, but not in the normal-hearing group. The differences between the normal-hearing and the ARHL groups concerning transfer to the speech-in-noise pseudowords test may be consistent with the processing model introduced in the introduction (Section Speech Processing in Younger and Older Adults). It is plausible that for normal-hearing participants the bottom-up acoustic information was still reliable and sufficient, therefore it matched the lexical representations and there was no need to divert attentional resources to low-level representations during training. In the ARHL group, lower-level and lexical representations did not automatically match, making it necessary to devote attentional resources to the matching process. This additional burden in the ARHL group may have increased the reliance on bottom-up perceptual processes which were generalized to pseudowords.</p></sec></sec><sec><title>Compliance and subjective outcomes</title><p>Our training paradigm tried to mimic the challenges of real-world listening and consisted of blocks of sentences in a wide variety of topics. In addition to enhancing reliance on top-down processes the aim of this approach was to enhance motivation and compliance with the training program. It was previously shown that increased time on task is positively associated with gain in understanding speech in noise (Levitt et al., <xref rid="B30" ref-type="bibr">2011</xref>). Thus, the training paradigm in the current study engaged participants resulting in a high rate of compliance (90%) similar to previous reports by Stecker et al. (<xref rid="B46" ref-type="bibr">2006</xref>). The improvement on the trained conditions suggests that participants with ARHL can benefit from an improved SNR adjustment to compensate for the inaudibility of high frequencies; such improvements though, are hard to accomplish in many everyday settings. However, despite the lack of evidence concerning transfer of learning in objective measures, more than 50% of the normal-hearing and 75% of the ARHL trained listeners reported that training was helpful in their communication with their grandchildren &#x0201c;<italic>especially those who speak really fast</italic>,&#x0201d; and &#x0201c;<italic>understanding what is being said in noisy environments</italic>&#x0201d; suggesting that training may result in subjective benefits.</p></sec></sec><sec sec-type="conclusions" id="s5"><title>Conclusions</title><p>We suggest that most training-related changes in the current study occurred at a higher level of task-specific cognitive processes in both groups, as evident by the lack of generalization to the sentence task, and to the frequency and duration discrimination tasks. Given that the difference between the normal-hearing and ARHL groups is defined based on lower level acoustic and perceptual processing, the larger learning gains in the normal-hearing group suggests an interaction between bottom-up and top-down processes. Namely, learning related changes in high level task-related cognitive processes is enhanced by the high quality of perceptual representations in the normal-hearing group.</p><p>Furthermore, the finding of generalization to pseudowords, only in the ARHL group, suggests that some learning related changes have also occurred at the level of identifying phonemic representations in this group. Presumably, because perceptual and phonemic representations were of low quality in the ARHL group, the training program has affected this level of representations in ARHL more than in the normal-hearing group.</p><p>Taken together, it was observed in the current study that the auditory training that was used, benefits people with mild-to-moderate hearing loss. It is left for future research to measure top-down processing strategies in order to enhance our understanding of the effects of training. There may be more effective training methods to add to the current training program; perhaps this requires more diverse training&#x02014;in many more tasks, or more intensive training over a very long period of time or change in the type of feedback used. Finally, studies into the training regimen that yields more generalization are needed.</p></sec><sec id="s6"><title>Author contributions</title><p>HK, TB, JA, and KB designed the study; HK collected and analyzed the data; HK, TB, and KB wrote the manuscript. All authors approved the final version of the manuscript.</p><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The reviewer, Carine Signoret, and handling Editor declared their shared affiliation, and the handling Editor states that the process nevertheless met the standards of a fair and objective review.</p></sec></sec></body><back><ack><p>This study was supported by Marie Curie International Reintegration (IRG 224763) and National Institute of Psychobiology in Israel grants to KB, by a feasibility grant from the Israel Ministry of Health to TB, JA, and KB, by Steiner's Fund for Hearing Research to JA, and by Excellence scholarships from the Council of Higher Education, Planning and Budgeting committee and the Graduate Studies Authority of the University of Haifa to HK.</p></ack><sec sec-type="supplementary-material" id="s7"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fpsyg.2015.02066">http://journal.frontiersin.org/article/10.3389/fpsyg.2015.02066</ext-link></p><supplementary-material content-type="local-data"><media xlink:href="DataSheet1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data"><media xlink:href="Image1.JPEG"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname><given-names>M.</given-names></name><name><surname>Nahum</surname><given-names>M.</given-names></name><name><surname>Nelken</surname><given-names>I.</given-names></name><name><surname>Hochstein</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>Reverse hierarchies and sensory learning</article-title>. <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
<volume>364</volume>, <fpage>285</fpage>&#x02013;<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2008.0253</pub-id><pub-id pub-id-type="pmid">18986968</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amitay</surname><given-names>S.</given-names></name><name><surname>Halliday</surname><given-names>L.</given-names></name><name><surname>Taylor</surname><given-names>J.</given-names></name><name><surname>Sohoglu</surname><given-names>E.</given-names></name><name><surname>Moore</surname><given-names>D. R.</given-names></name></person-group> (<year>2010</year>). <article-title>Motivation and intelligence drive auditory perceptual learning</article-title>. <source>PLoS ONE</source>
<volume>5</volume>:<fpage>e9816</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0009816</pub-id><pub-id pub-id-type="pmid">20352121</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amitay</surname><given-names>S.</given-names></name><name><surname>Irwin</surname><given-names>A.</given-names></name><name><surname>Moore</surname><given-names>D. R.</given-names></name></person-group> (<year>2006</year>). <article-title>Discrimination learning induced by training with identical stimuli</article-title>. <source>Nat. Neurosci.</source>
<volume>9</volume>, <fpage>1446</fpage>&#x02013;<lpage>1448</lpage>. <pub-id pub-id-type="doi">10.1038/nn1787</pub-id><pub-id pub-id-type="pmid">17028582</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amitay</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>Y. X.</given-names></name><name><surname>Jones</surname><given-names>P. R.</given-names></name><name><surname>Moore</surname><given-names>D. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Perceptual learning: top to bottom</article-title>. <source>Vision Res.</source>
<volume>99</volume>, <fpage>69</fpage>&#x02013;<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2013.11.006</pub-id><pub-id pub-id-type="pmid">24296314</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>S.</given-names></name><name><surname>White-Schwoch</surname><given-names>T.</given-names></name><name><surname>Choi</surname><given-names>H. J.</given-names></name><name><surname>Kraus</surname><given-names>N.</given-names></name></person-group> (<year>2013b</year>). <article-title>Training changes processing of speech cues in older adults with hearing loss</article-title>. <source>Front. Syst. Neurosci.</source>
<volume>7</volume>:<issue>97</issue>. <pub-id pub-id-type="doi">10.3389/fnsys.2013.00097</pub-id><pub-id pub-id-type="pmid">24348347</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>S.</given-names></name><name><surname>White-Schwoch</surname><given-names>T.</given-names></name><name><surname>Parbery-Clark</surname><given-names>A.</given-names></name><name><surname>Kraus</surname><given-names>N.</given-names></name></person-group> (<year>2013a</year>). <article-title>Reversal of age-related neural timing delays with training</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume>110</volume>, <fpage>4149</fpage>&#x02013;<lpage>4150</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1213555110</pub-id><pub-id pub-id-type="pmid">23401541</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banai</surname><given-names>K.</given-names></name><name><surname>Lavner</surname><given-names>Y.</given-names></name></person-group> (<year>2014</year>). <article-title>The effects of training length on the perceptual learning of time-compressed speech and its generalization</article-title>. <source>J. Acoust. Soc. Am.</source>
<volume>136</volume>, <fpage>1908</fpage>&#x02013;<lpage>1917</lpage>. <pub-id pub-id-type="doi">10.1121/1.4895684</pub-id><pub-id pub-id-type="pmid">25324090</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boothroyd</surname><given-names>A.</given-names></name></person-group> (<year>2007</year>). <article-title>Adult aural rehabilitation: what is it and doesit work?</article-title>
<source>Trends Amplif.</source>
<volume>11</volume>, <fpage>63</fpage>&#x02013;<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1177/1084713807301073</pub-id><pub-id pub-id-type="pmid">17494873</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borg</surname><given-names>E.</given-names></name></person-group> (<year>2000</year>). <article-title>Ecological aspects of auditory rehabilitation</article-title>. <source>Acta Otolaryngol.</source>
<volume>120</volume>, <fpage>234</fpage>&#x02013;<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1080/000164800750001008</pub-id><pub-id pub-id-type="pmid">11603781</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burk</surname><given-names>M. H.</given-names></name><name><surname>Humes</surname><given-names>L. E.</given-names></name></person-group> (<year>2008</year>). <article-title>Effects of long-term training on aided speech-recognition performance in noise in older adults</article-title>. <source>J. Speech Lang. Hear. Res.</source>
<volume>51</volume>, <fpage>759</fpage>&#x02013;<lpage>771</lpage>. <pub-id pub-id-type="doi">10.1044/1092-4388(2008/054)</pub-id><pub-id pub-id-type="pmid">18506049</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burk</surname><given-names>M. H.</given-names></name><name><surname>Humes</surname><given-names>L. E.</given-names></name><name><surname>Amos</surname><given-names>N. E.</given-names></name><name><surname>Strauser</surname><given-names>L. E.</given-names></name></person-group> (<year>2006</year>). <article-title>Effect of training on word-recognition performance in noise for young normal-hearing and older hearing-impaired listeners</article-title>. <source>Ear Hear.</source>
<volume>27</volume>, <fpage>263</fpage>&#x02013;<lpage>278</lpage>. <pub-id pub-id-type="doi">10.1097/01.aud.0000215980.21158.a2</pub-id><pub-id pub-id-type="pmid">16672795</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cainer</surname><given-names>K. E.</given-names></name><name><surname>James</surname><given-names>C.</given-names></name><name><surname>Rajan</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>Learning speech-in-noise discrimination in adult humans</article-title>. <source>Hear. Res.</source>
<volume>238</volume>, <fpage>155</fpage>&#x02013;<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2007.10.001</pub-id><pub-id pub-id-type="pmid">18024026</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>M. H.</given-names></name><name><surname>Johnsrude</surname><given-names>I. S.</given-names></name></person-group> (<year>2007</year>). <article-title>Hearing speech sounds: top-down influences on the interface between audition and speech perception</article-title>. <source>Hear. Res.</source>
<volume>229</volume>, <fpage>132</fpage>&#x02013;<lpage>147</lpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2007.01.014</pub-id><pub-id pub-id-type="pmid">17317056</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>M. H.</given-names></name><name><surname>Johnsrude</surname><given-names>I. S.</given-names></name><name><surname>Hervais-Adelman</surname><given-names>A.</given-names></name><name><surname>Taylor</surname><given-names>K.</given-names></name><name><surname>McGettigan</surname><given-names>C.</given-names></name></person-group> (<year>2005</year>). <article-title>Lexical information drives perceptual learning of distorted speech: evidence from the comprehension of noise-vocoded sentences</article-title>. <source>J. Exp. Psychol. Gen.</source>
<volume>134</volume>, <fpage>222</fpage>&#x02013;<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1037/0096-3445.134.2.222</pub-id><pub-id pub-id-type="pmid">15869347</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>M. A.</given-names></name><name><surname>Henshaw</surname><given-names>H.</given-names></name></person-group> (<year>2015a</year>). <article-title>Computer and internet interventions to optimize listening and learning for people with hearing loss: accessibility, use, and adherence</article-title>. <source>Am. J. Audiol.</source>
<volume>24</volume>, <fpage>338</fpage>&#x02013;<lpage>343</lpage>. <pub-id pub-id-type="doi">10.1044/2015_AJA-14-0090</pub-id><pub-id pub-id-type="pmid">26649543</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>M. A.</given-names></name><name><surname>Henshaw</surname><given-names>H.</given-names></name></person-group> (<year>2015b</year>). <article-title>Auditory training can improve working memory, attention, and communication in adverse conditions for adults with hearing loss</article-title>. <source>Front. Psychol.</source>
<volume>6</volume>:<issue>556</issue>. <pub-id pub-id-type="doi">10.3389/fpsyg.2015.00556</pub-id><pub-id pub-id-type="pmid">26074826</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>M. A.</given-names></name><name><surname>Henshaw</surname><given-names>H.</given-names></name><name><surname>Clark</surname><given-names>D. P. A.</given-names></name><name><surname>Moore</surname><given-names>D. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Benefits of phoneme discrimination training in a randomized controlled trial of 50- to 74-year-olds with mild hearing loss</article-title>. <source>Ear Hear.</source>
<volume>35</volume>, <fpage>e110</fpage>&#x02013;<lpage>e121</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0000000000000020</pub-id><pub-id pub-id-type="pmid">24752284</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fitzgibbons</surname><given-names>P. J.</given-names></name><name><surname>Gordon-Salant</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Behavioral studies with aging humans: hearing sensitivity and psychoacoustics</article-title>, in <source>The Aging Auditory System</source>, eds <person-group person-group-type="editor"><name><surname>Gordon-Salant</surname><given-names>S.</given-names></name><name><surname>Frisina</surname><given-names>R. D.</given-names></name><name><surname>Popper</surname><given-names>A. N.</given-names></name><name><surname>Fay</surname><given-names>R. R.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>111</fpage>&#x02013;<lpage>134</lpage>.</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gil</surname><given-names>D.</given-names></name><name><surname>Iorio</surname><given-names>M. C. M.</given-names></name></person-group> (<year>2010</year>). <article-title>Formal auditory training in adult hearing aid users</article-title>. <source>Clinics</source>
<volume>65</volume>, <fpage>165</fpage>&#x02013;<lpage>174</lpage>. <pub-id pub-id-type="doi">10.1590/S1807-59322010000200008</pub-id><pub-id pub-id-type="pmid">20186300</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon-Salant</surname><given-names>S.</given-names></name></person-group> (<year>2005</year>). <article-title>Hearing loss and aging: new research findings and clinical implications</article-title>. <source>J. Rehabil. Res. Dev.</source>
<volume>42</volume>, <fpage>9</fpage>&#x02013;<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1682/JRRD.2005.01.0006</pub-id><pub-id pub-id-type="pmid">16470462</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson Sabes</surname><given-names>J.</given-names></name><name><surname>Sweetow</surname><given-names>R. W.</given-names></name></person-group> (<year>2007</year>). <article-title>Variables predicting outcomes on listening and communication enhancement (LACETM) training</article-title>. <source>Int. J. Audiol.</source>
<volume>46</volume>, <fpage>374</fpage>&#x02013;<lpage>383</lpage>. <pub-id pub-id-type="doi">10.1080/14992020701297565</pub-id><pub-id pub-id-type="pmid">17680469</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henshaw</surname><given-names>H.</given-names></name><name><surname>Ferguson</surname><given-names>M. A.</given-names></name></person-group> (<year>2013</year>). <article-title>Efficacy of individual computer-based auditory training for people with hearing loss: a systematic review of the evidence</article-title>. <source>PLoS ONE</source>
<volume>8</volume>:<fpage>e62836</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0062836</pub-id><pub-id pub-id-type="pmid">23675431</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G.</given-names></name><name><surname>Poeppel</surname><given-names>D.</given-names></name></person-group> (<year>2007</year>). <article-title>The cortical organization of speech processing</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>8</volume>, <fpage>393</fpage>&#x02013;<lpage>402</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humes</surname><given-names>L. E.</given-names></name><name><surname>Burk</surname><given-names>M. H.</given-names></name><name><surname>Strauser</surname><given-names>L. E.</given-names></name><name><surname>Kinney</surname><given-names>D. L.</given-names></name></person-group> (<year>2009</year>). <article-title>Development and efficacy of a frequent-word auditory training protocol for older adults with impaired hearing</article-title>. <source>Ear Hear.</source>
<volume>30</volume>, <fpage>613</fpage>&#x02013;<lpage>627</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e3181b00d90</pub-id><pub-id pub-id-type="pmid">19633564</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Humes</surname><given-names>L. E.</given-names></name><name><surname>Dubno</surname><given-names>J. R.</given-names></name></person-group> (<year>2010</year>). <article-title>Factors affecting speech understanding in older adults</article-title>, in <source>The Aging Auditory System</source>, eds <person-group person-group-type="editor"><name><surname>Gordon-Salant</surname><given-names>S.</given-names></name><name><surname>Frisina</surname><given-names>R.</given-names></name><name><surname>Fay</surname><given-names>R.</given-names></name><name><surname>Popper</surname><given-names>A. N.</given-names></name></person-group> (<publisher-loc>New York. NY</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>211</fpage>&#x02013;<lpage>257</lpage>.</mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kochkin</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>Marke Trak V: &#x0201c;Why my hearing aids are in the drawer&#x0201d;: The consumers' perspective</article-title>. <source>Hear. J.</source>
<volume>53</volume>, <fpage>34</fpage>
<pub-id pub-id-type="doi">10.1097/00025572-200002000-00004</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname><given-names>L.</given-names></name><name><surname>Attias</surname><given-names>J.</given-names></name><name><surname>Karni</surname><given-names>A.</given-names></name></person-group> (<year>2013</year>). <article-title>Semi-structured listening experience (listening training) in hearing aid fitting: influence on dichotic listening</article-title>. <source>Am. J. Audiol.</source>
<volume>22</volume>, <fpage>347</fpage>&#x02013;<lpage>350</lpage>. <pub-id pub-id-type="doi">10.1044/1059-0889(2013/12-0083)</pub-id><pub-id pub-id-type="pmid">24096865</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname><given-names>L.</given-names></name><name><surname>Banai</surname><given-names>K.</given-names></name><name><surname>Attias</surname><given-names>J.</given-names></name><name><surname>Karni</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Better together: reduced compliance after sequential versus simultaneous bilateral hearing aids fitting</article-title>. <source>Am. J. Audiol.</source>
<volume>23</volume>, <fpage>93</fpage>&#x02013;<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1044/1059-0889(2013/13-0010)</pub-id><pub-id pub-id-type="pmid">24096862</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname><given-names>L.</given-names></name><name><surname>Banai</surname><given-names>K.</given-names></name><name><surname>Karni</surname><given-names>A.</given-names></name><name><surname>Attias</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Hearing aid-induced plasticity in the auditory system of older adults: evidence from speech perception</article-title>. <source>J. Speech Lang. Hear. Res.</source>
<volume>58</volume>, <fpage>1601</fpage>&#x02013;<lpage>1610</lpage>. <pub-id pub-id-type="doi">10.1044/2015_JSLHR-H-14-0225</pub-id><pub-id pub-id-type="pmid">26163676</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname><given-names>H.</given-names></name><name><surname>Oden</surname><given-names>C.</given-names></name><name><surname>Simon</surname><given-names>H.</given-names></name><name><surname>Noack</surname><given-names>C.</given-names></name><name><surname>Lotze</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Entertainment overcomes barriers of auditory training</article-title>. <source>Hear. J.</source>
<volume>64</volume>, <fpage>40</fpage>&#x02013;<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1097/01.HJ.0000403510.80465.7b</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>D. R.</given-names></name></person-group> (<year>2007</year>). <article-title>Auditory processing disorders: acquisition and treatment</article-title>. <source>J. Commun. Disord.</source>
<volume>40</volume>, <fpage>295</fpage>&#x02013;<lpage>304</lpage>. <pub-id pub-id-type="doi">10.1016/j.jcomdis.2007.03.005</pub-id><pub-id pub-id-type="pmid">17467002</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>J. E.</given-names></name><name><surname>Troiani</surname><given-names>V.</given-names></name><name><surname>Grossman</surname><given-names>M.</given-names></name><name><surname>Wingfield</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Hearing loss in older adults affects neural systems supporting speech comprehension</article-title>. <source>J. Neurosci.</source>
<volume>31</volume>, <fpage>12638</fpage>&#x02013;<lpage>12643</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2559-11.2011</pub-id><pub-id pub-id-type="pmid">21880924</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name></person-group> (<year>2008</year>). <article-title>Use of supportive context by younger and older adult listeners: balancing bottom-up and top-down information processing</article-title>. <source>Int. J. Audiol.</source>
<volume>47</volume>, <fpage>S72</fpage>&#x02013;<lpage>S82</lpage>. <pub-id pub-id-type="doi">10.1080/14992020802307404</pub-id><pub-id pub-id-type="pmid">19012114</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name><name><surname>Levitt</surname><given-names>H.</given-names></name></person-group> (<year>2012</year>). <article-title>Speech comprehension training and auditory and cognitive processing in older adults</article-title>. <source>Am. J. Audiol.</source>
<volume>21</volume>, <fpage>351</fpage>&#x02013;<lpage>357</lpage>. <pub-id pub-id-type="doi">10.1044/1059-0889(2012/12-0025)</pub-id><pub-id pub-id-type="pmid">23233521</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name><name><surname>Macdonald</surname><given-names>E.</given-names></name></person-group> (<year>2008</year>). <article-title>Auditory temporal processing deficits in older listeners: a review and overview</article-title>, in <source>Auditory Signal Processing In Hearing-Impaired Listeners: Proceedings of the First International Symposium on Audiological and Auditory Research (ISAAR 2007)</source>, eds <person-group person-group-type="editor"><name><surname>Dau</surname><given-names>T.</given-names></name><name><surname>Buchholz</surname><given-names>J.</given-names></name><name><surname>Harte</surname><given-names>J.</given-names></name><name><surname>Christiansen</surname><given-names>T.</given-names></name></person-group> (<publisher-loc>Holbaek</publisher-loc>: <publisher-name>Centertryk A/S.</publisher-name>), <fpage>297</fpage>&#x02013;<lpage>306</lpage>.</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name><name><surname>Schneider</surname><given-names>B. A.</given-names></name><name><surname>Daneman</surname><given-names>M.</given-names></name></person-group> (<year>1995</year>). <article-title>How young and old adults listen to and remember speech in noise</article-title>. <source>J. Acoust. Soc. Am.</source>
<volume>97</volume>, <fpage>593</fpage>&#x02013;<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1121/1.412282</pub-id><pub-id pub-id-type="pmid">7860836</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name><name><surname>Singh</surname><given-names>G.</given-names></name></person-group> (<year>2006</year>). <article-title>Effects of age on auditory and cognitive processing: implications for hearing aid fitting and audiologic rehabilitation</article-title>. <source>Trends Amplif.</source>
<volume>10</volume>, <fpage>29</fpage>&#x02013;<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1177/108471380601000103</pub-id><pub-id pub-id-type="pmid">16528429</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>C.</given-names></name><name><surname>Thierry</surname><given-names>G.</given-names></name><name><surname>Griffiths</surname><given-names>T.</given-names></name></person-group> (<year>2005</year>). <article-title>Speech-specific auditory processing: where is it?</article-title>
<source>Trends Cogn. Sci.</source>
<volume>9</volume>, <fpage>271</fpage>&#x02013;<lpage>276</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2005.03.009</pub-id><pub-id pub-id-type="pmid">15925805</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name><name><surname>Lunner</surname><given-names>T.</given-names></name><name><surname>Zekveld</surname><given-names>A.</given-names></name><name><surname>S&#x000f6;rqvist</surname><given-names>P.</given-names></name><name><surname>Danielsson</surname><given-names>H.</given-names></name><name><surname>Lyxell</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>The Ease of Language Understanding (ELU) model: theoretical, empirical, and clinical advances</article-title>. <source>Front. Syst. Neurosci.</source>
<volume>7</volume>:<issue>31</issue>. <pub-id pub-id-type="doi">10.3389/fnsys.2013.00031</pub-id><pub-id pub-id-type="pmid">23874273</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name><name><surname>Rudner</surname><given-names>M.</given-names></name><name><surname>Foo</surname><given-names>C.</given-names></name><name><surname>Lunner</surname><given-names>T.</given-names></name></person-group> (<year>2008</year>). <article-title>Cognition counts: a working memory system for ease of language understanding (ELU)</article-title>. <source>Int. J. Audiol.</source>
<volume>47</volume>, <fpage>S171</fpage>&#x02013;<lpage>S177</lpage>. <pub-id pub-id-type="doi">10.1080/14992020802301167</pub-id><pub-id pub-id-type="pmid">19012117</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>T. N.</given-names></name><name><surname>Hanebuth</surname><given-names>D.</given-names></name><name><surname>Probst</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>Prevalence of age-related hearing loss in Europe: a review</article-title>. <source>Eur. Arch. Otorhinolaryngol.</source>
<volume>268</volume>, <fpage>1101</fpage>&#x02013;<lpage>1107</lpage>. <pub-id pub-id-type="doi">10.1007/s00405-011-1597-8</pub-id><pub-id pub-id-type="pmid">21499871</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>B.</given-names></name><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name></person-group> (<year>2000</year>). <article-title>Implications of perceptual deterioration for cognitive aging research</article-title>, in <source>The Handbook of Aging and Cognition</source>, eds <person-group person-group-type="editor"><name><surname>Craik</surname><given-names>F. M. I.</given-names></name><name><surname>Salthouse</surname><given-names>T. A.</given-names></name></person-group> (<publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>), <fpage>155</fpage>&#x02013;<lpage>219</lpage>.</mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>B. A.</given-names></name><name><surname>Daneman</surname><given-names>M.</given-names></name><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name></person-group> (<year>2002</year>). <article-title>Listening in aging adults: from discourse comprehension to psychoacoustics</article-title>. <source>Can. J. Exp. Psychol.</source>
<volume>56</volume>, <fpage>139</fpage>. <pub-id pub-id-type="doi">10.1037/h0087392</pub-id><pub-id pub-id-type="pmid">12271745</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>G. E.</given-names></name><name><surname>Housen</surname><given-names>P.</given-names></name><name><surname>Yaffe</surname><given-names>K.</given-names></name><name><surname>Ruff</surname><given-names>R.</given-names></name><name><surname>Kennison</surname><given-names>R. F.</given-names></name><name><surname>Mahncke</surname><given-names>H. W.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>A cognitive training program based on principles of brain plasticity: results from the Improvement in Memory with Plasticity: based Adaptive Cognitive Training (IMPACT) study</article-title>. <source>J. Am. Geriatr. Soc.</source>
<volume>57</volume>, <fpage>594</fpage>&#x02013;<lpage>603</lpage>. <pub-id pub-id-type="doi">10.1111/j.1532-5415.2008.02167.x</pub-id><pub-id pub-id-type="pmid">19220558</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>J. H.</given-names></name><name><surname>Skoe</surname><given-names>E.</given-names></name><name><surname>Banai</surname><given-names>K.</given-names></name><name><surname>Kraus</surname><given-names>N.</given-names></name></person-group> (<year>2012</year>). <article-title>Training to improve hearing speech in noise: biological mechanisms</article-title>. <source>Cereb. Cortex</source>
<volume>22</volume>, <fpage>1180</fpage>&#x02013;<lpage>1190</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr196</pub-id><pub-id pub-id-type="pmid">21799207</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecker</surname><given-names>G. C.</given-names></name><name><surname>Bowman</surname><given-names>G. A.</given-names></name><name><surname>Yund</surname><given-names>E. W.</given-names></name><name><surname>Herron</surname><given-names>T. J.</given-names></name><name><surname>Roup</surname><given-names>C. M.</given-names></name><name><surname>Woods</surname><given-names>D. L.</given-names></name></person-group> (<year>2006</year>). <article-title>Perceptual training improves syllable identification in new and experienced hearing aid users</article-title>. <source>J. Rehabil. Res. Dev.</source>
<volume>43</volume>, <fpage>537</fpage>&#x02013;<lpage>551</lpage>. <pub-id pub-id-type="doi">10.1682/JRRD.2005.11.0171</pub-id><pub-id pub-id-type="pmid">17123192</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweetow</surname><given-names>R.</given-names></name><name><surname>Palmer</surname><given-names>C. V.</given-names></name></person-group> (<year>2005</year>). <article-title>Efficacy of individual auditory training in adults: a systematic review of the evidence</article-title>. <source>J. Am. Acad. Audiol.</source>
<volume>16</volume>, <fpage>494</fpage>. <pub-id pub-id-type="doi">10.3766/jaaa.16.7.9</pub-id><pub-id pub-id-type="pmid">16295236</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweetow</surname><given-names>R. W.</given-names></name><name><surname>Henderson Sabes</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>Auditory training and challenges associated with participation and compliance</article-title>. <source>J. Am. Acad. Audiol.</source>
<volume>21</volume>, <fpage>586</fpage>&#x02013;<lpage>593</lpage>. <pub-id pub-id-type="doi">10.3766/jaaa.21.9.4</pub-id><pub-id pub-id-type="pmid">21241646</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweetow</surname><given-names>R. W.</given-names></name><name><surname>Sabes</surname><given-names>H. J.</given-names></name></person-group> (<year>2006</year>). <article-title>The need for and development of an adaptive Listening and Communication Enhancement (LACE (TM)) program</article-title>. <source>J. Am. Acad. Audiol.</source>
<volume>17</volume>, <fpage>538</fpage>&#x02013;<lpage>558</lpage>. <pub-id pub-id-type="doi">10.3766/jaaa.17.8.2</pub-id><pub-id pub-id-type="pmid">16999250</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweetow</surname><given-names>R. W.</given-names></name><name><surname>Sabes</surname><given-names>J. H.</given-names></name></person-group> (<year>2007</year>). <article-title>Technologic advances in aural rehabilitation: applications and innovative methods of service delivery</article-title>. <source>Trends Amplif.</source>
<volume>11</volume>, <fpage>101</fpage>&#x02013;<lpage>111</lpage>. <pub-id pub-id-type="doi">10.1177/1084713807301321</pub-id><pub-id pub-id-type="pmid">17494876</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tice</surname><given-names>B.</given-names></name><name><surname>Carrell</surname><given-names>T.</given-names></name></person-group> (<year>1997</year>). <source>Tone</source>. <publisher-loc>Lincoln, NE</publisher-loc>: <publisher-name>University of Nebraska</publisher-name>.</mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tun</surname><given-names>P. A.</given-names></name><name><surname>Williams</surname><given-names>V. A.</given-names></name><name><surname>Small</surname><given-names>B. J.</given-names></name><name><surname>Hafter</surname><given-names>E. R.</given-names></name></person-group> (<year>2012</year>). <article-title>The effects of aging on auditory processing and cognition</article-title>. <source>Am. J. Audiol.</source>
<volume>21</volume>, <fpage>344</fpage>&#x02013;<lpage>350</lpage>. <pub-id pub-id-type="doi">10.1044/1059-0889(2012/12-0030)</pub-id><pub-id pub-id-type="pmid">23233520</pub-id></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wechsler</surname><given-names>D.</given-names></name></person-group> (<year>1997</year>). <source>Wechsler Adult Intelligence Scale.</source>
<publisher-loc>San Antonio, TX</publisher-loc>: <publisher-name>The Psychological Corporation</publisher-name>.</mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wingfield</surname><given-names>A.</given-names></name><name><surname>Grossman</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Language and the aging brain: patterns of neural compensation revealed by functional brain imaging</article-title>. <source>J. Neurophysiol.</source>
<volume>96</volume>, <fpage>2830</fpage>&#x02013;<lpage>2839</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00628.2006</pub-id><pub-id pub-id-type="pmid">17110737</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wingfield</surname><given-names>A.</given-names></name><name><surname>Stine-Morrow</surname><given-names>E. A. L.</given-names></name></person-group> (<year>2000</year>). <article-title>Language and speech</article-title>, in <source>The Handbook of Aging and Cognition, 2nd Edn.,</source> eds <person-group person-group-type="editor"><name><surname>Craik</surname><given-names>F. I. M.</given-names></name><name><surname>Salthouse</surname><given-names>T. A.</given-names></name></person-group> (<publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>), <fpage>359</fpage>&#x02013;<lpage>416</lpage>.</mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>P. C. M.</given-names></name><name><surname>Ettlinger</surname><given-names>M.</given-names></name><name><surname>Sheppard</surname><given-names>J. P.</given-names></name><name><surname>Gunasekera</surname><given-names>G. M.</given-names></name><name><surname>Dhar</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Neuroanatomical characteristics and speech perception in noise in older adults</article-title>. <source>Ear Hear.</source>
<volume>31</volume>, <fpage>471</fpage>&#x02013;<lpage>479</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e3181d709c2</pub-id><pub-id pub-id-type="pmid">20588117</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>B. A.</given-names></name><name><surname>Buonomano</surname><given-names>D. V.</given-names></name><name><surname>Mahncke</surname><given-names>H. W.</given-names></name><name><surname>Merzenich</surname><given-names>M. M.</given-names></name></person-group> (<year>1997</year>). <article-title>Learning and generalization of auditory temporal-interval discrimination in humans</article-title>. <source>J. Neurosci.</source>
<volume>17</volume>, <fpage>3956</fpage>&#x02013;<lpage>3963</lpage>. <pub-id pub-id-type="pmid">9133413</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yueh</surname><given-names>B.</given-names></name><name><surname>Shapiro</surname><given-names>N.</given-names></name><name><surname>Maclean</surname><given-names>C. H.</given-names></name><name><surname>Shekelle</surname><given-names>P. G.</given-names></name></person-group> (<year>2003</year>). <article-title>Screening and management of adult hearing loss in primary care: scientific review</article-title>. <source>JAMA</source>
<volume>289</volume>, <fpage>1976</fpage>&#x02013;<lpage>1985</lpage>. <pub-id pub-id-type="doi">10.1001/jama.289.15.1976</pub-id><pub-id pub-id-type="pmid">12697801</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>A. A.</given-names></name><name><surname>Heslenfeld</surname><given-names>D. J.</given-names></name><name><surname>Festen</surname><given-names>J. M.</given-names></name><name><surname>Schoonhoven</surname><given-names>R.</given-names></name></person-group> (<year>2006</year>). <article-title>Top-down and bottom-up processes in speech comprehension</article-title>. <source>Neuroimage</source>
<volume>32</volume>, <fpage>1826</fpage>&#x02013;<lpage>1836</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.04.199</pub-id><pub-id pub-id-type="pmid">16781167</pub-id></mixed-citation></ref></ref-list></back></article>
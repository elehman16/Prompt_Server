<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28715442</article-id><article-id pub-id-type="pmc">5513428</article-id><article-id pub-id-type="publisher-id">PONE-D-16-35943</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0180792</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Plants</subject><subj-group><subject>Trees</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Management Engineering</subject><subj-group><subject>Decision Analysis</subject><subj-group><subject>Decision Trees</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Decision Analysis</subject><subj-group><subject>Decision Trees</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Ecosystems</subject><subj-group><subject>Forests</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Ecology and Environmental Sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Ecosystems</subject><subj-group><subject>Forests</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Ecology and Environmental Sciences</subject><subj-group><subject>Terrestrial Environments</subject><subj-group><subject>Forests</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (Mathematics)</subject><subj-group><subject>Confidence Intervals</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Combining random forest with multi-block local binary pattern feature selection for multiclass head pose estimation</article-title><alt-title alt-title-type="running-head">Efficient head pose estimation with random forest and multi-block local binary pattern</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kang</surname><given-names>Min-Joo</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Data curation</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Jung-Kyung</given-names></name><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><name><surname>Kang</surname><given-names>Je-Won</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Formal analysis</role><role content-type="http://credit.casrai.org/">Software</role><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Validation</role><role content-type="http://credit.casrai.org/">Visualization</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="corresp" rid="cor001">*</xref><xref ref-type="aff" rid="aff001"/></contrib></contrib-group><aff id="aff001">
<addr-line>The Department of Electronics Engineering, Ewha W. University, Seoul, Republic of Korea</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Yudong</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>Nanjing Normal University, CHINA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>jewonk@ewha.ac.kr</email></corresp></author-notes><pub-date pub-type="collection"><year>2017</year></pub-date><pub-date pub-type="epub"><day>17</day><month>7</month><year>2017</year></pub-date><volume>12</volume><issue>7</issue><elocation-id>e0180792</elocation-id><history><date date-type="received"><day>7</day><month>9</month><year>2016</year></date><date date-type="accepted"><day>21</day><month>6</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2017 Kang et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Kang et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0180792.pdf"/><abstract><p>A new head pose estimation technique based on Random Forest (RF) and texture features for facial image analysis using a monocular camera is proposed in this paper, especially about how to efficiently combine the random forest and the features. In the proposed technique a randomized tree with useful attributes is trained to improve estimation accuracy and tolerance of occlusions and illumination. Specifically, a number of features including Multi-scale Block Local Block Pattern (MB-LBP) are extracted from an image, and random features such as the MB-LBP scale parameters, a block coordinate, and a layer of an image pyramid in the feature pool are used for training the tree. The randomized tree aims to maximize the information gain at each node while random samples traverse the nodes in the tree. To this aim, a split function considering the uniform property of the LBP feature is developed to move sample blocks to the left or the right children nodes. The trees are independently trained with random inputs, yet they are grouped to form a random forest so that the results collected from the trees are used for make the final decision. Precisely, we use a Maximum-A-Posteriori criterion in the decision. It is demonstrated with experimental results that the proposed technique provides significantly enhanced classification performance in the head pose estimation in various conditions of illumination, poses, expressions, and facial occlusions.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>Technology Advancement Research Program (TARP) funded by Ministry of Land, Infrastructure and Transport of Korean government</institution></funding-source><award-id>This research was supported by a grant(16CTAP-C114986-01) from Technology Advancement Research Program (TARP) funded by Ministry of Land, Infrastructure and Transport of Korean government</award-id><principal-award-recipient><name><surname>Kang</surname><given-names>Je-Won</given-names></name></principal-award-recipient></award-group><funding-statement>This research was supported by a grant (16CTAP-C114986-01) from Technology Advancement Research Program (TARP) funded by Ministry of Land, Infrastructure and Transport of Korean government.</funding-statement></funding-group><counts><fig-count count="20"/><table-count count="4"/><page-count count="24"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All facial data samples are available from the CMU Multi-PIE face database (<ext-link ext-link-type="uri" xlink:href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html">http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html</ext-link>). The minimal underlying data set is placed in the public available site (figshare). Please see the link below: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5142466">https://doi.org/10.6084/m9.figshare.5142466</ext-link> or <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/Pointing04_DB/5142466">https://figshare.com/articles/Pointing04_DB/5142466</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All facial data samples are available from the CMU Multi-PIE face database (<ext-link ext-link-type="uri" xlink:href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html">http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html</ext-link>). The minimal underlying data set is placed in the public available site (figshare). Please see the link below: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5142466">https://doi.org/10.6084/m9.figshare.5142466</ext-link> or <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/Pointing04_DB/5142466">https://figshare.com/articles/Pointing04_DB/5142466</ext-link>.</p></notes></front><body><sec id="sec001"><title>1 Introduction</title><p>Head pose estimation is the front-end technique to infer the changes in view points of a human face in an image as the heading estimation is important in human navigation and locomotion [<xref rid="pone.0180792.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0180792.ref002" ref-type="bibr">2</xref>]. Many face-related computer vision systems provide the best performance to the frontal views of faces even though the human faces in an image are often non-frontal with various poses. Thus, the head pose estimation aims to facilitate the computer vision applications. In [<xref rid="pone.0180792.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0180792.ref004" ref-type="bibr">4</xref>], the faces are rotated as a result of the pose estimation to perform face recognition and face expression analysis, respectively. In [<xref rid="pone.0180792.ref005" ref-type="bibr">5</xref>] the frontal faces are used for retrieving key frames in video summarization. In [<xref rid="pone.0180792.ref006" ref-type="bibr">6</xref>] head pose information is employed for gaze estimation and human activity recognition.</p><p>The algorithms can have different granularity though they handle the same vision task. At the coarse granularity, the algorithms is applied to determine a pose among several discrete orientations, e.g. typically 5&#x0223c;9 directions considering the degree of the freedom (DoF) of human heads [<xref rid="pone.0180792.ref007" ref-type="bibr">7</xref>&#x02013;<xref rid="pone.0180792.ref009" ref-type="bibr">9</xref>]. At the fine granularity, the algorithms estimate the continuous angles from regression methods in the full 3D position of a head [<xref rid="pone.0180792.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0180792.ref012" ref-type="bibr">12</xref>]. However, in practice, the ground truth of an accurate angle is difficult to obtain because the subject is not located at the same 3D space. For instance, Fanelli <italic>et al</italic>. use supplemental depth images to the estimation of a 3D position [<xref rid="pone.0180792.ref011" ref-type="bibr">11</xref>]. In [<xref rid="pone.0180792.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>], Kinect sensors are used for obtaining depth information and performing the regression in 3D coordinates.</p><p>Most of the head pose estimation techniques need a series of steps to interpret a high-level understanding of orientation from the face image [<xref rid="pone.0180792.ref014" ref-type="bibr">14</xref>]. In other words, a statistical model is established to transform the pixel-based representation of a head to the feature subspace, followed by an optimized classifier. The algorithms needs to be robust to a variety of image-changing factors, <italic>e</italic>.<italic>g</italic>. illumination changes, facial expressions, and the occlusions with hats and glasses. In the point of the view, a number of related works have been studied in the field of the head pose estimation. In [<xref rid="pone.0180792.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0180792.ref015" ref-type="bibr">15</xref>] the high dimensional spaces of face images are mapped into the lower dimensional manifolds. In [<xref rid="pone.0180792.ref016" ref-type="bibr">16</xref>] the pose variation as a 3-sphere manifold is modeled in the high-dimensional feature space. Statistical distributions of face appearances, named Active Appearance Model (AAM) are developed [<xref rid="pone.0180792.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0180792.ref018" ref-type="bibr">18</xref>]. Several low-level texture descriptors are used for distinguishing the facial features in the appearance [<xref rid="pone.0180792.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0180792.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>&#x02013;<xref rid="pone.0180792.ref023" ref-type="bibr">23</xref>]. In [<xref rid="pone.0180792.ref008" ref-type="bibr">8</xref>] Haar-like features trained with AdaBoost are used for detecting distinctive facial features. In [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>] a histogram of oriented gradients (HoG) descriptors are used for the face pose estimation. Local Binary Pattern (LBP)-based descriptors are widely used for the classification because they are compact and reliable to image changes. In [<xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>, <xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0180792.ref025" ref-type="bibr">25</xref>], the LBP-based feature descriptors including Gabor feature and run-length matrix are used for representing facial features. In [<xref rid="pone.0180792.ref025" ref-type="bibr">25</xref>] a local directional quaternary pattern (LDQP) is proposed to represent directional changes in pixels as a variation of an LBP. In addition, deep learning based image features are used for the pose estimation, trained from a large size of face data [<xref rid="pone.0180792.ref026" ref-type="bibr">26</xref>, <xref rid="pone.0180792.ref027" ref-type="bibr">27</xref>].</p><p>Random Forest (RF) refers to an ensemble of trained decision trees [<xref rid="pone.0180792.ref028" ref-type="bibr">28</xref>], shown to be effectively applied to classification problems in many computer vision applications. RF can naturally manage multiclass problems because leaf nodes in a tree correspond to classes. Each tree in the forest is independently trained with random samples, and it is combined together to construct a group of the trees, providing classification or regression. RF is also widely used for previous head pose estimation research [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0180792.ref029" ref-type="bibr">29</xref>&#x02013;<xref rid="pone.0180792.ref033" ref-type="bibr">33</xref>]. In the works the random forest improves the classification accuracy and the run-time efficiency as compared to the conventional approaches in the classification, <italic>e</italic>.<italic>g</italic>. PCA and SVM. The classification performance relies on how to maximize the discriminative power at each node in RF, achieved by an ability of the split function. Kim <italic>et al</italic>. use information gain to develop the random forest [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>] with a run-length matrix of bit patterns. Huang <italic>et al</italic>. discriminate various head poses using Gabor features and the linear discriminant analysis (LDA) at the node test [<xref rid="pone.0180792.ref031" ref-type="bibr">31</xref>]. In [<xref rid="pone.0180792.ref029" ref-type="bibr">29</xref>, <xref rid="pone.0180792.ref030" ref-type="bibr">30</xref>] the random forest regression is employed for the head pose estimation after detecting a human face. In [<xref rid="pone.0180792.ref032" ref-type="bibr">32</xref>] compressive features obtained from sparse responses of color and gradient components are used for random projection forest algorithm. In [<xref rid="pone.0180792.ref033" ref-type="bibr">33</xref>] a regression forest is trained from random face patches, which shows superior performance in the unrestricted databases. In [<xref rid="pone.0180792.ref032" ref-type="bibr">32</xref>, <xref rid="pone.0180792.ref033" ref-type="bibr">33</xref>], the random forest is shown to be robust to in-the-wild databases by learning image samples. However, in developing the split functions, the previous works have rarely considered the characteristics of features used for the facial data abstraction. As compared to the works, the proposed technique shows how to combine the random Forest with efficient facial analysis features for the head pose estimation.</p><p>In this paper, we propose the multiclass head pose estimation algorithm at the coarse-level prediction, which uses a randomized tree incorporating an multi-block LBP (MB-LBP) to be reliable with facial occlusions. In previous works [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0180792.ref029" ref-type="bibr">29</xref>, <xref rid="pone.0180792.ref031" ref-type="bibr">31</xref>&#x02013;<xref rid="pone.0180792.ref033" ref-type="bibr">33</xref>], the randomized trees with various image features have been introduced, yet there have been less efforts made in efficiently combining the trees and their ingredients to maximize the discriminative performance. In this work, we develop the randomized tree that includes an effective split function to learn important facial patterns represented by the LBP descriptors. Specifically, we consider the uniform property of MB-LBP in designing the split function. Furthermore several random attributes of image patches are taken into account in the construction of the random tree because the LBP-based descriptor alone may be too sensitive to local noises or occlusions. To this aim, we use Gaussian image pyramid and different sizes of block patches when encoding LPB patterns. In the classification, the trees grouped in the random forest are used for the final decision by using Maximum-A-Posteriori (MAP) criterion. It is demonstrated in the experimental results that the integration of the developed features and the random forest achieves significantly improved classification performance in various conditions of illumination, poses, expressions, and facial occlusions.</p></sec><sec id="sec002"><title>2 Preliminary</title><sec id="sec003"><title>2.1 Local binary pattern applied to face analysis</title><p>The original LBP operator assigns pixels in a 3 &#x000d7; 3 block into a binary string [<xref rid="pone.0180792.ref034" ref-type="bibr">34</xref>]. The operator compares the 8 immediately neighboring pixels to the center pixel and encodes the result as an eight-bin sequence. The LBP is robust to illumination changes because it computes the signs of pixel differences. However, the patterns may be readily distorted from the noises and small pixel variations. Therefore, the LBP is extended later in different applications [<xref rid="pone.0180792.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0180792.ref036" ref-type="bibr">36</xref>]. In one extension, the LBP operator is applied to the surrounding blocks at different scales, named multi-scale block LBP (MB-LBP) [<xref rid="pone.0180792.ref036" ref-type="bibr">36</xref>]. The multi-resolution analysis of a block uses the average values of surrounding sub-blocks when comparing those to the center block. <xref ref-type="fig" rid="pone.0180792.g001">Fig 1</xref> shows the original LBP and the MB-LBP when the size of the sub-block is 4. In <xref ref-type="fig" rid="pone.0180792.g001">Fig 1</xref> the pixel values are the averages of the sub-blocks, and &#x0201c;1&#x0201d; is assigned if the corresponding neighborhood is greater than or equal to the center value. Otherwise, &#x0201c;0&#x0201d; is assigned. The binary sequence created by MB-LBP is &#x0201c;11100011&#x0201d; (or 227 as a decimal number) in the example.</p><fig id="pone.0180792.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g001</object-id><label>Fig 1</label><caption><title>The original LBP and MB-LBP with a scale parameter <italic>s</italic>.</title></caption><graphic xlink:href="pone.0180792.g001"/></fig><p>The number of the possible LBP patterns can be too many as shown in the example, and the high dimensional feature space may incur an over-fitting problem in learning. Thus, in another extension, a sub-group of the LBP patterns, named a uniform LBP, is considered to resolve the problem. The uniform LBP is defined as a binary string that includes at most two bitwise transitions from 0 to 1 or vice versa in the circular presentation as shown in <xref ref-type="fig" rid="pone.0180792.g002">Fig 2</xref>. The uniform LBP shows several useful properties. First the nine spatial micro-structures are used for representative patterns, including a bright spot (0), edges and corners (1&#x0223c;7), and a homogeneous region (8) because they are frequently appeared in the textures. In [<xref rid="pone.0180792.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0180792.ref036" ref-type="bibr">36</xref>], it is observed that the uniform patterns account for around 90% of all LBP patterns in facial data while only the 58 patterns are uniform among 256 8-bit patterns. Second the uniform LBP is invariant in rotation, so the similar patterns can be compactly represented. Thus, considering the properties, the uniform LBP patterns can be used for a feature reduction of LBP.</p><fig id="pone.0180792.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g002</object-id><label>Fig 2</label><caption><title>Nine uniform LBP patterns.</title></caption><graphic xlink:href="pone.0180792.g002"/></fig><p>The LBP operator has been widely used in facial data analysis [<xref rid="pone.0180792.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0180792.ref037" ref-type="bibr">37</xref>&#x02013;<xref rid="pone.0180792.ref040" ref-type="bibr">40</xref>] because important facial features (e.g. a nose and eyes) incorporating distinctive micro texture patterns are well described by such operators. They consider the local descriptions of faces based on LBP features and combine them into global descriptions to be robust against pose and illumination variations. The local descriptor and the global descriptor intend to capture the micro-patterns of textures and some invariant properties, respectively. In [<xref rid="pone.0180792.ref035" ref-type="bibr">35</xref>] the facial image is divided into several sub-blocks where the LBP descriptors are extracted independently, and then they are linked together to the global descriptor of the face. The different sizes of the sub-blocks are used for the multi-resolution analysis of a facial image.</p></sec><sec id="sec004"><title>2.2 Review of random forest</title><p>In this section we review the training and testing of a random forest. Random forest (RF) turns out to be an efficient machine learning technique in many computer vision applications. It is shown in [<xref rid="pone.0180792.ref028" ref-type="bibr">28</xref>] that a group of randomized trees provides high generalization power while the decision tree alone may suffer from an overfitting problem. Thus, the random forest is formed with an ensemble of the trees as shown in <xref ref-type="fig" rid="pone.0180792.g003">Fig 3</xref>. Furthermore, to achieve the generalization, the trees are built with considering randomness in training. The training samples are randomly chosen either for growing the tree, for optimizing the node decision, or for the both.</p><fig id="pone.0180792.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g003</object-id><label>Fig 3</label><caption><title>Randomized trees including a root node, internal nodes, and leaf nodes and edges.</title><p>The random forest consists of the trees.</p></caption><graphic xlink:href="pone.0180792.g003"/></fig><p>A tree <italic>T</italic> in the forest <inline-formula id="pone.0180792.e001"><alternatives><graphic xlink:href="pone.0180792.e001.jpg" id="pone.0180792.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> consists of several nodes including a root node, internal nodes, and leaf node, and edges connecting with the nodes, shown in <xref ref-type="fig" rid="pone.0180792.g003">Fig 3</xref>. Learning a randomized tree is supervised, <italic>i</italic>.<italic>e</italic>., the training samples are annotated with label information. In the training, the goal is to maximize the classification performance when input samples traverse from a root node to a leaf node corresponding to each label. To this aim, each internal node needs to make its own optimal binary decision using a split function, formulated <italic>e</italic>.<italic>g</italic>. in the input data <inline-formula id="pone.0180792.e002"><alternatives><graphic xlink:href="pone.0180792.e002.jpg" id="pone.0180792.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mi>v</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> arriving at the <italic>i</italic>-th node,
<disp-formula id="pone.0180792.e003"><alternatives><graphic xlink:href="pone.0180792.e003.jpg" id="pone.0180792.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>&#x003d5;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="script">P</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>&#x003d5;</italic><sub><italic>i</italic></sub> denotes the split parameters associated with the <italic>i</italic>-th node in the set of all split parameters <inline-formula id="pone.0180792.e004"><alternatives><graphic xlink:href="pone.0180792.e004.jpg" id="pone.0180792.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mi mathvariant="script">P</mml:mi></mml:math></alternatives></inline-formula>, and 0 and 1 are understood as the left and the right children nodes to be placed.</p><p>There are several research works developing the binary tests in the pose estimation. Li <italic>et al</italic>. use the the pixel intensities at two different pixel positions [<xref rid="pone.0180792.ref029" ref-type="bibr">29</xref>]. Huang <italic>et al</italic>. apply linear discriminative analysis to the test [<xref rid="pone.0180792.ref031" ref-type="bibr">31</xref>]. However, the information gain (IG) is useful in general cases [<xref rid="pone.0180792.ref028" ref-type="bibr">28</xref>]. In information theory, IG is defined as the reduction of uncertainty when the training data arriving at the current node is divided into the children nodes. IG is mathematically defined as:
<disp-formula id="pone.0180792.e005"><alternatives><graphic xlink:href="pone.0180792.e005.jpg" id="pone.0180792.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic>S</italic><sub><italic>i</italic></sub> refers to the data set at the <italic>i</italic>-th node being split into the two subsets <italic>S</italic><sup><italic>L</italic></sup> and <italic>S</italic><sup><italic>R</italic></sup> in the left children and in the right children, respectively. <italic>H</italic>(<italic>S</italic>) is the entropy.</p><p>In testing, an unseen sample traverses the tree down to a leaf node by using the trained split functions with the associated parameters. The input sample is accordingly moved either to the left child node or to the right child node. The estimation is done when the sample is stopped to a leaf node. Note we construct a group of trees in the random forest. Therefore the final decision is made by considering the results from all the trees.</p></sec></sec><sec id="sec005"><title>3 Proposed technique</title><p>There are two subsequent tasks in the head pose estimation, <italic>i</italic>.<italic>e</italic>., the face detection in an image and the following pose estimation. In this paper, we assume facial data would be already localized for the pose estimation, and focus on the latter problem, as shown in <xref ref-type="fig" rid="pone.0180792.g004">Fig 4</xref>. Facial images obtained from monocular cameras are detected and cropped with face detection algorithm such as Viola-Jones method. For this, we use a standard facial image set named CMU Multi-PIE [<xref rid="pone.0180792.ref041" ref-type="bibr">41</xref>], including various face orientations, illumination conditions, and facial expressions, to resolve the problem. The image sets are annotated with pre-defined rotation angles that are quantized (e.g. 5&#x0223c;9) based on the degree-of-freedom of human faces [<xref rid="pone.0180792.ref014" ref-type="bibr">14</xref>].</p><fig id="pone.0180792.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g004</object-id><label>Fig 4</label><caption><title>The processing pipeline of the head pose estimation in the proposed technique.</title></caption><graphic xlink:href="pone.0180792.g004"/></fig><sec id="sec006"><title>3.1 Proposed feature space</title><p>In the proposed technique, a facial image is normalized to <italic>W</italic> &#x000d7; <italic>H</italic> size. Specifically <italic>W</italic> and <italic>H</italic> are equal to 108. Then, a gaussian image pyramid that is a sequence of low-pass filtered images of an original image is applied as a pre-processing step to an image patch. Because the LBP features can be too sensitive to local noises or occlusions, a gaussian image pyramid is applied to the input images before the feature extraction. The original image denoted by <italic>G</italic><sub>0</sub> is sequentially filtered with a Gaussian kernel <italic>w</italic> whose filter tab is 11 &#x000d7; 11 and the standard variation is set to 1. Then, the images are sub-sampled by a factor of two to generate the sequence of reduced resolution images <italic>G</italic><sub><italic>l</italic></sub>. The levels of the pyramid are obtained iteratively. Mathematically, they are given as,
<disp-formula id="pone.0180792.e006"><alternatives><graphic xlink:href="pone.0180792.e006.jpg" id="pone.0180792.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
In the proposed technique <italic>l</italic> is set to 0, 1, and 2. As the size of a facial image is normalized to 108, corresponding to <italic>G</italic><sub>0</sub>, the next layered images corresponding to <italic>G</italic><sub>1</sub> and <italic>G</italic><sub>2</sub> are equal to 54 and 27, respectively.</p><p><italic>M</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub> denotes an MB-LBP feature obtained from a randomly chosen block in an image. In <italic>M</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub>,<italic>s</italic> refers to a block size, which can be either 1, 4, 12, or 36. Thus, there are four MB-LBP feature spaces. <italic>g</italic> refers to a level of an image pyramid, which can be either 0, 1, and 2. <italic>k</italic> is the center pixel position of an MB-LBP block to retrieve the bit-pattern. The blocks can overlap one another during the feature extraction, so <italic>k</italic> can be any pixel position in a block if the block is fully contained in the image. <xref ref-type="fig" rid="pone.0180792.g005">Fig 5</xref> shows the proposed feature set. The features are used for constructing a feature set <italic>p</italic> in all possible parameter space <italic>P</italic>.</p><fig id="pone.0180792.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g005</object-id><label>Fig 5</label><caption><title>MB-LBP based feature set including the size of the block <italic>s</italic>, the center position <italic>k</italic>, and the scale of the Gaussian image pyramid <italic>g</italic>, and the upper and the lower thresholds, which are used for establishing a split function in each node of a randomized tree.</title></caption><graphic xlink:href="pone.0180792.g005"/></fig><p>The number of all the possible MB-LBP patterns is too large, which may cause an overfitting problem by the high dimensional feature spaces. Therefore we quantize the MB-LBP to a uniform MB-LBP denoted by <italic>U</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub> for a feature reduction. Among all the possible uniform MB-LBP patterns, <italic>U</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub> is formed as the closest bit-pattern from <italic>M</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub> with respect to the Hamming distance. For example, &#x0201c;11100101&#x0201d; is converted to &#x0201c;11100111.&#x0201d; If there are multiple candidates, the less significant bits are changed. It is observed in the facial image data that the uniform patterns are more than 90% of the LBP patterns while only the 58 patterns are uniform. Thus, we employ the properties for the feature reduction in the training.</p></sec><sec id="sec007"><title>3.2 Proposed random forest</title><p>Optimizing the split function is important in the developments of the random forest. The function needs to be tailored to the MB-LBP based features. For this, we propose a split function <italic>h</italic>(.) for <italic>U</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub> to be trained in a randomized tree <italic>T</italic>, defined as,
<disp-formula id="pone.0180792.e007"><alternatives><graphic xlink:href="pone.0180792.e007.jpg" id="pone.0180792.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02254;</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:msup><mml:mn>2</mml:mn><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:msup><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.em"/><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
where <italic>U</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub> is the uniform MB-LBP with a level of an image pyramid <italic>g</italic>, a scale parameter <italic>s</italic>, and a position <italic>k</italic>. <italic>&#x003c4;</italic><sub><italic>u</italic></sub> and <italic>&#x003c4;</italic><sub><italic>l</italic></sub> are two constraint thresholds that are, respectively, used for the upper bound and the lower bound of decimal representation of the uniform MB-LBP. All the parameters are exemplified in <xref ref-type="fig" rid="pone.0180792.g005">Fig 5</xref>. It is highlighted that the two constraints regarding the upper bound and the lower bound are used for compactly clustering the similar textures because there are at most two bit transitions in a uniform LBP. The selected parameter set is trained to determine the split function <italic>h</italic>, as shown in <xref ref-type="fig" rid="pone.0180792.g005">Fig 5</xref>.</p><p>The function is to map an input <italic>U</italic><sub><italic>g</italic>,<italic>s</italic>,<italic>k</italic></sub> to the binary outputs 0 and 1. Based on the binary test, the training samples at a node in a randomized tree are split into two children nodes. If the output of the function is true, the samples are sent to the left child node. Otherwise, they are sent to the right child node. The parameters at the nodes are learnt during the training to maximize an objective function. We employ the information gain function [<xref rid="pone.0180792.ref028" ref-type="bibr">28</xref>] that is defined as the difference between the differential entropy of the parent node and the sum of the differential entropies of the children nodes. The idea behind is that the information gain increases more when a child node contains less diversified classes, thus more discriminative capability of the tree. information gain function <italic>I</italic> is given as,
<disp-formula id="pone.0180792.e008"><alternatives><graphic xlink:href="pone.0180792.e008.jpg" id="pone.0180792.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(5)</label></disp-formula>
where <italic>H</italic>(<italic>U</italic>) is an entropy to measure uncertainty. The entropy in the proposed technique is defined as <italic>H</italic>(<italic>U</italic>) = &#x02212;&#x02211;<sub><italic>c</italic>&#x02208;<italic>C</italic></sub>
<italic>p</italic>(<italic>c</italic>|<italic>U</italic>) log <italic>p</italic>(<italic>c</italic>|<italic>U</italic>) where <italic>C</italic> is the set of classes and <italic>p</italic> is a probability of samples with a label <italic>c</italic> at a node specified by <italic>U</italic>. The distribution of the classes in the left and the right children is changed by <italic>U</italic> at a node, and the number of the classes is counted to compute the distribution. The information gain increases more if a child node has less diversified entries. Thus, the optimal parameter <inline-formula id="pone.0180792.e009"><alternatives><graphic xlink:href="pone.0180792.e009.jpg" id="pone.0180792.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in the split is given as,
<disp-formula id="pone.0180792.e010"><alternatives><graphic xlink:href="pone.0180792.e010.jpg" id="pone.0180792.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo form="prefix">arg</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(6)</label></disp-formula>
where <italic>P</italic><sub><italic>j</italic></sub> is the randomly chosen parameter space in all possible set <italic>P</italic> at the <italic>j</italic>-th node.</p><p>The optimized parameters are stored at internal nodes while constructing a randomized tree in the training. For example, in <xref ref-type="fig" rid="pone.0180792.g006">Fig 6</xref>, the optimal parameters maximizing the information gain are used in the node 5. The same optimization is repeatedly performed at each subsequent node. We also use a bagging that extracts random training samples from the image set for each tree. The bagging allows reliable performance results against large variants of input data while using less memory sizes in training. The training stops when the termination condition satisfies. In the standard RF training [<xref rid="pone.0180792.ref028" ref-type="bibr">28</xref>], the tree stops growing if it reaches to the pre-defined maximum depth, or if there are too few samples remaining in the current node. Specifically we set the maximum depth of a tree and the minimum samples in a node, respectively, to 9 and 5. We will show experimental results with respect to various termination conditions in the experimental results.</p><fig id="pone.0180792.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g006</object-id><label>Fig 6</label><caption><title>Construction of a randomized tree.</title></caption><graphic xlink:href="pone.0180792.g006"/></fig><p>Training a randomized tree is to build each optimal weak classifier corresponding to a node in the tree structure. On top of that, the tree also needs to provide an accurate prediction model at the leaf nodes. In the supervised learning, a subset of labeled training samples is associated with leaf nodes, and therefore the distributions of the labels can be used for the prediction. Precisely, we employ the conditional distributions after observing the associated samples, <italic>i</italic>.<italic>e</italic>., <italic>p</italic>(<italic>c</italic>|<italic>u</italic>) where <italic>c</italic> is the label of the head pose class in all possible set <italic>C</italic>, <italic>u</italic> is the uniform MB-LBP sample. Subsequently, we use a Maximum-A-Posteriori (MAP) for the predictor, defined as
<disp-formula id="pone.0180792.e011"><alternatives><graphic xlink:href="pone.0180792.e011.jpg" id="pone.0180792.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(7)</label></disp-formula>
For instance, in <xref ref-type="fig" rid="pone.0180792.g006">Fig 6</xref>, the node 11 and the node 12 are chosen, respectively, for the left and the frontal faces as they are major in the leaf nodes.</p><p>In a testing, a previously unseen sample traverses the tree down to a leaf node by going through the trained nodes. The split function at a node directs the samples either to the left child node or to the right child node, and, accordingly, the sample reaches to a leaf node. The estimation is done in the leaf node. Note that each randomized tree is grouped into a random forest. Therefore, in testing, all the prediction results need to be combined into a single forest prediction to make the final decision. The decision could be made with maintaining the whole conditional probability distributions. However, we use the major voting of the prediction results in the final decision as we compute the MAP prediction in a tree.</p></sec></sec><sec id="sec008"><title>4 Experimental results</title><sec id="sec009"><title>4.1 Test condition</title><p>In this section, the performance of the proposed technique is quantitatively evaluated. The experiments are performed using CMU Multi-PIE head pose image database [<xref rid="pone.0180792.ref041" ref-type="bibr">41</xref>], including 3,600 face images with 20 subjects with various face poses, lightening conditions, and facial expressions, controlled in a laboratory. We also use the AFLW [<xref rid="pone.0180792.ref042" ref-type="bibr">42</xref>], AFW [<xref rid="pone.0180792.ref043" ref-type="bibr">43</xref>], 300W [<xref rid="pone.0180792.ref044" ref-type="bibr">44</xref>], and Pointing04 [<xref rid="pone.0180792.ref045" ref-type="bibr">45</xref>]. AFLW, AFW, and 300W data bases are &#x0201c;in-the wild&#x0201d; data bases, and Pointing04 is another data base acquired from a laboratory condition. It is noted that any pre-processing technique to resolve the lightening variation is not applied to clearly show the performance of the proposed technique. Readers who are interested in the effects of the pre-processing such as histogram equalization may refer Tan&#x02019;s work [<xref rid="pone.0180792.ref046" ref-type="bibr">46</xref>]. We use the Viola-Jones method to detect the faces, and the image samples are resized to 108 &#x000d7; 108. In prior arts [<xref rid="pone.0180792.ref047" ref-type="bibr">47</xref>, <xref rid="pone.0180792.ref048" ref-type="bibr">48</xref>] an alignment process of a face sample has played important roles in pose estimation. In [<xref rid="pone.0180792.ref047" ref-type="bibr">47</xref>], a partial least squares regression-based method is used for reducing sensitivity to misalignment, thus providing better classification results. In our experiments, we use an alignment algorithm for the LBP-based descriptors to cope with geometric invariance. The facial feature points such as corners of the eyes and the tip of the nose are aligned in the samples by using trained feature sets, as in [<xref rid="pone.0180792.ref049" ref-type="bibr">49</xref>]. The process is automatically applied to all the facial samples that are used in the experiments.</p><p>The experiments are configured to predict the head rotation angles quantized into 3&#x0223c;9 classes, equally-spaced from &#x02212;90 to 90 degrees. Some of the subjects are occluded with glasses or hairs, which are used for demonstrating the reliable performance of the proposed technique against an occlusion. In training, we use 5-fold cross validation to avoid any over-fitting. For training the randomized tree, the maximum depth of tree is set to up to 9, and the minimal number of samples processed in a node is 5 to stop the tree growing. We train maximum 15 trees to create a forest. The parameters are empirically set to maximize the performance. In testing, the performance is evaluated by averaging the results in five times.</p><p>We perform the intra-data base experiments and inter-data base experiments. In the intra-data base experiments, two disjoint sets of facial samples from the same data base are separately used for training and testing. Specifically, the CMU MultiPIE data base is used for the intra-data base experiments. In the inter-data base experiments, the facial samples from the different data bases are separately used for training and testing. Specifically, the random forest is trained with the CMU MultiPIE data base, and then the trained model is tested using in-the-wild data bases [<xref rid="pone.0180792.ref042" ref-type="bibr">42</xref>&#x02013;<xref rid="pone.0180792.ref044" ref-type="bibr">44</xref>] and Pointing04 acquired in laboratory conditions [<xref rid="pone.0180792.ref045" ref-type="bibr">45</xref>]. The results of the inter-data base experiments are shown in Sec. 4.2.4. All experiments are performed with an Intel i7 @ 3.60GHz CPU and 8GB memory.</p></sec><sec id="sec010"><title>4.2 Performance evaluation</title><sec id="sec011"><title>4.2.1 Performance comparisons to conventional techniques</title><p>In this subsection, we present the results of the intra-data base experiments using the MultiPIE data base. We show the estimation accuracies of the proposed technique and the conventional techniques named &#x0201c;Conventional LBP&#x0201d; and &#x0201c;Conventional MB-LBP&#x0201d; with respect to the classes of the different head poses in <xref ref-type="fig" rid="pone.0180792.g007">Fig 7</xref>. &#x0201c;Conventional LBP&#x0201d; and &#x0201c;Conventional MB-LBP&#x0201d; refer to the algorithms using only the original LBP and the MB-LBP, respectively, combined with the same random forest classifiers. However, in the conventional algorithms, only one constraint parameter, <italic>i</italic>.<italic>e</italic>., <italic>&#x003c4;</italic> of the split function is used [<xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>]. In other words, in <xref ref-type="disp-formula" rid="pone.0180792.e003">Eq (1)</xref>, <italic>h</italic><sub><italic>&#x003d5;</italic></sub>(<italic>U</italic><sub><italic>&#x003d5;</italic></sub>) is true if a uniform MB-LBP <italic>U</italic><sub><italic>&#x003d5;</italic></sub> is less than a single threshold <italic>&#x003c4;</italic>, otherwise, it is false. Thus the performance difference shows mostly the impact of the proposed split function design on the estimation accuracy. As shown in <xref ref-type="fig" rid="pone.0180792.g007">Fig 7</xref> the proposed technique provides significantly improved estimation accuracies over the conventional algorithms in all the numbers of the head poses. The proposed technique provides the performance about 95%, 87.2%, 82%, 74%, respectively, in 3, 5, 7, and 9 pose cases. The average performance is 85%. As compared to the average, &#x0201c;Conventional LBP&#x0201d; and &#x0201c;Conventional MB-LBP&#x0201d; provide the average performance of 53% and 75%, respectively. Even though the classification performance is monotonically degraded with the number of the classes, the performance of the proposed technique is more gentle in the degradation than the conventional techniques because of the extended block sizes in the feature extraction. For instance, <xref ref-type="fig" rid="pone.0180792.g007">Fig 7</xref> shows 95 &#x0223c; 74% in &#x0201c;Proposed (NL)&#x0201d; while showing 90.2 &#x0223c; 52% in &#x0201c;Conventional MB-LBP&#x0201d;, and 75.8 &#x0223c; 28% in &#x0201c;Conventional LBP&#x0201d;, which is much unreliable. &#x0201c;Conventional MB-LBP&#x0201d; is comparable with &#x0201c;Proposed (NL)&#x0201d; in 3 and 5 poses. However, the differences in the performance become large in 7 and 9 poses about 7&#x0223c; 22%. We show the binomial confidence interval for 95% confidence in <xref ref-type="fig" rid="pone.0180792.g007">Fig 7</xref>. The error bar represents how much uncertainty the proposed technique has in the estimation. The ranges of the errors in the proposed techniques are around &#x000b1;0.9%&#x0223c;&#x000b1;1.6%, while those in the conventional LBP-based techniques are around &#x000b1;1.7%&#x0223c;&#x000b1;3.1%.</p><fig id="pone.0180792.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g007</object-id><label>Fig 7</label><caption><title>Estimation accuracies of the proposed technique with respect to the number of the classes, as compared to the conventional algorithms.</title><p>Proposed (NL) refers to the technique where the uniform MB-LBP is extracted from non-overlapped block patches in the proposed technique while Proposed (OL) uses overlapped block patches in the generation. The error bars represent 95% binomial confidence intervals.</p></caption><graphic xlink:href="pone.0180792.g007"/></fig><p>Furthermore we provide two variations of the proposed techniques, depicted as &#x0201c;Proposed (NL)&#x0201d; and &#x0201c;Proposed (OL).&#x0201d; The candidates of the block positions to extract the uniform MBLBP features are only the differences between the algorithms. &#x0201c;Proposed (NL)&#x0201d; extracts the uniform MB-LBP features from non-overlapped <italic>s</italic> &#x000d7; <italic>s</italic> blocks in the image sample. In other words, the pixel position <italic>k</italic> in <xref ref-type="disp-formula" rid="pone.0180792.e007">Eq (4)</xref> can be placed only on the grid of the image sample. As compared, in &#x0201c;Proposed (OL)&#x0201d;, the pixel position <italic>k</italic> can be any position in a block if the uniform MB-LBP feature is available. In implementation, we choose a subset of the overlapping <italic>s</italic> &#x000d7; <italic>s</italic> blocks during the training rather than to use all the possible pixel positions. As shown in <xref ref-type="fig" rid="pone.0180792.g007">Fig 7</xref> the average classification performance of &#x0201c;Proposed (OL)&#x0201d; is better than that of &#x0201c;Proposed (NL)&#x0201d; about 2.5 &#x0223c; 5.0%. Meanwhile the training time increases about 180% in &#x0201c;Proposed (OL)&#x0201d; because there can be more pixel positions, randomly selected in training the randomized tree. However, the test time is only slightly changed. Once the node parameters are determined, the classification is very quick, which is an important advantage of the random forest.</p><p>To show the reliable discriminative power to the occlusions, we reorganize the CMU MultiPIE database to include only the faces having occlusions such as hairs and glasses, and show the results in <xref ref-type="fig" rid="pone.0180792.g008">Fig 8</xref>. The performance of the proposed technique significantly outperforms those of the two other conventional algorithms in all the number of the poses as well. The average performance of the proposed technique (<italic>i</italic>.<italic>e</italic>., &#x0201c;Proposed (NL)&#x0201d;) is 82%, which is better than those of the other two conventional techniques, <italic>i</italic>.<italic>e</italic>. 53% and 70%, respectively in &#x0201c;Conventional LBP&#x0201d; and &#x0201c;Conventional MB-LBP&#x0201d;. As shown, the performance of the proposed technique is more reliable to the occlusions than those of the conventional techniques. It varies from 93.5% in 3 pose to 67% in 9 pose, <italic>i</italic>.<italic>e</italic>., the difference among the poses is about 26.5%. However, the differences in &#x0201c;Conventional LBP&#x0201d; and &#x0201c;Conventional MB-LBP&#x0201d; are about 47.7% and 39.2%, respectively. &#x0201c;Proposed (OL)&#x0201d; yields the highest classification performance about 88% on average. We also show the binomial confidence interval for 95% confidence in <xref ref-type="fig" rid="pone.0180792.g008">Fig 8</xref>. The ranges of the errors in the proposed techniques are around &#x000b1;1.1%&#x0223c;&#x000b1;2.7%. The ranges are slightly larger than in <xref ref-type="fig" rid="pone.0180792.g007">Fig 7</xref> as the occlusion gives higher variability in the inputs.</p><fig id="pone.0180792.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g008</object-id><label>Fig 8</label><caption><title>Estimation accuracies of the proposed technique with respect to the number of the classes, as compared to the conventional algorithms when the facial images have occlusions.</title><p>The error bars represent 95% binomial confidence intervals.</p></caption><graphic xlink:href="pone.0180792.g008"/></fig><p>Several confusion matrices obtained from in 7 and 9 poses are shown in Figs <xref ref-type="fig" rid="pone.0180792.g009">9</xref>&#x0223c;<xref ref-type="fig" rid="pone.0180792.g012">12</xref> to provide a more comprehensive analysis of the proposed technique. The matrices show that the proposed technique yields reliable performance to the estimation because most of the errors occur in neighboring angles. It is observed from Figs <xref ref-type="fig" rid="pone.0180792.g011">11</xref> and <xref ref-type="fig" rid="pone.0180792.g012">12</xref> that the performance is quite robust in estimating the frontal face and &#x02212;90 and 90 degrees, corresponding the class 5, 1, and 9, respectively. However the misclassification is relatively frequent in the intermediate angles. As compared to the 9 poses, Figs <xref ref-type="fig" rid="pone.0180792.g009">9</xref> and <xref ref-type="fig" rid="pone.0180792.g010">10</xref> depict in the 7 poses that the errors are evenly distributed at the most of the classes.</p><fig id="pone.0180792.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g009</object-id><label>Fig 9</label><caption><title>Confusion matrices of &#x0201c;Proposed (NL)&#x0201d; in 7 class case.</title></caption><graphic xlink:href="pone.0180792.g009"/></fig><fig id="pone.0180792.g010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g010</object-id><label>Fig 10</label><caption><title>Confusion matrices of &#x0201c;Proposed (OL)&#x0201d; in 7 class case.</title></caption><graphic xlink:href="pone.0180792.g010"/></fig><fig id="pone.0180792.g011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g011</object-id><label>Fig 11</label><caption><title>Confusion matrices of &#x0201c;Proposed (NL)&#x0201d; in 9 class case.</title></caption><graphic xlink:href="pone.0180792.g011"/></fig><fig id="pone.0180792.g012" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g012</object-id><label>Fig 12</label><caption><title>Confusion matrices of &#x0201c;Proposed (OL)&#x0201d; in 9 class case.</title></caption><graphic xlink:href="pone.0180792.g012"/></fig></sec><sec id="sec012"><title>4.2.2 Performance analysis in various conditions on parameters</title><p>The proposed technique incorporates various factors that can affect the overall performance. For the purpose of experimental analysis on the factors we first change the MB-LBP parameters. The proposed technique extracts four MB-LBP feature planes (<italic>i</italic>.<italic>e</italic>. the block sizes are either 1, 4, 12, or 36) for possible candidates in training while the compared algorithms do only few number of the features. We examine the proportions of the MB-LBP sizes, selected as the best feature at each node in a random forest to figure out which sizes affect the performance. We observe from the empirical results that the proportions of the blocks are 65.5%, 11.1%, 14.8%, 8.6%, respectively for the size 1, 4, 12, and 36 in training, as shown in <xref ref-type="table" rid="pone.0180792.t001">Table 1</xref>. In other words, the block size equal to 1 is largely selected among the candidates to maximize the information gain in the tree, and, thus we include the block size equal to 1 in all the comparisons. <xref ref-type="fig" rid="pone.0180792.g013">Fig 13</xref> shows the classification performance with respect to the MB-LBP sizes <italic>s</italic> in the 5 pose case. The performance shows 81.3% when 1 &#x000d7; 1 and 36 &#x000d7; 36 block-sized MB-LBP are used. As compared, the performance is close to the best when 1 &#x000d7; 1 and 12 &#x000d7; 12 block-sized MB-LBP are used. It is noted that the 4 &#x000d7; 4 block size provides slight changes to the performance. The phenomenon is because the features from 4 &#x000d7; 4 block size may be similar to 1 &#x000d7; 1 block size in the second level of the Gaussian pyramid. However all the block sizes somehow contributes to improving the overall performance as revealed in <xref ref-type="table" rid="pone.0180792.t001">Table 1</xref>. The proposed technique achieves the best performance when all the block sizes are used in the random forest.</p><table-wrap id="pone.0180792.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.t001</object-id><label>Table 1</label><caption><title>Proportions of MB-LBP block sizes, selected as the best feature at each node in a random forest.</title></caption><alternatives><graphic id="pone.0180792.t001g" xlink:href="pone.0180792.t001"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">MBLBP size(<italic>s</italic>)</th><th align="center" rowspan="1" colspan="1"><italic>s</italic> = 1</th><th align="center" rowspan="1" colspan="1"><italic>s</italic> = 4</th><th align="center" rowspan="1" colspan="1"><italic>s</italic> = 12</th><th align="center" rowspan="1" colspan="1"><italic>s</italic> = 36</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">Proportion (%)</td><td align="char" char="." rowspan="1" colspan="1">65.5%</td><td align="char" char="." rowspan="1" colspan="1">11.1%</td><td align="char" char="." rowspan="1" colspan="1">14.8%</td><td align="char" char="." rowspan="1" colspan="1">8.6%</td></tr></tbody></table></alternatives></table-wrap><fig id="pone.0180792.g013" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g013</object-id><label>Fig 13</label><caption><title>Performance changes with respect to the number of the MB-LBP feature planes.</title><p><italic>s</italic> refers to the size of the MB-LBP block. The error bars represent 95% binomial confidence intervals.</p></caption><graphic xlink:href="pone.0180792.g013"/></fig><p>Second, the performance of the proposed technique can rely on the different parameters of a random forest, and therefore we present the effects of the changes of the parameters. Precisely, the parameters that are the maximum depth (MD) of a tree, the minimum samples (MS) of a node, and the forest size (FS) are changed. The MD and MS are used for the early-termination condition in training as a random tree finishes its growth when the maximum depth or the minimum samples reaches to the pre-defined values. Figs <xref ref-type="fig" rid="pone.0180792.g014">14</xref>&#x0223c;<xref ref-type="fig" rid="pone.0180792.g016">16</xref> show the variations of the performance with the RF parameters MD, MS, and FS. In <xref ref-type="fig" rid="pone.0180792.g014">Fig 14</xref>, the proposed technique shows 91.2%, 94.1%, 95%, 93.2%, and 92.8% when the maximum depths (MD) are 5, 7, 9, 11, and 13, respectively. In <xref ref-type="fig" rid="pone.0180792.g015">Fig 15</xref> the proposed technique shows 95%, 92.8%, 92.4%, 92.1%, and 92.5% when the minimum samples (MS) are 5, 10, 15, 20, and 25, respectively. The forest size (FS) determines the number of trees comprising a forest. Each tree performs the classification in training/testing independently, and each of the result is combined to make the final decision. <xref ref-type="fig" rid="pone.0180792.g016">Fig 16</xref> shows the variations of the performance with respect to the forest size. The performance is 91.5%, 91.8%, 93.4%, 95%, and 94.5%, respectively when the sizes are 9, 11, 13, 15, and 17. We emphasize from the results that the variations of the classification performance are relatively small even though the RF parameters are different. Furthermore, the confidence intervals with respect to the different parameters are similar one another. This phenomenon highlights the robustness of the performance of the proposed technique over various conditions and practical advantages because subtle changes in the implementation do not affect significant changes in the performance.</p><fig id="pone.0180792.g014" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g014</object-id><label>Fig 14</label><caption><title>Performance changes with respect to the number of the maximum depth (MD) of the random tree.</title><p>The error bars represent 95% binomial confidence intervals.</p></caption><graphic xlink:href="pone.0180792.g014"/></fig><fig id="pone.0180792.g015" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g015</object-id><label>Fig 15</label><caption><title>Performance changes with respect to the number of the minimum samples (MS) in the tree.</title><p>The error bars represent 95% binomial confidence intervals.</p></caption><graphic xlink:href="pone.0180792.g015"/></fig><fig id="pone.0180792.g016" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g016</object-id><label>Fig 16</label><caption><title>Performance changes with respect to the number of the forest size (FS).</title><p>The error bars represent 95% binomial confidence intervals.</p></caption><graphic xlink:href="pone.0180792.g016"/></fig></sec><sec id="sec013"><title>4.2.3 Performance comparison with various feature descriptors</title><p>In this subsection we show the performance of the proposed technique as compared to previous research works using various feature descriptors. For this we choose the state-of-the-art methods using different image descriptors such as histogram of gradient (HoG) feature [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>], Gabor feature [<xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>], and bit-pattern run length (BPRL) feature [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>]. Support vector machine (SVM) is used for a classifier in [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>] while the random forest (RF) is used for [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>] as in the proposed technique. We select the compared algorithms using monocular cameras processing RGB color images but also some of the algorithms use supplemental depth images, obtained from Kinect sensor [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>]. Some of the algorithms perform the regression of the head poses [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>]. In the comparison, we choose a specific angle in the regression to evaluate the performance.</p><p>
<xref ref-type="table" rid="pone.0180792.t002">Table 2</xref> shows the results of using various image descriptors and classifiers for the head pose estimation. We observe from the results that the LBP-based descriptors provides superior performance as compared to the HoG-based descriptors. For instance, Ma <italic>et al</italic>. [<xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>] use a Gabor-filtered LBP followed by SVM, providing better classification performance than the HoG-based descriptors with SVM [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>]. The MB-LBP based descriptors yield more robust descriptors against occlusions and illumination variants in face analysis. However, the performance relies on the classifier as well. Drouard <italic>et al</italic>. [<xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>] show fairly good performance with HoG-based descriptors with the random forest. Furthermore the random forest achieves better performance with MB-LBP than with HoG, when seeing the performance of the proposed technique and the compared algorithms. The MB-LBP can provide higher generalization capability in the parameter selections. Accordingly, the proposed technique shows the best classification performance, <italic>i</italic>.<italic>e</italic>, classification error around 5.0% with 0.8% of 95% confidence interval and the mean absolute error around 4.17. The depth information enhances fair performance, observed from [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>] and [<xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>]. However, they need RGB+D camera sensors. We also show the cumulative head pose estimation error distributions (%) of test images with respect to a degree in <xref ref-type="fig" rid="pone.0180792.g017">Fig 17</xref>. As shown in <xref ref-type="fig" rid="pone.0180792.g017">Fig 17</xref> the proposed technique provides robust classification performance in errors.</p><table-wrap id="pone.0180792.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.t002</object-id><label>Table 2</label><caption><title>The classification errors (CE), the mean absolute errors (MAE) in degree of the head pose estimation algorithms using different features and classifiers in intra-bases experiments, and the standard deviation (STD) of the degrees.</title></caption><alternatives><graphic id="pone.0180792.t002g" xlink:href="pone.0180792.t002"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" rowspan="1" colspan="1">Feature/Classifier</th><th align="center" rowspan="1" colspan="1">CE</th><th align="center" rowspan="1" colspan="1">MAE(degree)</th><th align="center" rowspan="1" colspan="1">STD</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">Ma <italic>et al</italic>. [<xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>]</td><td align="center" rowspan="1" colspan="1">LGBP/SVM</td><td align="char" char="." rowspan="1" colspan="1">10.8</td><td align="char" char="." rowspan="1" colspan="1">5.22</td><td align="char" char="." rowspan="1" colspan="1">7.61</td></tr><tr><td align="center" rowspan="1" colspan="1">Kim <italic>et al</italic>. [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>]</td><td align="center" rowspan="1" colspan="1">BPRL/RF</td><td align="char" char="." rowspan="1" colspan="1">9.5</td><td align="char" char="." rowspan="1" colspan="1">5.17</td><td align="char" char="." rowspan="1" colspan="1">6.82</td></tr><tr><td align="center" rowspan="1" colspan="1">Drouard <italic>et al</italic>. [<xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>]</td><td align="center" rowspan="1" colspan="1">GLLiM+HoG/RF</td><td align="char" char="." rowspan="1" colspan="1">8.2</td><td align="char" char="." rowspan="1" colspan="1">4.83</td><td align="char" char="." rowspan="1" colspan="1">6.75</td></tr><tr><td align="center" rowspan="2" colspan="1">Yang <italic>et al</italic>. [<xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>]</td><td align="center" rowspan="1" colspan="1">HoG/SVM</td><td align="char" char="." rowspan="1" colspan="1">12.1</td><td align="char" char="." rowspan="1" colspan="1">5.35</td><td align="char" char="." rowspan="1" colspan="1">7.50</td></tr><tr><td align="center" rowspan="1" colspan="1">HoG+Depth/SVM</td><td align="char" char="." rowspan="1" colspan="1">8.3</td><td align="char" char="." rowspan="1" colspan="1">4.81</td><td align="char" char="." rowspan="1" colspan="1">6.49</td></tr><tr><td align="center" rowspan="2" colspan="1">Saeed <italic>et al</italic>. [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>]</td><td align="center" rowspan="1" colspan="1">HoG/SVM</td><td align="char" char="." rowspan="1" colspan="1">12.4</td><td align="char" char="." rowspan="1" colspan="1">5.37</td><td align="char" char="." rowspan="1" colspan="1">7.31</td></tr><tr><td align="center" rowspan="1" colspan="1">HoG+Depth/SVM</td><td align="char" char="." rowspan="1" colspan="1">8.9</td><td align="char" char="." rowspan="1" colspan="1">4.96</td><td align="char" char="." rowspan="1" colspan="1">7.05</td></tr><tr><td align="center" rowspan="1" colspan="1">MB-LBP+SVM</td><td align="center" rowspan="1" colspan="1">MB-LBP/SVM</td><td align="char" char="." rowspan="1" colspan="1">15.8</td><td align="char" char="." rowspan="1" colspan="1">5.84</td><td align="char" char="." rowspan="1" colspan="1">9.07</td></tr><tr><td align="center" rowspan="1" colspan="1">Proposed (NL)</td><td align="center" rowspan="1" colspan="1">MB-LBP/RF</td><td align="char" char="." rowspan="1" colspan="1">10.6</td><td align="char" char="." rowspan="1" colspan="1">5.20</td><td align="char" char="." rowspan="1" colspan="1">7.13</td></tr><tr><td align="center" rowspan="1" colspan="1">Proposed (OL)</td><td align="center" rowspan="1" colspan="1">MB-LBP/RF</td><td align="char" char="." rowspan="1" colspan="1">5.0</td><td align="char" char="." rowspan="1" colspan="1">4.17</td><td align="char" char="." rowspan="1" colspan="1">6.59</td></tr></tbody></table></alternatives></table-wrap><fig id="pone.0180792.g017" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g017</object-id><label>Fig 17</label><caption><title>Cumulative head pose estimation error (%) of test images with respect to a degree.</title></caption><graphic xlink:href="pone.0180792.g017"/></fig><p>We evaluate the classification accuracies with various feature selections. The classification performance relies on choices of feature subsets to avoid significant loss. The conventional feature selection usually goes through two independent procedures: a filtering process based on independent criteria of supervised learning and an embedding process to choose the best features subset [<xref rid="pone.0180792.ref050" ref-type="bibr">50</xref>]. In the proposed technique, the two steps are jointly combined with the random forest where each node tries to determines the best subset of the MB-LBP features and associated parameters in <xref ref-type="disp-formula" rid="pone.0180792.e010">Eq (6)</xref> during the training. Figs <xref ref-type="fig" rid="pone.0180792.g018">18</xref> and <xref ref-type="fig" rid="pone.0180792.g019">19</xref> show the classification error rates with the number of features, determined by the different classifiers and feature selection methods. We observe the performance with respect to the number of the chosen features in the 3-pose and the 7-pose cases. The original number of the features is 6 since <italic>k</italic> denotes the <italic>x</italic> &#x02212; <italic>y</italic> coordinate in an image. We leave <italic>k</italic> out of the feature selection as the MB-LBP is a local feature, so the number of the feature varies from 6 to 3. In <xref ref-type="fig" rid="pone.0180792.g018">Fig 18</xref> that &#x0201c;PROP&#x0201d; denotes the use of the proposed technique while restricting the maximal number of the features. &#x0201c;MBLBP(FMS)+RF&#x0201d; and &#x0201c;MBLBP(FMS)+SVM&#x0201d; denote the compared algorithms, using the independent procedures to choose the features. We apply Fisher-Markov Selector (FMS) with a linear polynomial order [<xref rid="pone.0180792.ref051" ref-type="bibr">51</xref>] as an explicit feature selector to the MB-LBP feature, followed by the random forest and SVM. It is observed in <xref ref-type="fig" rid="pone.0180792.g018">Fig 18</xref>, the &#x0201c;PROP&#x0201d; shows only the slight improvements over the two other algorithms. However, when the number of the class increase to 7 in <xref ref-type="fig" rid="pone.0180792.g019">Fig 19</xref>, the differences are more visible. That is because the proposed technique performs the joint optimization during the feature selection. The FMS is a generic feature selector, but it works well when the number of the features is much greater than the number of the classes [<xref rid="pone.0180792.ref051" ref-type="bibr">51</xref>].</p><fig id="pone.0180792.g018" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g018</object-id><label>Fig 18</label><caption><title>Performance changes with respect to the number of the MB-LBP feature in 3-pose case, using different features selectors and classifiers.</title></caption><graphic xlink:href="pone.0180792.g018"/></fig><fig id="pone.0180792.g019" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g019</object-id><label>Fig 19</label><caption><title>Performance changes with respect to the number of the MB-LBP feature in 7-pose case, using different features selectors and classifiers.</title></caption><graphic xlink:href="pone.0180792.g019"/></fig></sec><sec id="sec014"><title>4.2.4 Performance analysis in inter-data base experiments</title><p>In this subsection, we show the results of inter-data base experiments. The parameters in the random forest are trained with the MultiPIE data, and then the model is tested with different data bases such as AFLW, AFW, 300W, and Pointing04 [<xref rid="pone.0180792.ref042" ref-type="bibr">42</xref>&#x02013;<xref rid="pone.0180792.ref045" ref-type="bibr">45</xref>]. As 300W and AFW have smaller facial samples, we merge the same number of samples from the two data-bases into one named &#x0201c;AFW&#x00026;300W&#x0201d; in the evaluation.</p><p>
<xref ref-type="fig" rid="pone.0180792.g020">Fig 20</xref> shows the cumulative head pose estimation error (%) distributions using the wild data bases, denoted by &#x0201c;Pointing04&#x0201d;, &#x0201c;AFW&#x00026;300W&#x0201d;, and &#x0201c;AFLW&#x0201d;. The proposed technique provides fairly good performance when using in-the wild data bases such as &#x0201c;AFW&#x00026;300W&#x0201d; and &#x0201c;AFLW&#x0201d; but also provides comparable results with the intra-database experiments in &#x0201c;Pointing04.&#x0201d; Pointing04 data base is acquired in laboratory condition as in MultiPIE. Thus, the performance is similar to one another. In <xref ref-type="fig" rid="pone.0180792.g020">Fig 20</xref>, &#x0201c;Pointing04 (Mixed)&#x0201d;, &#x0201c;AFW&#x00026;300W (Mixed)&#x0201d;, and &#x0201c;AFLW (Mixed)&#x0201d; show the results when the training samples are evenly chosen from MultiPIE data base and the wild data bases and the testing samples are chosen solely from the corresponding wild data bases. As shown, the performance increases significantly, especially in &#x0201c;AFW&#x00026;300W&#x0201d; and &#x0201c;AFLW&#x0201d;. Tables <xref ref-type="table" rid="pone.0180792.t003">3</xref> and <xref ref-type="table" rid="pone.0180792.t004">4</xref> shows the classification errors (CE), the mean absolute errors (ME) of the degrees, and the standard deviation (STD) of the compared algorithms in inter-bases experiments and in mixed inter-bases experiments. According to the results, the proposed technique achieves the best performance among the compared algorithms. The random forest is used in the proposed technique, Kim <italic>et al</italic>. [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>], and Drouard <italic>et al</italic>. [<xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>] while the other three techniques [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>] use the support vector machine. It is observed that the techniques using the random forest provides much better performance in the inter-data base cases.</p><fig id="pone.0180792.g020" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.g020</object-id><label>Fig 20</label><caption><title>Cumulative head pose estimation error (%) of test images with respect to a degree in inter-db experiments.</title></caption><graphic xlink:href="pone.0180792.g020"/></fig><table-wrap id="pone.0180792.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.t003</object-id><label>Table 3</label><caption><title>The classification errors (CE)%, the mean absolute errors (MAE) in degree of the proposed technique, and the standard deviation (STD) of the degrees in inter-bases experiments.</title></caption><alternatives><graphic id="pone.0180792.t003g" xlink:href="pone.0180792.t003"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Database</th><th align="center" colspan="3" rowspan="1">Pointing04</th><th align="center" colspan="3" rowspan="1">AFLW</th><th align="center" colspan="3" rowspan="1">AFW&#x00026;300W</th></tr><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" rowspan="1" colspan="1">CE</th><th align="center" rowspan="1" colspan="1">MAE</th><th align="center" rowspan="1" colspan="1">STD</th><th align="center" rowspan="1" colspan="1">CE</th><th align="center" rowspan="1" colspan="1">MAE</th><th align="center" rowspan="1" colspan="1">STD</th><th align="center" rowspan="1" colspan="1">CE</th><th align="center" rowspan="1" colspan="1">MAE</th><th align="center" rowspan="1" colspan="1">STD</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">Ma [<xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>]</td><td align="char" char="." rowspan="1" colspan="1">17.1</td><td align="char" char="." rowspan="1" colspan="1">10.4</td><td align="char" char="." rowspan="1" colspan="1">13.6</td><td align="char" char="." rowspan="1" colspan="1">58.9</td><td align="char" char="." rowspan="1" colspan="1">76.5</td><td align="char" char="." rowspan="1" colspan="1">58.2</td><td align="char" char="." rowspan="1" colspan="1">70.3</td><td align="center" rowspan="1" colspan="1">79.2</td><td align="char" char="." rowspan="1" colspan="1">58.5</td></tr><tr><td align="center" rowspan="1" colspan="1">Kim [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>]</td><td align="char" char="." rowspan="1" colspan="1">15.2</td><td align="char" char="." rowspan="1" colspan="1">7.1</td><td align="char" char="." rowspan="1" colspan="1">8.3</td><td align="char" char="." rowspan="1" colspan="1">40.1</td><td align="char" char="." rowspan="1" colspan="1">37.2</td><td align="char" char="." rowspan="1" colspan="1">27.1</td><td align="char" char="." rowspan="1" colspan="1">50.7</td><td align="center" rowspan="1" colspan="1">53.9</td><td align="char" char="." rowspan="1" colspan="1">48.2</td></tr><tr><td align="center" rowspan="1" colspan="1">Drouard [<xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>]</td><td align="char" char="." rowspan="1" colspan="1">14.6</td><td align="char" char="." rowspan="1" colspan="1">6.5</td><td align="char" char="." rowspan="1" colspan="1">5.8</td><td align="char" char="." rowspan="1" colspan="1">41.5</td><td align="char" char="." rowspan="1" colspan="1">38.4</td><td align="char" char="." rowspan="1" colspan="1">24.0</td><td align="char" char="." rowspan="1" colspan="1">47.2</td><td align="center" rowspan="1" colspan="1">52.8</td><td align="char" char="." rowspan="1" colspan="1">31.1</td></tr><tr><td align="center" rowspan="1" colspan="1">Yang [<xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>]</td><td align="char" char="." rowspan="1" colspan="1">21.6</td><td align="char" char="." rowspan="1" colspan="1">16.9</td><td align="char" char="." rowspan="1" colspan="1">12.7</td><td align="char" char="." rowspan="1" colspan="1">56.3</td><td align="char" char="." rowspan="1" colspan="1">70.5</td><td align="char" char="." rowspan="1" colspan="1">63.8</td><td align="char" char="." rowspan="1" colspan="1">68.3</td><td align="center" rowspan="1" colspan="1">75</td><td align="char" char="." rowspan="1" colspan="1">63.1</td></tr><tr><td align="center" rowspan="1" colspan="1">Saeed [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>]</td><td align="char" char="." rowspan="1" colspan="1">18.7</td><td align="char" char="." rowspan="1" colspan="1">13.0</td><td align="char" char="." rowspan="1" colspan="1">15.9</td><td align="char" char="." rowspan="1" colspan="1">18.7</td><td align="char" char="." rowspan="1" colspan="1">13.0</td><td align="char" char="." rowspan="1" colspan="1">17.4</td><td align="char" char="." rowspan="1" colspan="1">60.8</td><td align="center" rowspan="1" colspan="1">71.6</td><td align="char" char="." rowspan="1" colspan="1">50.5</td></tr><tr><td align="center" rowspan="1" colspan="1">Proposed(OL)</td><td align="char" char="." rowspan="1" colspan="1">13.5</td><td align="char" char="." rowspan="1" colspan="1">6.1</td><td align="char" char="." rowspan="1" colspan="1">8.2</td><td align="char" char="." rowspan="1" colspan="1">36.6</td><td align="char" char="." rowspan="1" colspan="1">34.8</td><td align="char" char="." rowspan="1" colspan="1">27.5</td><td align="char" char="." rowspan="1" colspan="1">46.4</td><td align="center" rowspan="1" colspan="1">48.6</td><td align="char" char="." rowspan="1" colspan="1">32.9</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone.0180792.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0180792.t004</object-id><label>Table 4</label><caption><title>The classification errors (CE)%, the mean absolute errors (MAE) in degree of the proposed technique, and the standard deviation (STD) of the degrees in mixed inter-bases experiments.</title></caption><alternatives><graphic id="pone.0180792.t004g" xlink:href="pone.0180792.t004"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Database</th><th align="center" colspan="3" rowspan="1">Pointing04</th><th align="center" colspan="3" rowspan="1">AFLW</th><th align="center" colspan="3" rowspan="1">AFW&#x00026;300W</th></tr><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" rowspan="1" colspan="1">CE</th><th align="center" rowspan="1" colspan="1">MAE</th><th align="center" rowspan="1" colspan="1">STD</th><th align="center" rowspan="1" colspan="1">CE</th><th align="center" rowspan="1" colspan="1">MAE</th><th align="center" rowspan="1" colspan="1">STD</th><th align="center" rowspan="1" colspan="1">CE</th><th align="center" rowspan="1" colspan="1">MAE</th><th align="center" rowspan="1" colspan="1">STD</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">Ma [<xref rid="pone.0180792.ref021" ref-type="bibr">21</xref>]</td><td align="char" char="." rowspan="1" colspan="1">15.7</td><td align="char" char="." rowspan="1" colspan="1">7.3</td><td align="char" char="." rowspan="1" colspan="1">12.4</td><td align="char" char="." rowspan="1" colspan="1">27.3</td><td align="char" char="." rowspan="1" colspan="1">36.0</td><td align="char" char="." rowspan="1" colspan="1">25.6</td><td align="char" char="." rowspan="1" colspan="1">44.6</td><td align="char" char="." rowspan="1" colspan="1">52.3</td><td align="char" char="." rowspan="1" colspan="1">32.8</td></tr><tr><td align="center" rowspan="1" colspan="1">Kim [<xref rid="pone.0180792.ref022" ref-type="bibr">22</xref>]</td><td align="char" char="." rowspan="1" colspan="1">13.6</td><td align="char" char="." rowspan="1" colspan="1">5.8</td><td align="char" char="." rowspan="1" colspan="1">8.2</td><td align="char" char="." rowspan="1" colspan="1">19.5</td><td align="char" char="." rowspan="1" colspan="1">8.9</td><td align="char" char="." rowspan="1" colspan="1">13.7</td><td align="char" char="." rowspan="1" colspan="1">36.1</td><td align="char" char="." rowspan="1" colspan="1">38.0</td><td align="char" char="." rowspan="1" colspan="1">24.8</td></tr><tr><td align="center" rowspan="1" colspan="1">Drouard [<xref rid="pone.0180792.ref024" ref-type="bibr">24</xref>]</td><td align="char" char="." rowspan="1" colspan="1">13.2</td><td align="char" char="." rowspan="1" colspan="1">5.3</td><td align="char" char="." rowspan="1" colspan="1">7.4</td><td align="char" char="." rowspan="1" colspan="1">18.6</td><td align="char" char="." rowspan="1" colspan="1">8.3</td><td align="char" char="." rowspan="1" colspan="1">9.6</td><td align="char" char="." rowspan="1" colspan="1">38.2</td><td align="char" char="." rowspan="1" colspan="1">40.3</td><td align="char" char="." rowspan="1" colspan="1">29.5</td></tr><tr><td align="center" rowspan="1" colspan="1">Yang [<xref rid="pone.0180792.ref019" ref-type="bibr">19</xref>]</td><td align="char" char="." rowspan="1" colspan="1">16.2</td><td align="char" char="." rowspan="1" colspan="1">8.6</td><td align="char" char="." rowspan="1" colspan="1">10.3</td><td align="char" char="." rowspan="1" colspan="1">29.6</td><td align="char" char="." rowspan="1" colspan="1">37.4</td><td align="char" char="." rowspan="1" colspan="1">26.6</td><td align="char" char="." rowspan="1" colspan="1">51.9</td><td align="char" char="." rowspan="1" colspan="1">53.1</td><td align="char" char="." rowspan="1" colspan="1">38.7</td></tr><tr><td align="center" rowspan="1" colspan="1">Saeed [<xref rid="pone.0180792.ref013" ref-type="bibr">13</xref>]</td><td align="char" char="." rowspan="1" colspan="1">17.3</td><td align="char" char="." rowspan="1" colspan="1">10.8</td><td align="char" char="." rowspan="1" colspan="1">8.6</td><td align="char" char="." rowspan="1" colspan="1">28.5</td><td align="char" char="." rowspan="1" colspan="1">36.9</td><td align="char" char="." rowspan="1" colspan="1">29.5</td><td align="char" char="." rowspan="1" colspan="1">42.8</td><td align="char" char="." rowspan="1" colspan="1">50.6</td><td align="char" char="." rowspan="1" colspan="1">36.7</td></tr><tr><td align="center" rowspan="1" colspan="1">Proposed(OL)</td><td align="char" char="." rowspan="1" colspan="1">11.2</td><td align="char" char="." rowspan="1" colspan="1">2.8</td><td align="char" char="." rowspan="1" colspan="1">4.7</td><td align="char" char="." rowspan="1" colspan="1">16.0</td><td align="char" char="." rowspan="1" colspan="1">6.1</td><td align="char" char="." rowspan="1" colspan="1">7.8</td><td align="char" char="." rowspan="1" colspan="1">33.0</td><td align="char" char="." rowspan="1" colspan="1">35.6</td><td align="char" char="." rowspan="1" colspan="1">28.6</td></tr></tbody></table></alternatives></table-wrap></sec></sec></sec><sec id="sec015"><title>5 Conclusion</title><p>We proposed an efficient head pose estimation technique using random forest and texture analysis including gaussian pyramid, multi-scaled block LBP features. In the proposed technique a randomized tree with the feature parameters was trained to yield the improved accurate estimation performance. The features were used at each node for maximizing an information gain, and as a result, the distribution of a particular class of samples was compact in a leaf node. An efficient split function was also developed for each sample to efficiently traverse the tree. When making a decision, we use a Maximum-A-Posteriori criterion for determining the classes of the poses. In the experimental results, the proposed technique showed significantly improved classification performance in the head pose estimation in the various conditions of illumination and occlusions. In the future work, we plan to extend the key idea of the proposed technique to the deep learning framework.</p></sec></body><back><ack><p>This research was supported by a grant(16CTAP-C114986-01) from Technology Advancement Research Program (TARP) funded by Ministry of Land, Infrastructure and Transport of Korean government.</p></ack><ref-list><title>References</title><ref id="pone.0180792.ref001"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Cuturi</surname><given-names>LF</given-names></name>, <name><surname>MacNeilage</surname><given-names>PR</given-names></name>. <article-title>Systematic Biases in Human Heading Estimation</article-title>. <source>Plos One</source>. <year>2013</year>
<month>2</month>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/annotation/38466452-ba14-4357-b640-5582550bb3dd">10.1371/annotation/38466452-ba14-4357-b640-5582550bb3dd</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref002"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Ksander</surname><given-names>N</given-names></name>, <name><surname>Katliar</surname><given-names>M</given-names></name>, <name><surname>Bulthoff</surname><given-names>H</given-names></name>. <article-title>Forced Fusion in Multisensory Heading Estimation</article-title>. <source>Plos One</source>. <year>2015</year>
<month>5</month></mixed-citation></ref><ref id="pone.0180792.ref003"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Taigman</surname><given-names>Y</given-names></name>, <name><surname>Yang</surname><given-names>M</given-names></name>, <name><surname>Ranzato</surname><given-names>M</given-names></name>, <name><surname>Wolf</surname><given-names>L</given-names></name>. <article-title>Deepface: Closing the gap to human-level performance in face verification</article-title>. <source>Computer Vision and Pattern Recognition (CVPR)</source>. <year>2014</year>.</mixed-citation></ref><ref id="pone.0180792.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Guo</surname><given-names>G</given-names></name>, <name><surname>Dyer</surname><given-names>RC</given-names></name>. <article-title>Learning From Examples in the Small Sample Case:Face Expression Recognition</article-title>. <source>IEEE Transactions on Syst, Man, and Cybernetics-Part B: Cybernetcis</source>. <year>2005</year>
<month>6</month>;<volume>35</volume>(<issue>3</issue>):<fpage>477</fpage>&#x02013;<lpage>488</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TSMCB.2005.846658">10.1109/TSMCB.2005.846658</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Lee</surname><given-names>K</given-names></name>, <name><surname>Ghosh</surname><given-names>J</given-names></name>, <name><surname>Grauman</surname><given-names>K</given-names></name>. <article-title>Discovering Important People and Objects for Egocentric Video Summarization</article-title>. <source>Computer Vision and Pattern Recognition (CVPR)</source>
<year>2012</year>.</mixed-citation></ref><ref id="pone.0180792.ref006"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Valenti</surname><given-names>R</given-names></name>, <name><surname>Sebe</surname><given-names>N</given-names></name>, <name><surname>Gevers</surname><given-names>T</given-names></name>. <article-title>Combining head pose and eye location information for gaze estimation</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2012</year>
<month>2</month>;<volume>21</volume>(<issue>2</issue>):<fpage>802</fpage>&#x02013;<lpage>815</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TIP.2011.2162740">10.1109/TIP.2011.2162740</ext-link></comment>
<pub-id pub-id-type="pmid">21788191</pub-id></mixed-citation></ref><ref id="pone.0180792.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>Balasubramanian</surname><given-names>NV</given-names></name>, <name><surname>Ye</surname><given-names>J</given-names></name>, <name><surname>Panchanathan</surname><given-names>S</given-names></name>. <article-title>Biased manifold embedding: A framework for person-independent head pose estimation</article-title>. <source>Computer Vision and Pattern Recognition (CVPR)</source>
<year>2007</year>.</mixed-citation></ref><ref id="pone.0180792.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Vatahska</surname><given-names>T</given-names></name>, <name><surname>Bennewitz</surname><given-names>M</given-names></name>, <name><surname>Behnke</surname><given-names>S</given-names></name>. <article-title>Feature-based head pose estimation from images</article-title>. <source>Humanoids</source>
<year>2007</year>.</mixed-citation></ref><ref id="pone.0180792.ref009"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Matsumoto</surname><given-names>Y</given-names></name>, <name><surname>Zelinsky</surname><given-names>A</given-names></name>. <article-title>An algorithm for real-time stereo vision implementation of head pose and gaze direction measurement</article-title>. <source>Aut. Face and Gestures Rec</source>. <year>2000</year>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/AFGR.2000.840680">10.1109/AFGR.2000.840680</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref010"><label>10</label><mixed-citation publication-type="other">Guo G, Fu Y, Dyer CR, Huang ST. Head pose estimation: Classification or regression. Proc. 19th Int&#x02019;l Conf. Pattern Recognition 2008.</mixed-citation></ref><ref id="pone.0180792.ref011"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Fanelli</surname><given-names>G</given-names></name>, <name><surname>Gall</surname><given-names>J</given-names></name>, <name><surname>Gool</surname><given-names>VL</given-names></name>. <article-title>Real time head pose estimation with random regression forests</article-title>. <source>Computer Vision and Pattern Recognition (CVPR)</source>
<year>2011</year>.</mixed-citation></ref><ref id="pone.0180792.ref012"><label>12</label><mixed-citation publication-type="other">Niese R, Werner P, Al-Hamadi A. Accurate, Fast and Robust Realtime Face Pose Estimation using Kinect Camera. IEEE International Conference on Systems, Man, and Cybernetics 2013.</mixed-citation></ref><ref id="pone.0180792.ref013"><label>13</label><mixed-citation publication-type="other">Saeed A, Al-Hamadi A. Boosted Human Head Pose Estimation using Kinect Camera. International Conference on Image Processing (ICIP) 2015.</mixed-citation></ref><ref id="pone.0180792.ref014"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Erik</surname><given-names>M</given-names></name>, <name><surname>Trivedi</surname><given-names>MM</given-names></name>. <article-title>Head Pose Estimation in Computer Vision: A Survey</article-title>. <source>IEEE Transactions on Pateern Analysis and Machine Learning</source>. <year>2009</year>
<month>4</month>;<volume>31</volume>(<issue>4</issue>):<fpage>607</fpage>&#x02013;<lpage>625</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2008.106">10.1109/TPAMI.2008.106</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref015"><label>15</label><mixed-citation publication-type="other">Chen L, Zhang L, Hu Y, Li M, Zhang H. Head pose estimation using fisher manifold learning. Workshop on Analysis and Modeling of Faces and Gestures 2003.</mixed-citation></ref><ref id="pone.0180792.ref016"><label>16</label><mixed-citation publication-type="other">Peng X, Huang J, Hu Q, Zhang S, Metaxas DN. Three-dimensional head pose estimation in-the-wild. IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG) 2015.</mixed-citation></ref><ref id="pone.0180792.ref017"><label>17</label><mixed-citation publication-type="journal">
<name><surname>Cootes</surname><given-names>FT</given-names></name>, <name><surname>Edwards</surname><given-names>JG</given-names></name>, <name><surname>Taylor</surname><given-names>JC</given-names></name>. <article-title>Active appearance models</article-title>. <source>IEEE Transactions on Pateern Analysis and Machine Learning</source>. <year>2001</year>
<month>6</month>;<volume>23</volume>(<issue>6</issue>):<fpage>681</fpage>&#x02013;<lpage>685</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/34.927467">10.1109/34.927467</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref018"><label>18</label><mixed-citation publication-type="other">Storer M, Urschler M, Bischof H. 3D morphable appearance model for efficient fine head pose estimation from still images. Workshop on Subspace Methods 2009.</mixed-citation></ref><ref id="pone.0180792.ref019"><label>19</label><mixed-citation publication-type="other">Yang J, Liang W, Jia Y. Face Pose Estimation with Combined 2D and 3D HOG Features. International Conference on Pattern Recognition 2012.</mixed-citation></ref><ref id="pone.0180792.ref020"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Huang</surname><given-names>D</given-names></name>, <name><surname>Shan</surname><given-names>C</given-names></name>, <name><surname>Ardabilian</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>L</given-names></name>. <article-title>Local binary patterns and its application to facial image analysis: a survey</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics, Part C</source>. <year>2011</year>
<month>6</month>;<volume>41</volume>(<issue>6</issue>):<fpage>765</fpage>&#x02013;<lpage>781</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TSMCC.2011.2118750">10.1109/TSMCC.2011.2118750</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref021"><label>21</label><mixed-citation publication-type="other">Ma B, Zhang W, Shan S, Chen X, Gao W. Robust head pose estimation using LGBP. Proc. International Conference on Pattern Recognition 2006.</mixed-citation></ref><ref id="pone.0180792.ref022"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Kim</surname><given-names>H</given-names></name>, <name><surname>Lee</surname><given-names>S</given-names></name>, <name><surname>Sohn</surname><given-names>M</given-names></name>, <name><surname>Kim</surname><given-names>D</given-names></name>. <article-title>Illumination invariant head pose estimation using random forests classifier and binary pattern run length matrix</article-title>. <source>Human-centric Computing and Information Sciences</source>
<year>2014</year>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/s13673-014-0009-7">10.1186/s13673-014-0009-7</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>Moore</surname><given-names>S</given-names></name>, <name><surname>Bowden</surname><given-names>R</given-names></name>. <article-title>Local binary patterns for multi-view facial expression recognition</article-title>. <source>Computer Vision and Image Understanding</source>. <year>2011</year>
<month>4</month>;<volume>115</volume>(<issue>4</issue>):<fpage>541</fpage>&#x02013;<lpage>558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cviu.2010.12.001">10.1016/j.cviu.2010.12.001</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref024"><label>24</label><mixed-citation publication-type="other">Drouard V, Ba S, Evangelidis G, Deleforge A, Horaud R. Head Pose Estimation via Probabilistic High-Dimensional Regression. Internationan Conference on Image Processing (ICIP) 2015.</mixed-citation></ref><ref id="pone.0180792.ref025"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Han</surname><given-names>B</given-names></name>, <name><surname>Lee</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>SH</given-names></name>. <article-title>Head pose estimation using image abstraction and local directional quaternary patterns for multiclass classification</article-title>. <source>Pattern Recognition Letters</source>. <year>2014</year>
<month>8</month>;<volume>45</volume>(<issue>1</issue>):<fpage>145</fpage>&#x02013;<lpage>153</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.patrec.2014.03.017">10.1016/j.patrec.2014.03.017</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref026"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Ahn</surname><given-names>B</given-names></name>, <name><surname>Park</surname><given-names>J</given-names></name>, <name><surname>Kweon</surname><given-names>I</given-names></name>. <article-title>Real-time Head Orientation from a Monocular Camera using Deep Neural Network</article-title>. <source>ACCV</source>, <year>2014</year>.</mixed-citation></ref><ref id="pone.0180792.ref027"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Jain</surname><given-names>A</given-names></name>, <name><surname>Tompson</surname><given-names>J</given-names></name>, <name><surname>Andriluka</surname><given-names>M</given-names></name>. <article-title>Learning Human Pose Estimation Features with Convolutional Networks</article-title>. <source>Computer Vision and Pattern Recognition (CVPR)</source>
<year>2014</year>.</mixed-citation></ref><ref id="pone.0180792.ref028"><label>28</label><mixed-citation publication-type="journal">
<name><surname>Breiman</surname><given-names>L</given-names></name>. <article-title>Random Forests</article-title>. <source>Machine Learning</source>. <year>2001</year>
<month>10</month>;<volume>45</volume>(<issue>1</issue>):<fpage>5</fpage>&#x02013;<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1010933404324">10.1023/A:1010933404324</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref029"><label>29</label><mixed-citation publication-type="other">Li Y, Wang S, Ding X. Person-independent head pose estimation based on random forest regression. International Conference on Image Processing (ICIP) 2010.</mixed-citation></ref><ref id="pone.0180792.ref030"><label>30</label><mixed-citation publication-type="journal">
<name><surname>Min</surname><given-names>S</given-names></name>, <name><surname>Kohli</surname><given-names>P</given-names></name>, <name><surname>Shotton</surname><given-names>J</given-names></name>. <article-title>Conditional regression forests for human pose estimation</article-title>. <source>Computer Vision and Pattern Recognition (CVPR)</source>
<year>2012</year>.</mixed-citation></ref><ref id="pone.0180792.ref031"><label>31</label><mixed-citation publication-type="other">Huang C, Ding X, Fang C. Head pose estimation based on random forests for multiclass classification. International Conference on Pattern Recognition 2010.</mixed-citation></ref><ref id="pone.0180792.ref032"><label>32</label><mixed-citation publication-type="other">Lee D, Yang M, Oh S. Fast and Accurate Head Pose Estimation via Random Projection Forests. IEEE International Conference on Computer Vision (ICCV) 2015.</mixed-citation></ref><ref id="pone.0180792.ref033"><label>33</label><mixed-citation publication-type="journal">
<name><surname>Valle</surname><given-names>R</given-names></name>, <name><surname>Buenaposada</surname><given-names>JM</given-names></name>, <name><surname>Valdes</surname><given-names>A</given-names></name>, <name><surname>Baumela</surname><given-names>L</given-names></name>. <article-title>Head-Pose Estimation In-the-Wild Using a Random Forest</article-title>. <source>AMDO</source>
<year>2016</year>.</mixed-citation></ref><ref id="pone.0180792.ref034"><label>34</label><mixed-citation publication-type="journal">
<name><surname>Ojala</surname><given-names>T</given-names></name>, <name><surname>Pietikainen</surname><given-names>M</given-names></name>, <name><surname>Harwood</surname><given-names>D</given-names></name>. <article-title>A Comparative Study of Texture Measures with Classification Based on Feature Distributions</article-title>. <source>Pattern Recognition</source>. <year>1996</year>
<month>1</month>;<volume>29</volume>(<issue>1</issue>):<fpage>51</fpage>&#x02013;<lpage>59</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0031-3203(95)00067-4">10.1016/0031-3203(95)00067-4</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref035"><label>35</label><mixed-citation publication-type="journal">
<name><surname>Ahonen</surname><given-names>T</given-names></name>, <name><surname>Hadid</surname><given-names>A</given-names></name>, <name><surname>Pietikainen</surname><given-names>M</given-names></name>. <article-title>Face Description with Local Binary Patterns: Application to Face Recognition</article-title>. <source>IEEE Transaction on Pattern Analysis and Machine Intelligence</source>. <year>2006</year>
<month>12</month>;<volume>28</volume>(<issue>12</issue>):<fpage>2037</fpage>&#x02013;<lpage>2041</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2006.244">10.1109/TPAMI.2006.244</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref036"><label>36</label><mixed-citation publication-type="journal">
<name><surname>Ojala</surname><given-names>T</given-names></name>, <name><surname>Pietikainen</surname><given-names>M</given-names></name>, <name><surname>Maenpaa</surname><given-names>T</given-names></name>. <article-title>Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns</article-title>. <source>IEEE Trans. Pattern Analysis and Machine Intelligence</source>. <year>2002</year>
<month>8</month>;<volume>24</volume>(<issue>7</issue>):<fpage>971</fpage>&#x02013;<lpage>987</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2002.1017623">10.1109/TPAMI.2002.1017623</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref037"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Heisele</surname><given-names>B</given-names></name>, <name><surname>Ho</surname><given-names>P</given-names></name>, <name><surname>Wu</surname><given-names>J</given-names></name>, <name><surname>Poggio</surname><given-names>T</given-names></name>. <article-title>Face Recognition: Component Based versus Global Approaches</article-title>. <source>Compter Vision and Image Understanding</source>. <year>2003</year>
<month>8</month>;<volume>91</volume>(<issue>1</issue>):<fpage>6</fpage>&#x02013;<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1077-3142(03)00073-0">10.1016/S1077-3142(03)00073-0</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref038"><label>38</label><mixed-citation publication-type="journal">
<name><surname>Gottumukkal</surname><given-names>R</given-names></name>, <name><surname>Asari</surname><given-names>VK</given-names></name>. <article-title>An Improved Face Recognition Technique Based on Modular PCA Approach</article-title>. <source>Pattern Recognition Letters</source>. <year>2004</year>
<month>3</month>;<volume>25</volume>(<issue>3</issue>):<fpage>429</fpage>&#x02013;<lpage>436</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.patrec.2003.11.005">10.1016/j.patrec.2003.11.005</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref039"><label>39</label><mixed-citation publication-type="journal">
<name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>See</surname><given-names>J</given-names></name>, <name><surname>Phan</surname><given-names>CR</given-names></name>, <name><surname>Oh</surname><given-names>YH</given-names></name>. <article-title>Efficient Spatio-Temporal Local Binary Patterns for Spontaneous Facial Micro-Expression Recognition</article-title>. <source>Plos One</source>
<year>2015</year>
<month>5</month>.</mixed-citation></ref><ref id="pone.0180792.ref040"><label>40</label><mixed-citation publication-type="journal">
<name><surname>Ming</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>G</given-names></name>, <name><surname>Fan</surname><given-names>C</given-names></name>. <article-title>Uniform Local Binary Pattern Based Texture-Edge Feature for 3D Human Behavior Recognition</article-title>. <source>Plos One</source>
<year>2015</year>
<month>5</month>.</mixed-citation></ref><ref id="pone.0180792.ref041"><label>41</label><mixed-citation publication-type="journal">
<name><surname>Gross</surname><given-names>R.</given-names></name>, <name><surname>Mattews</surname><given-names>I.</given-names></name>, <name><surname>Kanade</surname><given-names>J.</given-names></name>, and <name><surname>Baker</surname><given-names>S.</given-names></name>. <article-title>Multi-PIE</article-title>. <source>Image and Vision Computing</source>. <year>2010</year>
<month>5</month>; <volume>28</volume>(<issue>5</issue>): <fpage>807</fpage>&#x02013;<lpage>813</lpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.imavis.2009.08.002">10.1016/j.imavis.2009.08.002</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref042"><label>42</label><mixed-citation publication-type="other">Koestinger M, Wohlhart P, Roth MP, Bischof H. Annotated Facial Landmarks in the Wild: A Large-scale, Real-world Database for Facial Landmark Localization. IEEE International Workshop on Benchmarking Facial Image Analysis Technologies 2011.</mixed-citation></ref><ref id="pone.0180792.ref043"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Zhu</surname><given-names>X</given-names></name>, <name><surname>Ramanan</surname><given-names>D</given-names></name>. <article-title>Face detection, pose estimation and landmark localization in the wild</article-title>. <source>Computer Vision and Pattern Recognition (CVPR)</source>
<year>2012</year>.</mixed-citation></ref><ref id="pone.0180792.ref044"><label>44</label><mixed-citation publication-type="book">
<name><surname>Sagonas</surname><given-names>C</given-names></name>, <name><surname>Antonakos</surname><given-names>E</given-names></name>, <name><surname>Tzimiropoulos</surname><given-names>G</given-names></name>, <name><surname>Zafeiriou</surname><given-names>S</given-names></name>, <name><surname>Pantic</surname><given-names>M</given-names></name>. <chapter-title>300 faces In-the-wild challenge: Database and results</chapter-title>
<source>Image and Vision Computing (IMAVIS), Special Issue on Facial Landmark Localisation In-The-Wild</source>. <year>2016</year>.</mixed-citation></ref><ref id="pone.0180792.ref045"><label>45</label><mixed-citation publication-type="other">Gourier N, Hall D, Crowley LJ. Estimating face orientation from robust detection of salient facial features. International Workshop on Visual Observation of Deictic Gestures 2004.</mixed-citation></ref><ref id="pone.0180792.ref046"><label>46</label><mixed-citation publication-type="journal">
<name><surname>Tan</surname><given-names>X</given-names></name>, <name><surname>Triggs</surname><given-names>B</given-names></name>. <article-title>Enhanced Local Texture Feature Sets for Face Recognition Under Difficult Lighting Conditions</article-title>. <source>IEEE Trans. Image Processing</source>. <year>2010</year>
<month>5</month>; <volume>19</volume>(<issue>6</issue>):<fpage>1635</fpage>&#x02013;<lpage>1650</lpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TIP.2010.2042645">10.1109/TIP.2010.2042645</ext-link></comment></mixed-citation></ref><ref id="pone.0180792.ref047"><label>47</label><mixed-citation publication-type="journal">
<name><surname>Haj</surname><given-names>M</given-names></name>, <name><surname>Gonzalez</surname><given-names>J</given-names></name>, <name><surname>Davis</surname><given-names>L</given-names></name>. <article-title>On partial least squares in head pose estimation: How to simultaneously deal with misalignment</article-title>. <source>CVPR</source>
<year>2012</year>.</mixed-citation></ref><ref id="pone.0180792.ref048"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Taigman</surname><given-names>Y</given-names></name>, <name><surname>Wolf</surname><given-names>L</given-names></name>, <name><surname>Hassner</surname><given-names>T</given-names></name>. <article-title>On partial least squares in head pose estimation: How to simultaneously deal with misalignment</article-title>. <source>CVPR</source>
<year>2012</year></mixed-citation></ref><ref id="pone.0180792.ref049"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Ai</surname><given-names>H</given-names></name>, <name><surname>Huang</surname><given-names>G</given-names></name>. <article-title>A two-stage approach to automatic face alignment</article-title>. <source>SPIE Proceeding</source>
<year>2003</year>.</mixed-citation></ref><ref id="pone.0180792.ref050"><label>50</label><mixed-citation publication-type="journal">
<name><surname>Guyon</surname><given-names>I</given-names></name>, <name><surname>Saffari</surname><given-names>A</given-names></name>, <name><surname>Dror</surname><given-names>G</given-names></name>, <name><surname>Cawley</surname><given-names>G</given-names></name>. <article-title>Model selection: beyond the bayesian and frequentist divide</article-title>. <source>Journal of Machine Learning Research</source>. <year>2010</year>
<month>1</month>; <volume>11</volume>(<issue>1</issue>):<fpage>61</fpage>&#x02013;<lpage>87</lpage></mixed-citation></ref><ref id="pone.0180792.ref051"><label>51</label><mixed-citation publication-type="journal">
<name><surname>Cheng</surname><given-names>Q</given-names></name>, <name><surname>Zhou</surname><given-names>H</given-names></name>, <name><surname>Cheng</surname><given-names>J</given-names></name>. <article-title>The Fisher-Markov Selector: Fast Selecting Maximally Separable Feature Subset for Multiclass Classification with Applications to High-Dimensional Data</article-title>. <source>IEEE Trans. Pattern Analysis and Machine Intelligence</source>. <year>2011</year>
<month>6</month>; <volume>33</volume>(<issue>6</issue>):<fpage>1217</fpage>&#x02013;<lpage>1233</lpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2010.195">10.1109/TPAMI.2010.195</ext-link></comment></mixed-citation></ref></ref-list></back></article>
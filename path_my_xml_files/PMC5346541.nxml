<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28348542</article-id><article-id pub-id-type="pmc">5346541</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2017.00368</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>The Efficacy of Short-term Gated Audiovisual Speech Training for Improving Auditory Sentence Identification in Noise in Elderly Hearing Aid Users</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Moradi</surname><given-names>Shahram</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/73987/overview"/></contrib><contrib contrib-type="author"><name><surname>Wahlin</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>H&#x000e4;llgren</surname><given-names>Mathias</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/166921/overview"/></contrib><contrib contrib-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>Jerker</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/75267/overview"/></contrib><contrib contrib-type="author"><name><surname>Lidestam</surname><given-names>Bj&#x000f6;rn</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/96987/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Linnaeus Centre HEAD, Department of Behavioral Sciences and Learning, Link&#x000f6;ping University,</institution><country>Link&#x000f6;ping, Sweden</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Otorhinolaryngology and Department of Clinical and Experimental Medicine, Link&#x000f6;ping University,</institution><country>Link&#x000f6;ping, Sweden</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Behavioral Sciences and Learning, Link&#x000f6;ping University,</institution><country>Link&#x000f6;ping, Sweden</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: <italic>Claude Alain, Rotman Research Institute, Canada</italic></p></fn><fn fn-type="edited-by"><p>Reviewed by: <italic>Samira Anderson, University of Maryland, College Park, USA; Christian F&#x000fc;llgrabe, MRC Institute of Hearing Research, UK</italic></p></fn><corresp id="fn001">*Correspondence: <italic>Shahram Moradi, <email xlink:type="simple">shahram.moradi@liu.se</email></italic></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Auditory Cognitive Neuroscience, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>13</day><month>3</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>8</volume><elocation-id>368</elocation-id><history><date date-type="received"><day>16</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>27</day><month>2</month><year>2017</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2017 Moradi, Wahlin, H&#x000e4;llgren, R&#x000f6;nnberg and Lidestam.</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Moradi, Wahlin, H&#x000e4;llgren, R&#x000f6;nnberg and Lidestam</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>This study aimed to examine the efficacy and maintenance of short-term (one-session) gated audiovisual speech training for improving auditory sentence identification in noise in experienced elderly hearing-aid users. Twenty-five hearing aid users (16 men and 9 women), with an average age of 70.8 years, were randomly divided into an experimental (audiovisual training, <italic>n</italic> = 14) and a control (auditory training, <italic>n</italic> = 11) group. Participants underwent gated speech identification tasks comprising Swedish consonants and words presented at 65 dB sound pressure level with a 0 dB signal-to-noise ratio (steady-state broadband noise), in audiovisual or auditory-only training conditions. The Hearing-in-Noise Test was employed to measure participants&#x02019; auditory sentence identification in noise before the training (pre-test), promptly after training (post-test), and 1 month after training (one-month follow-up). The results showed that audiovisual training improved auditory sentence identification in noise promptly after the training (post-test vs. pre-test scores); furthermore, this improvement was maintained 1 month after the training (one-month follow-up vs. pre-test scores). Such improvement was not observed in the control group, neither promptly after the training nor at the one-month follow-up. However, no significant between-groups difference nor an interaction between groups and session was observed. Conclusion: Audiovisual training may be considered in aural rehabilitation of hearing aid users to improve listening capabilities in noisy conditions. However, the lack of a significant between-groups effect (audiovisual vs. auditory) or an interaction between group and session calls for further research.</p></abstract><kwd-group><kwd>audiovisual training</kwd><kwd>auditory training</kwd><kwd>hearing aid</kwd><kwd>speech-in-noise identification</kwd></kwd-group><counts><fig-count count="2"/><table-count count="2"/><equation-count count="0"/><ref-count count="54"/><page-count count="10"/><word-count count="0"/></counts></article-meta></front><body><sec><title>Introduction</title><p>Age-related hearing loss (Presbycusis) is one of the most common disorders in elderly people, and hearing loss prevalence is growing because of an aging population (<xref rid="B24" ref-type="bibr">Lin et al., 2011</xref>; <xref rid="B52" ref-type="bibr">World Health Organization, 2012</xref>). The most evident negative consequence of hearing loss is difficulty in perceiving speech, especially in adverse listening conditions (e.g., <xref rid="B30" ref-type="bibr">Needleman and Crandell, 1995</xref>). This can lead to other difficulties such as social isolation, particularly in women (<xref rid="B25" ref-type="bibr">Mick et al., 2014</xref>); mental health problems (e.g., depression, anxiety; <xref rid="B20" ref-type="bibr">Li et al., 2014</xref>; <xref rid="B17" ref-type="bibr">Keidser et al., 2015</xref>); spouse relationship difficulties (<xref rid="B39" ref-type="bibr">Scarinci et al., 2012</xref>); and reduction in quality of life (<xref rid="B7" ref-type="bibr">Dalton et al., 2003</xref>).</p><p>Currently, the most common method to compensate for the speech perception difficulties of sensorineural hearing-impaired listeners is to prescribe hearing aids. Despite the use of advanced digital hearing aids, our previous findings have shown that elderly hearing aid users have inferior performance compared to their age-matched counterparts with normal hearing in perceiving speech stimuli when a prior semantic context is lacking (<xref rid="B26" ref-type="bibr">Moradi et al., 2014</xref>, <xref rid="B28" ref-type="bibr">2016</xref>). In addition, independent studies have shown that the amplification of sounds alone cannot fully restore difficulties in the auditory perception of speech stimuli in people with hearing loss (<xref rid="B9" ref-type="bibr">Dimitrijevic et al., 2004</xref>; <xref rid="B2" ref-type="bibr">Ahlstrom et al., 2014</xref>; Moradi et al., unpublished). As a consequence, people with hearing loss need other methods of rehabilitation, in addition to hearing aids, to compensate more fully for their difficulties in perceiving speech stimuli.</p><p>Auditory training has been reported to be an effective method in people with hearing loss to improve phoneme recognition (e.g., <xref rid="B46" ref-type="bibr">Stecker et al., 2006</xref>; <xref rid="B10" ref-type="bibr">Ferguson et al., 2014</xref>) and word recognition (e.g., <xref rid="B5" ref-type="bibr">Burk et al., 2006</xref>). In sentence recognition, however, the current research with regards to the effect of auditory training on subsequent sentence identification in noise is inconclusive. For instance, <xref rid="B19" ref-type="bibr">Levitt et al. (2011)</xref> and <xref rid="B33" ref-type="bibr">Rao et al. (2017)</xref>, both using ReadMyQuips auditory training program, and <xref rid="B47" ref-type="bibr">Sweetow and Sabes (2006)</xref> and <xref rid="B31" ref-type="bibr">Olson et al. (2013)</xref>, using Listening and Communication Enhancement auditory training program, demonstrated the efficiency of auditory training on auditory sentence identification in noise, in hearing-aid users. However, <xref rid="B3" ref-type="bibr">Bock and Abrams (2013)</xref> and <xref rid="B1" ref-type="bibr">Abrams et al. (2015)</xref> reported no effects of auditory training on sentence identification in noise in hearing aid users.</p><p><xref rid="B27" ref-type="bibr">Moradi et al. (2013)</xref>, using a gating paradigm, studied the extent to which the addition of visual cues to an auditory speech signal facilitates the identification of speech stimuli in both silence and noise in young normal-hearing listeners. In the gating paradigm, participants are presented with successive fragments of a given speech stimulus (e.g., a word) and their task is to suggest a word that can be a continuation to that presented fragments (<xref rid="B14" ref-type="bibr">Grosjean, 1980</xref>). The main purpose of the gating paradigm is to estimate the isolation point, which is the shortest time from the onset of a speech token that is required for correct identification. A secondary finding by <xref rid="B27" ref-type="bibr">Moradi et al. (2013)</xref> was that the participants who were first exposed to gated audiovisual speech stimuli (consonants, words, and words embedded within sentences, presented in both silence and noise) subsequently had better performance in a Swedish version of HINT (<xref rid="B15" ref-type="bibr">H&#x000e4;llgren et al., 2006</xref>) than those who were first exposed to those speech stimuli, but in an auditory-only modality. In order to investigate whether the secondary finding by <xref rid="B27" ref-type="bibr">Moradi et al. (2013)</xref> was a genuine effect, <xref rid="B23" ref-type="bibr">Lidestam et al. (2014)</xref> conducted a randomized control group study. Young participants with normal hearing were randomly divided into three training groups: gated audiovisual speech training group, gated auditory speech training group, and a control group that simply watched a movie clip. The auditory and audiovisual speech training tasks consisted of identifying gated consonants and words presented audiovisually or aurally. Their results replicated the finding by <xref rid="B27" ref-type="bibr">Moradi et al. (2013)</xref> by showing that only the participants in the audiovisual training group experienced an improvement in HINT performance, while the participants in the auditory-only training and control groups did not. Interestingly, the findings of these two studies showed that the effect of audiovisual training on improving sentence comprehension in noise was independent of the idiosyncrasies of the talker&#x02019;s articulation, as the talkers in the training and the HINT were not the same. This finding differs from cross-modal studies which have shown that familiarity with the talkers is a key factor in subsequent improvement in auditory speech processing (<xref rid="B38" ref-type="bibr">Rosenblum et al., 2007</xref>; <xref rid="B49" ref-type="bibr">von Kriegstein et al., 2008</xref>; <xref rid="B53" ref-type="bibr">Wu et al., 2013</xref>). Together, it seems that providing audiovisual training materials in a gating format is an efficient short-term approach in improving auditory speech-in-noise identification. This is most likely due to gated audiovisual speech stimulation tunes the attentional resources with richer speech input that subsequently reinforces the auditory map to phonological and lexical representations in the mental lexicon.</p><p>The present study is an extension of the study by <xref rid="B23" ref-type="bibr">Lidestam et al. (2014)</xref>, which focuses on experienced hearing aid users. To do this, we measured the HINT scores of elderly hearing-aid users before the training (pre-test scores), immediately after the training (post-test scores), and 1 month after the training (one-month follow-up scores).</p><p>In the present study, we predict that the gated audiovisual speech training results in better performance of HINT in terms of SNR in elderly hearing aid users immediately after the training (post-test vs. pre-test comparison) and that this audiovisual training effect might be maintained in the follow-up measurement (one-month follow-up vs. pre-test comparison). We also predict no improvement from one session of gated auditory training on HINT in elderly hearing aid users either immediately after the training (post-test vs. pre-test comparison) or 1 month after the training (one-month follow-up vs. pre-test comparison). These predictions are based on our prior studies in normal hearing listeners which showed that one session of gated auditory training did not improve subsequent HINT performance, but gated audiovisual training did.</p></sec><sec sec-type="materials|methods" id="s1"><title>Materials and Methods</title><sec><title>Participants</title><p>Twenty-five native Swedish speakers (16 men and 9 women) with a symmetrical bilateral mild to moderate hearing loss consented to participate in this study. They were recruited from an audiology clinic patient list at Link&#x000f6;ping University Hospital, Sweden. Their ages at the time of testing ranged from 63 to 75 years (<italic>M</italic> = 70.8, <italic>SD</italic> = 2.9 years). They had been hearing aids users for at least 6 months. The participants were randomized into experimental (audiovisual training, <italic>n</italic> = 14, mean age = 71.8 years, 10 men and 4 women) and control (auditory training, <italic>n</italic> = 11, mean age = 69.5 years, 6 men and 5 women) groups by coin flipping (hence the unequal group sizes).</p><p>The participants wore various in-the-ear (ITE), behind-the-ear (BTE), completely-in-the-canal (CIC), and receiver-in-the-ear (RITE) digital hearing aids. The hearing aids were fitted for them in their most recent visit at Link&#x000f6;ping University Hospital, based on each listener&#x02019;s individual needs, by licensed audiologists who were independent of the present study. All of these hearing aids used non-linear processing and had been fitted according to manufacturer instructions. Participants wore their own hearing aids both in the training condition and in the HINT condition, with no change in the amplification setting of their own hearing aids throughout the study. <bold>Table <xref ref-type="table" rid="T1">1</xref></bold> shows the information about age ranges, type and brand of hearing aid, time since first hearing aid fitting, and the pure-tune average (PTA) thresholds across seven frequencies (125, 250, 500, 1000, 2000, 4000, and 8000 Hz) for the left and right ears for each participant in the experimental and control groups.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Age ranges, time since first hearing fitting, PTA7 for each ear, and the type and brand of hearing aid of each participant in experimental and control groups.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Group</th><th valign="top" align="center" rowspan="1" colspan="1">Participant</th><th valign="top" align="center" rowspan="1" colspan="1">Age ranges (years)</th><th valign="top" align="center" rowspan="1" colspan="1">Time since first hearing aid fitting (years)</th><th valign="top" align="center" rowspan="1" colspan="1">PTA7 right ear (dB HL)</th><th valign="top" align="center" rowspan="1" colspan="1">PTA7 left ear (dB HL)</th><th valign="top" align="left" rowspan="1" colspan="1">Type and brand of hearing aid</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Experimental group (Audiovisual speech training)</td><td valign="top" align="center" rowspan="1" colspan="1">E1</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">9.6</td><td valign="top" align="center" rowspan="1" colspan="1">40</td><td valign="top" align="center" rowspan="1" colspan="1">35.7</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak, Ambra M H20</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E2</td><td valign="top" align="center" rowspan="1" colspan="1">65&#x02013;70</td><td valign="top" align="center" rowspan="1" colspan="1">3.9</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">50</td><td valign="top" align="left" rowspan="1" colspan="1">CIC, Widex, Mind 440 M4</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E3</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">9.8</td><td valign="top" align="center" rowspan="1" colspan="1">33.6</td><td valign="top" align="center" rowspan="1" colspan="1">37.9</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Oticon, K140 13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E4</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">1.8</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">31.4</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak Exelia Art Micro</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E5</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">0.8</td><td valign="top" align="center" rowspan="1" colspan="1">39.3</td><td valign="top" align="center" rowspan="1" colspan="1">33.6</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak, Audeo S Smart IX</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E6</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">1.2</td><td valign="top" align="center" rowspan="1" colspan="1">37.9</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak, Ambra M H20</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E7</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">1.7</td><td valign="top" align="center" rowspan="1" colspan="1">59.3</td><td valign="top" align="center" rowspan="1" colspan="1">51.4</td><td valign="top" align="left" rowspan="1" colspan="1">ITE, Oticon, K220</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E8</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">5.8</td><td valign="top" align="center" rowspan="1" colspan="1">45.7</td><td valign="top" align="center" rowspan="1" colspan="1">48.6</td><td valign="top" align="left" rowspan="1" colspan="1">RITE, Oticon, Vigo Pro</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E9</td><td valign="top" align="center" rowspan="1" colspan="1">65&#x02013;70</td><td valign="top" align="center" rowspan="1" colspan="1">3.8</td><td valign="top" align="center" rowspan="1" colspan="1">45.7</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="left" rowspan="1" colspan="1">ITE, Beltone, True9 35</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E10</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">4.3</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Widex, Dream D4-XP</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E11</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">7.8</td><td valign="top" align="center" rowspan="1" colspan="1">53.6</td><td valign="top" align="center" rowspan="1" colspan="1">57.1</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Oticon, Vigo Pro Power</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E12</td><td valign="top" align="center" rowspan="1" colspan="1">65&#x02013;70</td><td valign="top" align="center" rowspan="1" colspan="1">0.9</td><td valign="top" align="center" rowspan="1" colspan="1">45.7</td><td valign="top" align="center" rowspan="1" colspan="1">46.4</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Widex, Clear C4-9</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E13</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">7.7</td><td valign="top" align="center" rowspan="1" colspan="1">50</td><td valign="top" align="center" rowspan="1" colspan="1">51.4</td><td valign="top" align="left" rowspan="1" colspan="1">ITE, Oticon, K220</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">E14</td><td valign="top" align="center" rowspan="1" colspan="1">65&#x02013;70</td><td valign="top" align="center" rowspan="1" colspan="1">3.5</td><td valign="top" align="center" rowspan="1" colspan="1">43.4</td><td valign="top" align="center" rowspan="1" colspan="1">49.2</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak Exelia Art Micro</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Control Group (Auditory speech training)</td><td valign="top" align="center" rowspan="1" colspan="1">C1</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">7.1</td><td valign="top" align="center" rowspan="1" colspan="1">50.7</td><td valign="top" align="center" rowspan="1" colspan="1">42.9</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Oticon, K220 13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C2</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">1.4</td><td valign="top" align="center" rowspan="1" colspan="1">28.6</td><td valign="top" align="center" rowspan="1" colspan="1">35</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Oticon, EPOQ X W 13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C3</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">2.5</td><td valign="top" align="center" rowspan="1" colspan="1">32.1</td><td valign="top" align="center" rowspan="1" colspan="1">33.6</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak, Versata Art Mico</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C4</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">1.2</td><td valign="top" align="center" rowspan="1" colspan="1">39.3</td><td valign="top" align="center" rowspan="1" colspan="1">37.9</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak, Ambra M H20</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C5</td><td valign="top" align="center" rowspan="1" colspan="1">65&#x02013;70</td><td valign="top" align="center" rowspan="1" colspan="1">3.3</td><td valign="top" align="center" rowspan="1" colspan="1">32.1</td><td valign="top" align="center" rowspan="1" colspan="1">34.3</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak Exelia Art Micro</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C6</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">10.8</td><td valign="top" align="center" rowspan="1" colspan="1">42.1</td><td valign="top" align="center" rowspan="1" colspan="1">40.7</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak, Ambra Micro</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C7</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">1.1</td><td valign="top" align="center" rowspan="1" colspan="1">42.1</td><td valign="top" align="center" rowspan="1" colspan="1">40</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Resound, Alera9 AL962-DVIRW</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C8</td><td valign="top" align="center" rowspan="1" colspan="1">60&#x02013;65</td><td valign="top" align="center" rowspan="1" colspan="1">0.6</td><td valign="top" align="center" rowspan="1" colspan="1">37.9</td><td valign="top" align="center" rowspan="1" colspan="1">41.4</td><td valign="top" align="left" rowspan="1" colspan="1">RITE, Oticon, K220 mini</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C9</td><td valign="top" align="center" rowspan="1" colspan="1">65&#x02013;70</td><td valign="top" align="center" rowspan="1" colspan="1">18.4</td><td valign="top" align="center" rowspan="1" colspan="1">57.1</td><td valign="top" align="center" rowspan="1" colspan="1">58.6</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Oticon, K220 13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C10</td><td valign="top" align="center" rowspan="1" colspan="1">65&#x02013;70</td><td valign="top" align="center" rowspan="1" colspan="1">1.9</td><td valign="top" align="center" rowspan="1" colspan="1">37.9</td><td valign="top" align="center" rowspan="1" colspan="1">44.3</td><td valign="top" align="left" rowspan="1" colspan="1">BTE, Phonak Exelia Art Micro</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">C11</td><td valign="top" align="center" rowspan="1" colspan="1">70&#x02013;75</td><td valign="top" align="center" rowspan="1" colspan="1">1.0</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">53.6</td><td valign="top" align="center" rowspan="1" colspan="1">BTE, Phonak Exelia Art Micro</td></tr></tbody></table></table-wrap><p>The participants reported themselves to be in good health and had normal or corrected-to-normal vision with glasses.</p><p>The study inclusion criteria were as follows: (1) age over 50 years but less than 80 years; (2) Swedish as the native language; and (3) bilateral hearing loss with an average threshold of more than 35 dB HL for pure tone frequencies of 500, 1000, 2000, and 4000 Hz.</p><p>All participants were fully informed about the study and gave written consent for their participation in this study. In addition, they were paid (500 SEK) in return for their participation in this study. The Link&#x000f6;ping regional ethical review board approved the study, including the informational materials and consent procedure.</p></sec><sec><title>Speech Identification Training Conditions</title><p>The speech identification tasks comprised the same speech stimuli that were used by <xref rid="B23" ref-type="bibr">Lidestam et al. (2014)</xref>. Participants were presented with gated Swedish consonants and words for both the audiovisual (experimental) and auditory-only (control) training conditions. <bold>Figure <xref ref-type="fig" rid="F1">1</xref></bold> represents an example of the gated audiovisual consonant identification task.</p><fig id="F1" position="float"><label>FIGURE 1</label><caption><p><bold>An illustration of gated presentation of speech stimuli.</bold> Reprinted from <xref rid="B28" ref-type="bibr">Moradi et al. (2016)</xref>. Copyright 2016 by Sage Publications Inc. Adapted with permission.</p></caption><graphic xlink:href="fpsyg-08-00368-g001"/></fig><p>In the audiovisual speech presentation, the talker&#x02019;s hair, face, and shoulders were shown. The average overall sound pressure level (SPL) for the presentation of gated speech stimuli was set at 65 dB SPL, at a SNR of 0 dB, in both the auditory and audiovisual training conditions. This was measured in the vicinity of the participant&#x02019;s head with a Larson Davis System 824 sound level meter (Provo, UT, USA) in a free field. The background noise was a steady-state broadband noise, from <xref rid="B15" ref-type="bibr">H&#x000e4;llgren et al. (2006)</xref>, which was resampled and spectrally matched to the speech stimuli used in the present study. The onset and offset of background noise were simultaneous to the onset and offset of speech stimuli. The participants in the auditory training group received the same exact speech stimuli as the participants in audiovisual training group, but without visual input. Matlab (2010b) was used to associate the background noise to the auditory speech stimuli in a gating format at 0 dB SNR. To present speech stimuli in an audiovisual modality, Matlab and Psychophysics Toolbox (<xref rid="B4" ref-type="bibr">Brainard, 1997</xref>; <xref rid="B32" ref-type="bibr">Pelli, 1997</xref>; <xref rid="B18" ref-type="bibr">Kleiner et al., 2007</xref>) were used to synchronize auditory and visual speech stimuli. Detailed information about the synchronization of video and audio speech signal and the Matlab scripts used to gate them is available in <xref rid="B22" ref-type="bibr">Lidestam (2014)</xref>.</p></sec><sec><title>Consonants</title><p>Eighteen Swedish consonants were presented to participants in a vowel-consonant-vowel syllable format (/aba, ada, afa, aga, aja, aha, aka, ala, ama, ana, a&#x0014b;a, apa, ara, a&#x00288;a, asa, a<italic>&#x0222b;</italic>a, ata, and ava/) in the audiovisual and auditory-only training conditions. The gate size for consonants was set at 16.67 ms. Gating started after the first vowel (/a/) onset and at the beginning of the consonant onset. Hence, the first gate included the vowel /a/ plus the initial 16.67 ms of the consonant, the second gate provided an additional 16.67 ms of the consonant (a total of 33.33 ms), and so on. The consonant gating task took 10&#x02013;15 min to complete.</p></sec><sec><title>Words</title><p>Twenty-three Swedish monosyllabic words were presented to participants in a consonant-vowel-consonant (CVC) format (all nouns) in the audiovisual and auditory-only training conditions. The selected words had average to high frequencies according to the Swedish language corpus PAROLE [<xref rid="B45" ref-type="bibr">Spr&#x000e5;kbanken (The Swedish Language Bank), 2011</xref>] and also had a small-to-average number of phonological neighbors (three to six different words with similar articulation of the first two phonemes). For instance the word /dos/ has the neighbors /dog, dok, dop, don/. The gate size was 33.3 ms, similar to our previous studies (e.g., <xref rid="B27" ref-type="bibr">Moradi et al., 2013</xref>; <xref rid="B23" ref-type="bibr">Lidestam et al., 2014</xref>). The rationale for this gate size was based on our pilot findings, which showed that the identification of words with a gate size of 16.67 ms (starting from the first phoneme) in CVC format lead to exhaustion and loss of motivation. Hence, a double gate size (33.3 ms) starting from the onset of second phoneme was used to avoid fatigue in participants. The word gating task took 20&#x02013;25 min to complete.</p></sec><sec><title>Hearing-In-Noise Test</title><p>We used a Swedish version of the HINT (<xref rid="B15" ref-type="bibr">H&#x000e4;llgren et al., 2006</xref>) to measure participants&#x02019; sentence identification in noise ability. The HINT consisted of everyday sentences, from minimum three to maximum seven words in length on a background of steady-state speech-shaped noise. The first sentence in each list (in both the practice and experimental lists) was presented at 65 dB SPL and 0 dB SNR. The participants were asked to listen and repeat each sentence correctly. An automatic, adaptive up-down procedure was used to determine the SNR of each participant at a correct response rate of 50%. If all words were correctly repeated, the SNR was decreased by 2dB and if one or more words were not correctly repeated, the SNR was raised by 2 dB.</p><p>In the present study, the HINT scores of participants were collected in three sessions (pre-test, post-test, and one-month follow-up). Each session consisted of a practice list (with 10 sentences) and an experimental list (with 20 sentences). In each session, participants were first familiarized with the test using a 10-sentence practice list. To determine the SNR for each participant, a 20-sentence list was used in each session. Hence, 30 sentences in each session were used. There are 5 practice lists and 25 experimental lists in HINT. We chose three practice and three experimental lists that had the same SNRs based on norm data in normal hearing listeners (see <xref rid="B15" ref-type="bibr">H&#x000e4;llgren et al., 2006</xref>). To avoid repetition effects, we randomized the presentation of lists (practice and experimental lists) across participants in each training group, such that no list (or item) was repeated for any participant. The HINT took approximately 10 min to complete in each session.</p><p>It should be noted that the talkers of the HINT and training speech materials were different.</p></sec><sec><title>Procedure</title><p>Participants were tested individually in a quiet room at Link&#x000f6;ping University. They were sat in front of a Mitsubishi Diamond Pro 2070 SB cathode ray tube (CRT) monitor (Mitsubishi Electric, Tokyo, Japan). The monitor was turned off during auditory-only presentation. Auditory speech stimuli were delivered via an iMac computer, which was routed to the input of two loudspeakers (Genelec 8030A) located to the right and left of the CRT monitor. The experimenter used the iMac to present the gated stimuli and monitor the participants&#x02019; progress. There was a screen between the iMac monitor and the CRT monitor used for stimulus presentation, preventing participants from seeing the experimenter&#x02019;s monitor and the response sheets.</p><p>The study was conducted over two separate sessions. The first session for both groups started with the pre-test measurement of participants&#x02019; HINT scores. Participants subsequently underwent gated audiovisual or auditory speech identification training. This entailed a practice session to allow participants to become familiarized with the gated presentation of stimuli. The practice session comprised three gated consonants (/v k &#x0014b;/) and two gated words (/tum [inch]/ and /bil [car]/). Oral feedback was provided during the practice session, but not during the experiment.</p><p>After the practice session, the gating paradigm started. All participants began with the consonant identification task, followed by the word identification task. There were short rest periods to prevent fatigue. The order of item presentation within each gated task (i.e., consonants and words) varied among the participants. Participants gave their responses orally and the experimenter wrote these down. The presentation of gates continued until the target item was correctly recognized on five consecutive presentations to avoid random guessing. If the target item was not correctly recognized, presentation continued until the end of the stimulus. After gated identification training, the post-test HINT scores were obtained. The first session finished at this point. Similar to <xref rid="B23" ref-type="bibr">Lidestam et al. (2014)</xref>, after each training condition (audiovisual or auditory gated speech identification training, respectively), the participants rated the effort required for speech identification tasks on a questionnaire on a visual analog scale from 0 (no effort) to 100 (maximum effort).</p><p>One month after the first session, participants returned to the laboratory for the measurement of their HINT follow-up scores and obtaining their pure-tone hearing thresholds using an audiometer (Interacoustics AC40).</p></sec></sec><sec><title>Results</title><sec><title>Comparison of Gated Audiovisual and Auditory Speech Identification Tasks</title><p><bold>Table <xref ref-type="table" rid="T2">2</xref></bold> shows the mean number of gates required for correct identification of consonants and words in the auditory and audiovisual modalities, respectively. The participants in the audiovisual training were able to identify both consonants and words with fewer gates (faster identification) than the participants in the auditory training group. These findings are in agreement with our prior studies that showed that the association of visual cues to auditory speech signal resulted in faster identification of consonants and words in aided hearing-impaired listeners (<xref rid="B28" ref-type="bibr">Moradi et al., 2016</xref>; Moradi et al., unpublished).</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Mean number of gates (with Standard Deviations in parentheses) required for correct identification of consonants and words in auditory and audiovisual modalities, respectively.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Type of gated task</th><th valign="top" align="left" rowspan="1" colspan="1">Audiovisual</th><th valign="top" align="left" rowspan="1" colspan="1">Auditory</th><th valign="top" align="left" rowspan="1" colspan="1">Inferential statistics</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Consonants</td><td valign="top" align="left" rowspan="1" colspan="1">9.17 (1.81)</td><td valign="top" align="left" rowspan="1" colspan="1">10.69 (1.56)</td><td valign="top" align="left" rowspan="1" colspan="1"><italic>t</italic>(23) = 2.20, <italic>p</italic> = 0.038, <italic>d</italic> = 0.88</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Words</td><td valign="top" align="left" rowspan="1" colspan="1">12.65 (2.02)</td><td valign="top" align="left" rowspan="1" colspan="1">14.22 (1.35)</td><td valign="top" align="left" rowspan="1" colspan="1"><italic>t</italic>(23) = 2.21, <italic>p</italic> = 0.037, <italic>d</italic> = 0.89</td></tr></tbody></table></table-wrap></sec><sec><title>Effects of Gated Audiovisual and Auditory Speech Training on HINT Performance</title><p><bold>Figure <xref ref-type="fig" rid="F2">2</xref></bold> shows the mean scores and standard errors for HINT performance from before the training, promptly after the training, and from the one-month follow-up. Prior to training, the mean HINT scores in audiovisual and auditory training groups were 1.85 dB SNR (<italic>SD</italic> = 2.54) and 0.44 dB SNR (<italic>SD</italic> = 1.96), respectively, which means that the audiovisual training group had about 1.5 dB higher SNR in HINT than the auditory training group. However, there was no significant difference between the audiovisual and auditory training groups in HINT performance before the training, <italic>t</italic>(23) = 1.52, <italic>p</italic> = 0.14 (ns. <italic>d</italic> = 0.61).</p><fig id="F2" position="float"><label>FIGURE 2</label><caption><p><bold>HINT scores (<italic>M</italic> &#x000b1; <italic>SE</italic>) for the experimental and control groups in the pre-test (before the training), post-test (promptly after the training), and one-month follow-up (1 month after the training)</bold>.</p></caption><graphic xlink:href="fpsyg-08-00368-g002"/></fig><p>A 2 (Training condition: audiovisual, auditory) &#x000d7; 3 (Session: pre, post-, follow-up) split-plot factorial analysis of variance (ANOVA) with repeated measures on the second factor was conducted to examine the effect of training conditions on HINT performance across sessions. Results showed the main effect of training condition was not significant, <italic>F</italic>(1,23) = 0.90, <italic>p</italic> = 0.35. In addition, the main effect of session was not significant, <italic>F</italic>(2,46) = 2.29, <italic>p</italic> = 0.11. The interaction between training condition and session was not also significant, <italic>F</italic>(2,46) = 1.99, <italic>p</italic> = 0.15.</p><p>In the next step of analysis, we analyze the data in a within-subject manner to test our <italic>a priori</italic> hypothesis that gated audiovisual speech training subsequently improved HINT performance and this improvement was retained after one-month follow-up; whereas there should be little or no improvement for the gated auditory training group.</p></sec><sec><title>Pre- to Post-test Improvements and Pre- to One-month Follow-Up Test Maintenance</title><p>A one-way (Sessions [before, after, and one-month follow-up]) repeated measures ANOVA was conducted to examine the differences between HINT scores before and after the training in the experimental and control groups, respectively.</p><p>In the audiovisual training group, the main effect of sessions was significant, <italic>F</italic>(2,26) = 4.97, <italic>p</italic> = 0.015, <inline-formula><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.28. Planned comparisons showed that audiovisual training improved sentence comprehension in noise ability promptly after the training (post-test vs. pre-test comparison), <italic>t</italic>(13) = 3.32, <italic>p</italic> = 0.006, <italic>d</italic> = 0.89. When comparing the HINT scores between one-month follow-up and before the training, the effect of audiovisual training in terms of improving sentence comprehension in noise was maintained, <italic>t</italic>(13) = 2.35, <italic>p</italic> = 0.035, <italic>d</italic> = 0.63. In addition, the difference between the post-test and one-month follow-up was not significant, <italic>t</italic>(13) = 0.43, <italic>p</italic> = 0.67 (ns. <italic>d</italic> = 0.12).</p><p>In the auditory training group, the main effect of session was not significant, <italic>F</italic>(2,20) = 0.31, <italic>p</italic> = 0.74 (ns. <inline-formula><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.03). Planned comparisons also showed no effect of auditory training in terms of improving sentence comprehension in noise ability neither promptly after the training, <italic>t</italic>(10) = 0.35, <italic>p</italic> = 0.73 (ns. <italic>d</italic> = 0.11), nor at one-month follow-up, <italic>t</italic>(10) = 0.37, <italic>p</italic> = 0.72 (ns. <italic>d</italic> = 0.11).</p></sec><sec><title>Self-effort Rating for the Identification of Gated Speech Training Tasks</title><p>The mean self-rated effort for the identification of gated speech tasks was 53.18 (<italic>SD</italic> = 13.09) in the auditory training group and 62.14 (<italic>SD</italic> = 20.64) in the audiovisual training group. A <italic>t</italic>-test comparison showed no significant difference between two groups in their subjective effort required for the identification of gated speech training tasks, <italic>t</italic>(23) = 1.25, <italic>p</italic> = 0.22. This finding corroborates <xref rid="B23" ref-type="bibr">Lidestam et al. (2014)</xref> by suggesting that the differences in HINT scores (particularly post-test scores) are not associated with subjective effort of participants in training conditions for the identification of speech stimuli.</p></sec></sec><sec><title>Discussion</title><p>The findings of the present study extend our previous studies in elderly hearing-aid users, by showing that prior exposure to gated audiovisual speech identification tasks subsequently improves auditory sentence identification in noise ability. In addition, this audiovisual speech training effect was independent of the idiosyncrasy of talkers as the talkers in the training materials and HINT, respectively, were different. Furthermore, for the first time, we showed that the effect of audiovisual speech training was retained 1 month after the training (the HINT performance was better at both post-test and one-month follow-up in comparison to the pre-test; and there was no difference between post-test and one-month follow-up). This suggests that gated audiovisual speech training can be used in a reliable aural rehabilitation program for people with hearing loss. Further, our findings are in line with those of <xref rid="B34" ref-type="bibr">Richie and Kewley-Port (2008)</xref>, who showed that audiovisual vowel training improved auditory vowel recognition and the auditory identification of key words in sentences in participants with normal hearing. Together, our findings suggest that a sensory-rich speech training program comprising audiovisual speech training (containing complementary audio and visual speech cues) improves auditory identification of speech stimuli in people with hearing loss.</p><p>In the auditory training group, and despite providing an interfering noise during the training, there was no significant improvement in the HINT performance at neither post-test nor one-month follow-up when comparing to the pre-test. Most likely, this was due to using only one session of training in the present study. One may speculate that by providing several sessions of auditory training when background noise is added to training materials, then auditory training becomes an active training method (i.e., more challenging due to more degradation of the speech signal, see <xref rid="B33" ref-type="bibr">Rao et al., 2017</xref>) that subsequently would result in better HINT performance. In their review, however, <xref rid="B16" ref-type="bibr">Henshaw and Ferguson (2013)</xref> evaluated the effects of auditory training on aural rehabilitation of people with hearing loss. In terms of on-task improvement (i.e., improvement in a given speech task that was the exactly same task as the one trained), their review revealed that most of the studies reported significant on-task improvement after auditory training. In terms of far-task improvement (i.e., improvement in a speech task or a test that had not been directly trained), their review showed that auditory training studies have generally failed to show such a &#x0201c;far-transfer&#x0201d; improvement. Our lack of effect on auditory training is in agreement with the conclusion by <xref rid="B16" ref-type="bibr">Henshaw and Ferguson (2013)</xref>. We used consonants and words in the training materials whereas the outcome measure was identification of sentences (in noise) in the HINT task, where no improvement was observed.</p><p>One explanation for the efficiency of this short audiovisual speech training program is that the gated audiovisual identification of consonants and words represents an active audiovisual speech identification training program. This is because it forces listeners to allocate their attention to the auditory and visual components of phonetic and lexical incoming speech signals and quickly map them onto their corresponding phonological and lexical structures in the brain. In addition, a background noise was added to the gated audiovisual speech identification training in both the present study and in that by <xref rid="B23" ref-type="bibr">Lidestam et al. (2014)</xref> in order to increase listeners&#x02019; attention (or cognitive effort) for pairing the auditory and visual components of incoming speech signals to create a coherent audiovisual speech token. According to the Ease of Language Understanding model (<xref rid="B36" ref-type="bibr">R&#x000f6;nnberg et al., 2013</xref>) working memory plays a critical role in perceiving speech stimuli in degraded listening conditions such as in background noise or in people with hearing loss (for an opposing view, see <xref rid="B11" ref-type="bibr">F&#x000fc;llgrabe and Rosen, 2016</xref>). Hence, an increase in cognitive effort (e.g., the addition of background noise) might to some extent reinforce the effect of gated audiovisual speech identification training (see <xref rid="B51" ref-type="bibr">Wayne and Johnsrude, 2012</xref>). In addition, we speculate that longer audiovisual training sessions would improve auditory sentence identification in noise ability even more. An alternative approach for providing longer sessions of audiovisual speech training could be provided in the form of &#x0201c;e-health rehabilitation,&#x0201d; by developing computer software, mobile applications (&#x0201c;apps&#x0201d;), or internet interventions, to be used at home by people with hearing loss.</p><p>We also suggest that audiovisual speech training in noise with multiple talkers could be another efficient training approach for improving speech-in-noise in people with hearing loss. In such a training condition, participants focus more on the visual component of the audiovisual speech materials for extracting phonetic features, in order to correctly identify those speech tokens. Simultaneously, they have to ignore the distractive speech from multiple talkers, which makes this task more cognitively demanding than simple audiovisual speech training in noise. A comparison between gated audiovisual speech training in background noise (as used in the present study) and audiovisual speech training in noise with multiple talkers would be a very interesting future research topic.</p><p>According to <xref rid="B43" ref-type="bibr">Shams and Seitz (2008)</xref>, our interaction with the external world is usually multisensory rather than unisensory, and the human brain has evolved to process, operate, and learn optimally in multisensory rather than unisensory conditions. Consequently, the efficiency of training protocols would be optimized if they consisted of multisensory materials that are more approximate to natural settings. In fact, the ecological validity of audiovisual speech training is more evident for people with hearing loss as, due to their hearing loss, they rely more on visual speech cues to disambiguate the identity of a target speech signal than their counterparts with normal hearing when both auditory and visual speech cues are available (<xref rid="B50" ref-type="bibr">Walden et al., 1990</xref>; <xref rid="B8" ref-type="bibr">Desai et al., 2008</xref>).</p><p>In a magnetoencephalographic study, <xref rid="B54" ref-type="bibr">Zion Golumbic et al. (2013)</xref> revealed that audiovisual relative to auditory presentation enhanced the capacity of the auditory cortex to track the temporal speech envelope of the talker, particularly in &#x0201c;cocktail party&#x0201d; conditions. Similarly, <xref rid="B6" ref-type="bibr">Crosse et al. (2015)</xref> showed that cortical representation of the speech envelope was enhanced by congruent audiovisual presentation, even in a noise-free condition. In addition, <xref rid="B12" ref-type="bibr">Ganesh et al. (2014)</xref> in an electroencephalography study showed that early audiovisual presentation subsequently reduced the amplitude and latency of P2 response (a speech specific component that presumably is related to processing of physical characteristics of a speech sound prior to its categorization, see <xref rid="B29" ref-type="bibr">N&#x000e4;&#x000e4;t&#x000e4;nen and Winkler, 1999</xref>). The <xref rid="B12" ref-type="bibr">Ganesh et al. (2014)</xref> study in fact denotes that audiovisual relative to auditory presentation speeds up the auditory processing of speech stimuli for their identification. Further, <xref rid="B21" ref-type="bibr">Li et al. (2011)</xref>, in a functional magnetic resonance imaging study, also showed that audiovisual relative to auditory-only presentation facilitated access to the neural representation of semantic content, in terms of both within-class reproducibility (discriminability of semantic content within the same semantic category) and between-class discriminability (discriminability of semantic content between two different semantic categories). In their review, <xref rid="B44" ref-type="bibr">Shams et al. (2011)</xref> suggested that multisensory training can boost subsequent unisensory processing, most likely because early exposure to multisensory stimuli quickly recalibrates unisensory maps in the brain, creates a new connection between unisensory cortical areas, or because the unisensory representation of stimuli (i.e., auditory-only or visual-only representation of stimuli) are integrated in a multisensory manner.</p><p>Together, we hypothesize that there are two mechanisms that, independently or together, account for the efficiency of gated audiovisual speech training on subsequent auditory identification performance. First, gated audiovisual speech training reinforces auditory routes for phonological and lexical representations in long-term memory (<xref rid="B44" ref-type="bibr">Shams et al., 2011</xref>), which subsequently facilitates access to those representations in an auditory-only modality condition (<xref rid="B21" ref-type="bibr">Li et al., 2011</xref>). In such a case, competing words in the target&#x02019;s cohort are readily eliminated in subsequent sentence comprehension in noise by the previous audiovisual speech processing (training). Second, audiovisual speech training enhances the capacity of the auditory cortex to detect the perceptual changes in the target speech due to background noise (<xref rid="B54" ref-type="bibr">Zion Golumbic et al., 2013</xref>; <xref rid="B12" ref-type="bibr">Ganesh et al., 2014</xref>; <xref rid="B6" ref-type="bibr">Crosse et al., 2015</xref>); helping listeners to identify the target words at higher noise levels (i.e., lower SNRs).</p><p>The participants in the auditory training group may have been somewhat discouraged by their poorer performance as they needed longer exposure (i.e., higher number of gated) in auditory speech identification training tasks than the participants in the audiovisual training group. This might have had a negative influence on subsequent speech-in-noise task performance. Hence, the better performance in subsequent auditory speech-in-noise task in audiovisual training group was perhaps not solely due to the association of visual cues with auditory speech training materials. The effect may, to some extent, have been due to higher motivation and compliance caused by audiovisual speech training materials. Nevertheless, the results showed no significant difference in self-effort ratings for the training speech materials between audiovisual and auditory training groups. In addition, if the abovementioned argument was true, we believe that it is a merit for the use of audiovisual speech training over auditory speech training to improve participants&#x02019; motivation and compliance. <xref rid="B48" ref-type="bibr">Sweetow and Sabes (2010)</xref> showed that compliance with home-based auditory-only training programs in hearing-aid users was low, and most participants did not complete the training. The interaction between compliance and participation, the modality of speech training (auditory vs. audiovisually), and the amount of benefit provided by the training materials needs to be investigated in future studies.</p><p>Note that the data from the present study did not reveal that that the gated audiovisual speech training is better than auditory speech training since there were no significant differences in HINT scores between audiovisual and auditory training groups in each of three sessions (prior to the training, promptly post-training, and one-month follow-up). Future studies are needed to better evaluate audiovisual versus auditory speech training on subsequent on-task and far-task speech improvement and the compliance with the training programs in people with hearing loss.</p><sec><title>Idiosyncrasy of the Talkers</title><p>The idiosyncrasy of talkers is a key factor in perceptual learning studies, as previously acquired knowledge about a talker (obtained in the audiovisual training condition) should be available in the subsequent unisensory modality task (<xref rid="B40" ref-type="bibr">Schall et al., 2013</xref>; <xref rid="B41" ref-type="bibr">Schall and von Kriegstein, 2014</xref>; <xref rid="B42" ref-type="bibr">Schelinski et al., 2014</xref>). <xref rid="B35" ref-type="bibr">Riedel et al. (2015)</xref> suggested an &#x0201c;audiovisual feed-forward model&#x0201d; to explain how multisensory training with familiar persons subsequently improves the auditory-only speech recognition of those persons. According to this model, the human brain quickly and efficiently learns about &#x0201c;a new person&#x0201d; by his or her own auditory and visual characteristics that are relevant in the auditory-only or the visual-only identification conditions (forming an audiovisual simulation of that person). When a visual signal is lacking, this simulation feeds back to auditory areas and improves the auditory-only recognition (voice) of that person. In the present study and in our earlier studies, the talkers in the gated audiovisual training and auditory sentence comprehension in noise tests were not the same; hence, the audiovisual feed-forward model (<xref rid="B35" ref-type="bibr">Riedel et al., 2015</xref>) is at variance with our findings. We have coined the term &#x0201c;perceptual doping&#x0201d; to refer to the fact that the visual component of the audiovisual speech signal is much more distinct than the auditory-alone component for retuning (or setting up) phonological and lexical maps in the mental lexicon. As a consequence, the maps to phonological and lexical representations become more distinct and easier to access &#x02013; <italic>without effort</italic>. These distinct and more accessible maps (doped, updated, or enriched maps) are maintained after the (gated) audiovisual speech training, which subsequently facilitates auditory route mapping to those phonological and lexical representations in the mental lexicon. The gating format presumably adds to the efficiency by its task characteristics. Successive repetition and extension of the audiovisual speech stimuli allow listeners to allocate their attention to more fine-grained phonological features, which are much easier to find in audiovisual format.</p><p>The perceptual doping hypothesis is also supported by neuroimaging studies which show that audiovisual relative to auditory-alone presentation improves the auditory route map for speech comprehension (<xref rid="B21" ref-type="bibr">Li et al., 2011</xref>; <xref rid="B54" ref-type="bibr">Zion Golumbic et al., 2013</xref>; <xref rid="B12" ref-type="bibr">Ganesh et al., 2014</xref>; <xref rid="B6" ref-type="bibr">Crosse et al., 2015</xref>). The exact encoding of visual characteristics of a given talker (audiovisual feed-forward model; <xref rid="B35" ref-type="bibr">Riedel et al., 2015</xref>) does not seem necessary for subsequent improvement in auditory-only speech recognition. The perceptual doping effect is more about the default mode of speech processing which is multisensory (<xref rid="B37" ref-type="bibr">Rosenblum, 2005</xref>; <xref rid="B13" ref-type="bibr">Ghazanfar, 2010</xref>) and its instant and maintained consequences on unisensory mapping of speech signal onto a linguistic representation in the brain and is generalizable across hearing status of individuals.</p><p>One limitation of the present study is the sample size in the audiovisual and auditory training groups. Since hearing-impaired listeners are more heterogeneous in perceiving speech stimuli than normal hearing listeners, we recommend that future studies evaluating training programs in people with hearing loss consider a larger sample size to detect significant differences that we could not achieve in the present study (e.g., interaction between group and session).</p><p>In the present study, we randomized the participants to the auditory or auditory training groups. Due to the heterogeneity of people with hearing loss in perceiving speech stimuli, we suggest that future studies allocate their participants to training groups based on matching instead of randomization. This is because prior to the training conditions in the present study, the audiovisual and auditory training groups differed around 1.5 dB SNR, which makes the interpretation the data more difficult.</p></sec></sec><sec><title>Conclusion</title><p>The findings of the present study highlight the efficiency of gated audiovisual speech training for improving auditory sentence identification in noise ability in elderly hearing aid users; furthermore, this efficiency persisted 1 month after the training. The present study did not show that the audiovisual speech training is better than auditory speech training (in terms of the between-groups comparison), however. A controlled comparison of audiovisual and auditory speech training on subsequent auditory improvement of speech stimuli using larger sample size is needed for future studies. In addition, we suggest examining the efficiency of in-home audiovisual speech training programs for the aural rehabilitation of hearing aid users, as they can offer longer periods of training that can boost auditory speech identification in people with hearing loss.</p></sec><sec><title>Author Contributions</title><p>The present study was designed by the SM, BL, and AW collected the data. MH helped us in recruiting the participants based on inclusion and exclusion criteria. The data was analyzed with SM and BL with comments from the JR. The manuscript was written with SM, BL, and JR helped us in theoretical aspects of the manuscript. In addition, MH commented and helped in Materials and Methods section.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>The authors thank Carl-Fredrik Neikter and Amin Saremi for their technical support. In addition, the authors thank two reviewers for their comments on this manuscript.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrams</surname><given-names>H. B.</given-names></name><name><surname>Bock</surname><given-names>K.</given-names></name><name><surname>Irey</surname><given-names>R. L.</given-names></name></person-group> (<year>2015</year>). <article-title>Can a remotely delivered auditory training program improve speech-in-noise understanding?</article-title>
<source><italic>Am. J. Audiol.</italic></source>
<volume>24</volume>
<fpage>333</fpage>&#x02013;<lpage>337</lpage>. <pub-id pub-id-type="doi">10.1044/2015_AJA-15-0002</pub-id><pub-id pub-id-type="pmid">26649542</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahlstrom</surname><given-names>J. B.</given-names></name><name><surname>Horwitz</surname><given-names>A. R.</given-names></name><name><surname>Dubno</surname><given-names>J. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Spatial separation benefit for unaided and aided listening.</article-title>
<source><italic>Ear Hear.</italic></source>
<volume>35</volume>
<fpage>72</fpage>&#x02013;<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e3182a02274</pub-id><pub-id pub-id-type="pmid">24121648</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>K.</given-names></name><name><surname>Abrams</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>). <source><italic>An Evaluation of the Efficacy of a Remotely Delivered Auditory Training Program. Starkey Hearing Technologies, Technology Paper.</italic></source> Available at: <ext-link ext-link-type="uri" xlink:href="https://starkeypro.com/pdfs/technicalpapers/An%20Evaluation%20of%20the%20Efficacy_Remotely_Delivered_Auditory_Training_Program.pdf">https://starkeypro.com/pdfs/technicalpapers/An Evaluation of the Efficacy_Remotely_Delivered_Auditory_Training_Program.pdf</ext-link>.</mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>D. H.</given-names></name></person-group> (<year>1997</year>). <article-title>The psychophysics toolbox.</article-title>
<source><italic>Spat. Vis.</italic></source>
<volume>10</volume>
<fpage>433</fpage>&#x02013;<lpage>436</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burk</surname><given-names>M. H.</given-names></name><name><surname>Humes</surname><given-names>L. E.</given-names></name><name><surname>Amos</surname><given-names>N. E.</given-names></name><name><surname>Strausser</surname><given-names>L. E.</given-names></name></person-group> (<year>2006</year>). <article-title>Effect of training on word-recognition performance in noise for young normal-hearing and older hearing-impaired listeners.</article-title>
<source><italic>Ear Hear.</italic></source>
<volume>27</volume>
<fpage>263</fpage>&#x02013;<lpage>278</lpage>. <pub-id pub-id-type="doi">10.1097/01.aud.0000215980.21158.a2</pub-id><pub-id pub-id-type="pmid">16672795</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>M. J.</given-names></name><name><surname>Butler</surname><given-names>J. S.</given-names></name><name><surname>Lalor</surname><given-names>E. C.</given-names></name></person-group> (<year>2015</year>). <article-title>Congruent visual speech enhances cortical entrainment to continuous auditory speech in noise-free conditions.</article-title>
<source><italic>J. Neurosci.</italic></source>
<volume>35</volume>
<fpage>14195</fpage>&#x02013;<lpage>14204</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1829-15.2015</pub-id><pub-id pub-id-type="pmid">26490860</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalton</surname><given-names>D. S.</given-names></name><name><surname>Cruickshanks</surname><given-names>K. J.</given-names></name><name><surname>Klein</surname><given-names>B. E.</given-names></name><name><surname>Klein</surname><given-names>R.</given-names></name><name><surname>Wiley</surname><given-names>T. L.</given-names></name><name><surname>Nondahl</surname><given-names>D. M.</given-names></name></person-group> (<year>2003</year>). <article-title>The impact of hearing loss on quality of life in older adults.</article-title>
<source><italic>Gerontologist</italic></source>
<volume>43</volume>
<fpage>661</fpage>&#x02013;<lpage>668</lpage>. <pub-id pub-id-type="doi">10.1093/geront/43.5.661</pub-id><pub-id pub-id-type="pmid">14570962</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desai</surname><given-names>S.</given-names></name><name><surname>Stickney</surname><given-names>G.</given-names></name><name><surname>Zeng</surname><given-names>F.-G.</given-names></name></person-group> (<year>2008</year>). <article-title>Auditory-visual speech perception in normal-hearing and cochlear-implant listeners.</article-title>
<source><italic>J. Acoust. Soc. Am.</italic></source>
<volume>123</volume>
<fpage>428</fpage>&#x02013;<lpage>440</lpage>. <pub-id pub-id-type="doi">10.1121/1.2816573</pub-id><pub-id pub-id-type="pmid">18177171</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimitrijevic</surname><given-names>A.</given-names></name><name><surname>John</surname><given-names>M. S.</given-names></name><name><surname>Picton</surname><given-names>T. W.</given-names></name></person-group> (<year>2004</year>). <article-title>Auditory steady-state responses and word recognition scores in normal-hearing and hearing-impaired adults.</article-title>
<source><italic>Ear Hear.</italic></source>
<volume>25</volume>
<fpage>68</fpage>&#x02013;<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1097/01.AUD.0000111545.71693.48</pub-id><pub-id pub-id-type="pmid">14770019</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>M. A.</given-names></name><name><surname>Henshaw</surname><given-names>H.</given-names></name><name><surname>Clark</surname><given-names>D. P. A.</given-names></name><name><surname>Moore</surname><given-names>D. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Benefits of phoneme discrimination training in a randomized controlled trial of 50-to 74-year-olds with mild hearing loss.</article-title>
<source><italic>Ear Hear.</italic></source>
<volume>35</volume>
<fpage>e110</fpage>&#x02013;<lpage>e121</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0000000000000020</pub-id><pub-id pub-id-type="pmid">24752284</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>F&#x000fc;llgrabe</surname><given-names>C.</given-names></name><name><surname>Rosen</surname><given-names>S.</given-names></name></person-group> (<year>2016</year>). <article-title>On the (un)importance of working memory in speech-in-noise processing for listeners with normal hearing thresholds.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>7</volume>:<issue>1268</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2016.01268</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganesh</surname><given-names>A. C.</given-names></name><name><surname>Berthommier</surname><given-names>F.</given-names></name><name><surname>Vilain</surname><given-names>C.</given-names></name><name><surname>Sato</surname><given-names>M.</given-names></name><name><surname>Schwartz</surname><given-names>J.-L.</given-names></name></person-group> (<year>2014</year>). <article-title>A possible neurophysiological correlate of audiovisual binding and unbinding in speech perception.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>5</volume>:<issue>1340</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2014.01340</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>A. A.</given-names></name></person-group> (<year>2010</year>). <article-title>&#x0201c;The default mode of primate vocal communication and its correlates,&#x0201d; in</article-title>
<source><italic>Multisensory Object Perception in the Primate Brain</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Naumer</surname><given-names>M. J.</given-names></name><name><surname>Kaiser</surname><given-names>J.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>139</fpage>&#x02013;<lpage>153</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-4419-5615-6_9</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosjean</surname><given-names>F.</given-names></name></person-group> (<year>1980</year>). <article-title>Spoken word recognition processes and gating paradigm.</article-title>
<source><italic>Percept. Psychophys.</italic></source>
<volume>28</volume>
<fpage>267</fpage>&#x02013;<lpage>283</lpage>. <pub-id pub-id-type="doi">10.3758/BF03204386</pub-id><pub-id pub-id-type="pmid">7465310</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000e4;llgren</surname><given-names>M.</given-names></name><name><surname>Larsby</surname><given-names>B.</given-names></name><name><surname>Arlinger</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). <article-title>A Swedish version of the Hearing In Noise Test (HINT) for measurement of speech recognition.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>45</volume>
<fpage>227</fpage>&#x02013;<lpage>237</lpage>. <pub-id pub-id-type="doi">10.1080/14992020500429583</pub-id><pub-id pub-id-type="pmid">16684704</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henshaw</surname><given-names>H.</given-names></name><name><surname>Ferguson</surname><given-names>M. A.</given-names></name></person-group> (<year>2013</year>). <article-title>Efficacy of individual computer-based auditory training for people with hearing loss: a systematic review of the evidence.</article-title>
<source><italic>PLoS ONE</italic></source>
<volume>8</volume>:<issue>e62836</issue>
<pub-id pub-id-type="doi">10.1371/journal.pone.0062836</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keidser</surname><given-names>G.</given-names></name><name><surname>Seeto</surname><given-names>M.</given-names></name><name><surname>Rudner</surname><given-names>M.</given-names></name><name><surname>Hygge</surname><given-names>S.</given-names></name><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>On the relationship between functional hearing and depression.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>54</volume>
<fpage>653</fpage>&#x02013;<lpage>664</lpage>. <pub-id pub-id-type="doi">10.3109/14992027.2015.1046503</pub-id><pub-id pub-id-type="pmid">26070470</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M.</given-names></name><name><surname>Brainard</surname><given-names>D.</given-names></name><name><surname>Pelli</surname><given-names>D.</given-names></name></person-group> (<year>2007</year>). &#x0201c;<article-title>What&#x02019;s new in Psychtoolbox-3?</article-title>,&#x0201d; in <source><italic>Proceedings of the Talk Presented at 30th European Conference on Visual Perception</italic></source>
<publisher-loc>Arezzo</publisher-loc>.</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname><given-names>H.</given-names></name><name><surname>Oden</surname><given-names>C.</given-names></name><name><surname>Simon</surname><given-names>H.</given-names></name><name><surname>Noack</surname><given-names>C.</given-names></name><name><surname>Lotze</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Entertainment overcomes barriers of auditory training.</article-title>
<source><italic>Hear. J.</italic></source>
<volume>64</volume>
<fpage>40</fpage>&#x02013;<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1097/01.HJ.0000403510.80465.7b</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C. M.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Hoffman</surname><given-names>H. J.</given-names></name><name><surname>Cotch</surname><given-names>M. F.</given-names></name><name><surname>Themann</surname><given-names>C. L.</given-names></name><name><surname>Wilson</surname><given-names>M. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Hearing impairment associated with depression in US adults, national health and nutrition examination survey 2005&#x02013;2010.</article-title>
<source><italic>JAMA. Otolaryngol. Head Neck Surg.</italic></source>
<volume>140</volume>
<fpage>293</fpage>&#x02013;<lpage>302</lpage>. <pub-id pub-id-type="doi">10.1001/jamaoto.2014.42</pub-id><pub-id pub-id-type="pmid">24604103</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>G.</given-names></name><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Yu</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>B.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Reproducibility and discriminability of brain patterns of semantic categories enhanced by congruent audiovisual stimuli.</article-title>
<source><italic>PLoS ONE</italic></source>
<volume>6</volume>:<issue>e20801</issue>
<pub-id pub-id-type="doi">10.1371/journal.pone.0020801</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lidestam</surname><given-names>B.</given-names></name></person-group> (<year>2014</year>). <article-title>Audiovisual presentation of video-recorded stimuli at a high frame rate.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>46</volume>
<fpage>499</fpage>&#x02013;<lpage>516</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-013-0394-2</pub-id><pub-id pub-id-type="pmid">24197711</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lidestam</surname><given-names>B.</given-names></name><name><surname>Moradi</surname><given-names>S.</given-names></name><name><surname>Petterson</surname><given-names>R.</given-names></name><name><surname>Ricklefs</surname><given-names>T.</given-names></name></person-group> (<year>2014</year>). <article-title>Audiovisual training is better than auditory-only training for auditory only speech-in-noise identification.</article-title>
<source><italic>J. Acoust. Soc. Am.</italic></source>
<volume>136</volume> EL142&#x02013;EL147. <pub-id pub-id-type="doi">10.1121/1.4890200</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>F. R.</given-names></name><name><surname>Thorpe</surname><given-names>R.</given-names></name><name><surname>Gordon-Salant</surname><given-names>S.</given-names></name><name><surname>Ferrucci</surname><given-names>L.</given-names></name></person-group> (<year>2011</year>). <article-title>Hearing loss prevalence and risk factors among older adults in the United States.</article-title>
<source><italic>J. Gerontol. A Biol. Sci. Med. Sci.</italic></source>
<volume>66</volume>
<fpage>582</fpage>&#x02013;<lpage>590</lpage>. <pub-id pub-id-type="doi">10.1093/gerona/glr002</pub-id><pub-id pub-id-type="pmid">21357188</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mick</surname><given-names>P.</given-names></name><name><surname>Kawachi</surname><given-names>I.</given-names></name><name><surname>Lin</surname><given-names>F. R.</given-names></name></person-group> (<year>2014</year>). <article-title>The association between hearing loss and social isolation in older adults.</article-title>
<source><italic>Otolaryngol. Head Neck Surg.</italic></source>
<volume>150</volume>
<fpage>378</fpage>&#x02013;<lpage>384</lpage>. <pub-id pub-id-type="doi">10.1177/0194599813518021</pub-id><pub-id pub-id-type="pmid">24384545</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moradi</surname><given-names>S.</given-names></name><name><surname>Lidestam</surname><given-names>B.</given-names></name><name><surname>H&#x000e4;llgren</surname><given-names>M.</given-names></name><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Gated auditory speech perception in elderly hearing aid users and elderly normal-hearing individuals: effects of hearing impairment and cognitive capacity.</article-title>
<source><italic>Trends Hear.</italic></source>
<volume>18</volume>
<fpage>1</fpage>&#x02013;<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1177/2331216514545406</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moradi</surname><given-names>S.</given-names></name><name><surname>Lidestam</surname><given-names>B.</given-names></name><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Gated audiovisual speech identification in silence vs. noise: effects on time and accuracy.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>4</volume>:<issue>359</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2013.00359</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moradi</surname><given-names>S.</given-names></name><name><surname>Lidestam</surname><given-names>B.</given-names></name><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Comparison of gated audiovisual speech identification in elderly hearing aid users and elderly normal-hearing individuals: effects of adding visual cues to auditory speech stimuli.</article-title>
<source><italic>Trends Hear.</italic></source>
<volume>20</volume>
<fpage>1</fpage>&#x02013;<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1177/2331216516653355</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>N&#x000e4;&#x000e4;t&#x000e4;nen</surname><given-names>O.</given-names></name><name><surname>Winkler</surname><given-names>I.</given-names></name></person-group> (<year>1999</year>). <article-title>The concept of auditory stimulus representation in cognitive neuroscience.</article-title>
<source><italic>Psychol. Bull.</italic></source>
<volume>6</volume>
<fpage>826</fpage>&#x02013;<lpage>859</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.125.6.826</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Needleman</surname><given-names>A. R.</given-names></name><name><surname>Crandell</surname><given-names>C. C.</given-names></name></person-group> (<year>1995</year>). <article-title>Speech recognition in noise by hearing-impaired and noise masked normal-hearing listeners.</article-title>
<source><italic>J. Am. Acad. Audiol.</italic></source>
<volume>6</volume>
<fpage>414</fpage>&#x02013;<lpage>424</lpage>.<pub-id pub-id-type="pmid">8580501</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olson</surname><given-names>A. D.</given-names></name><name><surname>Preminger</surname><given-names>J. E.</given-names></name><name><surname>Shinn</surname><given-names>J. B.</given-names></name></person-group> (<year>2013</year>). <article-title>The effect of LACE DVD training in new and experienced hearing aid users.</article-title>
<source><italic>J. Am. Acad. Audiol.</italic></source>
<volume>24</volume>
<fpage>214</fpage>&#x02013;<lpage>230</lpage>. <pub-id pub-id-type="doi">10.3766/jaaa.24.3.7</pub-id><pub-id pub-id-type="pmid">23506666</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>D. G.</given-names></name></person-group> (<year>1997</year>). <article-title>The video toolbox software for visual psychophysics: transforming numbers into movies.</article-title>
<source><italic>Spat. Vis.</italic></source>
<volume>10</volume>
<fpage>437</fpage>&#x02013;<lpage>442</lpage>. <pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>A.</given-names></name><name><surname>Rishiq</surname><given-names>D.</given-names></name><name><surname>Yu</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Abrams</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>Neural correlates of selective attention with hearing aid use followed by ReadMyQuips auditory training program.</article-title>
<source><italic>Ear Hear.</italic></source>
<volume>38</volume>
<fpage>28</fpage>&#x02013;<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0000000000000348</pub-id><pub-id pub-id-type="pmid">27556531</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richie</surname><given-names>C.</given-names></name><name><surname>Kewley-Port</surname><given-names>D.</given-names></name></person-group> (<year>2008</year>). <article-title>The effects of auditory-visual vowel identification training on speech recognition under difficult listening conditions.</article-title>
<source><italic>J. Speech Lang. Hear. Res.</italic></source>
<volume>51</volume>
<fpage>1607</fpage>&#x02013;<lpage>1619</lpage>. <pub-id pub-id-type="doi">10.1044/1092-4388(2008/07-0069)</pub-id><pub-id pub-id-type="pmid">18695021</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riedel</surname><given-names>P.</given-names></name><name><surname>Ragert</surname><given-names>P.</given-names></name><name><surname>Schelinski</surname><given-names>S.</given-names></name><name><surname>Kiebel</surname><given-names>S. J.</given-names></name><name><surname>von Kriegstein</surname><given-names>K.</given-names></name></person-group> (<year>2015</year>). <article-title>Visual face-movement sensitive cortex is relevant for auditory-only speech recognition.</article-title>
<source><italic>Cortex</italic></source>
<volume>68</volume>
<fpage>86</fpage>&#x02013;<lpage>99</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2014.11.016</pub-id><pub-id pub-id-type="pmid">25650106</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name><name><surname>Lunner</surname><given-names>T.</given-names></name><name><surname>Zekveld</surname><given-names>A.</given-names></name><name><surname>S&#x000f6;rqvist</surname><given-names>P.</given-names></name><name><surname>Danielsson</surname><given-names>H.</given-names></name><name><surname>Lyxell</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>The Ease of Language Understanding (ELU) model: theoretical, empirical, and clinical advances.</article-title>
<source><italic>Front. Syst. Neurosci.</italic></source>
<volume>7</volume>:<issue>31</issue>
<pub-id pub-id-type="doi">10.3389/fnsys.2013.00031</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosenblum</surname><given-names>L. D.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Primacy of multimodal speech perception,&#x0201d; in</article-title>
<source><italic>Handbook of Speech Perception</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Pisoni</surname><given-names>D. B.</given-names></name><name><surname>Remez</surname><given-names>R. E.</given-names></name></person-group> (<publisher-loc>Malden, MA</publisher-loc>: <publisher-name>Blackwell</publisher-name>), <fpage>51</fpage>&#x02013;<lpage>78</lpage>.</mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblum</surname><given-names>L. D.</given-names></name><name><surname>Miller</surname><given-names>M.</given-names></name><name><surname>Sanchez</surname><given-names>K.</given-names></name></person-group> (<year>2007</year>). <article-title>Lip-read me now, hear me better later: cross-modal transfer of talker familiarity effects.</article-title>
<source><italic>Psychol. Sci.</italic></source>
<volume>18</volume>
<fpage>392</fpage>&#x02013;<lpage>396</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9280.2007.01911.x</pub-id><pub-id pub-id-type="pmid">17576277</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scarinci</surname><given-names>N.</given-names></name><name><surname>Worrall</surname><given-names>L.</given-names></name><name><surname>Hickson</surname><given-names>L.</given-names></name></person-group> (<year>2012</year>). <article-title>Factors associated with third-party disability in spouses of older people with hearing impairment.</article-title>
<source><italic>Ear Hear.</italic></source>
<volume>33</volume>
<fpage>618</fpage>&#x02013;<lpage>708</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e31825aab39</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schall</surname><given-names>S.</given-names></name><name><surname>Kiebel</surname><given-names>S. J.</given-names></name><name><surname>Maess</surname><given-names>B.</given-names></name><name><surname>von Kriegstein</surname><given-names>K.</given-names></name></person-group> (<year>2013</year>). <article-title>Early auditory sensory processing of voices is facilitated by visual mechanisms.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>77</volume>
<fpage>237</fpage>&#x02013;<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.043</pub-id><pub-id pub-id-type="pmid">23563227</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schall</surname><given-names>S.</given-names></name><name><surname>von Kriegstein</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>). <article-title>Functional connectivity between face-movement and speech-intelligibility areas during auditory-only speech perception.</article-title>
<source><italic>PLoS ONE</italic></source>
<volume>9</volume>:<issue>e86325</issue>
<pub-id pub-id-type="doi">10.1371/journal.pone.0086325</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schelinski</surname><given-names>S.</given-names></name><name><surname>Riedel</surname><given-names>P.</given-names></name><name><surname>von Kriegstein</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>). <article-title>Visual abilities are important for auditory-only speech recognition: evidence from autism spectrum disorder.</article-title>
<source><italic>Neuropsychologia</italic></source>
<volume>65</volume>
<fpage>1</fpage>&#x02013;<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.09.031</pub-id><pub-id pub-id-type="pmid">25283605</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L.</given-names></name><name><surname>Seitz</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>Benefits of multisensory learning.</article-title>
<source><italic>Trends Cogn. Sci.</italic></source>
<volume>12</volume>
<fpage>411</fpage>&#x02013;<lpage>417</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2008.07.006</pub-id><pub-id pub-id-type="pmid">18805039</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L.</given-names></name><name><surname>Wozny</surname><given-names>D. R.</given-names></name><name><surname>Kim</surname><given-names>R.</given-names></name><name><surname>Seitz</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Influences of multisensory experience on subsequent unisensory processing.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>2</volume>:<issue>264</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2011.00264</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><collab>Spr&#x000e5;kbanken (The Swedish Language Bank)</collab> (<year>2011</year>). Available at: <ext-link ext-link-type="uri" xlink:href="http://spraakbanken.gu.se/">http://spraakbanken.gu.se/</ext-link> [accessed November <volume>30</volume>
<issue>2011</issue>]</mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecker</surname><given-names>G. C.</given-names></name><name><surname>Bowman</surname><given-names>G. A.</given-names></name><name><surname>Yund</surname><given-names>E. W.</given-names></name><name><surname>Herron</surname><given-names>T. J.</given-names></name><name><surname>Roup</surname><given-names>C. M.</given-names></name><name><surname>Woods</surname><given-names>D. L.</given-names></name></person-group> (<year>2006</year>). <article-title>Perceptual training improves syllable identification in new and experienced hearing-aid users.</article-title>
<source><italic>J. Rehabil. Res. Dev.</italic></source>
<volume>43</volume>
<fpage>537</fpage>&#x02013;<lpage>552</lpage>. <pub-id pub-id-type="doi">10.1682/JRRD.2005.11.0171</pub-id><pub-id pub-id-type="pmid">17123192</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweetow</surname><given-names>R. W.</given-names></name><name><surname>Sabes</surname><given-names>J. H.</given-names></name></person-group> (<year>2006</year>). <article-title>The need for and development of an adaptive listening and communication enhancement (LACETM) program.</article-title>
<source><italic>J. Am. Acad. Audiol.</italic></source>
<volume>17</volume>
<fpage>538</fpage>&#x02013;<lpage>558</lpage>. <pub-id pub-id-type="doi">10.3766/jaaa.17.8.2</pub-id><pub-id pub-id-type="pmid">16999250</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweetow</surname><given-names>R. W.</given-names></name><name><surname>Sabes</surname><given-names>J. H.</given-names></name></person-group> (<year>2010</year>). <article-title>Auditory training and challenges associated with participation and compliance.</article-title>
<source><italic>J. Am. Acad. Audiol.</italic></source>
<volume>21</volume>
<fpage>586</fpage>&#x02013;<lpage>593</lpage>. <pub-id pub-id-type="doi">10.3766/jaaa.21.9.4</pub-id><pub-id pub-id-type="pmid">21241646</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Kriegstein</surname><given-names>K.</given-names></name><name><surname>Dogan</surname><given-names>&#x000d6;</given-names></name><name><surname>Gr&#x000fc;ter</surname><given-names>M.</given-names></name><name><surname>Giraud</surname><given-names>A. L.</given-names></name><name><surname>Kell</surname><given-names>C. A.</given-names></name><name><surname>Gr&#x000fc;ter</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2008</year>). <article-title>Simulation of talking faces in the human brain improves auditory speech recognition.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U.S.A.</italic></source>
<volume>105</volume>
<fpage>6747</fpage>&#x02013;<lpage>6752</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0710826105</pub-id><pub-id pub-id-type="pmid">18436648</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walden</surname><given-names>B. E.</given-names></name><name><surname>Montgomery</surname><given-names>A. A.</given-names></name><name><surname>Prosek</surname><given-names>R. A.</given-names></name><name><surname>Hawkins</surname><given-names>D. B.</given-names></name></person-group> (<year>1990</year>). <article-title>Visual biasing nor normal and hearing impaired auditory speech perception.</article-title>
<source><italic>J. Speech Lang. Hear. Res.</italic></source>
<volume>33</volume>
<fpage>163</fpage>&#x02013;<lpage>173</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.3301.163</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wayne</surname><given-names>R. V.</given-names></name><name><surname>Johnsrude</surname><given-names>I. S.</given-names></name></person-group> (<year>2012</year>). <article-title>The role of visual speech information in supporting perceptual learning of degraded speech.</article-title>
<source><italic>J. Exp. Psychol. Appl.</italic></source>
<volume>18</volume>
<fpage>419</fpage>&#x02013;<lpage>435</lpage>. <pub-id pub-id-type="doi">10.1037/a0031042</pub-id><pub-id pub-id-type="pmid">23294284</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="book"><collab>World Health Organization</collab> (<year>2012</year>). <source><italic>WHO Global Estimates on Prevalence of Hearing Loss.</italic></source>
<publisher-loc>Geneva</publisher-loc>: <publisher-name>WHO Press.</publisher-name></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>C.</given-names></name><name><surname>Cao</surname><given-names>S.</given-names></name><name><surname>Wu</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>L.</given-names></name></person-group> (<year>2013</year>). <article-title>Temporally pre-presented lipreading cues release speech from informational masking.</article-title>
<source><italic>J. Acoust. Soc. Am.</italic></source>
<volume>133</volume> EL281&#x02013;EL285. <pub-id pub-id-type="doi">10.1121/1.4794933</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname><given-names>E.</given-names></name><name><surname>Cogan</surname><given-names>G. B.</given-names></name><name><surname>Schroeder</surname><given-names>C. E.</given-names></name><name><surname>Poeppel</surname><given-names>D.</given-names></name></person-group> (<year>2013</year>). <article-title>Visual input enhances selective speech envelope tracking in a auditory cortex at a &#x02018;Cocktail Party&#x02019;.</article-title>
<source><italic>J. Neurosci.</italic></source>
<volume>33</volume>
<fpage>1417</fpage>&#x02013;<lpage>1426</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3675-12.2013</pub-id><pub-id pub-id-type="pmid">23345218</pub-id></mixed-citation></ref></ref-list></back></article>
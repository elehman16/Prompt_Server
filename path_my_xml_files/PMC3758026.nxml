<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Med Internet Res</journal-id><journal-id journal-id-type="publisher-id">JMIR</journal-id><journal-title-group><journal-title>Journal of Medical Internet Research</journal-title></journal-title-group><issn pub-type="ppub">1439-4456</issn><issn pub-type="epub">1438-8871</issn><publisher><publisher-name>JMIR Publications Inc.</publisher-name><publisher-loc>Toronto, Canada</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">23963306</article-id><article-id pub-id-type="pmc">3758026</article-id><article-id pub-id-type="publisher-id">v15i8e182</article-id><article-id pub-id-type="doi">10.2196/jmir.2497</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>Effects of Mobile Augmented Reality Learning Compared to Textbook Learning on Medical Students: Randomized Controlled Pilot Study</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Eysenbach</surname><given-names>Gunther</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Brack</surname><given-names>Charlotte</given-names></name></contrib><contrib contrib-type="reviewer"><name><surname>Alipour</surname><given-names>Sadaf</given-names></name></contrib><contrib contrib-type="reviewer"><name><surname>Pimmer</surname><given-names>Christoph</given-names></name></contrib></contrib-group><contrib-group><contrib id="contrib1" contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Albrecht</surname><given-names>Urs-Vito</given-names></name><degrees>Dr med, MPH</degrees><xref ref-type="aff" rid="aff1">1</xref><address><institution>PL Reichertz Institute for Medical Informatics</institution><institution>Hannover Medical School</institution><addr-line>Carl-Neuberg-Str 1</addr-line><addr-line>Hannover, 30625</addr-line><country>Germany</country><phone>49 511532 ext 3508</phone><fax>49 5115322517</fax><email>albrecht.urs-vito@mh-hannover.de</email></address></contrib><contrib id="contrib2" contrib-type="author" equal-contrib="yes"><name><surname>Folta-Schoofs</surname><given-names>Kristian</given-names></name><degrees>Dr rer nat</degrees><xref ref-type="aff" rid="aff2">2</xref></contrib><contrib id="contrib3" contrib-type="author" equal-contrib="yes"><name><surname>Behrends</surname><given-names>Marianne</given-names></name><degrees>Dr rer biol hum</degrees><xref ref-type="aff" rid="aff1">1</xref></contrib><contrib id="contrib4" contrib-type="author" equal-contrib="yes"><name><surname>von Jan</surname><given-names>Ute</given-names></name><degrees>Dr rer biol hum</degrees><xref ref-type="aff" rid="aff1">1</xref></contrib></contrib-group><aff id="aff1" rid="aff1"><sup>1</sup><institution>PL Reichertz Institute for Medical Informatics</institution><institution>Hannover Medical School</institution><addr-line>Hannover</addr-line><country>Germany</country></aff><aff id="aff2" rid="aff2"><sup>2</sup><institution>Institute of Psychology</institution><institution>University of Hildesheim</institution><addr-line>Hildesheim</addr-line><country>Germany</country></aff><author-notes><corresp>Corresponding Author: Urs-Vito Albrecht <email>albrecht.urs-vito@mh-hannover.de</email></corresp></author-notes><pub-date pub-type="collection"><month>8</month><year>2013</year></pub-date><pub-date pub-type="epub"><day>20</day><month>8</month><year>2013</year></pub-date><volume>15</volume><issue>8</issue><elocation-id>e182</elocation-id><history><date date-type="received"><day>15</day><month>12</month><year>2012</year></date><date date-type="rev-request"><day>22</day><month>1</month><year>2013</year></date><date date-type="rev-recd"><day>12</day><month>6</month><year>2013</year></date><date date-type="accepted"><day>29</day><month>6</month><year>2013</year></date></history><permissions><copyright-statement>&#x000a9;Urs-Vito Albrecht, Kristian Folta-Schoofs, Marianne Behrends, Ute von Jan. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 20.08.2013. </copyright-statement><copyright-year>2013</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0/"><license-p><!--CREATIVE COMMONS-->This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0/">http://creativecommons.org/licenses/by/2.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on <ext-link ext-link-type="uri" xlink:href="http://www.jmir.org/,">http://www.jmir.org/,</ext-link> as well as this copyright and license information must be included.</license-p></license></permissions><self-uri xlink:type="simple" xlink:href="http://www.jmir.org/2013/8/e182/"/><abstract><sec sec-type="background"><title>Background</title><p>By adding new levels of experience, mobile Augmented Reality (mAR) can significantly increase the attractiveness of mobile learning applications in medical education.</p></sec><sec sec-type="objective"><title>Objective</title><p>To compare the impact of the heightened realism of a self-developed mAR blended learning environment (mARble) on learners to textbook material, especially for ethically sensitive subjects such as forensic medicine, while taking into account basic psychological aspects (usability and higher level of emotional involvement) as well as learning outcomes (increased learning efficiency).</p></sec><sec sec-type="methods"><title>Methods</title><p>A prestudy was conducted based on a convenience sample of 10 third-year medical students. The initial emotional status was captured using the &#x0201c;Profile of Mood States&#x0201d; questionnaire (POMS, German variation); previous knowledge about forensic medicine was determined using a 10-item single-choice (SC) test. During the 30-minute learning period, the students were randomized into two groups: the first group consisted of pairs of students, each equipped with one iPhone with a preinstalled copy of mARble, while the second group was provided with textbook material. Subsequently, both groups were asked to once again complete the POMS questionnaire and SC test to measure changes in emotional state and knowledge gain. Usability as well as pragmatic and hedonic qualities of the learning material was captured using AttrakDiff2 questionnaires. Data evaluation was conducted anonymously. Descriptive statistics for the score in total and the subgroups were calculated before and after the intervention. The scores of both groups were tested against each other using paired and unpaired signed-rank tests. An item analysis was performed for the SC test to objectify difficulty and selectivity.</p></sec><sec sec-type="results"><title>Results</title><p>Statistically significant, the mARble group (6/10) showed greater knowledge gain than the control group (4/10) (Wilcoxon <italic>z</italic>=2.232, <italic>P</italic>=.03). The item analysis of the SC test showed a difficulty of <italic>P</italic>=0.768 (s=0.09) and a selectivity of RPB=0.2. For mARble, fatigue (<italic>z</italic>=2.214, <italic>P</italic>=.03) and numbness (<italic>z</italic>=2.07, <italic>P</italic>=.04) decreased with statistical significance when comparing pre- and post-tests. Vigor rose slightly, while irritability did not increase significantly. Changes in the control group were insignificant. Regarding hedonic quality (identification, stimulation, attractiveness), there were significant differences between mARble (mean 1.179, CI &#x02212;0.440 to 0.440) and the book chapter (mean &#x02212;0.982, CI &#x02212;0.959 to 0.959); the pragmatic quality mean only differed slightly.</p></sec><sec sec-type="conclusions"><title>Conclusions</title><p>The mARble group performed considerably better regarding learning efficiency; there are hints for activating components of the mAR concept that may serve to fascinate the participants and possibly boost interest in the topic for the remainder of the class. While the small sample size reduces our study&#x02019;s conclusiveness, its design seems appropriate for determining the effects of interactive eLearning material with respect to emotions, learning efficiency, and hedonic and pragmatic qualities using a larger group.</p></sec><sec><title>Trial Registration</title><p>German Clinical Trial Register (DRKS), DRKS-ID: DRKS00004685; https://drks-neu.uniklinik-freiburg.de/drks_web/navigate.do?navigationId=trial.HTML&#x00026;TRIAL_ID=DRKS00004685.</p></sec></abstract><kwd-group><kwd>problem-based learning</kwd><kwd>cellular phone</kwd><kwd>education</kwd><kwd>medical</kwd><kwd>emotions</kwd></kwd-group></article-meta></front><body><sec><title> Introduction</title><p>Mobile Augmented Reality (AR) offers valuable learning opportunities and may have the potential to significantly improve the learning environment and the attractiveness of the learning process. Mobile AR blended learning environments offer a new level of experience for learners, especially in areas such as forensic medicine where ethical constraints may have to be placed on learning specific subjects in real-life scenarios. For nonmedical education, a number of studies have shown beneficial effects for AR-supported study modules. Many of these make use of AR in a mobile setting [<xref ref-type="bibr" rid="ref1">1</xref>,<xref ref-type="bibr" rid="ref2">2</xref>]. If used appropriately, this allows users to &#x0201c;immerse&#x0201d; themselves in the subject at hand [<xref ref-type="bibr" rid="ref3">3</xref>] and to become involved in their own learning process.</p><p>Although there are a number of projects that integrate mobile AR for basic science education, for example, for middle-school or high-school students&#x02014;and some of these also touch on subjects related to medicine [<xref ref-type="bibr" rid="ref4">4</xref>]&#x02014;projects employing such concepts for basic medical education are still rare. Regarding medicine in general, AR-based applications have so far been put to use mostly for supporting diagnostic or therapeutic purposes [<xref ref-type="bibr" rid="ref5">5</xref>,<xref ref-type="bibr" rid="ref6">6</xref>]. Other projects provide more or less complex simulations, such as for surgical training [<xref ref-type="bibr" rid="ref7">7</xref>]. Although these approaches generally use Augmented Reality for complex scenarios, they all have in common that the technology is used in a stationary way that&#x02014;even when used for educational purposes&#x02014;keeps the users emotionally detached from the subject at hand. Often, they only serve to teach physicians about the use of specific tools, such as laparoscopic tools [<xref ref-type="bibr" rid="ref7">7</xref>], or diagnostic methods, where a certain distance to the patient would be kept even in real-life scenarios. Although such projects certainly enhance learning by giving users experiences they would otherwise not be able to have, the aforementioned stationary AR-based diagnostic and training applications also usually do not allow full immersion of the users into the learning experience. They do not make them an integral part of the learning experience, for example, by projecting the learning content on the learner&#x02019;s body and thus potentially evoking emotional responses in them that might have an additional influence on the learning process.</p><p>The current paper describes a methodological approach and study design that can be used for the purpose of measuring basic cognitive and emotional factors that must be dealt with when integrating AR-based mobile applications into medical teaching. To allow experimental testing of the aforementioned approach and study design, a mobile AR-based prototype app (mARble) was developed that can serve to provide medical students and their educators with a versatile mobile learning environment, making it possible to simulate situations that are either ethically problematic or only rarely encountered in real life [<xref ref-type="bibr" rid="ref8">8</xref>-<xref ref-type="bibr" rid="ref11">11</xref>]. This prototype included content for forensic medicine. Education in this field often suffers from specific cases either being unavailable or unusable due to ethical restrictions, since&#x02014;especially when dealing with survivors of a crime&#x02014;additional traumatization must be strictly avoided.</p><p>In the context of this mobile learning environment, the mobile device serves to meet two basic demands for almost realistic wound pattern simulations. First, it is a portable and highly capable multimedia device, making it an ideal choice for using the technology in various learning situations. Second, through its highly advanced features, it even allows for augmented reality in these learning situations. Thus, it becomes possible to provide new, more realistic elements for the learning setting, such as the projection of wound patterns on the skin of the students, and to possibly provide a new learning experience. Using such an approach, the learners themselves become objects in their own learning process. Thus, they may more easily identify themselves with their role as a patient or an assault victim.</p><p>So far, little is known about the impact of mobile AR applications on the learner during the learning process. It is still unclear which emotions and cognitive effects are provoked in the recipient due to a higher level of realism combined with a very personal experience in a simulated setting. According to Edelmann [<xref ref-type="bibr" rid="ref12">12</xref>], emotions may have an influence on various aspects of learning. When an individual processes information, facts are attributed with &#x0201c;subjective significance&#x0201d; based on the triggered emotions and thus become a part of that individual&#x02019;s interpretative system. Depending on the perceived success or failure of the learning process, for example, determined by exams, this can also have an effect on an individual&#x02019;s subjective well-being [<xref ref-type="bibr" rid="ref13">13</xref>]. In general, emotions that are evoked while learning are not only important when considering single individuals. They also have a big influence on the communication processes within groups of learners as well as with their teachers and are thus one of the key factors for overall learning success.</p><p>When taking a closer look at the significance of emotions on the learning process, a number of important questions arise: How can emotions be classified? How can their effects and benefits for the learning process be reliably quantified? In literature, there is currently no uniform scheme that sufficiently covers all of these aspects. This is additionally complicated by the fact that the impact of emotions also depends on the sociocultural context [<xref ref-type="bibr" rid="ref14">14</xref>]. Another problem is that it is hard to differentiate emotional aspects from related psychological concepts such as &#x0201c;motivation&#x0201d; [<xref ref-type="bibr" rid="ref13">13</xref>]. The influence of certain emotions on the success of specific learning methods, for example, if someone is in favor of authoritative or more liberal teaching methods, may also depend on an individual&#x02019;s ideological perspective [<xref ref-type="bibr" rid="ref13">13</xref>]. When trying to describe emotions, subjective assessments must also be taken under consideration since terms such as &#x0201c;disgust&#x0201d;, &#x0201c;modesty&#x0201d;, &#x0201c;fear&#x0201d;, or &#x0201c;insecurity&#x0201d; may not always describe the exact same emotions for different individuals. It is also difficult for people to quantify their emotions exactly since emotions are regularly perceived on an instinctive, subjective, and nonverbal level.</p><p>For the purpose of the current paper, three core dimensions were identified for the evaluation of our AR-based learning environment, specifically to be able to confirm our expectations that emotional involvement during the learning process as well as learning efficiency for students learning with mobile augmented reality rise compared to those using only textbooks.</p><p>The first dimension was defined as learning effectiveness, which quantifies the influence of a learning method on the acquisition of knowledge. It was of special interest to investigate whether an improvement in knowledge is possible by means of training based on a specific learning method. The second dimension deals with the learning experience itself. This includes the usability of the provided material (practicability), the user&#x02019;s identification with the learning method, the stimulation it provides, as well as its attractiveness for students and educators. The third dimension that was identified as having an influence on learning success, as indicated above, is emotion. There may be a change in the emotional status of learners after using a certain learning method. This could speak to an additional emotional involvement that may be due to the chosen learning method, for example, additional realism when using modern tools and applications that integrate augmented reality. The students become their own learning subjects, which offers a chance for experiencing an additional layer of learning: potential personal involvement.</p></sec><sec><title> Methods</title><sec><title>Participants</title><p>Ten third-year medical students (6 male, 4 female, mean age: 23.7 years, standard deviation: 2 years) were included in the prestudy after giving their informed consent to participate in the trial. The students had not previously participated in any regular courses dealing with the learning topic presented during the trial. Since all participants had already completed the mandatory curriculum of medical informatics, where, aside from theoretical knowledge, they were also introduced to practical aspects of using computers, it was assumed that all of them had attained at least a basic level of computer literacy.</p><p>The study was approved by the Institutional Review Board of Hannover Medical School, (ID: 1653-2012).</p><p>As shown in <xref ref-type="fig" rid="figure1">Figure 1</xref>, to measure the emotional state of the students before the training session, all students were asked to fill out the German variation of the &#x0201c;Profile of Mood States&#x0201d; (POMS) questionnaire within a period of 5 minutes (T3a). To establish a baseline with respect to a priori knowledge of the learning topic, a 10-question standard multiple choice test about &#x0201c;gunshot wounds&#x0201d; (T1a) was given to the students, which they were asked to complete within 15 minutes. After the initial testing, the students were randomly assigned to two subgroups, named group A (6 students) and group B (4 students). Group B participated in the conservative learning situation, finding themselves in a quiet room and reading a 10-page excerpt from a standard textbook in forensic medicine [<xref ref-type="bibr" rid="ref15">15</xref>] about &#x0201c;gunshot wounds&#x0201d;. The students were instructed to read and learn about the topic using the textbook material for a learning period of 30 minutes. While learning, they were allowed to use additional supplies such as pencils, pens, and paper to take notes and highlighters to work in the provided copies of the text material. Also, the students were free to discuss the learning topics and were specifically instructed to interact freely with other participants in their group. After 30 minutes, the students were again asked to complete the previous standard multiple choice tests comprising 10 questions (T1b). During the tests, the participants were not allowed to refer to the textbook material or their notes; they were given 15 minutes to complete the test (10 questions, 90 seconds for each answer). Afterwards, the students were asked to provide information about their learning experience (T2, 10 minutes). The POMS (T3b) questionnaire was administered to determine their emotional state after the training session. During the trial, a member of the study personnel was placed in the same room for direct observation (T4) and also to provide feedback to the students if necessary. At the end, the study participants were thanked and invited to come back on another day if they wished to try out mARble.</p><p>Group A joined the interventional arm of the trial. The group was divided into 3 pairs and each subgroup received an iPhone 4, on which the app &#x0201c;mARble Forensics&#x0201d; had already been installed, and a set of 3 paper markers. After a short greeting and introduction to the application, working with the provided markers and the learning task, the 3 pairs of students were directed into the corners of another quiet room, away from group B. The task was to learn about &#x0201c;gunshot wounds&#x0201d; using the provided iPhone and the preinstalled mARble application. The information in the mARble application contained all information relevant for later solving the multiple choice test. After 30 minutes, the students of group A had to solve the same tests as those in group B (T1b), including the multiple choice test, a questionnaire about their learning experience (T2), and the POMS questionnaire (T3b) to determine their emotional state. Just as for group B, during the trial, a member of the study personnel stayed with the students of group A for providing feedback and for direct observation (T4). After completion of all tests, participants were thanked and dismissed. The complete timeline of the individual test elements for both groups is shown in <xref ref-type="fig" rid="figure1">Figure 1</xref>.</p><fig id="figure1" position="float"><label>Figure 1</label><caption><p>Timeline and applied tests. In the text, individual blocks are referenced via labels (T1a/b, T2, and T3a/b).</p></caption><graphic xlink:href="jmir_v15i8e182_fig1"/></fig></sec><sec><title> Learning Material Provided</title><sec><title> The Application</title><p>mARble is an iOS application that was developed at the Peter L. Reichertz Institute for Medical Informatics (PLRI) at the Hannover Medical School. Using AR, virtual information can be linked to objects in the real environment, thus providing an additional layer of information to the users [<xref ref-type="bibr" rid="ref10">10</xref>]. Code and content for mARble are kept separately. Information can easily be edited or added based on an XML-based file format. The content for the forensic module of mARble used during the course of our study was derived from and corresponded to the textbook-based learning material that was provided to the group of learners belonging to the &#x0201c;conventional&#x0201d; learning group.</p><p>Based on this module, mARble was able to detect and interpret predefined markers representing various pathologies commonly found in forensic medicine. Each marker corresponded to a wound pattern that the students were expected to explore. When placing a marker on the student&#x02019;s body, for example, on the neck, the image acquired by the iPhone&#x02019;s camera was automatically overlaid with the corresponding wound pattern, such as the entrance wound of a bullet, as seen in <xref ref-type="fig" rid="figure2">Figure 2</xref>. Through the virtual flashcard system included in mARble, it was also possible to view textual and multimedia background information (ie, images, drawings, video, and audio) linked to the current marker and work with the provided questions and tasks (<xref ref-type="fig" rid="figure2">Figure 2</xref>). Through the described approach, learners were able to construct various fictive cases by combining markers for the desired set of findings. The learning process could be documented by adding snapshots of the augmented image to a personal image gallery. Previously taken snapshots and findings could be used for review, for discussions with fellow students, or presentation purposes. When reviewing an image, it was also possible to trigger the corresponding background information as well as associated questions and tasks. Using their iPhone, students were able to examine the provided wound patterns either on themselves or on their partner; thus, they could easily immerse themselves in the learning topic.</p></sec><sec><title> The Conventional Learning Material</title><p>We chose the 10-page chapter &#x0201c;gunshot wounds&#x0201d; of a popular short compendium of forensic medicine in Germany [<xref ref-type="bibr" rid="ref15">15</xref>] as learning material for group B. The textbook is very well-equipped with color and black/white pictures, schemes, and tables, as well as small repetitive summaries in colored boxes. Roughly 50% of the material consists of images and drawings.</p><fig id="figure2" position="float"><label>Figure 2</label><caption><p>The mobile Augmented Reality blended learning environment with the module &#x0201c;Forensic Medicine&#x0201d;: AR simulation of a gunshot wound and connected multimedia content.</p></caption><graphic xlink:href="jmir_v15i8e182_fig2"/></fig></sec></sec><sec><title> Overall Learning Experience: Evaluation Tools</title><sec><title> Learning Success: Multiple Choice Test (T1a, T1b)</title><p>A paper-based test consisting of 10 questions with single choice answers was used to measure the learning effectiveness. The questions and related answers were collected from a pool of material compiled by a member of the staff of the forensic medicine department. Before the trial, two members of the staff evaluated the multiple choice questionnaire with respect to comprehensibility, solvability, and time consumption. Beforehand, both the textbook extracts as well as the content provided in mARble were reviewed to determine whether the content necessary for answering all questions was sufficiently covered. Also, an item analysis before and after the learning period was conducted to take into account test difficulty (p), item discrimination (RPB), and item selectivity.</p></sec><sec><title>Statistical Analysis</title><p>To determine learning effectiveness (T1a and T1b), descriptive statistics were calculated, including the mean, standard deviation (SD), and mean for the score in total and the subgroups, before and after the intervention. In a noninferiority design (unpaired rank sum, Mann-Whitney U, 2-sided, Cronbach alpha=.05), the scores reached by learning with mARble were tested against the scores achieved when using the classical learning material. The calculation of T1a and T1b was based on the sum of the item values. All questionnaires were included. Those with one missing item per scale were corrected with the scale mean.</p></sec><sec><title> Learning Experience (T2): AttrakDiff2</title><p>Over the past few years, &#x0201c;user experience&#x0201d; has become an important factor for the acceptance of all technical innovations. Nevertheless, only vague definitions exist and there are many unanswered questions concerning the factors contributing to a good user experience [<xref ref-type="bibr" rid="ref16">16</xref>]. Hassenzahl et al [<xref ref-type="bibr" rid="ref17">17</xref>-<xref ref-type="bibr" rid="ref19">19</xref>] designed a model that classifies the attributes necessary for describing products according to their pragmatic or hedonic quality and can thus be employed to describe the subjective attractiveness. This experience design concept was integrated into the test design. Following Hassenzahl&#x02019;s theoretical work model, the pragmatic and hedonic qualities of an application influence a user&#x02019;s subjective perception of attractiveness, resulting in respective behavioral and emotional responses. In this context, hedonic quality describes the emotional impact of a product or system, while measuring pragmatic quality offers insights into its usability or usefulness [<xref ref-type="bibr" rid="ref20">20</xref>].</p><p>AttrakDiff2 [<xref ref-type="bibr" rid="ref19">19</xref>] was developed as a tool by Hassenzahl&#x02019;s research group to be able to quantify these qualities. The tool uses 4x7 anchor scales, in total 28 questions. The anchors are presented in the form of semantic differentials and a 7-point Likert scale is employed for rating the intensity of the items. The poles of each item are opposite adjectives (eg, &#x0201c;confusing-clear&#x0201d;, &#x0201c;unusual-ordinary&#x0201d;, &#x0201c;good-bad&#x0201d;). Each of the mean values of an item group creates a scale value for pragmatic quality (PQ), hedonic stimulation (HQ-S), hedonic identification (HQ-I), and attractiveness (ATT).</p><p>Attributes in the PQ group describe how easy the user finds it to work with the provided program or environment. Pairs of words belonging in this group are for example &#x0201c;technical vs human&#x0201d;, &#x0201c;complicated vs simple&#x0201d;, or &#x0201c;impractical vs practical&#x0201d;.</p><p>Attributes belonging to HQ-S describe factors that encourage the personal growth of users and provide stimulation to give them the opportunity to enhance their knowledge and development. Stimulating factors can be delivered in many different ways, such as by presenting things in a novel way or by providing a new interaction style. Anchors for hedonic stimulation include attributes such as &#x0201c;professional vs unprofessional&#x0201d;, &#x0201c;stylish vs tacky&#x0201d;, or &#x0201c;isolating vs connective&#x0201d;.</p><p>The attributes falling into the HQ-I category make it possible to identify the social impact that using a product can have for users, including the &#x0201c;messages&#x0201d; that are communicated by using the evaluated product [<xref ref-type="bibr" rid="ref20">20</xref>]. Anchors belonging in this group are, for example, &#x0201c;ordinary vs novel&#x0201d;, &#x0201c;conservative vs innovative&#x0201d;, or &#x0201c;undemanding vs challenging&#x0201d;.</p><p>Last, the attributes of the ATT group depict the overall experience a product has to offer to its users, that is, its attractiveness. Contributing attributes are, for example, &#x0201c;pleasant vs unpleasant&#x0201d;, &#x0201c;ugly vs attractive&#x0201d;, or &#x0201c;appealing vs unappealing&#x0201d;.</p></sec><sec><title>Statistical Methods</title><p>A Mann-Whitney U test for independent random sampling (Cronbach alpha=.05) was conducted to discriminate a possibly significant difference within the categories. Overall scores for the individual 28 attributes as well as aggregated values for each category were obtained by calculating the average values of the ratings provided by the users. To better visualize the relationship between pragmatic and hedonic qualities, the values calculated for PQ are shown on one axis and the values for HQ-I as well as HQ-S on the other axis. Combined with the confidence interval (CI) for each value, the values obtained for the ratings allow a clear differentiation between students using text-based learning or mARble.</p></sec><sec><title> Emotional Involvement (T3a+T3b): POMS Questionnaire, German Version</title><p>To measure the emotional state and possible psychological distress before (T3a) and after (T3b) the learning phase, we asked all students to answer the German variation of the POMS questionnaire by McNair et al [<xref ref-type="bibr" rid="ref21">21</xref>]. This version, modified by Biehl, Dangel, and Reiser [<xref ref-type="bibr" rid="ref22">22</xref>], consists of 35 items (adjectives) that can be divided into 4 groups describing mood disturbances, including fatigue-inertia (14 items), vigor-activity (7 items), tension-anxiety (7 items), and depression-dejection (7 items). Participants rated each item on a 7-point rating scale according to the experienced intensity (eg, &#x0201c;not at all&#x0201d;, &#x0201c;very little&#x0201d;, &#x0201c;a little&#x0201d;, &#x0201c;somewhat&#x0201d;, &#x0201c;fairly&#x0201d;, &#x0201c;strongly&#x0201d;, &#x0201c;very strongly&#x0201d;) of the corresponding mood disturbance. The triggering question was formulated as: &#x0201c;How do you rate your current emotional state?&#x0201d;</p><p>The study participants were asked to finish the survey in approximately 5 minutes. Internal consistency estimates range between Cronbach alpha=.89 and Cronbach alpha=.95 [<xref ref-type="bibr" rid="ref23">23</xref>]. We decided to use this instrument due to its known validity and its broad usage in medical [<xref ref-type="bibr" rid="ref24">24</xref>-<xref ref-type="bibr" rid="ref26">26</xref>] and psychological [<xref ref-type="bibr" rid="ref27">27</xref>] disciplines.</p></sec><sec><title> Direct Observation of the Participants (T4)</title><p>To be able to extensively evaluate the learning situation, observations were included in the study design. A nonparticipant observation was chosen for data collection. During the trial, the behavior of participants from both groups was observed by trained personnel. The observers, all having at least 5 years of experience teaching at a university level, were required to focus on a priori defined criteria of learning behavior as the primary basis for organizing and reporting results. Notes of additional observations of any kind were allowed and were used for further qualitative analysis. The development of observation criteria referred to statements of Schulmeister [<xref ref-type="bibr" rid="ref28">28</xref>] on learning psychology-based factors of virtual teaching and learning, where the importance of social presence in learning settings is emphasized.</p><p>This well-known classification provided the basis for observing both groups in order to examine possible differences in participants&#x02019; learning behavior. According to this classification, the following observation criteria have been selected: (1) student&#x02019;s communication and interactivity with peers, (2) student&#x02019;s focus on or distraction from the learning material, and (3) the way the student dealt with the learning object (learning material).</p></sec></sec></sec><sec><title> Results</title><sec><title> Learning Success: Multiple Choice Test (T1a, T1b)</title><p>Comparing the results of the multiple choice tests before and after the learning period, on average, all participants showed an improvement regarding correct answers (<xref ref-type="fig" rid="figure3">Figure 3</xref>). Stratified for the learning method, the improvement was higher in the mARble group with 4.7 questions (SD 2.9) compared to the control group showing an improvement of 3 questions, but also with smaller variability (SD 1.5). The difference in improvement within the mARble group was statistically significant (Wilcoxon, <italic>z</italic>=2.232, <italic>P</italic>=.03). The multiple choice test difficulty was calculated with <italic>P</italic>=0.768 (SD 0.09) with an item discrimination of RPB=0.2.</p></sec><sec><title> Learning Experience (T2): AttrakDiff2</title><p>Statistical analysis revealed significant differences between the mARble and the textbook groups (<xref ref-type="table" rid="table1">Table 1</xref>) for the hedonic qualities, HQ-S &#x0201c;stimulation&#x0201d; (Mann-Whitney U, <italic>z</italic>=6.506, <italic>P</italic>&#x0003c;.001), HQ-I &#x0201c;identification&#x0201d; (Mann-Whitney U, <italic>z</italic>=2.825, <italic>P</italic>=.005), and ATT &#x0201c;attractiveness&#x0201d; (Mann-Whitney U, <italic>z</italic>=5.179, <italic>P</italic>&#x0003c;.001). mARble obtained more positive ratings. The confidence interval (CI) for the hedonic quality of the mARble group was smaller than for the textbook group (<xref ref-type="fig" rid="figure4">Figure 4</xref>), since mARble&#x02019;s users were more consistent in their evaluation; therefore, mARble&#x02019;s ratings were applied with greater certainty. When comparing the values for pragmatic quality, the textbook group performed better than the mARble group, although this difference is not statistically significant (Mann-Whitney U, <italic>z</italic>=&#x02212;1.616, <italic>P</italic>=.11). However, for the identity aspect of hedonic quality, the mARble group performed significantly better than the textbook group (Mann-Whitney U, <italic>z</italic>=2.825 <italic>P</italic>=.005). Furthermore, considering the hedonic quality &#x0201c;stimulation&#x0201d;, participants in the mARble group performed much better than the textbook group (Mann-Whitney U, <italic>z</italic>=6.506, <italic>P</italic>&#x0003c;.001). This resulted in a difference in participants&#x02019; ratings of attractiveness (Mann-Whitney U, <italic>z</italic>=5.179, <italic>P</italic>&#x0003c;.001), with mARble again receiving better ratings (<xref ref-type="fig" rid="figure4">Figures 4</xref> and <xref ref-type="fig" rid="figure5">5</xref>). <xref ref-type="fig" rid="figure6">Figure 6</xref> describes the profile of the mean values and standard deviations for the word pairs stratified for the learning methods.</p><fig id="figure3" position="float"><label>Figure 3</label><caption><p>Number of incorrectly and correctly answered questions before (left) and after (right) the learning period.</p></caption><graphic xlink:href="jmir_v15i8e182_fig3"/></fig><table-wrap id="table1" position="float"><label>Table 1</label><caption><p>Aggregated values calculated for the 4 qualities covered by AttrakDiff2: pragmatic quality (PQ), identification (HQ-I), stimulation (HQ-S), and attractiveness (ATT).</p></caption><table frame="hsides" rules="groups" width="628" border="0" cellpadding="2" cellspacing="0"><col width="162" span="1"/><col width="111" span="1"/><col width="112" span="1"/><col width="111" span="1"/><col width="112" span="1"/><thead><tr valign="top"><td rowspan="1" colspan="1">Group</td><td rowspan="1" colspan="1">PQ, <break/>
mean (SD)</td><td rowspan="1" colspan="1">HQ-I, <break/>
mean (SD)</td><td rowspan="1" colspan="1">HQ-S, <break/>
mean (SD)</td><td rowspan="1" colspan="1">ATT, <break/>
mean (SD)</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">A: mARble (n=6)</td><td rowspan="1" colspan="1">0.381 (1.168)</td><td rowspan="1" colspan="1">0.9048 (0.932)</td><td rowspan="1" colspan="1">1.452 (1.087)</td><td rowspan="1" colspan="1">1.24 (0.726)</td></tr><tr valign="top"><td rowspan="1" colspan="1">B: textbook (n=4)</td><td rowspan="1" colspan="1">0.857 (1.758)</td><td rowspan="1" colspan="1">&#x02212;0.143 (1.780)</td><td rowspan="1" colspan="1">&#x02212;1.821 (1.39)</td><td rowspan="1" colspan="1">&#x02212;0.57 (1.451)</td></tr></tbody></table></table-wrap><fig id="figure4" position="float"><label>Figure 4</label><caption><p>Portfolio with average values of the dimensions PQ and HQ and the respective confidence rectangles of A (mARble) and B (textbook) on left, modified following Hassenzahl et al; corresponding values on right.</p></caption><graphic xlink:href="jmir_v15i8e182_fig4"/></fig><fig id="figure5" position="float"><label>Figure 5</label><caption><p>Average values for pragmatic quality (PQ), hedonic quality &#x02013; identification (HQ-I), hedonic quality &#x02013; stimulation (HQ-S), and attractiveness (ATT), based on evaluation of the AttrakDiff2 questionnaire (solid line: mARble group (6/10); dashed line: textbook group (4/10)).</p></caption><graphic xlink:href="jmir_v15i8e182_fig5"/></fig><fig id="figure6" position="float"><label>Figure 6</label><caption><p>Description of word-pairs and calculated values: comparison between mARble (solid line) and the textbook material (dashed line).</p></caption><graphic xlink:href="jmir_v15i8e182_fig6"/></fig></sec><sec><title> Emotional Involvement (T3a+T3b): POMS Questionnaire, German Version</title><p>In our study, POMS was used to measure a change in the emotional state change before and after learning. The answers according to the dimensions numbness, fatigue, vigor, and irritability are shown in <xref ref-type="table" rid="table2">Table 2</xref> and visualized in <xref ref-type="fig" rid="figure7">Figure 7</xref> and were aggregated from the item values recorded for both groups of participants. A comparison of pre- and post-test values (<xref ref-type="fig" rid="figure7">Figure 7</xref>) showed a statistically significant decrease of fatigue (<italic>z</italic>=2.214, <italic>P</italic>=.03) and numbness (<italic>z</italic>=2.07, <italic>P</italic>=.04) for the mARble group; vigor increased slightly. Irritability did not change significantly (<italic>z</italic>=1.166, <italic>P</italic>=.24). The control group did not show significant changes on any of the variables.</p></sec><sec><title> Direct Observation of the Participants (T4)</title><sec><title> Group A: mARble</title><p>Nonparticipant observations of the two groups showed a highly heterogeneous pattern of results. Group A (alternative learning method mARble) comprised 6 participants (P1-6) that were assigned to one of three subgroups (SG1 to SG3). In the beginning of the learning phase, all students explored the functionality of the mARble app on the iPhones. They used the marker and tried to get the picture of the linked object on their iPhone display. There was one group consisting of male participants, one with female participants, and one mixed group. Both same-sex groups showed a high level of interaction, placing the markers on their bodies, discussing the content, and taking notes. Although the mixed group also interactively used and discussed the content, they refrained from placing the markers on their skin, instead simply placing them on the table.</p></sec><sec><title> Group B: Textbook</title><p>Group B (conservative learning method) consisted of 4 participants (P1-P4) being paired and assigned to two subgroups (SG1: male, male; SG2: female, male). All participants were instructed to learn in their usual manner, but also asked to discuss the text material with their learning partner and with the whole group if they desired. Still, there were almost no dialogues with either the whole group or between the members of SG1 or SG2. From the beginning of the learning phase, all participants worked in a focused and concentrated manner, and read the text quietly. There were only very short interruptions: two participants briefly talked to each other and there was one distraction due to disruptive environmental influences. Only near the end of the learning phase was there some exchange between the participants. This did not exclusively relate to the learning content but also encompassed private matters. Further kinds of interactivity (other than communication) did not take place. The way students had worked with the text could be tracked by looking at what they had highlighted or underlined.</p><table-wrap id="table2" position="float"><label>Table 2</label><caption><p>Aggregated values for numbness, vigor, fatigue, and irritability.</p></caption><table frame="hsides" rules="groups" width="639" border="0" cellpadding="8" cellspacing="0"><col width="125" span="1"/><col width="43" span="1"/><col width="87" span="1"/><col width="96" span="1"/><col width="96" span="1"/><col width="96" span="1"/><thead><tr valign="top"><td rowspan="1" colspan="1">Group</td><td rowspan="1" colspan="1">Phase</td><td colspan="4" rowspan="1">Dimensions</td></tr><tr valign="top"><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">Numbness, <break/>
mean (SD)</td><td rowspan="1" colspan="1">Vigor, <break/>
mean (SD)</td><td rowspan="1" colspan="1">Fatigue, <break/>
mean (SD)</td><td rowspan="1" colspan="1">Irritability, <break/>
mean(SD)</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">
<bold>A: mARble (n=6)</bold>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td></tr><tr valign="top"><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">pre</td><td rowspan="1" colspan="1">19.5 (4.637)</td><td rowspan="1" colspan="1">21.0 (4.561)</td><td rowspan="1" colspan="1">24.33 (3.204</td><td rowspan="1" colspan="1">9.17 (2.041)</td></tr><tr valign="top"><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">post</td><td rowspan="1" colspan="1">15.5 (3.209)</td><td rowspan="1" colspan="1">23.83 (9.326)</td><td rowspan="1" colspan="1">18.0 (4.147)</td><td rowspan="1" colspan="1">8.17 (1.472)</td></tr><tr valign="top"><td rowspan="1" colspan="1">
<bold>B: textbook (n=4)</bold>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">
<break/>
</td></tr><tr valign="top"><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">pre</td><td rowspan="1" colspan="1">23.5 (2.887)</td><td rowspan="1" colspan="1">29.5 (9.037)</td><td rowspan="1" colspan="1">20.75 (9.570)</td><td rowspan="1" colspan="1">17.0 (5.944)</td></tr><tr valign="top"><td rowspan="1" colspan="1">
<break/>
</td><td rowspan="1" colspan="1">post</td><td rowspan="1" colspan="1">19.5 (4.041)</td><td rowspan="1" colspan="1">29.0 (7.616)</td><td rowspan="1" colspan="1">20.0 (7.528)</td><td rowspan="1" colspan="1">10.25 (3.594)</td></tr></tbody></table></table-wrap><fig id="figure7" position="float"><label>Figure 7</label><caption><p>Learning affection: pre-test (dashed line) and post-test (solid line) comparison of the aggregated values for numbness, vigor, fatigue, and irritability for both groups.</p></caption><graphic xlink:href="jmir_v15i8e182_fig7"/></fig></sec></sec></sec><sec><title> Discussion</title><sec><title>Principal Findings</title><p>The use of mobile devices, especially when augmented reality comes into the picture, can considerably change the learning experience as well as shift it to an entirely new level [<xref ref-type="bibr" rid="ref3">3</xref>,<xref ref-type="bibr" rid="ref29">29</xref>], thereby providing learning experiences that are simply not possible in a conventional learning setting.</p><p>For the study, a mobile AR learning environment was developed for almost realistic wound pattern simulations in medical settings, where learners become emotionally involved in their learning process. For us, it was important to determine whether the use of AR-based solutions might trigger negative emotions or irritations in learners: when learning with mARble, by placing the markers representing specific findings on their bodies and viewing the findings on their own skin, they become emotionally involved and a part of the learning process. For the current study, we carefully investigated the emotional and cognitive impact mobile AR applications might have on the learner and the learning process in contrast to a control group of subjects that learned in a conventional medical learning environment. Since emotional reactions are often domain-specific [<xref ref-type="bibr" rid="ref30">30</xref>], the control group served to make sure that the measured effects were not caused by the content but by the learning medium.</p><p>Although our study showed that both learning environments induced significant cognitive improvement with respect to an increased knowledge, in direct comparison, the mARble group performed significantly better.</p><p>Regarding pragmatic quality, both methods were given average ratings. While mARble&#x02019;s user interface was interpreted as fairly self-oriented, the textbook was rated as rather task-oriented. Nevertheless, mARble is much more attractive to its users. The ratings also point to a significant stimulation offered by mARble. Solely in terms of pragmatic quality, the textbook material was located in the above-average region. It meets ordinary standards with regard to hedonic quality&#x02013;identity. For hedonic quality&#x02013;stimulation, the textbook is located in the below-average region. It does not have a stimulating effect on users. Insufficient stimulation results in a lack of motivation when using the product. Should products of similar pragmatic quality be available, users would gladly change products. The attractiveness value is located in the average region. The overall impression of the product is moderately attractive.</p><p>In comparison to the textbook material, mARble obtained better ratings with respect to vigor; its users were less fatigued after using mARble. There was no indication of irritating properties for either of the two learning methods. Similarly, during observation, the participants showed no signs of emotional irritation. Nonetheless, the different behavior of the participants in the two groups suggests that learning with mARble and using markers on the body might provoke emotions such as shame or shyness. The participants in the gender-mixed groups did not use the markers directly on their skin but only on the table&#x02019;s surface. However, these aspects require further investigation.</p></sec><sec><title>Limitations</title><p>It might be argued whether the observed results were due to the medium, textbook vs mARble, or rather due to the chosen approach, individual learning vs social interaction. Both factors are probably closely interrelated. For example, for the control group learning with the textbook material, discussion and interaction between the participants was not prohibited in any way; rather, students were specifically asked to interact with other members of their learning group if they desired. Although they sat close to each other, they chose individual learning instead of social interaction. However, due to the limited number of participants in this prestudy, we refrained from specifically using separate groups for testing both textbook and mARble in individual learning sessions as well as in an interactive way, and instead let the participants choose what suited them best.</p><p>Nevertheless, the effects of both learning approaches cannot be completely separated from the effects of the chosen learning material. Even if students learning with textbooks were to choose social interaction during their learning phase, they would still need time periods to read the material. On the other hand, with AR-based approaches, they can collaboratively use the presented material by listening to the content, looking at overlaid images and additional material right away, which encourages social interaction. Still, the possible bias of the results we achieved by direct comparison between the textbook-based learning, which is assumed to be an individual learning method when the students are not explicitly ordered to collaborate vs the AR-based learning, where we expected a more collaborative learning process by deploying one device to each pair of students, remains a limitation of the presented study. An upcoming study should consider this and take the effects of social interaction or missing interaction during the learning process into account, for example by using individual as well interactive setups for both textbook-based and AR-based learning and comparing the results not only between the learning methods themselves but also between collaborative as well as individual learning settings for both methods.</p><p>Additionally, the multiple choice questions used for rating the students&#x02019; performance should be selected more carefully. When looking at the results of the item analysis of the multiple choice questions, it became clear that the item difficulty and item discrimination of the questions used during the presented study still had room for improvement. This could, for example, be alleviated by selecting items where the values for these criteria are already known, such as by choosing questions from previously conducted official exams rather than self-developed items.</p></sec><sec><title>Conclusions</title><p>Although limited by the small sample size, and the limited amount of time and content available, the chosen evaluation setup allowed for certain conclusions regarding the desired factors; a study using a larger group of participants, based on our current study&#x02019;s design, may provide more conclusive results regarding various aspects of interactive mobile learning tools such as mARble in comparison to conventional learning material.</p></sec></sec></body><back><ack><p>We would like to thank Bernhard H&#x000e4;ussermann and Felix Str&#x000fc;bing for technical support during the development of mARble, Anke Mittelst&#x000e4;dt, Christoph Noll for assistance with the photography, and Dr. J&#x000f6;rn Kr&#x000fc;ckeberg for stimulating discussions. Special thanks also go to Prof Dr Herbert Matthies (PLRI) and Prof Dr Michael Klintschar, Institute of Legal Medicine, Hannover Medical School for their effort and expertise while conducting this study. We also acknowledge support by Deutsche Forschungsgemeinschaft for the publication costs. The manuscript was checked against the CONSORT-EHEALTH checklist [<xref ref-type="bibr" rid="ref31">31</xref>].</p></ack><fn-group><fn fn-type="conflict"><p>Conflicts of Interest: None declared.</p></fn></fn-group><glossary><title>Abbreviations</title><def-list><def-item><term id="abb1">AR</term><def><p>Augmented Reality</p></def></def-item><def-item><term id="abb2">mAR</term><def><p>mobile Augmented Reality</p></def></def-item><def-item><term id="abb3">mARble</term><def><p>mobile Augmented Reality blended learning environment</p></def></def-item><def-item><term id="abb4">PLRI</term><def><p>Peter L Reichertz Institute for Medical Informatics</p></def></def-item><def-item><term id="abb5">POMS</term><def><p>Profile of Mood States questionnaire</p></def></def-item><def-item><term id="abb6">SC</term><def><p>single choice</p></def></def-item></def-list></glossary><ref-list><ref id="ref1"><label>1</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Parhizkar</surname><given-names>B</given-names></name><name><surname>Gebril</surname><given-names>ZM</given-names></name><name><surname>Obeidy</surname><given-names>WK</given-names></name><name><surname>Ngan</surname><given-names>MNA</given-names></name><name><surname>Chowdhury</surname><given-names>SA</given-names></name><name><surname>Lashkari</surname><given-names>AH</given-names></name></person-group><article-title>Android mobile augmented reality application based on different learning theories for primary school children</article-title><source>Proceedings of the International Conference on Multimedia Computing and Systems</source><year>2012</year><conf-name>Multimedia Computing and Systems (ICMCS), International Conference</conf-name><conf-date>May 10-12, 2012</conf-date><conf-loc>Tangier, Morocco</conf-loc><fpage>404</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1109/ICMCS.2012.6320114</pub-id></element-citation></ref><ref id="ref2"><label>2</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>YC</given-names></name><name><surname>Buchholz</surname><given-names>H</given-names></name><name><surname>Brosda</surname><given-names>C</given-names></name><name><surname>Bogner</surname><given-names>FX</given-names></name></person-group><article-title>Evaluation of a portable and interactive augmented reality learning system by teachers and students</article-title><source>Augmented Reality in Education</source><year>2011</year><conf-name>EDEN - 2011 Open Classroom Conference</conf-name><conf-date>October 27 - 29, 2011</conf-date><conf-loc>Ellinogermaniki Agogi, Athens, Greece</conf-loc><fpage>41</fpage><lpage>50</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.sctg.eu/materials/sctgo_proceedings_low.pdf"/></comment></element-citation></ref><ref id="ref3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dede</surname><given-names>C</given-names></name></person-group><article-title>Immersive interfaces for engagement and learning</article-title><source>Science</source><year>2009</year><month>1</month><day>2</day><volume>323</volume><issue>5910</issue><fpage>66</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1126/science.1167311</pub-id><pub-id pub-id-type="medline">19119219</pub-id><!--<pub-id pub-id-type="pii">323/5910/66</pub-id>--><pub-id pub-id-type="pmid">19119219</pub-id></element-citation></ref><ref id="ref4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vilkoniene</surname><given-names>M</given-names></name></person-group><article-title>Influence of augmented reality technology upon pupils' knowledge about human digestive system: The results of the experiment</article-title><source>US-China Education Review</source><year>2009</year><volume>6</volume><issue>1</issue><fpage>36</fpage><lpage>43</lpage></element-citation></ref><ref id="ref5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weidert</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>von der Heide</surname><given-names>A</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Euler</surname><given-names>E</given-names></name></person-group><article-title>[Intraoperative augmented reality visualization. Current state of development and initial experiences with the CamC]</article-title><source>Unfallchirurg</source><year>2012</year><month>3</month><volume>115</volume><issue>3</issue><fpage>209</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1007/s00113-011-2121-8</pub-id><pub-id pub-id-type="medline">22406917</pub-id><pub-id pub-id-type="pmid">22406917</pub-id></element-citation></ref><ref id="ref6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten-Oertel</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>SS</given-names></name><name><surname>Drouin</surname><given-names>S</given-names></name><name><surname>Sinclair</surname><given-names>DS</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><article-title>Augmented reality visualization for guidance in neurovascular surgery</article-title><source>Stud Health Technol Inform</source><year>2012</year><volume>173</volume><fpage>225</fpage><lpage>9</lpage><pub-id pub-id-type="medline">22356991</pub-id><pub-id pub-id-type="pmid">22356991</pub-id></element-citation></ref><ref id="ref7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nugent</surname><given-names>E</given-names></name><name><surname>Shirilla</surname><given-names>N</given-names></name><name><surname>Hafeez</surname><given-names>A</given-names></name><name><surname>O'Riordain</surname><given-names>DS</given-names></name><name><surname>Traynor</surname><given-names>O</given-names></name><name><surname>Harrison</surname><given-names>AM</given-names></name><name><surname>Neary</surname><given-names>P</given-names></name></person-group><article-title>Development and evaluation of a simulator-based laparoscopic training program for surgical novices</article-title><source>Surg Endosc</source><year>2013</year><month>1</month><volume>27</volume><issue>1</issue><fpage>214</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1007/s00464-012-2423-0</pub-id><pub-id pub-id-type="medline">22773232</pub-id><pub-id pub-id-type="pmid">22773232</pub-id></element-citation></ref><ref id="ref8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Jan</surname><given-names>U</given-names></name><name><surname>Noll</surname><given-names>C</given-names></name><name><surname>Behrends</surname><given-names>M</given-names></name><name><surname>Albrecht</surname><given-names>UV</given-names></name></person-group><article-title>mARble - Augmented Reality in Medical Education</article-title><source>Biomed Tech (Berl)</source><year>2012</year><volume>57</volume><issue>Suppl. 1</issue><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1515/bmt-2012-4252</pub-id><pub-id pub-id-type="medline">22944785</pub-id><!--<pub-id pub-id-type="pii">/j/bmte.2012.57.issue-s1-A/bmt-2012-4252/bmt-2012-4252.xml</pub-id>--></element-citation></ref><ref id="ref9"><label>9</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Albrecht</surname><given-names>U</given-names></name><name><surname>von Jan</surname><given-names>U</given-names></name><name><surname>H&#x000e4;ussermann</surname><given-names>B</given-names></name><name><surname>Kr&#x000fc;ckeberg</surname><given-names>J</given-names></name><name><surname>Matthies</surname><given-names>H</given-names></name></person-group><article-title>Hands-on mobile Augmented Reality simulations to learn about ethically sensitive topics in medicine through self-experience. Schooling of awareness and understanding</article-title><source>Proceedings of the IADIS International Conference Mobile Learning</source><year>2012</year><conf-name>IADIS International Conference Mobile Learning</conf-name><conf-date>March 11-13, 2012</conf-date><conf-loc>Berlin, Germany</conf-loc><publisher-name>IADIS Press</publisher-name><fpage>447</fpage><lpage>52</lpage></element-citation></ref><ref id="ref10"><label>10</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Albrecht</surname><given-names>UV</given-names></name><name><surname>von Jan</surname><given-names>U</given-names></name><name><surname>Kr&#x000fc;ckeberg</surname><given-names>J</given-names></name><name><surname>Behrends</surname><given-names>M</given-names></name><name><surname>Matthies</surname><given-names>HK</given-names></name></person-group><article-title>Medical Students Experience the Mobile Augmented Reality Blended Learning Envirionment mARble &#x02013; An attractive concept for the Net-generation?</article-title><source>Proceedings of the IADIS International Conference on Cognition and Exploratory Learning in the Digital Age (CELDA 2011)</source><year>2011</year><conf-name>IADIS International Conference on Cognition and Exploratory Learning in the Digital Age</conf-name><conf-date>March 10-12, 2011</conf-date><conf-loc>Avila, Spain</conf-loc><publisher-name>IADIS Press</publisher-name><fpage>263</fpage><lpage>66</lpage></element-citation></ref><ref id="ref11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Noll</surname><given-names>C</given-names></name><name><surname>von Jan</surname><given-names>U</given-names></name><name><surname>Schaft</surname><given-names>T</given-names></name><name><surname>Matthies</surname><given-names>HK</given-names></name><name><surname>Albrecht</surname><given-names>UV</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Reiterer</surname><given-names>H</given-names></name><name><surname>Deussen</surname><given-names>O</given-names></name></person-group><article-title>Sehen, F&#x000fc;hlen, Erfahren &#x02013; Rechtsmedizin mit mARble erleben</article-title><source>Mensch &#x00026; Computer 2012 &#x02013; Workshopband: Interaktiv informiert &#x02013; allgegenw&#x000e4;rtig und allumfassend!?</source><year>2012</year><publisher-loc>Munich</publisher-loc><publisher-name>Oldenbourg Verlag</publisher-name><fpage>241</fpage><lpage>246</lpage></element-citation></ref><ref id="ref12"><label>12</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Edelmann</surname><given-names>W</given-names></name></person-group><source>Lernpsychologie. 6th ed</source><year>2000</year><publisher-loc>Weinheim</publisher-loc><publisher-name>Psychologie Verlags Union</publisher-name></element-citation></ref><ref id="ref13"><label>13</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>G&#x000f6;tz</surname><given-names>T</given-names></name><name><surname>Zirngibl</surname><given-names>A</given-names></name><name><surname>Pekrun</surname><given-names>R</given-names></name><name><surname>Hall</surname><given-names>N</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Mayring</surname><given-names>P</given-names></name><name><surname>von Rhoeneck</surname><given-names>C</given-names></name></person-group><article-title>Emotions, learning and achievement from an educational-psychological perspective</article-title><source>Learning emotions: the influence of affective factors on classroom learning</source><year>2003</year><publisher-loc>Frankfurt</publisher-loc><publisher-name>Peter Lang</publisher-name><fpage>9</fpage><lpage>28</lpage></element-citation></ref><ref id="ref14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Regan</surname><given-names>K</given-names></name></person-group><article-title>Emotion and eLearning</article-title><source>Journal of Asynchronous Learning Networks</source><year>2003</year><month>9</month><volume>7</volume><issue>3</issue><fpage>78</fpage><lpage>92</lpage></element-citation></ref><ref id="ref15"><label>15</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Madea</surname><given-names>B</given-names></name></person-group><source>Praxis Rechtsmedizin: Befunderhebung, Rekonstruktion, Begutachtung</source><year>2006</year><publisher-loc>Heidelberg</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="ref16"><label>16</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Isleifsd&#x000f3;ttir</surname><given-names>J</given-names></name><name><surname>L&#x000e1;rusd&#x000f3;ttir</surname><given-names>M</given-names></name></person-group><article-title>Measuring the User Experience of a Task Oriented Software</article-title><source>Proceedings of the International Workshop on Meaningful Measures</source><year>2008</year><conf-name>International Workshop on Meaningful Measures</conf-name><conf-date>June 18, 2008</conf-date><conf-loc>Reykjavik</conf-loc><publisher-loc>Toulouse, France</publisher-loc><publisher-name>Institute of Research in Informatics of Toulouse (IRIT)</publisher-name><fpage>27</fpage><lpage>31</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.7123&#x00026;rep=rep1&#x00026;type=pdf#page=29"/></comment></element-citation></ref><ref id="ref17"><label>17</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hassenzahl</surname><given-names>M</given-names></name><name><surname>Burmester</surname><given-names>M</given-names></name><name><surname>Koller</surname><given-names>F</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Ziegler</surname><given-names>J</given-names></name><name><surname>Szwillus</surname><given-names>G</given-names></name></person-group><article-title>AttrakDiff: Ein Fragebogen zur Messung wahrgenommener hedonischer und pragmatischer Qualitat</article-title><source>Mensch &#x00026; Computer 2003. Interaktion in Bewegung</source><year>2003</year><publisher-loc>Stuttgart</publisher-loc><publisher-name>B.G. Teubner</publisher-name><fpage>187</fpage><lpage>196</lpage></element-citation></ref><ref id="ref18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassenzahl</surname><given-names>M</given-names></name></person-group><article-title>The Interplay of Beauty, Goodness, and Usability in Interactive Products</article-title><source>Human-Comp. Interaction</source><year>2004</year><month>12</month><day>1</day><volume>19</volume><issue>4</issue><fpage>319</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1207/s15327051hci1904_2</pub-id></element-citation></ref><ref id="ref19"><label>19</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hassenzahl</surname><given-names>M</given-names></name><name><surname>Burmester</surname><given-names>M</given-names></name><name><surname>Koller</surname><given-names>F</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Brau</surname><given-names>H</given-names></name><name><surname>Diefenbach</surname><given-names>S</given-names></name><name><surname>Hassenzahl</surname><given-names>M</given-names></name><name><surname>Koller</surname><given-names>F</given-names></name><name><surname>Peissner</surname><given-names>M</given-names></name><name><surname>R&#x000f6;se</surname><given-names>K</given-names></name></person-group><article-title>Der User Experience (UX) auf der Spur: Zum Einsatz von www.attrakdiff.de</article-title><source>Usability Professionals</source><year>2008</year><publisher-loc>Stuttgart</publisher-loc><publisher-name>German Chapter der Usability Professionals Association</publisher-name><fpage>78</fpage><lpage>82</lpage></element-citation></ref><ref id="ref20"><label>20</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hartson</surname><given-names>R</given-names></name><name><surname>Pardha</surname><given-names>P</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Roumeliotis</surname><given-names>R</given-names></name><name><surname>Bevans</surname><given-names>D</given-names></name></person-group><source>The UX Book: Process and Guidelines for Ensuring a Quality User Experience</source><year>2012</year><publisher-loc>Waltham, MA, USA</publisher-loc><publisher-name>Morgan Kaufmann (Elsevier)</publisher-name></element-citation></ref><ref id="ref21"><label>21</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McNair</surname><given-names>DM</given-names></name><name><surname>Lorr</surname><given-names>M</given-names></name><name><surname>Droppleman</surname><given-names>LF</given-names></name></person-group><source>Manual for the Profile of Mood States</source><year>1971</year><publisher-loc>San Diego, CA</publisher-loc><publisher-name>Educational and Industrial Testing Services</publisher-name></element-citation></ref><ref id="ref22"><label>22</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Biehl</surname><given-names>B</given-names></name><name><surname>Dangel</surname><given-names>S</given-names></name><name><surname>Reiser</surname><given-names>A</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Collegium Internationale Psychiatriae Scalarum (CIPS)</surname><given-names/></name></person-group><article-title>POMS: Profile of Mood States &#x02013; deutsche Fassung</article-title><source>Internationale Skalen f&#x000fc;r Psychiatrie</source><year>1981</year><publisher-loc>Weinheim</publisher-loc><publisher-name>Beltz Test Gesellschaft</publisher-name></element-citation></ref><ref id="ref23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albani</surname><given-names>C</given-names></name><name><surname>Blaser</surname><given-names>G</given-names></name><name><surname>Geyer</surname><given-names>M</given-names></name><name><surname>Schmutzer</surname><given-names>G</given-names></name><name><surname>Br&#x000e4;hler</surname><given-names>E</given-names></name><name><surname>Bailer</surname><given-names>H</given-names></name><name><surname>Grulke</surname><given-names>N</given-names></name></person-group><article-title>[The German short version of "Profile of Mood States" (POMS): psychometric evaluation in a representative sample]</article-title><source>Psychother Psychosom Med Psychol</source><year>2005</year><month>7</month><volume>55</volume><issue>7</issue><fpage>324</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1055/s-2004-834727</pub-id><pub-id pub-id-type="medline">15986282</pub-id><pub-id pub-id-type="pmid">15986282</pub-id></element-citation></ref><ref id="ref24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesudovs</surname><given-names>K</given-names></name><name><surname>Weisinger</surname><given-names>HS</given-names></name><name><surname>Coster</surname><given-names>DJ</given-names></name></person-group><article-title>Cataract surgery and changes in quality of life measures</article-title><source>Clin Exp Optom</source><year>2003</year><month>1</month><volume>86</volume><issue>1</issue><fpage>34</fpage><lpage>41</lpage><pub-id pub-id-type="medline">12568649</pub-id><!--<pub-id pub-id-type="pii">ceo861034</pub-id>--><pub-id pub-id-type="pmid">12568649</pub-id></element-citation></ref><ref id="ref25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cramer</surname><given-names>JA</given-names></name><name><surname>Hammer</surname><given-names>AE</given-names></name><name><surname>Kustra</surname><given-names>RP</given-names></name></person-group><article-title>Improved mood states with lamotrigine in patients with epilepsy</article-title><source>Epilepsy Behav</source><year>2004</year><month>10</month><volume>5</volume><issue>5</issue><fpage>702</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.yebeh.2004.07.005</pub-id><pub-id pub-id-type="medline">15380122</pub-id><!--<pub-id pub-id-type="pii">S1525-5050(04)00226-4</pub-id>--><pub-id pub-id-type="pmid">15380122</pub-id></element-citation></ref><ref id="ref26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crumlish</surname><given-names>CM</given-names></name></person-group><article-title>Coping and emotional response in cardiac surgery patients</article-title><source>West J Nurs Res</source><year>1994</year><month>2</month><volume>16</volume><issue>1</issue><fpage>57</fpage><lpage>68</lpage><pub-id pub-id-type="medline">8128669</pub-id><pub-id pub-id-type="pmid">8128669</pub-id></element-citation></ref><ref id="ref27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolbeault</surname><given-names>S</given-names></name><name><surname>Cayrou</surname><given-names>S</given-names></name><name><surname>Br&#x000e9;dart</surname><given-names>A</given-names></name><name><surname>Viala</surname><given-names>AL</given-names></name><name><surname>Desclaux</surname><given-names>B</given-names></name><name><surname>Saltel</surname><given-names>P</given-names></name><name><surname>Gauvain-Piquard</surname><given-names>A</given-names></name><name><surname>Hardy</surname><given-names>P</given-names></name><name><surname>Dickes</surname><given-names>P</given-names></name></person-group><article-title>The effectiveness of a psycho-educational group after early-stage breast cancer treatment: results of a randomized French study</article-title><source>Psychooncology</source><year>2009</year><month>6</month><volume>18</volume><issue>6</issue><fpage>647</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1002/pon.1440</pub-id><pub-id pub-id-type="medline">19039808</pub-id><pub-id pub-id-type="pmid">19039808</pub-id></element-citation></ref><ref id="ref28"><label>28</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schulmeister</surname><given-names>R</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Rinn</surname><given-names>U</given-names></name><name><surname>Meister</surname><given-names>DM</given-names></name></person-group><article-title>Didaktisches Design aus hochschuldidaktischer Sicht &#x02013; Ein Pl&#x000e4;doyer f&#x000fc;r offene Lernsituationen</article-title><source>Didaktik und Neue Medien. Konzepte und Anwendungen in der Hochschule</source><year>2004</year><publisher-loc>M&#x000fc;nster</publisher-loc><publisher-name>Waxmann</publisher-name></element-citation></ref><ref id="ref29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dede</surname><given-names>C</given-names></name><name><surname>Barab</surname><given-names>S</given-names></name></person-group><article-title>Emerging Technologies for Learning Science: A Time of Rapid Advances</article-title><source>J Sci Educ Technol</source><year>2009</year><month>8</month><day>13</day><volume>18</volume><issue>4</issue><fpage>301</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1007/s10956-009-9172-4</pub-id></element-citation></ref><ref id="ref30"><label>30</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>G&#x000f6;tz</surname><given-names>T</given-names></name><name><surname>Zirngibl</surname><given-names>A</given-names></name><name><surname>Pekrun</surname><given-names>R</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Hascher</surname><given-names>T</given-names></name></person-group><article-title>Lern- und Leistungsemotionen von Sch&#x000fc;lerinnen und Sch&#x000fc;lern</article-title><source>Schule positiv erleben. Erkenntnisse und Ergebnisse zum Wohlbefinden von Sch&#x000fc;lerinnen und Sch&#x000fc;lern</source><year>2004</year><publisher-loc>Switzerland</publisher-loc><publisher-name>Haupt AG</publisher-name><fpage>49</fpage><lpage>66</lpage></element-citation></ref><ref id="ref31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eysenbach</surname><given-names>G</given-names></name><collab>CONSORT-EHEALTH Group</collab></person-group><article-title>CONSORT-EHEALTH: improving and standardizing evaluation reports of Web-based and mobile health interventions</article-title><source>J Med Internet Res</source><year>2011</year><month>12</month><volume>13</volume><issue>4</issue><fpage>e126</fpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.jmir.org/2011/4/e126/"/></comment><pub-id pub-id-type="doi">10.2196/jmir.1923</pub-id><pub-id pub-id-type="medline">22209829</pub-id><!--<pub-id pub-id-type="pii">v13i4e126</pub-id>--><!--<pub-id pub-id-type="pmcid">PMC3278112</pub-id>--><pub-id pub-id-type="pmid">22209829</pub-id></element-citation></ref></ref-list></back></article>
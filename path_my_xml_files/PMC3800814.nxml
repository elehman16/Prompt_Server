<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">24155738</article-id><article-id pub-id-type="pmc">3800814</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2013.00779</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research Article</subject></subj-group></subj-group></article-categories><title-group><article-title>Iconic gestures prime words: comparison of priming effects when gestures are presented alone and when they are accompanying speech</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>So</surname><given-names>Wing-Chee</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref></contrib><contrib contrib-type="author"><name><surname>Yi-Feng</surname><given-names>Alvan Low</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Yap</surname><given-names>De-Fu</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Kheng</surname><given-names>Eugene</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Yap</surname><given-names>Ju-Min Melvin</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Educational Psychology, The Chinese University of Hong Kong</institution><country>Hong Kong, China</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Psychology, National University of Singapore</institution><country>Singapore, Singapore</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Psychology, University of Chicago</institution><country>Chicago, IL, USA</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Gabriella Vigliocco, University College London, UK</p></fn><fn fn-type="edited-by"><p>Reviewed by: Asli Ozyurek, Max Planck Institute for Psycholinguistics, Netherlands; Itziar Laka, University of the Basque Country UPV/EHU, Spain</p></fn><corresp id="fn001">*Correspondence: Wing-Chee So, Department of Educational Psychology, The Chinese University of Hong Kong, Room 208, Ho Tim Building, Hong Kong, China e-mail: <email xlink:type="simple">wingchee@cuhk.edu.hk</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Language Sciences, a section of the journal Frontiers in Psychology.</p></fn></author-notes><pub-date pub-type="epub"><day>21</day><month>10</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>4</volume><elocation-id>779</elocation-id><history><date date-type="received"><day>05</day><month>4</month><year>2013</year></date><date date-type="accepted"><day>03</day><month>10</month><year>2013</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2013 So, Yi-Feng, Yap, Kheng and Yap.</copyright-statement><copyright-year>2013</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Previous studies have shown that iconic gestures presented in an isolated manner prime visually presented semantically related words. Since gestures and speech are almost always produced together, this study examined whether iconic gestures accompanying speech would prime words and compared the priming effect of iconic gestures with speech to that of iconic gestures presented alone. Adult participants (<italic>N</italic> = 180) were randomly assigned to one of three conditions in a lexical decision task: Gestures-Only (the primes were iconic gestures presented alone); Speech-Only (the primes were auditory tokens conveying the same meaning as the iconic gestures); Gestures-Accompanying-Speech (the primes were the simultaneous coupling of iconic gestures and their corresponding auditory tokens). Our findings revealed significant priming effects in all three conditions. However, the priming effect in the Gestures-Accompanying-Speech condition was comparable to that in the Speech-Only condition and was significantly weaker than that in the Gestures-Only condition, suggesting that the facilitatory effect of iconic gestures accompanying speech may be constrained by the level of language processing required in the lexical decision task where linguistic processing of words forms is more dominant than semantic processing. Hence, the priming effect afforded by the co-speech iconic gestures was weakened.</p></abstract><kwd-group><kwd>co-speech gestures</kwd><kwd>cross-modal priming</kwd><kwd>lexical decision</kwd><kwd>language processing</kwd></kwd-group><counts><fig-count count="1"/><table-count count="3"/><equation-count count="0"/><ref-count count="53"/><page-count count="9"/><word-count count="8178"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>We gesture when we talk. Among different types of gestures, iconic gestures display physical properties and movements of objects or actions. As suggested by McNeill (<xref ref-type="bibr" rid="B30">1992</xref>; p. 155), iconic gestures represent thought or are so-called &#x0201c;<italic>material carriers of thinking.</italic>&#x0201d; Previous research has shown that iconic gestures often convey semantic information relevant to that in speech (e.g., McNeill, <xref ref-type="bibr" rid="B30">1992</xref>; Kita and &#x000d6;zy&#x000fc;rek, <xref ref-type="bibr" rid="B25">2003</xref>; &#x000d6;zy&#x000fc;rek et al., <xref ref-type="bibr" rid="B35">2005</xref>). For example, a speaker extends his little finger and thumb (with thumb pointing up and little finger pointing down) and puts his hand next to his ear while saying, &#x0201c;I'll call you tonight.&#x0201d; This iconic gesture resembles the action of talking on a phone and reinforces the semantic content expressed in speech.</p><p>The question of interest is whether <italic>encoding</italic> an iconic gesture would activate semantically related words. This question can be addressed in a cross-modal semantic priming experiment. Semantic priming refers to an increase in speed or accuracy in responding to a target stimulus when it is preceded by a semantically related stimulus, compared to when it is presented by a semantically unrelated stimulus (McNamara, <xref ref-type="bibr" rid="B29">2005</xref>). For example, in the primed lexical decision task, participants initially view a prime word (e.g., DOG) followed by a target stimulus. This target is either a word, semantically related (e.g., CAT) or unrelated to the prime (e.g., HAT), or a non-word (e.g., NOM). Participants then determine whether the targets are words or non-words. Substantial research has shown that participants respond faster to semantically related words than to semantically unrelated words (see Neely, <xref ref-type="bibr" rid="B33">1991</xref> and McNamara, <xref ref-type="bibr" rid="B29">2005</xref> for reviews).</p><p>In cross-modal priming experiments, which examine whether there is a semantic link between iconic gestures and lexical representations, word primes in the semantic priming paradigm are replaced by iconic gestures. To date, two studies have explored cross-modal priming with gesture primes. An initial study by Bernardis et al. (<xref ref-type="bibr" rid="B5">2008</xref>) reported that iconic gestures did <italic>not</italic> prime semantically related words. They selected 40 video clips of gestures that were paired with 80 word targets, with half of the targets semantically related to the gestures and the rest unrelated. Participants in the gesture group were asked to view the gesture video clips (each lasting ~3600 ms), and then to read the following word targets as rapidly as possible. Participants in the baseline group were also told to name the same word targets but these words were presented in isolation (i.e., without gesture clips). The difference in response time between the two groups was then computed, where response time refers to the time elapsed between the presentation of word targets and the onset of the participant's vocal response. The findings indicated that naming target words that were semantically related to preceding gestures was not significantly faster than naming target words that were presented alone, possibly suggesting that iconic gestures did not prime semantically related words.</p><p>However, the findings in Bernardis et al.'s (<xref ref-type="bibr" rid="B5">2008</xref>) study should be interpreted with caution for two reasons. First, naming target words presented alone and naming the same words preceded by semantically related gestures might involve different processing demands. Specifically, it was very likely that participants in the gesture group needed additional amount of time to encode target words that were preceded by semantically related gestures because these gestures were informative and thus required time to process. However, this extra time and effort in processing target words was not relevant in the baseline group. Hence, comparing the response time of participants in the baseline group to that of the gesture group might eventually attenuate the facilitatory effects of gestures (see Yap et al., <xref ref-type="bibr" rid="B46">2011</xref> for more discussion). Indeed, one should calculate the overall priming effect, which is the difference in response time to semantically-related words (e.g., CAT&#x02014;DOG) and that to semantically-unrelated words (e.g., HAT&#x02014;DOG) (Neely, <xref ref-type="bibr" rid="B50">1977</xref>; Jonides and Mack, <xref ref-type="bibr" rid="B49">1984</xref>). As such, Yap et al. (<xref ref-type="bibr" rid="B46">2011</xref>) re-examined Bernardis et al.'s (<xref ref-type="bibr" rid="B5">2008</xref>) results and found a significant overall priming effect. Specifically, participants responded faster when reading words that were semantically related to the preceding gestures than when reading words that were not semantically related to the preceding gestures.</p><p>However, there is still an alternative explanation for the significant overall priming effect. The average duration of each gesture clip in their study was longer than 3000 ms, which might in principle provide participants with sufficient time to strategically recode gestures with verbal labels. This pertains to the second limitation in Bernardis et al.'s study. Labeling gestures would in turn activate semantically-related words, which might serve as lexical primes. Henceforth, the overall priming effect in the naming task could be attributed to lexical primes instead of gesture primes. In order to prevent participants from naming gesture primes, one should keep each gesture clip shorter than 3000 ms.</p><p>Based on the limitations described above, Yap et al. (<xref ref-type="bibr" rid="B46">2011</xref>) conducted another cross-modal priming experiment wherein they administered a lexical decision task. In both experiments in Yap et al.'s study, they presented participants video clips of iconic gestures that carried semantic meaning (e.g., a two-hands flapping gesture) followed by lexical targets which were either words (e.g., &#x0201c;bird,&#x0201d; &#x0201c;brush&#x0201d;) or non-words (&#x0201c;blurds&#x0201d;) and then asked participants to make lexical decisions. The iconic gestures clips lasted for 3500 ms (Experiment 1; also see Bernardis et al., <xref ref-type="bibr" rid="B5">2008</xref>) as well as 1000 ms (Experiment 2). The results in both experiments showed that participants responded faster and more accurately to words which were semantically related to gestures (e.g., &#x0201c;bird,&#x0201d; which is semantically related to a two-hands flapping gesture) than to words which were not semantically related to gestures (e.g., &#x0201c;bad&#x0201d;). These findings suggest that iconic gestures facilitate recognition of semantically related words. They are also consistent with the theories which suggest that our memorial representations are encoded in multiple formats or levels, including gestures and their lexical affiliates (Krauss, <xref ref-type="bibr" rid="B26">1998</xref>) and gesture and language form <italic>single</italic> system within a unified conceptual framework (McNeill, <xref ref-type="bibr" rid="B30">1992</xref>, <xref ref-type="bibr" rid="B31">2005</xref>).</p><p>However, a critical issue remains unaddressed in both Bernardis et al. (<xref ref-type="bibr" rid="B5">2008</xref>) and Yap et al.'s (<xref ref-type="bibr" rid="B46">2011</xref>) studies. These two studies examined gestures when they <italic>were</italic> presented alone rather than co-occurring with speech. However, gestures themselves are idiosyncratic as their forms do not conform to any standard, thereby making it difficult for participants to interpret their meaning, especially when they are presented in an isolated manner (Feyereisen et al., <xref ref-type="bibr" rid="B10">1988</xref>; Krauss et al., <xref ref-type="bibr" rid="B28">1991</xref>, <xref ref-type="bibr" rid="B27">2000</xref>). Therefore, some researchers have concluded that gestures can only be appropriately interpreted together with accompanying speech (Krauss et al., <xref ref-type="bibr" rid="B28">1991</xref>, <xref ref-type="bibr" rid="B27">2000</xref>). Having said that, the iconic gesture primes examined in Yap et al.'s (<xref ref-type="bibr" rid="B46">2011</xref>) study were well recognized by majority of participants (at least 70% agreement rate in a separate norming study). Hence, it is possible that some iconic gestures, compared to others, possess a more direct and transparent relationship with their meaning, thereby allowing them to be recognized more easily (Beattie and Shovelton, <xref ref-type="bibr" rid="B4">2002</xref>) even in the absence of speech.</p><p>Nevertheless, iconic gestures and speech are almost always produced together in a natural communicative setting. According to McNeill (<xref ref-type="bibr" rid="B31">2005</xref>), 90% of gestures are produced in tandem with speech. In addition, both gestural and spoken modalities are tightly integrated temporally, semantically and pragmatically (McNeill, <xref ref-type="bibr" rid="B30">1992</xref>; Kita, <xref ref-type="bibr" rid="B24">2000</xref>; So et al., <xref ref-type="bibr" rid="B41">2009</xref>). Temporally, Morrel-Samuels and Krauss (<xref ref-type="bibr" rid="B32">1992</xref>) found that gestures are initiated either prior to or simultaneously with the onset of their lexical affiliate, thus suggesting a tight temporal link between the gesture and language systems during communication. Semantically, So et al. (<xref ref-type="bibr" rid="B41">2009</xref>) reported that participants are more likely to identify a referent in gestures when the referent is also lexically specified in speech than when it is not, thus suggesting a parallel semantic relationship between gestures and their co-occurring speech. Pragmatically, Kita (<xref ref-type="bibr" rid="B24">2000</xref>) proposed that gestures facilitate speaking by packaging preverbal spatial-motoric information into units suitable for verbalization (the Information Packaging Hypothesis).</p><p>The wealth of evidence has demonstrated the tight integration of gesture and speech in the realm of language production. One significant implication of these findings is that people are exposed to gestures and their accompanying speech during language comprehension. As a result, it is necessary to test for priming when gestures are paired with speech, i.e., studying speech and gestures as a combined system rather than as separate modalities.</p><p>In the present study, we aimed to explore the interplay between co-speech iconic gestures and the lexical processing system. Specifically, we examined whether iconic gestures accompanying speech would prime semantically related words and whether such priming was comparable in size to that produced by iconic gestures presented alone in a lexical decision task (Yap et al., <xref ref-type="bibr" rid="B46">2011</xref>). We also set out to examine whether the priming effect of iconic gestures accompanying speech was comparable to that of speech alone. The findings in the present study will shed more light on the influence of iconic gestures (either presented alone or co-occurring with speech) on the word recognition process.</p><p>We used three types of primes in the present study; iconic gestures alone (Gestures-Only condition), speech alone (Speech-Only condition), and iconic gestures accompanying speech (Gestures-Accompanying-Speech condition). The prime stimuli in the Gestures-Only condition were identical to those in (Yap et al.'s <xref ref-type="bibr" rid="B46">2011</xref>) study. Those in the Speech-Only condition were auditory tokens that labeled iconic gestures examined in the Gestures-Only condition. Those in the Gestures-Accompanying-Speech condition contained the simultaneous coupling of gesture clips in the Gestures-Only and auditory vocal tokens in the Speech-Only condition. Hence, the iconic gestures and auditory vocal tokens were conveying the same meaning. However, it might not be naturalistic for not having a speaker producing gestures and speech in a synchronized manner. It could be a potential limitation in this condition.</p><p>In line with (Yap et al.'s <xref ref-type="bibr" rid="B46">2011</xref>) study, we predicted that iconic gesture primes presented alone would result in faster response times to semantically related than to semantically unrelated words. We also predicted that words preceded by semantically related auditory primes presented alone should be responded faster than those preceded by semantically unrelated auditory primes. This expectation is in line with other cross-modal studies in the literature (Holcomb and Neville, <xref ref-type="bibr" rid="B13">1990</xref>; Holcomb and Anderson, <xref ref-type="bibr" rid="B12">1993</xref>). For example, a study by Holcomb and Neville (<xref ref-type="bibr" rid="B13">1990</xref>) showed that auditory primes yielded faster response rate to visually presented related words than to unrelated words in both behavioral (response latencies) and electrophysiological (event-related potentials) experiments.</p><p>The focus of our interest was the priming effect of iconic gestures accompanying speech, which was not examined in previous research. We predicted that the priming effect of co-speech iconic gestures would be significant because these gestures were the same as those in the Gestures-Only condition. However, would the priming effect in the Gestures-Accompanying-Speech condition be stronger than that in the Gesture-Only condition, or vice versa?</p><p>On one hand, processing iconic gestures that are co-occurring with speech would induce a <italic>stronger</italic> priming effect than processing the same iconic gestures on their own and processing speech alone. According to the Integrated Systems Hypothesis (Kelly et al., <xref ref-type="bibr" rid="B18">2010</xref>), iconic gestures and their co-occurring speech are tightly integrated in the process of language comprehension (for a review, see Kelly et al., <xref ref-type="bibr" rid="B23">2008</xref>). This gesture-speech integration in language comprehension is driven by obligatory interaction between speech and its co-occurring iconic gestures. According to this view, participants would automatically process the information presented in speech while taking into account the information presented in gestures (Kelly et al., <xref ref-type="bibr" rid="B18">2010</xref>). This hypothesis has found empirical support in recent studies (e.g., Kelly et al., <xref ref-type="bibr" rid="B22">2004</xref>; Holle and Gunter, <xref ref-type="bibr" rid="B14">2007</xref>; &#x000d6;zy&#x000fc;rek et al., <xref ref-type="bibr" rid="B36">2007</xref>; Wu and Coulson, <xref ref-type="bibr" rid="B45">2007</xref>; Hubbard et al., <xref ref-type="bibr" rid="B51">2009</xref>). For example, previous studies which used event related potentials have found that iconic gestures are integrated with speech during the process of language comprehension (Kelly et al., <xref ref-type="bibr" rid="B22">2004</xref>; &#x000d6;zy&#x000fc;rek et al., <xref ref-type="bibr" rid="B36">2007</xref>). In the present study, iconic gestures and their co-occurring speech conveyed congruent semantic meaning. Hence, the presence of the accompanying speech would reinforce and consolidate the meaning of iconic gestures (Krauss et al., <xref ref-type="bibr" rid="B27">2000</xref>), thereby yielding a stronger priming effect, when compared to a condition when iconic gestures were presented on their own.</p><p>Similarly, iconic gestures accompanying speech should yield stronger priming effect than speech alone. There is abundant behavioral evidence reporting that co-speech gestures aid language comprehension and memory (e.g., Cohen and Otterbein, <xref ref-type="bibr" rid="B7">1992</xref>; Beattie and Shovelton, <xref ref-type="bibr" rid="B3">1999</xref>; Kelly et al., <xref ref-type="bibr" rid="B17">1999</xref>; Feyereisen, <xref ref-type="bibr" rid="B9">2006</xref>; So et al., <xref ref-type="bibr" rid="B42">2012</xref>). Physiological studies have also documented that gestures in general (iconic gestures and speech beats) influence speech perception and modulate neural activities (e.g., Dick et al., <xref ref-type="bibr" rid="B8">2009</xref>; Biau and Soto-Faraco, <xref ref-type="bibr" rid="B6">2013</xref>). Other studies have also shown benefits of gesture and speech integration on comprehension (Kelly et al., <xref ref-type="bibr" rid="B22">2004</xref>; Wu and Coulson, <xref ref-type="bibr" rid="B44">2005</xref>; &#x000d6;zy&#x000fc;rek et al., <xref ref-type="bibr" rid="B36">2007</xref>). Although the communicative power of gestures is greater when gestures are non-redundant with co-occurring speech than when they are completely redundant (Hostetter, <xref ref-type="bibr" rid="B15">2011</xref>), presenting redundant iconic gestures would still direct listeners' attention to the accompanying speech (Kelly and Goldsmith, <xref ref-type="bibr" rid="B19">2004</xref>), thereby enhancing the priming effect.</p><p>On the other hand, the priming effect of co-speech gestures might be weaker than that of iconic gestures presented on their own. As mentioned earlier, speech helps reinforce and consolidate the meaning of co-occurring iconic gestures gestures (Krauss et al., <xref ref-type="bibr" rid="B27">2000</xref>). As opposed to iconic gestures accompanying speech (and speech alone), those presented alone are far less constrained in their meaning. This is because, as compared to words, iconic gestures might be seen as more idiosyncratic and less associated with conventional labels (McNeill, <xref ref-type="bibr" rid="B30">1992</xref>, <xref ref-type="bibr" rid="B53">2000</xref>). Due to the absence of standardized forms and meanings, iconic gestures presented alone might be semantically related to a wider range of words or concepts, compared to iconic gestures accompanying speech and speech presented alone, henceforth strengthening its priming effect.</p><p>In addition, the priming afforded by speech accompanied by iconic gestures might not be necessarily stronger than that afforded by speech presented alone. Much of the previous work supporting the Integrated Systems Hypothesis have examined the gesture-speech integrated relationship at the &#x0201c;semantic&#x0201d; level, which emphasizes relatively high level language processing. In many of these studies, participants were required to answer questions pertaining to their understanding of the gestures and speech (Beattie and Shovelton, <xref ref-type="bibr" rid="B3">1999</xref>; Kelly et al., <xref ref-type="bibr" rid="B17">1999</xref>, <xref ref-type="bibr" rid="B22">2004</xref>; Dick et al., <xref ref-type="bibr" rid="B8">2009</xref>) or to determine whether gestures and their accompanying speech conveyed congruent or incongruent semantic meaning (Kelly et al., <xref ref-type="bibr" rid="B18">2010</xref>, <xref ref-type="bibr" rid="B20">2012</xref>). This leaves open the possibility that iconic gestures, which are accompanying speech, enhance higher-level <italic>semantic</italic> processing, but not lower-level <italic>word form</italic> processing. This argument has found some empirical support. In Kelly and Lee's study (<xref ref-type="bibr" rid="B21">2012</xref>), native English-speaking participants were taught the meaning of novel Japanese words, which were accompanied or not accompanied by congruent iconic gestures. They then performed two tasks, a semantic task (where they chose the correct English translation of a Japanese word) and a phonetic task (where they determined whether the presented Japanese word contained a geminate or singleton). Their findings showed that co-speech iconic gestures facilitated learning in the semantic task but not in the phonetic task, suggesting that co-speech iconic gestures might not aid processing of phonetic forms (also see Hirata and Kelly, <xref ref-type="bibr" rid="B11">2010</xref> for speech beats).</p><p>Like the phonetic task, the reliance on semantic processing is relatively low in the lexical decision task. In the lexical decision task, participants are attempting to discriminate real words from orthographically similar distracters. While lexical semantic variables such as concreteness, imageability and valence affect lexical decision times, emphasis is placed more on the forms of words than their meanings when doing this task. According to the Theory of Language and Situated Simulation (LASS; Barsalou et al., <xref ref-type="bibr" rid="B2">2008</xref>), making lexical decisions (i.e., discriminating words from non-words) can be based on processing linguistic forms alone without accessing deep conceptual meaning. This perspective is consistent with findings from Pexman et al.'s (<xref ref-type="bibr" rid="B54">2008</xref>) study, which indicated that the lexical decision task focused considerably more on word form than on word meaning, whereas the semantic classification task (i.e., discriminate words from different semantic categories, e.g., animate vs. inanimate) focused more on word meaning than on word form. In short, the findings from Kelly and Lee (<xref ref-type="bibr" rid="B21">2012</xref>) and Pexman et al. (<xref ref-type="bibr" rid="B54">2008</xref>) suggest that iconic gestures accompanying speech might not facilitate word recognition when making lexical decision.</p><p>Related to this line of argument, the priming effect of speech accompanied by iconic gestures might yield a <italic>comparable</italic> priming effect to that of speech presented alone. According to the LASS theory (Barsalou et al., <xref ref-type="bibr" rid="B2">2008</xref>), the processing of linguistic forms would start first and become dominant, followed by deeper and conceptual processing of word meaning when making a lexical decision. This proposition has found empirical support in an fMRI (functional magnetic resonance imaging) study (Simmons et al., <xref ref-type="bibr" rid="B40">2008</xref>). Similarly, Kelly and Lee (<xref ref-type="bibr" rid="B21">2012</xref>) proposed that iconic gestures might be integrated with accompanying speech only <italic>after</italic> the speech system finishes its processing of linguistic forms of words. Therefore, iconic gestures and their accompanying words might not be well integrated in lower level language processing (e.g., recognition of word forms in a lexical decision task) and linguistic forms might be processed prior to the iconic gestures. According to this view, participants in the Gestures-Accompanying-Speech condition would process the presented words prior to recognizing the forms of iconic gestures, resulting in comparable priming effects in both Gestures-Accompanying-Speech and Speech-Alone conditions.</p><p>The present study explored the aforementioned possibilities by looking at the priming effects in three conditions: Gestures-Only condition, Speech-Only condition and Gestures-Accompanying-Speech condition, with the types of primes manipulated. Participants were randomly assigned to one of the three conditions and were asked to make lexical decisions to targets that could be words semantically related to iconic gestures, words semantically unrelated to gestures, or non-words. The response times and accuracy rates of the participants in three conditions were examined.</p></sec><sec sec-type="methods" id="s2"><title>Method</title><sec><title>Participants</title><p>One-hundred and eighty undergraduates (85 males; ages 18&#x02013;21 years old) from the National University of Singapore participated in this experiment. They all had correct or corrected-to-normal vision and were native English speakers<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>. They were either awarded course credit or a voucher of SGD$5 for participation.</p></sec><sec><title>Materials</title><sec><title>Primes</title><p>Forty gesture primes were used in the Gestures-Only condition. These gestures were the same as those used in Yap et al.'s (<xref ref-type="bibr" rid="B46">2011</xref>) study. They were shown in the video clips in which the person who gestured was clearly seen. In their study, Yap et al. asked a separate group of 45 English-speaking undergraduates from the National University of Singapore to watch 80 silent videotaped gestures, each lasting for 3&#x02013;4 s on a computer screen in a speechless context, and to write a single word that best described the meaning of a gesture. A gesture whose meaning was agreed upon by 70% of the participants was considered to have a common interpretation (see Goh et al., <xref ref-type="bibr" rid="B55">2009</xref>). Using this criterion, 40 gestures (out of 80) had consistency rates of 70% of above and thus were selected to become gesture primes. The present study used all these forty gesture primes, with each gesture clip 1000 ms long (Yap et al., <xref ref-type="bibr" rid="B46">2011</xref>; Experiment 2). Appendix Table <xref ref-type="table" rid="TA1">A1</xref> contains a list of gestures and their associated meanings based on participants' interpretation in Yap et al.'s (<xref ref-type="bibr" rid="B46">2011</xref>) study.</p><p>We then used the gesture labels provided by the participants in Yap et al.'s (<xref ref-type="bibr" rid="B46">2011</xref>) to create the prime stimuli in the Speech Only condition. Forty auditory clips, which contained vocal tokens conveying the same semantic meaning as the accompanying gestures, were made. For example, a vocal token of &#x0201c;<italic>rabbit</italic>&#x0201d; that matched the gesture video clip of RABBIT (i.e., the index and middle fingers of both hands are flexed and un-flexed above the head) was recorded. The average duration of auditory clips varied across different vocal tokens (<italic>M</italic> = 655.13 ms, <italic>SD</italic> = 81.96 ms). All vocal tokens were recorded by a linguistically trained female speaker. A <italic>post-hoc</italic> test on the intelligibility of the vocal tokens was conducted on a group of 30 participants. The average agreement rate was 95.92%. However, two out of the 40 vocal tokens had agreement rates below 70%<xref ref-type="fn" rid="fn0002"><sup>2</sup></xref> (see Appendix Table <xref ref-type="table" rid="TA2">A2</xref> for a list of vocal tokens and their intelligibility rates).</p><p>The prime stimuli in the Gestures-Accompanying-Speech condition reflected the simultaneous coupling of gesture and audio clips. Both gesture and its corresponding speech started at the same time. Therefore, participants would watch the gesture videos while listening to the auditory inputs. Each gesture + speech clip lasted for 1000 ms.</p></sec><sec><title>Targets</title><p>The lexical targets consisted of words and non-words. Lexical words were derived from the interpretation of gestures given by participants in Yap et al.'s (<xref ref-type="bibr" rid="B46">2011</xref>) study. Then they were matched with the words listed in Nelson et al.'s (<xref ref-type="bibr" rid="B34">1998</xref>) free association norms. The strongest associate in the Nelson et al. norms for each lexical target was chosen. The non-words were then created by matching their length, number of orthographic neighbors, and number of syllables based on the ARC Non-word Database (Rastle et al., <xref ref-type="bibr" rid="B37">2002</xref>) and the English Lexicon Project (Balota et al., <xref ref-type="bibr" rid="B1">2007</xref>). Each participant was presented with 20 word and 20 non-word targets. Of the word targets, half were semantically related to the primes (gestures, vocal tokens, or both) and half were not.</p></sec></sec><sec><title>Procedure</title><p>All stimuli were presented on computers using the E-Prime 2.0 software (Schneider et al., <xref ref-type="bibr" rid="B38">2002</xref>). Participants in all conditions were instructed to classify letter strings as words and non-words, where they pressed &#x0201c;/&#x0201d; on the keyboard for a word and &#x0201c;z&#x0201d; for a non-word.</p><p>The participants were given ten practice trials before the actual experiment. Each trial began with a black fixation sign (#) located at the center of the screen that was displayed for 1000 ms followed by a blank white screen for 200 ms. A prime was then presented to the participants. It was a gesture clip shown on a screen (Gestures-Only condition), an audio clip played binaurally through the headphones connected to a computer (Speech-Only condition), or a combination of both (Gestures-Accompanying-Speech condition). The prime type was manipulated between subjects. After the prime, a blank white screen was shown for 200 ms. Finally, a lexical target, in black lowercase letter, was displayed at the center of the screen for 3000 ms or until participants made a lexical decision. A blank white screen was shown for 1000 ms for correct responses, while a signal of &#x0201c;Incorrect answer&#x0201d; was shown for 1000 ms for incorrect answers. A signal of &#x0201c;No response&#x0201d; was shown if participants did not make a decision after 3000 ms.</p><p>The experiment ended after participants had completed all forty trials (10 related and 10 unrelated prime-target pairs, and 20 non-words).</p><p>The primes in the three conditions (related, unrelated, and non-word) were counterbalanced across participants such that each prime had an equal chance of appearing in each of the three conditions. The order of the trials was also randomized anew for every participant. Accuracy rates and responses latencies of lexical decisions were recorded.</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><p>Accuracy was almost at ceiling in all three conditions. However, four participants who scored below 85% in accuracy were removed from the analyses (one from the Gestures-Only condition, one from the Speech-Only condition, and two from the Gestures-Accompanying-Speech condition). A box-plot analysis was conducted on the mean response latencies of the participants to identify any outlier participants. Errors (4.1% for &#x0201c;Gesture&#x0201d; condition, 3.7% for &#x0201c;Auditory&#x0201d; condition and 2.8% for &#x0201c;Gesture + Auditory&#x0201d; condition) and response latencies faster than 200 ms or slower than 3000 ms were excluded from the analyses. Response latencies more than 2.5 <italic>SD</italic>s above or below each participant in each condition were excluded from the analyses as well (2.4% for the Gestures-Only condition, 2.6% for the Speech-Only condition and 2.4% for the Gestures-Accompanying-Speech condition). This removed a further 2.5% of the responses.</p><p>A 2 (Relatedness: Related vs. Unrelated) &#x000d7; 3 (Conditions: Gestures-Only, Speech-Only and Gesture Accompanying Speech) mixed analysis of variance (ANOVA) was conducted on response latencies and accuracy rates separately. Figure <xref ref-type="fig" rid="F1">1</xref> shows the mean response latencies and standard errors in three conditions. Regarding the analyses on response latencies, the main effect of Relatedness was significant by both participants, <italic>F</italic><sub><italic>p</italic>(1, 173)</sub> = 30.08, <italic>p</italic> &#x0003c; 0.001, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.15; and items, <italic>F</italic><sub><italic>i</italic>(1, 39)</sub> = 5.26, <italic>p</italic> = 0.027, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.12. Overall, response latencies were faster when primes and targets were semantically related (<italic>M</italic> = 566.26 ms, <italic>SD</italic> = 160.80 ms) compared to when they were not (<italic>M</italic> = 590.24 ms, <italic>SD</italic> = 161.43 ms). The main effect of Condition was significant for items, <italic>F</italic><sub><italic>i</italic>(2, 78)</sub> = 18.21, <italic>p</italic> &#x0003c; 0.001, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.32, but not significant for participants, <italic>F</italic><sub><italic>p</italic>(2, 173)</sub> = 1.67, <italic>p</italic> = 0.190, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.02. Response latencies in the Gestures-Only condition (<italic>M</italic> = 591.22 ms, <italic>SD</italic> = 174.63 ms), Speech-Only condition (<italic>M</italic> = 585.28, <italic>SD</italic> = 162.06 ms) and Gestures-Accompanying-Speech condition (<italic>M</italic> = 558.26 ms, <italic>SD</italic> = 144.79 ms) were not significantly different from each other.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>The mean response latencies to semantically related words (gray bars) and to semantically unrelated words (white bars) in three conditions</bold>.</p></caption><graphic xlink:href="fpsyg-04-00779-g0001"/></fig><p>The interaction between Relatedness and Condition was significant by participants, <italic>F</italic><sub><italic>p</italic>(2, 173)</sub> = 3.73, <italic>p</italic> = 0.026, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.04, but not significant by items, <italic>F</italic><sub><italic>i</italic>(2, 78)</sub> = 1.17, <italic>p</italic> = 0.317, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.03. Bonferonni pairwise comparisons were then conducted to examine whether all three conditions showed significant priming effects. The priming effect in the Gestures-Only condition was significant, <italic>M</italic> = 38.66, <italic>SD</italic> = 54.71, <italic>F</italic><sub>(1, 173)</sub> = 28.44, <italic>p</italic> &#x0003c; 0.001, Cohen's <italic>d</italic> = 5.33, same as that in the Gestures-Accompanying-Speech condition, <italic>M</italic> = 18.76, <italic>SD</italic> = 58.28, <italic>F</italic><sub>(1, 173)</sub> = 6.59, <italic>p</italic> = 0.006, Cohen's <italic>d</italic> = 2.57. However, the priming effect in the Speech-Only condition was marginally significant, <italic>M</italic> = 11.65, <italic>SD</italic> = 54.02, <italic>F</italic><sub>(1, 173)</sub> = 2.58, <italic>p</italic> = 0.055, Cohen's <italic>d</italic> = 1.61.</p><p>Following this, we compared the strength of priming effects across different conditions. A <italic>post-hoc</italic> test (LSD) revealed that the priming effect in the Gestures-Accompanying-Speech condition (<italic>M</italic> = 18.76 ms, <italic>SD</italic> = 58.28 ms) was significantly weaker than that in the Gestures-Only condition (<italic>M</italic> = 38.66 ms, <italic>SD</italic> = 54.71 ms), <italic>p</italic> = 0.028, but was comparable to that in the Speech-Only condition (<italic>M</italic> = 11.65 ms, <italic>SD</italic> = 54.02 ms), <italic>p</italic> = 0.245. The priming effect in the Gestures-Only condition was significantly stronger than that in the Speech-Only condition, <italic>p</italic> = 0.005.</p><sec><title>Accuracy</title><p>Table <xref ref-type="table" rid="T1">1</xref> shows the mean accuracy rates and standard deviations in three conditions. The main effect of Relatedness was not significant by participants <italic>F</italic><sub><italic>p</italic>(1, 173)</sub> = 0.66, <italic>p</italic> = 0.419, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.00; and items, <italic>F</italic><sub><italic>i</italic>(1, 39)</sub> = 0.84, <italic>p</italic> = 0.366, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.02. The main effect of Condition was also not significant by participants, <italic>F</italic><sub><italic>p</italic>(2, 173)</sub> = 0.79, <italic>p</italic> = 0.456, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.01; and items, <italic>F</italic><sub><italic>i</italic>(2, 78)</sub> = 0.98, <italic>p</italic> = 0.382, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.02. The interaction between Relatedness and Condition was also not significant by participants, <italic>F</italic><sub><italic>p</italic>(2, 173)</sub> = 0.06, <italic>p</italic> = 0.939, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.00; and items, <italic>F</italic><sub><italic>i</italic>(2, 78)</sub> = 0.07, <italic>p</italic> = 0.933, &#x003b7;<sup>2</sup><sub><italic>p</italic></sub> = 0.00.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>The mean (and standard deviation) of response accuracies in lexical decision of semantically related and unrelated words in three conditions</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1"><bold>Mean</bold></th><th align="left" rowspan="1" colspan="1"><bold><italic>SD</italic></bold></th><th align="left" rowspan="1" colspan="1"><bold>Mean</bold></th><th align="left" rowspan="1" colspan="1"><bold><italic>SD</italic></bold></th></tr><tr><th rowspan="1" colspan="1"/><th align="center" colspan="2" rowspan="1"><bold>Related</bold></th><th align="center" colspan="2" rowspan="1"><bold>Unrelated</bold></th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Gestures-only</td><td align="left" rowspan="1" colspan="1">0.977</td><td align="left" rowspan="1" colspan="1">0.064</td><td align="left" rowspan="1" colspan="1">0.974</td><td align="left" rowspan="1" colspan="1">0.056</td></tr><tr><td align="left" rowspan="1" colspan="1">Speech-only</td><td align="left" rowspan="1" colspan="1">0.984</td><td align="left" rowspan="1" colspan="1">0.039</td><td align="left" rowspan="1" colspan="1">0.979</td><td align="left" rowspan="1" colspan="1">0.061</td></tr><tr><td align="left" rowspan="1" colspan="1">Gestures-accompanying-speech</td><td align="left" rowspan="1" colspan="1">0.986</td><td align="left" rowspan="1" colspan="1">0.034</td><td align="left" rowspan="1" colspan="1">0.984</td><td align="left" rowspan="1" colspan="1">0.042</td></tr></tbody></table></table-wrap></sec></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>Our study set out to examine whether iconic gestures accompanying speech prime semantically-related words and to compare priming in this condition to when gestures or speech are presented alone. The findings revealed that the priming effect in the Gestures-Accompanying-Speech condition was significant but it was weaker than that in the Gestures-Only condition and comparable to that in the Speech-Only condition. We will first discuss the findings in the Gestures-Only and Speech-Only conditions, followed by that those in the Gestures-Accompanying-Speech condition.</p><p>The results in the Gestures-Only condition replicated the findings in Yap et al.'s (<xref ref-type="bibr" rid="B46">2011</xref>) study, showing that iconic gestures prime semantically related target words in a lexical decision task. These results are consistent with previous research that support the semantic link between the gestural and lexical representational systems. The forms of iconic gestures tested in the present study (also in Yap et al., <xref ref-type="bibr" rid="B46">2011</xref>) might possess a relatively direct and transparent relationship with their meaning such that they were well recognized despite the fact they were presented without speech. Hence, the rich imagistic meaning of iconic gestures would provide useful semantic information and allow participants to activate semantically related words. A significant priming effect was also found in the Speech-Only condition. This finding converges with previous research (Holcomb and Neville, <xref ref-type="bibr" rid="B13">1990</xref>; Holcomb and Anderson, <xref ref-type="bibr" rid="B12">1993</xref>), and demonstrates that cross-modal priming from an auditory prime to a visual target is reliably obtained across different studies. The foregoing results demonstrate that cross-modal priming can be generalized across different modalities (i.e., gesture-visual as well as auditory-visual). This reinforces the idea that our semantic representation is represented in multiple formats, such as gestures, audio tokens, and words (Krauss, <xref ref-type="bibr" rid="B26">1998</xref>).</p><p>Although it was not one of our major predictions, we found that the priming yielded by iconic gestures presented alone was stronger than that of speech alone and that of speech accompanied by iconic gestures. What might account for this? It is possible that processing iconic gestures presented alone might result in a deeper semantic processing than processing words and words accompanied by iconic gestures. Since iconic gestures do not have conventional labels and standardized forms (McNeill, <xref ref-type="bibr" rid="B53">2000</xref>), they might be connected to a wider range of semantic concepts than words, thus strengthening priming. In contrast, words have conventional labels and can consolidate the meanings of accompanying iconic gestures. We acknowledge that the foregoing proposal is <italic>post-hoc</italic> and speculative, given that no study to date has directly compared the degree of semantic processing engaged by iconic gestures (either presented alone or co-occurring with speech) and words in a lexical decision task. This will be an interesting research direction to pursue in the future.</p><p>However, one might contend that iconic gestures presented alone are less constrained in their meanings, and thus participants would have to work harder in order to process the meanings of gesture, compared to spoken word, primes. Previous work have established that larger priming effects are produced when more time is taken to process a prime (Hutchison et al., <xref ref-type="bibr" rid="B56">2008</xref>). That said, we should emphasize that the gesture primes we have selected for the present study (and also Yap et al.'s study in <xref ref-type="bibr" rid="B46">2011</xref>) received a relatively high agreement rate (70%) in a norming study. Note that this is the same threshold adopted when one decides whether a particular auditory token can be correctly identified by a subject. While we cannot entirely exclude the possibility that the extra effort involved in processing gestures might induce stronger priming, we have attempted to minimize this possibility by choosing only high-agreement gesture primes.</p><p>Our findings also showed that gestures accompanying speech prime semantically related words. While previous studies have focused primarily on the priming effects when iconic gestures were presented alone (Bernardis et al., <xref ref-type="bibr" rid="B5">2008</xref>; Yap et al., <xref ref-type="bibr" rid="B46">2011</xref>), our results extend the literature by demonstrating that such priming also occurs even in a naturalistic setting where iconic gestures are not presented alone, but co-occurring with speech.</p><p>Intriguingly, the priming effect in the Gestures-Accompanying-Speech condition was comparable in size to that in the Speech-Only condition. However, this finding can be well explained by the Theory of Language and Situated Simulation (LASS; Barsalou et al., <xref ref-type="bibr" rid="B2">2008</xref>). According to this theory, making lexical decisions (i.e., discriminating words from non-words) can be mainly driven by the processing of linguistic forms alone, without accessing deep conceptual meaning. In addition, the linguistic system dominates the early stage of conceptual processing (Simmons et al., <xref ref-type="bibr" rid="B40">2008</xref>). Because of the <italic>asynchrony</italic> in the time-course of linguistic (first) and conceptual (second) processing, the facilitating effect of co-speech iconic gestures might therefore be weakened. Participants in the Gestures-Accompanying-Speech condition (as well as the Speech-Only condition) might mainly drive their lexical decision from activity in the linguistic system. However, we want to be careful to point out that we are <italic>not</italic> arguing that participants did not process the deep conceptual information of the presented targets. Instead, the speech and iconic gestures presented in the Gestures-Accompanying-Speech would actually activate associated simulations (Barsalou et al., <xref ref-type="bibr" rid="B2">2008</xref>). However, such activation might occur <italic>after</italic> the linguistic system has recognized the presented words, at which point lexical decisions have already been made.</p><p>The comparable priming effects in the Gestures-Accompanying-Speech and Speech-Alone conditions might seem to be inconsistent with the extant literature, which demonstrates the facilitatory effect of gestures on processing when they accompany speech (e.g., Beattie and Shovelton, <xref ref-type="bibr" rid="B3">1999</xref>; Cohen and Otterbein, <xref ref-type="bibr" rid="B7">1992</xref>; Kelly et al., <xref ref-type="bibr" rid="B22">2004</xref>; Feyereisen, <xref ref-type="bibr" rid="B9">2006</xref>; &#x000d6;zy&#x000fc;rek et al., <xref ref-type="bibr" rid="B36">2007</xref>). However, the tasks in which participants engaged in previous studies were not lexical decision tasks; rather they involved language comprehension and memory. Our findings thus shed light on the role of co-speech iconic gestures in facilitating word recognition. Specifically, the facilitatory effect of co-speech iconic gestures is more salient in higher level language processing (e.g., understanding conceptual meaning of words) than in lower level of language processing (e.g., discriminating words from non-words in a lexical decision task).</p><p>One might also contend that the comparable priming effect in the Gestures-Accompanying-Speech and Speech-Alone conditions was attributed to a task-specific effect. In this study, iconic gestures always conveyed the same information as the accompanying speech. Therefore, participants might have realized that iconic gestures did not help them to do better on the task or they even might have stopped attending to the gestures. This could explain why the priming effect in the Gestures-Accompanying-Speech was similar to that Speech-Alone condition. In addition, the non-naturalistic nature of the gestures accompanying speech stimuli (the person who gestured in the video did not say the words) might offer a possible alternative explanation for why the priming effect in the Gestures-Accompanying-Speech condition was less significant than that in the gesture only condition (e.g., subjects may just decide to listen to the speech and ignore the visually presented gesture). However, there is evidence against the idea that participants ignore iconic gestures that reinforce semantic contents of accompanying speech. Specifically, in a study by So et al. (<xref ref-type="bibr" rid="B42">2012</xref>), participants watched three different videos, each consisting of a list of 10 words, in 3 conditions (words accompanied by iconic gestures, words accompanied by speech beat gestures, and words not accompanied by gestures). The semantic meaning conveyed in these iconic gestures was the same as that in the accompanying speech. The findings showed that participants recalled more words when encoding them with iconic gestures than when encoding them with speech beats or without gestures. Therefore, presenting iconic gestures that convey the same semantic meaning with accompanying speech might not necessarily prevent participants from attending to these gestures.</p><p>In fact, our result is consistent with Kelly and Lee's study (<xref ref-type="bibr" rid="B21">2012</xref>) in which they did not find facilitatory effects of co-speech iconic gestures on learning the phonetic forms of a second language. In future work, we could explore the effects of co-speech gestures on a lexical processing task which weights semantic information more heavily, such as the semantic classification task.</p><p>To conclude, our study provides additional empirical support for cross-modal semantic priming. That is, gestures presented alone or with speech prime semantically related words in a lexical decision task. However, the priming effect of iconic gestures accompanying speech is weaker than that of iconic gestures presented alone, suggesting that the facilitatory effect of iconic gestures may be limited in tasks which rely more heavily on linguistic, compared to semantic, processing.</p><sec><title>Conflict of interest statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></sec></body><back><fn-group><fn id="fn0001"><p><sup>1</sup>In Singapore, English is the medium of instruction at all levels of education and is also the major official language. The last author of this manuscript routinely collect standardized vocabulary knowledge scores from his participant (Shipley, <xref ref-type="bibr" rid="B39">1940</xref>). Participants in the National University of Singapore have a mean Shipley vocabulary age of about 18 (Yap et al., <xref ref-type="bibr" rid="B47">2013</xref>), which is lower than the mean vocabulary age of a highly selective North American college (Washington University: 18.75) but higher than that of a typical state college (SUNY-Albany: 17.25) (Yap et al., <xref ref-type="bibr" rid="B48">2009</xref>).</p></fn><fn id="fn0002"><p><sup>2</sup>The removal of these two vocal tokens from our analyses did not result in any significant differences in our results or discussion.</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balota</surname><given-names>D. A.</given-names></name><name><surname>Yap</surname><given-names>M. J.</given-names></name><name><surname>Cortese</surname><given-names>M. J.</given-names></name><name><surname>Hutchison</surname><given-names>K. A.</given-names></name><name><surname>Kessler</surname><given-names>B.</given-names></name><name><surname>Loftis</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2007</year>). <article-title>The English Lexicon Project</article-title>. <source>Behav. Res. Methods</source>
<volume>39</volume>, <fpage>445</fpage>&#x02013;<lpage>459</lpage>
<pub-id pub-id-type="doi">10.3758/BF03193014</pub-id><pub-id pub-id-type="pmid">17958156</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>L. W.</given-names></name><name><surname>Santos</surname><given-names>A.</given-names></name><name><surname>Simmons</surname><given-names>W. K.</given-names></name><name><surname>Wilson</surname><given-names>C. D.</given-names></name></person-group> (<year>2008</year>). <article-title>Language and simulation in conceptual processing</article-title>, in <source>Symbols, Embodiment, and Meaning</source>, eds <person-group person-group-type="editor"><name><surname>De Vega</surname><given-names>M.</given-names></name><name><surname>Glenberg</surname><given-names>A. M.</given-names></name><name><surname>Graesser</surname><given-names>A. C.</given-names></name></person-group> (<publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>), <fpage>245</fpage>&#x02013;<lpage>283</lpage>
<pub-id pub-id-type="doi">10.1093/acprof:oso/9780199217274.003.0013</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beattie</surname><given-names>G.</given-names></name><name><surname>Shovelton</surname><given-names>H.</given-names></name></person-group> (<year>1999</year>). <article-title>Mapping the range of information contained in the iconic hand gestures that accompany spontaneous speech</article-title>. <source>J. Lang. Soc. Psychol</source>. <volume>18</volume>, <fpage>438</fpage>&#x02013;<lpage>462</lpage>
<pub-id pub-id-type="doi">10.1177/0261927X99018004005</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beattie</surname><given-names>G.</given-names></name><name><surname>Shovelton</surname><given-names>H.</given-names></name></person-group> (<year>2002</year>). <article-title>What properties of talk are associated with the generation of spontaneous iconic hand gestures?</article-title>
<source>Br. J. Soc. Psychol</source>. <volume>41</volume>, <fpage>403</fpage>&#x02013;<lpage>417</lpage>
<pub-id pub-id-type="doi">10.1348/014466602760344287</pub-id><pub-id pub-id-type="pmid">12419010</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernardis</surname><given-names>P.</given-names></name><name><surname>Salillas</surname><given-names>E.</given-names></name><name><surname>Caramelli</surname><given-names>N.</given-names></name></person-group> (<year>2008</year>). <article-title>Behavioural and neuropsychological evidence of semantic interaction between iconic gestures and words</article-title>. <source>Cogn. Neuropsychol</source>. <volume>25</volume>, <fpage>1114</fpage>&#x02013;<lpage>1128</lpage>
<pub-id pub-id-type="doi">10.1080/02643290801921707</pub-id><pub-id pub-id-type="pmid">18608334</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biau</surname><given-names>E.</given-names></name><name><surname>Soto-Faraco</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <article-title>Beat gestures modulate auditory integration in speech perception</article-title>. <source>Brain Lang</source>. <volume>124</volume>, <fpage>143</fpage>&#x02013;<lpage>152</lpage>
<pub-id pub-id-type="doi">10.1016/j.bandl.2012.10.008</pub-id><pub-id pub-id-type="pmid">23333667</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>R. L.</given-names></name><name><surname>Otterbein</surname><given-names>N.</given-names></name></person-group> (<year>1992</year>). <article-title>The mnemonic effect of speech gestures: pantomimic and non-pantomimic gestures compared</article-title>. <source>Eur. J. Cogn. Psychol</source>. <volume>4</volume>, <fpage>113</fpage>&#x02013;<lpage>139</lpage>
<pub-id pub-id-type="doi">10.1080/09541449208406246</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dick</surname><given-names>A. S.</given-names></name><name><surname>Goldin-Meadow</surname><given-names>S.</given-names></name><name><surname>Hasson</surname><given-names>U.</given-names></name><name><surname>Skipper</surname><given-names>J. I.</given-names></name><name><surname>Small</surname><given-names>S. L.</given-names></name></person-group> (<year>2009</year>). <article-title>Co-speech gestures influence neural activity in brain regions associated with processing semantic information</article-title>. <source>Hum. Brain Mapp</source>. <volume>30</volume>, <fpage>3509</fpage>&#x02013;<lpage>3526</lpage>
<pub-id pub-id-type="doi">10.1002/hbm.20774</pub-id><pub-id pub-id-type="pmid">19384890</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feyereisen</surname><given-names>P.</given-names></name></person-group> (<year>2006</year>). <article-title>How could gesture facilitate lexical access?</article-title>
<source>Adv. Speech Lang. Pathol</source>. <volume>8</volume>, <fpage>128</fpage>&#x02013;<lpage>133</lpage>
<pub-id pub-id-type="doi">10.1080/14417040600667293</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feyereisen</surname><given-names>P.</given-names></name><name><surname>Van de Wiele</surname><given-names>M.</given-names></name><name><surname>Dubois</surname><given-names>F.</given-names></name></person-group> (<year>1988</year>). <article-title>The meaning of gestures: what can be understood without speech?</article-title>
<source>Cahiers Psychol. Cogn</source>. <volume>8</volume>, <fpage>3</fpage>&#x02013;<lpage>25</lpage>
<pub-id pub-id-type="pmid">17683939</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goh</surname><given-names>W. D.</given-names></name><name><surname>Su&#x000e1;rez</surname><given-names>L.</given-names></name><name><surname>Yap</surname><given-names>M. J.</given-names></name><name><surname>Tan</surname><given-names>S. H.</given-names></name></person-group> (<year>2009</year>). <article-title>Distributional analyses in auditory lexical decision: neighbourhood density and word frequency effects</article-title>. <source>Psychon. Bull. and Rev</source>. <volume>16</volume>, <fpage>882</fpage>&#x02013;<lpage>887</lpage>
<pub-id pub-id-type="doi">10.3758/PBR.16.5.882</pub-id><pub-id pub-id-type="pmid">19815793</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirata</surname><given-names>Y.</given-names></name><name><surname>Kelly</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>The effects of lips and hands on auditory learning of second language speech sounds</article-title>. <source>J. Speech Lang. Hear. Res</source>. <volume>53</volume>, <fpage>298</fpage>&#x02013;<lpage>310</lpage>
<pub-id pub-id-type="doi">10.1044/1092-4388(2009/08-0243)</pub-id><pub-id pub-id-type="pmid">20220023</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holcomb</surname><given-names>P.</given-names></name><name><surname>Anderson</surname><given-names>J. E.</given-names></name></person-group> (<year>1993</year>). <article-title>Cross-modal semantic priming: a time-course analysis using event-related brain potentials</article-title>. <source>Lang. Cogn. Process</source>. <volume>8</volume>, <fpage>379</fpage>&#x02013;<lpage>411</lpage>
<pub-id pub-id-type="doi">10.1080/01690969308407583</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holcomb</surname><given-names>P. J.</given-names></name><name><surname>Neville</surname><given-names>H. J.</given-names></name></person-group> (<year>1990</year>). <article-title>Auditory and visual semantic priming in lexical decision: a comparison using event-related brain potentials</article-title>. <source>Lang. Cogn. Process</source>. <volume>5</volume>, <fpage>281</fpage>&#x02013;<lpage>312</lpage>
<pub-id pub-id-type="doi">10.1080/01690969008407065</pub-id><pub-id pub-id-type="pmid">18056222</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holle</surname><given-names>H.</given-names></name><name><surname>Gunter</surname><given-names>T. C.</given-names></name></person-group> (<year>2007</year>). <article-title>The role of iconic gestures in speech disambiguation: ERP evidence</article-title>. <source>J. Cogn. Neurosci</source>. <volume>19</volume>, <fpage>1175</fpage>&#x02013;<lpage>1192</lpage>
<pub-id pub-id-type="doi">10.1162/jocn.2007.19.7.1175</pub-id><pub-id pub-id-type="pmid">17583993</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hostetter</surname><given-names>A. B.</given-names></name></person-group> (<year>2011</year>). <article-title>When do gestures communicate? A meta-analysis</article-title>. <source>Psychol. Bull</source>. <volume>137</volume>, <fpage>297</fpage>&#x02013;<lpage>315</lpage>
<pub-id pub-id-type="doi">10.1037/a0022128</pub-id><pub-id pub-id-type="pmid">21355631</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubbard</surname><given-names>A. L.</given-names></name><name><surname>Wilson</surname><given-names>S. M.</given-names></name><name><surname>Callan</surname><given-names>D. E.</given-names></name><name><surname>Dapretto</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>Giving speech a hand: gesture modulates activity in auditory cortex during speech perception</article-title>. <source>Hum. Brain Mapp</source>. <volume>30</volume>, <fpage>1028</fpage>&#x02013;<lpage>1037</lpage>
<pub-id pub-id-type="doi">10.1002/hbm.20565</pub-id><pub-id pub-id-type="pmid">18412134</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutchison</surname><given-names>K. A.</given-names></name><name><surname>Balota</surname><given-names>D. A.</given-names></name><name><surname>Cortese</surname><given-names>M.</given-names></name><name><surname>Watson</surname><given-names>J. M.</given-names></name></person-group> (<year>2008</year>). <article-title>Predicting semantic priming at the item-level</article-title>. <source>Q. J. Exp. Psychol</source>. <volume>61</volume>, <fpage>1036</fpage>&#x02013;<lpage>1066</lpage>
<pub-id pub-id-type="doi">10.1080/17470210701438111</pub-id><pub-id pub-id-type="pmid">17853227</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonides</surname><given-names>J.</given-names></name><name><surname>Mack</surname><given-names>R.</given-names></name></person-group> (<year>1984</year>). <article-title>On the cost and benefit of cost and benefit</article-title>. <source>Psychol. Bull</source>. <volume>96</volume>, <fpage>29</fpage>&#x02013;<lpage>44</lpage>
<pub-id pub-id-type="doi">10.1037/0033-2909.96.1.29</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>S. D.</given-names></name><name><surname>Barr</surname><given-names>D. J.</given-names></name><name><surname>Church</surname><given-names>R. B.</given-names></name><name><surname>Lynch</surname><given-names>K.</given-names></name></person-group> (<year>1999</year>). <article-title>Offering a hand to pragmatic understanding: the role of speech and gesture in comprehension and memory</article-title>. <source>J. Mem. Lang</source>. <volume>40</volume>, <fpage>577</fpage>&#x02013;<lpage>592</lpage>
<pub-id pub-id-type="doi">10.1006/jmla.1999.2634</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>S. D.</given-names></name><name><surname>Creigh</surname><given-names>P.</given-names></name><name><surname>Bartolotti</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>Integrating speech and iconic gestures in a Stroop-like task: evidence for automatic processing</article-title>. <source>J. Cogn. Neurosci</source>. <volume>22</volume>, <fpage>683</fpage>&#x02013;<lpage>694</lpage>
<pub-id pub-id-type="doi">10.1162/jocn.2009.21254</pub-id><pub-id pub-id-type="pmid">19413483</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>S. D.</given-names></name><name><surname>Goldsmith</surname><given-names>L.</given-names></name></person-group> (<year>2004</year>). <article-title>Gesture and right hemisphere involvement in evaluating lecture material</article-title>. <source>Gesture</source>
<volume>4</volume>, <fpage>25</fpage>&#x02013;<lpage>42</lpage>
<pub-id pub-id-type="doi">10.1075/gest.4.1.03kel</pub-id><pub-id pub-id-type="pmid">19413483</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>S. D.</given-names></name><name><surname>Hansen</surname><given-names>B. C.</given-names></name><name><surname>Clark</surname><given-names>D. T.</given-names></name></person-group> (<year>2012</year>). <article-title>&#x0201c;Slight&#x0201d; of hand: the processing of visually degraded gestures with speech</article-title>. <source>PLoS ONE</source>
<volume>7</volume>:<fpage>e42620</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0042620</pub-id><pub-id pub-id-type="pmid">22912715</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>S. D.</given-names></name><name><surname>Lee</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>When actions speak too much louder than words: gesture disrupts word learning when phonetic demands are high</article-title>. <source>Lang. Cogn. Process</source>. <volume>27</volume>, <fpage>793</fpage>&#x02013;<lpage>807</lpage>
<pub-id pub-id-type="doi">10.1080/01690965.2011.581125</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>S. D.</given-names></name><name><surname>Kravitz</surname><given-names>C.</given-names></name><name><surname>Hopkins</surname><given-names>M.</given-names></name></person-group> (<year>2004</year>). <article-title>Neural correlates of bimodal speech and gesture comprehension</article-title>. <source>Brain Lang</source>. <volume>89</volume>, <fpage>253</fpage>&#x02013;<lpage>260</lpage>
<pub-id pub-id-type="doi">10.1016/S0093-934X(03)00335-3</pub-id><pub-id pub-id-type="pmid">15010257</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>S. D.</given-names></name><name><surname>Manning</surname><given-names>S.</given-names></name><name><surname>Rodak</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>Gesture gives a hand to language and learning: perspectives from cognitive neuroscience, developmental psychology and education</article-title>. <source>Lang. Linguist. Compass</source>
<volume>2</volume>, <fpage>569</fpage>&#x02013;<lpage>588</lpage>
<pub-id pub-id-type="doi">10.1111/j.1749-818X.2008.00067.x</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kita</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>How representational gestures help speaking</article-title>, in <source>Language and Gesture Window into Thought and Action</source>, ed <person-group person-group-type="editor"><name><surname>McNeill</surname><given-names>D.</given-names></name></person-group> (<publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>), <fpage>162</fpage>&#x02013;<lpage>185</lpage></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kita</surname><given-names>S.</given-names></name><name><surname>&#x000d6;zy&#x000fc;rek</surname><given-names>A.</given-names></name></person-group> (<year>2003</year>). <article-title>What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: evidence for an interface representation of spatial thinking and speaking</article-title>. <source>J. Mem. Lang</source>. <volume>48</volume>, <fpage>16</fpage>&#x02013;<lpage>32</lpage>
<pub-id pub-id-type="doi">10.1016/S0749-596X(02)00505-3</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>R. M.</given-names></name></person-group> (<year>1998</year>). <article-title>Why do we gesture when we speak?</article-title>
<source>Curr. Dir. Psychol. Sci</source>. <volume>7</volume>, <fpage>54</fpage>&#x02013;<lpage>60</lpage>
<pub-id pub-id-type="doi">10.1111/1467-8721.ep13175642</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>R. M.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Gottesman</surname><given-names>R. F.</given-names></name></person-group> (<year>2000</year>). <article-title>Lexical gestures and lexical access: a process model</article-title>, in <source>Language and Gesture</source>, ed <person-group person-group-type="editor"><name><surname>McNeill</surname><given-names>D.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>), <fpage>261</fpage>&#x02013;<lpage>283</lpage>
<pub-id pub-id-type="doi">10.1017/CBO9780511620850.017</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>R. M.</given-names></name><name><surname>Morrel-Samuels</surname><given-names>P.</given-names></name><name><surname>Colasante</surname><given-names>C.</given-names></name></person-group> (<year>1991</year>). <article-title>Do conversational hand gestures communicate?</article-title>
<source>J. Pers. Soc. Psychol</source>. <volume>61</volume>, <fpage>743</fpage>&#x02013;<lpage>754</lpage>
<pub-id pub-id-type="doi">10.1037/0022-3514.61.5.743</pub-id><pub-id pub-id-type="pmid">1753329</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>McNamara</surname><given-names>T.</given-names></name></person-group> (<year>2005</year>). <source>Semantic Priming: Perspectives from Memory and Word Recognition</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Psychological Press</publisher-name>
<pub-id pub-id-type="doi">10.4324/9780203338001</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>McNeill</surname><given-names>D.</given-names></name></person-group> (<year>1992</year>). <source>Hand and Mind: What Gestures Reveal About Thought</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>McNeill</surname><given-names>D.</given-names></name></person-group> (<year>2000</year>). <article-title>Catchments and contexts: non-modular factors in speech and gesture production</article-title>, in <source>Language and Gesture</source>, ed <person-group person-group-type="editor"><name><surname>McNeill</surname><given-names>D.</given-names></name></person-group> (<publisher-name>Cambridge University Press</publisher-name>), <fpage>312</fpage>&#x02013;<lpage>328</lpage></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>McNeill</surname><given-names>D.</given-names></name></person-group> (<year>2005</year>). <source>Gesture and Thought</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>
<pub-id pub-id-type="doi">10.7208/chicago/9780226514642.001.0001</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrel-Samuels</surname><given-names>P.</given-names></name><name><surname>Krauss</surname><given-names>R. M.</given-names></name></person-group> (<year>1992</year>). <article-title>Word familiarity predicts temporal asynchrony of hand gestures and speech</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>18</volume>, <fpage>615</fpage>&#x02013;<lpage>623</lpage>
<pub-id pub-id-type="doi">10.1037/0278-7393.18.3.615</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neely</surname><given-names>J. H.</given-names></name></person-group> (<year>1977</year>). <article-title>Semantic priming and retrieval from lexical memory: roles of inhibitionless spreading activation and limited-capacity attention</article-title>. <source>J. Exp. Psychol. Gen</source>. <volume>106</volume>, <fpage>226</fpage>&#x02013;<lpage>254</lpage>
<pub-id pub-id-type="doi">10.1037/0096-3445.106.3.226</pub-id><pub-id pub-id-type="pmid">10076091</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neely</surname><given-names>J. H.</given-names></name></person-group> (<year>1991</year>). <article-title>Semantic priming effects in visual word recognition: a selective review of current findings and theories</article-title>, in <source>Basic Processes in Reading: Visual Word Recognition</source>, eds <person-group person-group-type="editor"><name><surname>Besner</surname><given-names>D.</given-names></name><name><surname>Humphreys</surname><given-names>G.</given-names></name></person-group> (<publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>), <fpage>236</fpage>&#x02013;<lpage>264</lpage></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>D. L.</given-names></name><name><surname>McEvoy</surname><given-names>C. L.</given-names></name><name><surname>Schreiber</surname><given-names>T. A.</given-names></name></person-group> (<year>1998</year>). <source>The University of South Florida Word Association, Rhyme, and Word Fragment Norms</source>. Available online at <ext-link ext-link-type="uri" xlink:href="http://w3.usf.edu/FreeAssociation/">http://w3.usf.edu/FreeAssociation/</ext-link> (Accessed July 21, 2012).</mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>&#x000d6;zy&#x000fc;rek</surname><given-names>A.</given-names></name><name><surname>Kita</surname><given-names>S.</given-names></name><name><surname>Allen</surname><given-names>S.</given-names></name><name><surname>Furman</surname><given-names>R.</given-names></name><name><surname>Brown</surname><given-names>A.</given-names></name></person-group> (<year>2005</year>). <article-title>How does linguistic framing of events influence co-speech gestures? Insights from cross-linguistic variations and similarities</article-title>. <source>Gesture</source>
<volume>5</volume>, <fpage>215</fpage>&#x02013;<lpage>237</lpage>
<pub-id pub-id-type="doi">10.1075/gest.5.1.15ozy</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>&#x000d6;zy&#x000fc;rek</surname><given-names>A.</given-names></name><name><surname>Willems</surname><given-names>R. M.</given-names></name><name><surname>Kita</surname><given-names>S.</given-names></name><name><surname>Hagoort</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <article-title>On-line integration of semantic information from speech and gesture: Insights from event-related brain potentials</article-title>. <source>J. Cogn. Neurosci</source>. <volume>19</volume>, <fpage>605</fpage>&#x02013;<lpage>616</lpage>
<pub-id pub-id-type="doi">10.1162/jocn.2007.19.4.605</pub-id><pub-id pub-id-type="pmid">17381252</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pexman</surname><given-names>P. M.</given-names></name><name><surname>Hargreaves</surname><given-names>I. S.</given-names></name><name><surname>Siakaluk</surname><given-names>P. D.</given-names></name><name><surname>Bodner</surname><given-names>G. E.</given-names></name><name><surname>Pope</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>There are many ways to be rich: effects of three measures of semantic richness on visual word recognition</article-title>. <source>Psychon. Bull. Rev</source>. <volume>15</volume>, <fpage>161</fpage>&#x02013;<lpage>167</lpage>
<pub-id pub-id-type="doi">10.3758/PBR.15.1.161</pub-id><pub-id pub-id-type="pmid">18605497</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rastle</surname><given-names>K.</given-names></name><name><surname>Harrington</surname><given-names>J.</given-names></name><name><surname>Coltheart</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>358,534 nonwords: the ARC nonword database</article-title>. <source>Q. J. Exp. Psychol</source>. <volume>55A</volume>, <fpage>1339</fpage>&#x02013;<lpage>1362</lpage>
<pub-id pub-id-type="doi">10.1080/02724980244000099</pub-id><pub-id pub-id-type="pmid">12420998</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>W.</given-names></name><name><surname>Eschmann</surname><given-names>A.</given-names></name><name><surname>Zuccolotto</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <source>E-Prime User's Guide</source>. <publisher-loc>Pittsburgh, PA</publisher-loc>: <publisher-name>Psychology Software Tools, Inc</publisher-name></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shipley</surname><given-names>W. C.</given-names></name></person-group> (<year>1940</year>). <article-title>A self-administering scale for measuring intellectual impairment and deterioration</article-title>. <source>J. Psychol</source>. <volume>9</volume>, <fpage>371</fpage>&#x02013;<lpage>377</lpage>
<pub-id pub-id-type="doi">10.1080/00223980.1940.9917704</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>W. K.</given-names></name><name><surname>Hamann</surname><given-names>S. B.</given-names></name><name><surname>Harenski</surname><given-names>C. N.</given-names></name><name><surname>Hu</surname><given-names>X. P.</given-names></name><name><surname>Barsalou</surname><given-names>L. W.</given-names></name></person-group> (<year>2008</year>). <article-title>fMRI evidence for word association and situated simulation in conceptual processing</article-title>. <source>J. Physiol. Paris</source>
<volume>102</volume>, <fpage>106</fpage>&#x02013;<lpage>119</lpage>
<pub-id pub-id-type="doi">10.1016/j.jphysparis.2008.03.014</pub-id><pub-id pub-id-type="pmid">18468869</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>So</surname><given-names>W. C.</given-names></name><name><surname>Kita</surname><given-names>S.</given-names></name><name><surname>Goldin-Meadow</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>Using the hands to identify who does what to whom: speech and gesture go hand-in-hand</article-title>. <source>Cogn. Sci</source>. <volume>33</volume>, <fpage>115</fpage>&#x02013;<lpage>125</lpage>
<pub-id pub-id-type="doi">10.1111/j.1551-6709.2008.01006.x</pub-id><pub-id pub-id-type="pmid">20126430</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>So</surname><given-names>W. C.</given-names></name><name><surname>Sim</surname><given-names>C.</given-names></name><name><surname>Low</surname><given-names>W. S.</given-names></name></person-group> (<year>2012</year>). <article-title>Mnemonic effect of iconic gesture and beat in adults and children. Is meaning important for memory recall?</article-title>
<source>Lang. Cogn. Process</source>. <volume>5</volume>, <fpage>665</fpage>&#x02013;<lpage>681</lpage>
<pub-id pub-id-type="doi">10.1080/01690965.2011.573220</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y. C.</given-names></name><name><surname>Coulson</surname><given-names>S.</given-names></name></person-group> (<year>2005</year>). <article-title>Meaningful gestures: electrophysiological indices of iconic gesture comprehension</article-title>. <source>Psychophysiology</source>
<volume>42</volume>, <fpage>654</fpage>&#x02013;<lpage>667</lpage>
<pub-id pub-id-type="doi">10.1111/j.1469-8986.2005.00356.x</pub-id><pub-id pub-id-type="pmid">16364061</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y. C.</given-names></name><name><surname>Coulson</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>How iconic gestures enhance communication: an ERP study</article-title>. <source>Brain Lang</source>. <volume>101</volume>, <fpage>234</fpage>&#x02013;<lpage>245</lpage>
<pub-id pub-id-type="doi">10.1016/j.bandl.2006.12.003</pub-id><pub-id pub-id-type="pmid">17222897</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yap</surname><given-names>D.-F.</given-names></name><name><surname>So</surname><given-names>W. C.</given-names></name><name><surname>Yap</surname><given-names>J.-M. M.</given-names></name><name><surname>Tan</surname><given-names>Y.-Q.</given-names></name><name><surname>Teoh</surname><given-names>R.-L. S.</given-names></name></person-group> (<year>2011</year>). <article-title>Iconic gestures prime words</article-title>. <source>Cogn. Sci</source>. <volume>35</volume>, <fpage>171</fpage>&#x02013;<lpage>183</lpage>
<pub-id pub-id-type="doi">10.1111/j.1551-6709.2010.01141.x</pub-id><pub-id pub-id-type="pmid">21428996</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yap</surname><given-names>M. J.</given-names></name><name><surname>Balota</surname><given-names>D. A.</given-names></name><name><surname>Tan</surname><given-names>S. E.</given-names></name></person-group> (<year>2013</year>). <article-title>Additive and interactive effects in semantic priming: Isolating lexical and decision processes in the lexical decision task</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn</source>. <volume>39</volume>, <fpage>140</fpage>&#x02013;<lpage>158</lpage>
<pub-id pub-id-type="doi">10.1037/a0028520</pub-id><pub-id pub-id-type="pmid">22612169</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yap</surname><given-names>M. J.</given-names></name><name><surname>Tse</surname><given-names>C.-S.</given-names></name><name><surname>Balota</surname><given-names>D. A.</given-names></name></person-group> (<year>2009</year>). <article-title>Individual differences in the joint effects of semantic priming and word frequency: the role of lexical integrity</article-title>. <source>J. Mem. Lang</source>. <volume>61</volume>, <fpage>303</fpage>&#x02013;<lpage>325</lpage>
<pub-id pub-id-type="doi">10.1016/j.jml.2009.07.001</pub-id><pub-id pub-id-type="pmid">20161653</pub-id></mixed-citation></ref></ref-list><app-group><app id="A1"><title>Appendix</title><table-wrap id="TA1" position="anchor"><label>Table A1</label><caption><p><bold>List of gestures and their associated meanings as given by participants</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Gesture meaning (Yap et al., <xref ref-type="bibr" rid="B46">2011</xref>)</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>Lexical targets (Nelson et al., <xref ref-type="bibr" rid="B34">1998</xref>)</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>Prime-target associative strength (Nelson et al., <xref ref-type="bibr" rid="B34">1998</xref>)</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>Non-words (ARC; ELP)</bold></th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Baby</td><td align="left" rowspan="1" colspan="1">Child</td><td align="left" rowspan="1" colspan="1">0.20</td><td align="left" rowspan="1" colspan="1">Youns</td></tr><tr><td align="left" rowspan="1" colspan="1">Ball</td><td align="left" rowspan="1" colspan="1">Round</td><td align="left" rowspan="1" colspan="1">0.15</td><td align="left" rowspan="1" colspan="1">Sorms</td></tr><tr><td align="left" rowspan="1" colspan="1">Brush</td><td align="left" rowspan="1" colspan="1">Hair</td><td align="left" rowspan="1" colspan="1">0.20</td><td align="left" rowspan="1" colspan="1">Salm</td></tr><tr><td align="left" rowspan="1" colspan="1">Cap</td><td align="left" rowspan="1" colspan="1">Hat</td><td align="left" rowspan="1" colspan="1">0.06</td><td align="left" rowspan="1" colspan="1">Cug</td></tr><tr><td align="left" rowspan="1" colspan="1">Carry</td><td align="left" rowspan="1" colspan="1">Hold</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">Lown</td></tr><tr><td align="left" rowspan="1" colspan="1">Circle</td><td align="left" rowspan="1" colspan="1">Square<xref ref-type="table-fn" rid="TN1"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">0.47</td><td align="left" rowspan="1" colspan="1">Blurds</td></tr><tr><td align="left" rowspan="1" colspan="1">Climb</td><td align="left" rowspan="1" colspan="1">Mountain</td><td align="left" rowspan="1" colspan="1">0.09</td><td align="left" rowspan="1" colspan="1">Sprounge</td></tr><tr><td align="left" rowspan="1" colspan="1">Cold</td><td align="left" rowspan="1" colspan="1">Hot</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">Tib</td></tr><tr><td align="left" rowspan="1" colspan="1">Comb</td><td align="left" rowspan="1" colspan="1">Brush</td><td align="left" rowspan="1" colspan="1">0.16</td><td align="left" rowspan="1" colspan="1">Vonks</td></tr><tr><td align="left" rowspan="1" colspan="1">Curve</td><td align="left" rowspan="1" colspan="1">Straight</td><td align="left" rowspan="1" colspan="1">0.08</td><td align="left" rowspan="1" colspan="1">Phlieves</td></tr><tr><td align="left" rowspan="1" colspan="1">Cut</td><td align="left" rowspan="1" colspan="1">Blood</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">Skent</td></tr><tr><td align="left" rowspan="1" colspan="1">Cycle</td><td align="left" rowspan="1" colspan="1">Bike</td><td align="left" rowspan="1" colspan="1">&#x02013;<xref ref-type="table-fn" rid="TN2"><sup>**</sup></xref></td><td align="left" rowspan="1" colspan="1">Mout</td></tr><tr><td align="left" rowspan="1" colspan="1">Down</td><td align="left" rowspan="1" colspan="1">Up</td><td align="left" rowspan="1" colspan="1">0.84</td><td align="left" rowspan="1" colspan="1">Vu</td></tr><tr><td align="left" rowspan="1" colspan="1">Drive</td><td align="left" rowspan="1" colspan="1">Car</td><td align="left" rowspan="1" colspan="1">0.12</td><td align="left" rowspan="1" colspan="1">Dar</td></tr><tr><td align="left" rowspan="1" colspan="1">Fat</td><td align="left" rowspan="1" colspan="1">Skinny</td><td align="left" rowspan="1" colspan="1">0.41</td><td align="left" rowspan="1" colspan="1">Aggous</td></tr><tr><td align="left" rowspan="1" colspan="1">Fill</td><td align="left" rowspan="1" colspan="1">Empty<xref ref-type="table-fn" rid="TN1"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">&#x02013;<xref ref-type="table-fn" rid="TN2"><sup>**</sup></xref></td><td align="left" rowspan="1" colspan="1">Chuzz</td></tr><tr><td align="left" rowspan="1" colspan="1">Fly</td><td align="left" rowspan="1" colspan="1">Bird</td><td align="left" rowspan="1" colspan="1">0.32</td><td align="left" rowspan="1" colspan="1">Vuch</td></tr><tr><td align="left" rowspan="1" colspan="1">Full</td><td align="left" rowspan="1" colspan="1">Empty<xref ref-type="table-fn" rid="TN1"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">0.61</td><td align="left" rowspan="1" colspan="1">Slolt</td></tr><tr><td align="left" rowspan="1" colspan="1">Grab</td><td align="left" rowspan="1" colspan="1">Take</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">Jole</td></tr><tr><td align="left" rowspan="1" colspan="1">Guitar</td><td align="left" rowspan="1" colspan="1">String</td><td align="left" rowspan="1" colspan="1">0.06</td><td align="left" rowspan="1" colspan="1">Terked</td></tr><tr><td align="left" rowspan="1" colspan="1">Hot</td><td align="left" rowspan="1" colspan="1">Cold</td><td align="left" rowspan="1" colspan="1">0.41</td><td align="left" rowspan="1" colspan="1">Boke</td></tr><tr><td align="left" rowspan="1" colspan="1">Open</td><td align="left" rowspan="1" colspan="1">Close</td><td align="left" rowspan="1" colspan="1">0.44</td><td align="left" rowspan="1" colspan="1">Nubes</td></tr><tr><td align="left" rowspan="1" colspan="1">Photo</td><td align="left" rowspan="1" colspan="1">Picture</td><td align="left" rowspan="1" colspan="1">0.06</td><td align="left" rowspan="1" colspan="1">Iddings</td></tr><tr><td align="left" rowspan="1" colspan="1">Pray</td><td align="left" rowspan="1" colspan="1">God</td><td align="left" rowspan="1" colspan="1">&#x02013;<xref ref-type="table-fn" rid="TN2"><sup>**</sup></xref></td><td align="left" rowspan="1" colspan="1">Gos</td></tr><tr><td align="left" rowspan="1" colspan="1">Push</td><td align="left" rowspan="1" colspan="1">Pull</td><td align="left" rowspan="1" colspan="1">0.59</td><td align="left" rowspan="1" colspan="1">Pome</td></tr><tr><td align="left" rowspan="1" colspan="1">Rabbit</td><td align="left" rowspan="1" colspan="1">Bunny</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">Trief</td></tr><tr><td align="left" rowspan="1" colspan="1">Read</td><td align="left" rowspan="1" colspan="1">Book</td><td align="left" rowspan="1" colspan="1">0.42</td><td align="left" rowspan="1" colspan="1">Teap</td></tr><tr><td align="left" rowspan="1" colspan="1">Rectangle</td><td align="left" rowspan="1" colspan="1">Square<xref ref-type="table-fn" rid="TN1"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">&#x02013;<xref ref-type="table-fn" rid="TN2"><sup>**</sup></xref></td><td align="left" rowspan="1" colspan="1">Blurds</td></tr><tr><td align="left" rowspan="1" colspan="1">Run</td><td align="left" rowspan="1" colspan="1">Walk</td><td align="left" rowspan="1" colspan="1">0.46</td><td align="left" rowspan="1" colspan="1">Joth</td></tr><tr><td align="left" rowspan="1" colspan="1">Slap</td><td align="left" rowspan="1" colspan="1">Hit</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">Cug</td></tr><tr><td align="left" rowspan="1" colspan="1">Stir</td><td align="left" rowspan="1" colspan="1">Mix</td><td align="left" rowspan="1" colspan="1">0.19</td><td align="left" rowspan="1" colspan="1">Zix</td></tr><tr><td align="left" rowspan="1" colspan="1">Swim</td><td align="left" rowspan="1" colspan="1">Water</td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">Aiped</td></tr><tr><td align="left" rowspan="1" colspan="1">Think</td><td align="left" rowspan="1" colspan="1">Brain</td><td align="left" rowspan="1" colspan="1">0.23</td><td align="left" rowspan="1" colspan="1">Polfs</td></tr><tr><td align="left" rowspan="1" colspan="1">Throw</td><td align="left" rowspan="1" colspan="1">Ball</td><td align="left" rowspan="1" colspan="1">0.07</td><td align="left" rowspan="1" colspan="1">Sogs</td></tr><tr><td align="left" rowspan="1" colspan="1">Tie</td><td align="left" rowspan="1" colspan="1">Neck</td><td align="left" rowspan="1" colspan="1">0.07</td><td align="left" rowspan="1" colspan="1">Tesh</td></tr><tr><td align="left" rowspan="1" colspan="1">Tiny</td><td align="left" rowspan="1" colspan="1">Small</td><td align="left" rowspan="1" colspan="1">0.08</td><td align="left" rowspan="1" colspan="1">Crosh</td></tr><tr><td align="left" rowspan="1" colspan="1">Triangle</td><td align="left" rowspan="1" colspan="1">Square<xref ref-type="table-fn" rid="TN1"><sup>*</sup></xref></td><td align="left" rowspan="1" colspan="1">0.04</td><td align="left" rowspan="1" colspan="1">Quescs</td></tr><tr><td align="left" rowspan="1" colspan="1">Up</td><td align="left" rowspan="1" colspan="1">Down</td><td align="left" rowspan="1" colspan="1">0.58</td><td align="left" rowspan="1" colspan="1">Datt</td></tr><tr><td align="left" rowspan="1" colspan="1">Walk</td><td align="left" rowspan="1" colspan="1">Run</td><td align="left" rowspan="1" colspan="1">0.49</td><td align="left" rowspan="1" colspan="1">Ven</td></tr><tr><td align="left" rowspan="1" colspan="1">Write</td><td align="left" rowspan="1" colspan="1">Read</td><td align="left" rowspan="1" colspan="1">0.33</td><td align="left" rowspan="1" colspan="1">Dunt</td></tr></tbody></table><table-wrap-foot><fn id="TN1"><label>*</label><p>The repeated stimuli were not presented in the same block within participants after counterbalancing.</p></fn><fn id="TN2"><label>**</label><p>No values were reported for these stimuli because they were selected to fit the additional cues or information conveyed by the gestures and were therefore not found in Nelson et al.'s (<xref ref-type="bibr" rid="B34">1998</xref>) norms.</p></fn></table-wrap-foot></table-wrap><table-wrap id="TA2" position="anchor"><label>Table A2</label><caption><p><bold>List of vocal tokens and their participant agreement rate</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1"><bold>Vocal tokens</bold></th><th align="left" rowspan="1" colspan="1"><bold>Agreement rate (%)</bold></th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Baby</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Ball</td><td align="left" rowspan="1" colspan="1">93.33</td></tr><tr><td align="left" rowspan="1" colspan="1">Brush</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Cap</td><td align="left" rowspan="1" colspan="1">93.33</td></tr><tr><td align="left" rowspan="1" colspan="1">Carry</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Circle</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Climb</td><td align="left" rowspan="1" colspan="1">96.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Cold</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Comb</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Curve</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Cut</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Cycle</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Down</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Drive</td><td align="left" rowspan="1" colspan="1">96.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Fat</td><td align="left" rowspan="1" colspan="1">86.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Fill</td><td align="left" rowspan="1" colspan="1">83.33</td></tr><tr><td align="left" rowspan="1" colspan="1">Fly</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Full</td><td align="left" rowspan="1" colspan="1">53.33<xref ref-type="table-fn" rid="TN3"><sup>*</sup></xref></td></tr><tr><td align="left" rowspan="1" colspan="1">Grab</td><td align="left" rowspan="1" colspan="1">96.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Guitar</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Hot</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Open</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Photo</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Pray</td><td align="left" rowspan="1" colspan="1">93.33</td></tr><tr><td align="left" rowspan="1" colspan="1">Push</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Rabbit</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Read</td><td align="left" rowspan="1" colspan="1">96.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Rectangle</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Run</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Slap</td><td align="left" rowspan="1" colspan="1">96.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Stir</td><td align="left" rowspan="1" colspan="1">93.33</td></tr><tr><td align="left" rowspan="1" colspan="1">Swim</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Think</td><td align="left" rowspan="1" colspan="1">96.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Throw</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Tie</td><td align="left" rowspan="1" colspan="1">96.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Tiny</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Triangle</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Up</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Walk</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Write</td><td align="left" rowspan="1" colspan="1">63.33<xref ref-type="table-fn" rid="TN3"><sup>*</sup></xref></td></tr></tbody></table><table-wrap-foot><fn id="TN3"><label>*</label><p>There were two vocal tokens with an agreement rate below 70%.</p></fn></table-wrap-foot></table-wrap></app></app-group></back></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26594186</article-id><article-id pub-id-type="pmc">4633501</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2015.01681</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Multimodal retrieval of autobiographical memories: sensory information contributes differently to the recollection of events</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Willander</surname><given-names>Johan</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/239518/overview"/></contrib><contrib contrib-type="author"><name><surname>Sikstr&#x000f6;m</surname><given-names>Sverker</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/156236/overview"/></contrib><contrib contrib-type="author"><name><surname>Karlsson</surname><given-names>Kristina</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/137315/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Social Work and Psychology, University of G&#x000e4;vle</institution> G&#x000e4;<country>vle, Sweden</country></aff><aff id="aff2"><sup>2</sup><institution>G&#x000f6;sta Ekman Laboratory, Department of Psychology, Stockholm University</institution><country>Stockholm, Sweden</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Psychology, Lund University</institution><country>Lund, Sweden</country></aff><aff id="aff4"><sup>4</sup><institution>Department of Psychology, Stockholm University</institution><country>Stockholm, Sweden</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: <italic>Gezinus Wolters, Leiden University, Netherlands</italic></p></fn><fn fn-type="edited-by"><p>Reviewed by: <italic>Peter Verkoeijen, Erasmus University Rotterdam, Netherlands; Steve M. J. Janssen, University of Nottingham-Malaysia Campus, Malaysia</italic></p></fn><corresp id="fn001">*Correspondence: <italic>Johan Willander, <email xlink:type="simple">johan.willander@hig.se</email></italic></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Cognition, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>05</day><month>11</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>6</volume><elocation-id>1681</elocation-id><history><date date-type="received"><day>20</day><month>5</month><year>2015</year></date><date date-type="accepted"><day>19</day><month>10</month><year>2015</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2015 Willander, Sikstr&#x000f6;m and Karlsson.</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Willander, Sikstr&#x000f6;m and Karlsson</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Previous studies on autobiographical memory have focused on unimodal retrieval cues (i.e., cues pertaining to one modality). However, from an ecological perspective multimodal cues (i.e., cues pertaining to several modalities) are highly important to investigate. In the present study we investigated age distributions and experiential ratings of autobiographical memories retrieved with unimodal and multimodal cues. Sixty-two participants were randomized to one of four cue-conditions: visual, olfactory, auditory, or multimodal. The results showed that the peak of the distributions depends on the modality of the retrieval cue. The results indicated that multimodal retrieval seemed to be driven by visual and auditory information to a larger extent and to a lesser extent by olfactory information. Finally, no differences were observed in the number of retrieved memories or experiential ratings across the four cue-conditions.</p></abstract><kwd-group><kwd>autobiographical memory</kwd><kwd>multimodal</kwd><kwd>multisensory</kwd><kwd>unimodal</kwd><kwd>age distribution</kwd><kwd>experiential ratings</kwd></kwd-group><funding-group><award-group><funding-source id="cn001">Vetenskapsr&#x000e5;det<named-content content-type="fundref-id">10.13039/501100004359</named-content></funding-source></award-group><award-group><funding-source id="cn002">Riksbankens Jubileumsfond<named-content content-type="fundref-id">10.13039/501100004472</named-content></funding-source></award-group></funding-group><counts><fig-count count="3"/><table-count count="3"/><equation-count count="0"/><ref-count count="31"/><page-count count="8"/><word-count count="0"/></counts></article-meta></front><body><sec><title>Introduction</title><p>Autobiographical memory has been defined as personal events from one&#x02019;s life (<xref rid="B6" ref-type="bibr">Conway and Pleydell-Pearce, 2000</xref>). These personally experienced memories may be evoked by various cues (e.g., photographs). Previous research investigating cued retrieval of autobiographical memories has mostly focused on unimodal cues and studies on multimodal cueing of autobiographical memory are scarce (<xref rid="B14" ref-type="bibr">Karlsson et al., 2013</xref>).</p><p>In the unimodal paradigm, participants are presented with a cue of a single modality and asked to retrieve an autobiographical memory related to the cue. However, in everyday life individuals are exposed to combinations of sensory inputs originating from several modalities simultaneously (e.g., vision, audition, and olfaction). Therefore the overall aim of the present study was to investigate multimodal cueing of autobiographical memories, i.e., cues pertaining to several modalities simultaneously. More specifically, we investigated these memories with respect to the age distribution and experiential ratings (e.g., valence, the feeling of being brought back in time to the occurrence of the event). In addition modelling was carried out to determine the relative contribution of visual, olfactory, and auditory sensory information in multimodal retrieval of autobiographical memories.</p><p>A well-established finding in the autobiographical memory literature is the age distribution (<xref rid="B22" ref-type="bibr">Rubin and Schulkind, 1997</xref>). It has consistently been shown that the frequency distribution of autobiographical memories across the lifespan follows a specific pattern that comprises three components, i.e., the childhood amnesia, the bump, and recency (<xref rid="B22" ref-type="bibr">Rubin and Schulkind, 1997</xref>; <xref rid="B6" ref-type="bibr">Conway and Pleydell-Pearce, 2000</xref>; <xref rid="B1" ref-type="bibr">Bruce et al., 2005</xref>). The childhood amnesia reflects the inability to retrieve memories that occurred prior to three to 4 years of age (<xref rid="B21" ref-type="bibr">Rubin et al., 1998</xref>; <xref rid="B6" ref-type="bibr">Conway and Pleydell-Pearce, 2000</xref>). The childhood amnesia is succeeded by the bump period reflecting a peak in the age distribution typically located to 10&#x02013;30 years for verbally and visually evoked memories (<xref rid="B21" ref-type="bibr">Rubin et al., 1998</xref>; <xref rid="B6" ref-type="bibr">Conway and Pleydell-Pearce, 2000</xref>; <xref rid="B28" ref-type="bibr">Willander and Larsson, 2006</xref>; for a review see <xref rid="B15" ref-type="bibr">Koppel and Berntsen, 2015</xref>). Likewise, the age distribution for memories evoked by musical cues peak between 10&#x02013;20 years (<xref rid="B25" ref-type="bibr">Schulkind and Woldorf, 2005</xref>). However, for olfactory evoked memories a different pattern emerges, where the bump is located before 10 years of age (<xref rid="B3" ref-type="bibr">Chu and Downes, 2000</xref>; <xref rid="B28" ref-type="bibr">Willander and Larsson, 2006</xref>, <xref rid="B29" ref-type="bibr">2007</xref>, <xref rid="B30" ref-type="bibr">2008</xref>; <xref rid="B17" ref-type="bibr">Larsson et al., 2014</xref>; cf. <xref rid="B20" ref-type="bibr">Rubin et al., 1984</xref>).</p><p>Memories evoked by verbal, visual, or olfactory cues differ also with respect to experiential ratings. For example, it has been demonstrated that odor-evoked memories are rated as more pleasant than memories evoked by words (<xref rid="B4" ref-type="bibr">Chu and Downes, 2002</xref>; <xref rid="B12" ref-type="bibr">Herz and Schooler, 2002</xref>; <xref rid="B29" ref-type="bibr">Willander and Larsson, 2007</xref>). <xref rid="B11" ref-type="bibr">Herz (2004)</xref> also demonstrated that odor-evoked memories are experienced as more pleasant than memories evoked by pictures or sounds. In contrast, when individuals are asked to retroactively rate the emotionality experienced at the occurrence of the event, picture-evoked events are reported as more emotional than word- and odor-evoked memories (<xref rid="B28" ref-type="bibr">Willander and Larsson, 2006</xref>; cf. <xref rid="B8" ref-type="bibr">Ehrlichman and Bastone, 1992</xref>).</p><p>Differences in the experiential ratings of events as a function of cue-modality have also been observed on mental time travel (i.e., the feeling of being brought back in time to the occurrence of the event). <xref rid="B12" ref-type="bibr">Herz and Schooler (2002)</xref> demonstrated that memories evoked by odors are associated with a stronger feeling of being brought back than memories evoked by verbal cues. This finding has been replicated and extended in two experiments by <xref rid="B28" ref-type="bibr">Willander and Larsson (2006</xref>, <xref rid="B29" ref-type="bibr">2007</xref>), where odor-evoked memories were found to be associated with a stronger feeling of being brought back than memories evoked by words or pictures.</p><p>Another important experiential dimension in autobio raphical memory recollection is vividness; although the results are somewhat mixed. For example, <xref rid="B9" ref-type="bibr">Goddard et al. (2005)</xref> found that verbal cues result in more vividly recollected events compared to pictorial or olfactory evoked events. However, neither <xref rid="B11" ref-type="bibr">Herz (2004)</xref> nor <xref rid="B28" ref-type="bibr">Willander and Larsson (2006</xref>, <xref rid="B29" ref-type="bibr">2007</xref>) were able to detect any differences in vividness of retrieved autobiographical events across cue-modality (i.e., words, pictures, or odors).</p><p>In the autobiographical memory literature, sensory infor ation from different modalities are treated as separate entities rather than integrated multimodal representations. However, a growing body of research on multisensory perception indicates that unimodal information may be integrated into multimodal representations (e.g., <xref rid="B7" ref-type="bibr">Driver and Spence, 2000</xref>). Although unimodal perceptual information may be integrated, the relative influence of the respective modalities may differ. A well-established finding in perception is that the visual modality dominates over other modalities (e.g., <xref rid="B5" ref-type="bibr">Colavita and Weisberg, 1979</xref>; <xref rid="B16" ref-type="bibr">Koppen and Spence, 2007</xref>). For example, <xref rid="B5" ref-type="bibr">Colavita and Weisberg (1979)</xref> demonstrated that individuals were more prone to respond to the termination of visual (light) stimuli compared to auditory stimuli, even though these modalities were presented simultaneously.</p><p>Few studies have yet addressed multimodal cueing of events in autobiographical memory. <xref rid="B29" ref-type="bibr">Willander and Larsson (2007)</xref> indirectly addressed bimodal cues when presenting participants with odors in conjunction with their respective names. Here it was found that when odors were presented with its corresponding name, the age distribution and experiential ratings took an intermediate position compared to the unimodal cueing conditions (<xref rid="B29" ref-type="bibr">Willander and Larsson, 2007</xref>). However, it may be argued that a word/semantic cue does not adequately reflect or fully represent the sensory information perceived at the occurrence of an event since individuals rarely perceive sensory information in conjunction with its corresponding verbal label. In a study by <xref rid="B14" ref-type="bibr">Karlsson et al. (2013)</xref>, the semantic representation of autobiographical memories evoked by multimodal cues were contrasted with events triggered by unimodal cues. In this study the analyses were based on the verbal reports of the retrieved autobiographical memories of the same data set as the current study. The aim of <xref rid="B14" ref-type="bibr">Karlsson et al. (2013)</xref> was to highlight multimodal retrieval of events from a content perspective rather than a distributional perspective like the current study. The results of <xref rid="B14" ref-type="bibr">Karlsson et al. (2013)</xref> study indicated that the semantic representation of multimodally evoked memories could be described as a combination of the three unimodal conditions (visual, olfactory, and auditory). Furthermore, the visual and auditory conditions contributed more than the olfactory condition to the semantic representation of the multimodal condition. This finding provided further support for the notion of a modality hierarchy (<xref rid="B14" ref-type="bibr">Karlsson et al., 2013</xref>).</p><p>The aim of the present study was to investigate naturalistic multimodal cues, and compare them with corresponding unimodal cues presented in the visual, auditory, and odor modalities, in the context of autobiographical memory retrieval. The following hypotheses were made: based on previous studies on the age distribution of autobiographical memories, the bump of odor-evoked events should be located earlier (i.e., to the first decade of life) than the bump of visually and auditory evoked events. Also, the multimodal age distribution should fall in between the unimodal distributions, and the relative weight of the odor distribution should be smaller than the auditory and visual distributions. With regard to experiential ratings, it is hypothesized that odor-evoked memories are rated as more pleasant and associated with a stronger feeling of being brought back as compared the to visual and auditory conditions. The unimodal conditions should not differ with regard to vividness and importance.</p></sec><sec sec-type="materials|methods" id="s1"><title>Materials and Methods</title><sec><title>Participants</title><p>Sixty-two students (see <bold>Table <xref ref-type="table" rid="T1">1</xref></bold> for participant information) at the Department of Psychology, Stockholm University, took part in the study for course credits. All participants provided informed consents. The current project was approved by the Ethical Committee Stockholm (EPN Stockholm).</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Participant information across the four conditions.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="left" rowspan="1" colspan="1">Total</th><th valign="top" align="left" rowspan="1" colspan="1">Visual</th><th valign="top" align="left" rowspan="1" colspan="1">Auditory</th><th valign="top" align="left" rowspan="1" colspan="1">Olfactory</th><th valign="top" align="left" rowspan="1" colspan="1">Multimodal</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>n</italic><sub>total</sub></td><td valign="top" align="left" rowspan="1" colspan="1">62</td><td valign="top" align="left" rowspan="1" colspan="1">17</td><td valign="top" align="left" rowspan="1" colspan="1">16</td><td valign="top" align="left" rowspan="1" colspan="1">13</td><td valign="top" align="left" rowspan="1" colspan="1">16</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>n</italic><sub>women</sub></td><td valign="top" align="left" rowspan="1" colspan="1">46</td><td valign="top" align="left" rowspan="1" colspan="1">13</td><td valign="top" align="left" rowspan="1" colspan="1">12</td><td valign="top" align="left" rowspan="1" colspan="1">10</td><td valign="top" align="left" rowspan="1" colspan="1">11</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><italic>n</italic><sub>men</sub></td><td valign="top" align="left" rowspan="1" colspan="1">16</td><td valign="top" align="left" rowspan="1" colspan="1">4</td><td valign="top" align="left" rowspan="1" colspan="1">4</td><td valign="top" align="left" rowspan="1" colspan="1">3</td><td valign="top" align="left" rowspan="1" colspan="1">5</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Mean age years (SD)</td><td valign="top" align="left" rowspan="1" colspan="1">23.94 (2.70)</td><td valign="top" align="left" rowspan="1" colspan="1">23.53 (2.67)</td><td valign="top" align="left" rowspan="1" colspan="1">23.56 (1.90)</td><td valign="top" align="left" rowspan="1" colspan="1">24.62 (2.84)</td><td valign="top" align="left" rowspan="1" colspan="1">24.19 (3.35)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Age range years</td><td valign="top" align="left" rowspan="1" colspan="1">20&#x02013;30</td><td valign="top" align="left" rowspan="1" colspan="1">20&#x02013;29</td><td valign="top" align="left" rowspan="1" colspan="1">20&#x02013;29</td><td valign="top" align="left" rowspan="1" colspan="1">20&#x02013;30</td><td valign="top" align="left" rowspan="1" colspan="1">20&#x02013;30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table></table-wrap></sec><sec><title>Design</title><p>The design was a four-way between subjects design, where the participants were randomly assigned to one of four conditions (three unimodal or one multimodal).</p></sec><sec><title>Materials</title><p>The stimuli materials consisted of 15 pictures, 15 odors, and 15 sounds (see Appendix A for the respective cues). In the unimodal conditions all cues were presented separately whereas in the multimodal condition cues from the three unimodal conditions were presented simultaneously. The unimodal cues were selected so that they could be combined into a multimodal naturalistic context (see Appendix A). Pilot testing resulted in 15 contexts and their unimodal constituents. For example, the context <italic>harbor</italic> was represented by a photo of a harbor by the sea containing fishing boats, sounds from fishing boats, sea birds, sea waves, and the smell of fish. The photographs were real world scenes depicting the visual environment of the context. The sounds consisted of real world sounds typical of the contexts. Finally, the olfactory stimuli consisted of odors typical for the contexts. The olfactory cues consisted of non-synthethic odors, which were kept in non-translucent glass jars to prevent visual inspection.</p><p>The multimodal cues were constructed by combining pictures, odors, and sounds from the three unimodal conditions in such a way that all unimodal cues came from the same natural context.</p></sec><sec><title>Experiential Ratings</title><p>Retrieved memories were rated on the following experiential dimensions: (1) How strong is your feeling of being brought back to the occurrence of the event? (2) How emotional do you experience the event? (3) How pleasant do you experience the event? (4) How important do you experience the event (5) How vivid is your memory of the event? For question 1, 2, 4, and 5, a 5-point a Likert scale was used, where 1 = not at all, 5 = very much. For question 3, a 5-point a Likert scale was used, where 1 = very unpleasant, 5 = very pleasant.</p></sec><sec><title>Procedure</title><p>All subjects were tested individually and were given the following instruction: <italic>You will be presented with a number of memory cues. Your task is to try to remember specific events related to the respective cues. The event may have taken place at any time in your life. Once you remember an event please describe the event verbally as detailed as possible (if possible provide sensory information, feelings and so on). You will be given three minutes to describe the event verbally. You will also be asked to rate the event on some phenomenological dimensions.</italic></p><p>Thirty seconds per cue were allowed for retrieval. Given that the participant were able to generate a memory, three minutes were devoted for the verbal description, which was recorded with a digital audio recorder. The presentation order of the 15 cues was randomized for each participant. For the unimodal conditions the cues were administered in the following way: In the visual condition the pictures were presented on a 22-inch LCD computer screen. Auditory cues were presented with a pair of AKG 701 reference headphones connected to the same computer controlling the visual presentation. The odors were provided in non-translucent glass jars. The participants held the jars themselves and were given a cue to signal the start of sniffing. In the multimodal condition, the cues were presented simultaneously and in the same way as in the respective unimodal conditions.</p><p>Each event was rated on the five experiential dimensions, as specified in the <italic>experiential ratings</italic> section, immediately after the verbal description. Lastly, following the retrieval phase all memories were dated according to the age of the participants at the occurrence of the events.</p></sec></sec><sec><title>Results</title><sec><title>Number of Evoked Memories</title><p>The number of evoked memories for each participant were analyzed using a one-way ANOVA with modality, i.e., visual (<italic>M</italic> = 11.53, <italic>SD</italic> = 3.50), auditory (<italic>M</italic> = 9.81, <italic>SD</italic> = 2.93), olfactory (<italic>M</italic> = 9.00, <italic>SD</italic> = 3.21), and multimodal (<italic>M</italic> = 10.88, <italic>SD</italic> = 3.20), as between-subject factor. No main effect of modality was observed [<italic>F</italic>(3,58) = 1.81, <italic>p</italic> &#x0003e; 0.05, &#x003b7;<sup>2</sup> = 0.085].</p></sec><sec><title>Age Distributions</title><p>In order to statistically analyze the age distributions, the lifespan was segmented into six 5-year intervals, (i.e., 0&#x02013;5, 6&#x02013;10, 11&#x02013;15, 16&#x02013;20, 21&#x02013;25, and 26&#x02013;30 years). Next, the proportions of memories in each interval for the respective participants were calculated by dividing the number of memories in a particular interval with the total number of retrieved events for that participant. The proportions of memories were then analyzed using four separate one-way ANOVAs (i.e., visual, auditory, olfactory, and multimodal) with interval as within group factor. Given that the age distributions were analyzed with four separate ANOVAs the <italic>p</italic>-level was Bonferroni adjusted (i.e., <italic>p</italic> = 0.05/4).</p><p>First, for the visual age distribution Mauchly&#x02019;s test of sphericity indicated unequal variances across intervals and thus the ANOVA was Greenhouse&#x02013;Geisser corrected. The ANOVA indicated a significant main effect of interval [<italic>F</italic>(2.62,41.97) = 6.24, <italic>p</italic> &#x0003c; 0.01, &#x003b7;<sup>2</sup> = 0.28]. Repeated contrasts (SPSS 21) indicated that the proportion of memories in the 6&#x02013;10 year interval was higher than in the 0&#x02013;5 year interval, the 16&#x02013;20 year interval contained a higher proportion of memories than 11&#x02013;15 (<italic>p</italic> &#x0003c; 0.05), and the 21&#x02013;25 year interval contained a higher proportion of memories than the 26&#x02013;30 year interval. No other repeated contrasts were significant (<italic>p</italic>s &#x0003e; 0.05). The participants of the four conditions retrieved between 4&#x02013;15 memories. However, in the visual condition one of the participants only retrieved two memories, therefore an additional ANOVA without this participant was computed. The results did not change following the exclusion of the participant with only two memories [<italic>F</italic>(2.59,38.81) = 5.19, <italic>p</italic> &#x0003c; 0.01, &#x003b7;<sup>2</sup> = 0.26].</p><p>For the auditory age distribution Mauchly&#x02019;s test of sphericity indicated unequal variances across intervals, consequently the ANOVA was Greenhouse&#x02013;Geisser corrected. The ANOVA indicated a significant main effect of interval [<italic>F</italic>(3.00,44.96) = 5.58<italic>, p</italic> &#x0003c; 0.01, &#x003b7;<sup>2</sup> = 0.27]. Repeated contrasts indicated that the proportion of memories in the 6&#x02013;10 year interval was higher than in the 0&#x02013;5 year interval and the proportion of memories was higher in the 21&#x02013;25 year interval compared to the 26&#x02013;30 year interval (<italic>p</italic>s &#x0003c; 0.05). No other comparisons were significant (<italic>p</italic>s &#x0003e; 0.05).</p><p>For the olfactory condition the ANOVA indicated a marginally significant main effect of interval [<italic>F</italic>(5,60) = 2.76, <italic>p</italic> = 0.026, &#x003b7;<sup>2</sup> = 0.19]. Repeated contrasts indicated that the 6&#x02013;10 year interval contained a higher proportion of memories than the 0&#x02013;5 year interval and the 11&#x02013;15 year interval (<italic>p</italic>s &#x0003c; 0.05). No other comparisons were significant (<italic>p</italic>s &#x0003e; 0.05).</p><p>Finally, for the multimodal age distribution Mauchly&#x02019;s test of sphericity indicated unequal variances across intervals. Therefore, the ANOVA was Greenhouse&#x02013;Geisser corrected. No significant effect of interval was observed [<italic>F</italic>(2.80,75) = 2.53<italic>, p</italic> = 0.074, &#x003b7;<sup>2</sup> = 0.14].</p><p>The age distributions are displayed in <bold>Figure <xref ref-type="fig" rid="F1">1</xref></bold>.</p><fig id="F1" position="float"><label>FIGURE 1</label><caption><p><bold>The age distributions of the four cue-conditions</bold>.</p></caption><graphic xlink:href="fpsyg-06-01681-g001"/></fig></sec><sec><title>Decomposing the Age at Event Distribution into the Early Life Bumps and Forgetting</title><p>Conceptually the autobiographical memory distribution can also be divided into an early life bump, which in previous literature typically has been found to occur around 10&#x02013;30 years at the event (<xref rid="B15" ref-type="bibr">Koppel and Berntsen, 2015</xref>), and a recency gradient that presumably occurs due to more recent memories are less forgotten compared to older ones. Typically, these two components can be observed in age at event data; however, more recently <xref rid="B13" ref-type="bibr">Janssen et al. (2011)</xref> suggested a method where these two components can be decomposed through a mathematical procedure. In the present study we investigated how cues of different sensory modalities quantitatively impact on the bump and the forgetting curve components of autobiographical memories. We studied this by applying the <xref rid="B13" ref-type="bibr">Janssen et al. (2011)</xref> procedure to our data.</p><p>This procedure consists of six steps that are carefully described in <xref rid="B13" ref-type="bibr">Janssen et al. (2011)</xref>, and here we briefly summarize the steps; (1) The proportion of events are calculate for each participant separately and (2) a power-function is fitted to all participants based on the 10 most recent years. (3) A predicted value of the forgetting curve is calculated for each participant, using the fitted value from step (2) and empirical data from step (1). (4) The result from step (1) is divided with the results from step (3). (5) The resulting distribution from step (4) is normalized for each participant and (6) averaged over all participants. This procedure was conducted separately for each of the four conditions in our dataset.</p><sec><title>The Autobiographical Memory Bump</title><p><bold>Figure <xref ref-type="fig" rid="F2">2</xref></bold> shows the resulting probability density distributions, where the originally data is smoothed with a moving average methods consisting of 5 years. In essence this figure shows the age at event distributions when the forgetting component is mathematically removed from the dataset. Several interesting findings can be noted from this analysis. First, the perhaps most apparent results is that olfactory distribution (blue) has a peak around six years of age that is earlier than the visual and auditory modalities. Second, in contrast, the auditory and visual modalities has two peaks, an early peak around eight, which appears to be slightly later than the olfactory peak, and a later peak around the age of 20 that is absent in the olfactory modality. Third, although, the distributions of visual and auditory age at events are similar, the early auditory peak is larger than later auditory peaks, whereas the later visual peak is larger than the early visual peak. Fourth, the multimodal peak is more similar to the visual and auditory modalities, than the olfactory modality; however, the multimodal condition does not have the pronounced peaks in the interval from five to twenty years of the events.</p><fig id="F2" position="float"><label>FIGURE 2</label><caption><p><bold>The corrected autobiographical memory distributions for the four conditions</bold>.</p></caption><graphic xlink:href="fpsyg-06-01681-g002"/></fig><p>To further measure the similarity of the age at event density distributions, we calculated the dot products between the vector representing this distribution (i.e., the length of each distribution was first normalized to one, and two distributions where multiplied and summed, and finally the square root of the resulting value was presented). The results (see <bold>Table <xref ref-type="table" rid="T2">2</xref></bold>) show that visual and the auditory distributions are more similar, whereas the olfactory distribution is more dissimilar to these distributions. In contrast, the multimodal distribution is approximately as similar to the three unimodal conditions.</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>The dot product between the densities of the age at event distributions for the four modalities.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1">Visual</th><th valign="top" align="center" rowspan="1" colspan="1">Auditory</th><th valign="top" align="center" rowspan="1" colspan="1">Olfactory</th><th valign="top" align="center" rowspan="1" colspan="1">Multimodal</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Visual</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">0.9769</td><td valign="top" align="center" rowspan="1" colspan="1">0.9237</td><td valign="top" align="center" rowspan="1" colspan="1">0.9719</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Auditory</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">0.9728</td><td valign="top" align="center" rowspan="1" colspan="1">0.9823</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Olfactory</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td><td valign="top" align="center" rowspan="1" colspan="1">0.9711</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Multimodal</td><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1"/><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table></table-wrap></sec><sec><title>Forgetting Curves</title><p>The <xref rid="B13" ref-type="bibr">Janssen et al. (2011)</xref> procedure also estimates parameters to power-function forgetting curves that are fitted to the 10 most recent years. These curves are plotted in <bold>Figure <xref ref-type="fig" rid="F3">3</xref></bold>. The results shows that the visual and auditory modalities are more strongly remembered for the most recent years, but at the same time show a faster forgetting over the years, compared to the olfactory condition [<italic>F</italic>(3,58) = 57.07, <italic>p</italic> &#x0003c; 0.05 &#x003b7;<sup>2</sup> = 0.125]. This effect is slightly stronger for the auditory compared to the visual modality. In contrast the olfactory condition shows a rather shallow forgetting and a moderate memory for recent memories. Finally, the multimodal condition show intermediate slope and recency effects, although this conditions is more similar to the visual and auditory conditions compared to the olfactory condition. The exponents of the power-function for the visual, auditory, olfactory and the multimodal conditions where &#x02013;0.74, &#x02013;0.88, &#x02013;0.40, and &#x02013;0.66, respectively, and their associated intercepts where 0.38, 0.43, 0.25, and 0.33, respectively.</p><fig id="F3" position="float"><label>FIGURE 3</label><caption><p><bold>The forgetting curves of the four conditions</bold>.</p></caption><graphic xlink:href="fpsyg-06-01681-g003"/></fig></sec></sec><sec><title>Experiential Ratings</title><p>Mean values for the five experiential ratings were calculated for each participant. Next, the mean values were analyzed using five separate one-way ANOVAs with modality as between group factor. The ANOVAs did not yield any significant differences (<italic>p</italic>s &#x0003e; 0.05) across the four modalities. See <bold>Table <xref ref-type="table" rid="T3">3</xref></bold> for further details.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>Mean experiential ratings (and SD) across the four conditions and &#x003b7;<sup>2</sup> for the five ANOVAs.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" colspan="5" rowspan="1">Condition</th></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" colspan="5" rowspan="1"><hr/></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Experiential rating</th><th valign="top" align="center" rowspan="1" colspan="1">Visual</th><th valign="top" align="center" rowspan="1" colspan="1">Auditory</th><th valign="top" align="center" rowspan="1" colspan="1">Olfactory</th><th valign="top" align="center" rowspan="1" colspan="1">Multimodal</th><th valign="top" align="center" rowspan="1" colspan="1">&#x003b7;<sup>2</sup></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Pleasantness</td><td valign="top" align="center" rowspan="1" colspan="1">3.29 (0.43)</td><td valign="top" align="center" rowspan="1" colspan="1">3.52 (0.48)</td><td valign="top" align="center" rowspan="1" colspan="1">3.47 (0.54)</td><td valign="top" align="center" rowspan="1" colspan="1">3.47 (0.43)</td><td valign="top" align="center" rowspan="1" colspan="1">0.037</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Feeling of being brought back in time</td><td valign="top" align="center" rowspan="1" colspan="1">3.50 (0.57)</td><td valign="top" align="center" rowspan="1" colspan="1">3.32 (0.50)</td><td valign="top" align="center" rowspan="1" colspan="1">3.58 (0.52)</td><td valign="top" align="center" rowspan="1" colspan="1">3.61 (0.47)</td><td valign="top" align="center" rowspan="1" colspan="1">0.049</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Vividness</td><td valign="top" align="center" rowspan="1" colspan="1">3.52 (0.50)</td><td valign="top" align="center" rowspan="1" colspan="1">3.48 (0.54)</td><td valign="top" align="center" rowspan="1" colspan="1">3.53 (0.60)</td><td valign="top" align="center" rowspan="1" colspan="1">3.50 (0.57)</td><td valign="top" align="center" rowspan="1" colspan="1">0.001</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Importance</td><td valign="top" align="center" rowspan="1" colspan="1">2.45 (0.47)</td><td valign="top" align="center" rowspan="1" colspan="1">2.40 (0.88)</td><td valign="top" align="center" rowspan="1" colspan="1">2.56 (0.68)</td><td valign="top" align="center" rowspan="1" colspan="1">2.79 (0.68)</td><td valign="top" align="center" rowspan="1" colspan="1">0.049</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Emotionality</td><td valign="top" align="center" rowspan="1" colspan="1">2.74 (0.31)</td><td valign="top" align="center" rowspan="1" colspan="1">2.64 (0.46)</td><td valign="top" align="center" rowspan="1" colspan="1">2.83 (0.74)</td><td valign="top" align="center" rowspan="1" colspan="1">2.92 (0.64)</td><td valign="top" align="center" rowspan="1" colspan="1">0.040</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"/></tr></tbody></table></table-wrap></sec></sec><sec><title>Discussion</title><p>The present study is the first to investigate the age distribution of multimodal cues in autobiographical memory retrieval. Results showed that the age distribution of autobiographical memories varies as a function of cue-modality. In addition to replicating previous findings on the age distributions for visual and olfactory retrieval cues, two novel distributions were also shown, i.e., the age distributions for naturalistic auditory cues (cf. <xref rid="B24" ref-type="bibr">Schulkind et al., 1999</xref>; <xref rid="B25" ref-type="bibr">Schulkind and Woldorf, 2005</xref>; <xref rid="B2" ref-type="bibr">Cady et al., 2008</xref>) and multimodal cues. Importantly, modelling of the age distributions suggests that multimodal is more similar to visual and auditory age distributions than the olfactory distribution, and that the multimodal retrieval of autobiographical memories may be more driven by visual and auditory information. No differences were observed across the retrieval cues regarding experiential ratings or number of retrieved events.</p><p>In conjunction with previous research on visual dominance the present study suggest that there is a hierarchy among the modalities contained in a multimodal cue. We suggest based on modelling of the age distributions that visual and auditory information are the dominant sensory input when probing autobiographical memories with multimodal retrieval cues. The findings of the current study are in line with the results of <xref rid="B14" ref-type="bibr">Karlsson et al. (2013)</xref>. Thus, the suggested modality hierarchy has been demonstrated using two different types of data, i.e., age distribution data and the content of autobiographical memories. Extrapolating from previous studies on visual dominance (e.g., <xref rid="B23" ref-type="bibr">Schmid et al., 2011</xref>), it may be speculated that visual information contributes more than auditory information to the retrieval outcome. This view is in line with <xref rid="B10" ref-type="bibr">Greenberg and Rubin (2003)</xref> who suggest that the visual system is central for autobiographical memory and that sensory systems can activate each other during autobiographical memory recall in a cascading fashion.</p><p>Potentially, the underlying mechanism for this suggested cue-dominance could be related to attentional processing of sensory information. For example, from perception experiments on visual dominance utilizing non-autobiographical protocols it has been suggested that visual dominance is the result of more attention being directed toward visual information compared to information pertaining to other sensory systems (e.g., <xref rid="B19" ref-type="bibr">Posner et al., 1976</xref>; <xref rid="B27" ref-type="bibr">Sinnett et al., 2007</xref>). Although the present study did not address the underlying mechanism for the suggested cue-dominance we suggest that this effect may be related to an asymmetry in attention occurring either at (a) encoding or (b) at retrieval when sensory input (i.e., the retrieval cue) is processed.</p><p>No synergistic effects were observed among the modalities in the multimodal cue. A synergistic effect between visual, olfactory, and auditory information in the multimodal cue would likely have been manifested as a higher frequency of retrieved events, higher experiential ratings, and a unique age distribution (i.e., not being modelled on the three unimodal distributions) for the multimodal condition compared to the unimodal conditions. None of this was the case in the present data. The lack of synergistic effects provides further support for the view that there is a dominating modality (e.g., vision) within the multimodal cues.</p><p>Despite the seemingly smaller contribution of olfaction on multimodal retrieval it is possible that olfaction influence multimodal retrieval indirectly by modulating attention. For example, <xref rid="B26" ref-type="bibr">Seo et al. (2010)</xref> demonstrated using an eye-tracking protocol that participants were more prone to attend odor-congruent pictures when presented simultaneously with odor-congruent and -incongruent food pictures.</p><p>It was also shown that the forgetting curves of the visual, auditory, and multimodal conditions were relatively similar. However, the forgetting curve of olfactory condition was less steep compared to the other three conditions. It could be speculated that forgetting curve of the olfactory condition is in part related to proactive interference. A stronger proactive interference would predict a less steep forgetting curve. The notion of stronger proactive interference in odor memory compared to other modalities and a unique neural representation for first-learned olfactory associations has been supported in several studies (e.g., <xref rid="B18" ref-type="bibr">Lawless and Engen, 1977</xref>; <xref rid="B31" ref-type="bibr">Yeshurun et al., 2009</xref>). We suggest that the similarity between the forgetting curves of the visual, auditory, and multimodal conditions provide further support for the notion that multimodal retrieval of autobiographical memory is mainly driven by visual and auditory processes.</p><p>In line with previous work on childhood amnesia (e.g., <xref rid="B22" ref-type="bibr">Rubin and Schulkind, 1997</xref>; <xref rid="B1" ref-type="bibr">Bruce et al., 2005</xref>) the present study indicated that few memories were retrieved from the 0&#x02013;5 years interval. <bold>Figure <xref ref-type="fig" rid="F3">3</xref></bold> suggest that odor-evoked memories may have an earlier onset than memories retrieved by pictures or sounds. However, conclusions concerning cue-modality differences with regard to the childhood amnesia would require a follow-up study targeting childhood amnesia specifically.</p><p>It should be noted that the participants in the current study represent a young sample (20&#x02013;30 years). Typically, the age range is around 50 years and upward in age distribution studies. Although the present study did detected differences in the age distributions across cue conditions it is suggested that further studies are carried out with older participants.</p><p>Given that several (unimodal) studies have reported differences in experiential ratings (e.g., pleasantness/emotiona lity, vividness, and the feeling of being brought back in time) as a function of cue-modality it is somewhat surprising that no significant differences were observed in the experiential ratings of the present study (cf. <xref rid="B8" ref-type="bibr">Ehrlichman and Bastone, 1992</xref>; <xref rid="B4" ref-type="bibr">Chu and Downes, 2002</xref>; <xref rid="B12" ref-type="bibr">Herz and Schooler, 2002</xref>; <xref rid="B11" ref-type="bibr">Herz, 2004</xref>; <xref rid="B9" ref-type="bibr">Goddard et al., 2005</xref>; <xref rid="B28" ref-type="bibr">Willander and Larsson, 2006</xref>, <xref rid="B29" ref-type="bibr">2007</xref>). However, null hypothesis significance testing does not permit any conclusions regarding null findings and the lack of differences regarding experiential ratings could potentially be related to power. An potential alternative explanation could also be that the effect sizes of the studied variables are weaker in the current study compared to previous work due to, e.g., the age of the participants or sample size. The effect sizes (Cohen&#x02019;s <italic>d</italic>) typically range between 0 and 1 in previous studies where the phenomenological experiences (valence/emotionality, vividness, feeling of being brought back) of autobiographical memories cued by different modalities were contrasted (e.g., <xref rid="B20" ref-type="bibr">Rubin et al., 1984</xref>; <xref rid="B4" ref-type="bibr">Chu and Downes, 2002</xref>; <xref rid="B9" ref-type="bibr">Goddard et al., 2005</xref>; <xref rid="B28" ref-type="bibr">Willander and Larsson, 2006</xref>, <xref rid="B29" ref-type="bibr">2007</xref>). However, in some instances the effect sizes exceed <italic>d</italic> = 3 (<xref rid="B12" ref-type="bibr">Herz and Schooler, 2002</xref>; <xref rid="B11" ref-type="bibr">Herz, 2004</xref>). Overall these effect sizes from previous work indicated that odor-cued memories are rated as more emotional and associated with stronger feelings of being brought back in time to the occurrence of the event. Regarding vividness the results are less consistent across studies. Based on the effect sizes of the current study the multimodally and olfactory cued memories seems to evoke somewhat stronger phenomenological experiences than the visual and auditory cued events, although these differences were non-significant. Thus, with regard to the magnitude and direction of the effect sizes, it seems as if the present study is in line with previous work. Based on the results of the present study we suggest that experiential ratings could be insufficient and that thorough analyses of different experiential dimensions based on content (i.e., verbal descriptions) and semantic scales are needed as a complement to better understand potential differences across modalities (cf. <xref rid="B14" ref-type="bibr">Karlsson et al., 2013</xref>).</p><p>In summary, the present study replicated and extended previous findings concerning the age distributions for autobiographical retrieval cues. More importantly, we suggest that there is a modality hierarchy in multimodal retrieval cues, such that multimodal retrieval is mainly driven by visual and auditory information.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>The project &#x0201c;Multimodal processes in the retrieval of autobiographical memory&#x0201d; was funded by the Swedish Research Council (2008&#x02013;2415) to JW and by The Swedish Foundation for Humanities and Social Sciences (M14-0375:1).</p></ack><sec sec-type="supplementary material"><title>Supplementary Material</title><p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01681">http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01681</ext-link></p><supplementary-material content-type="local-data" id="SM1"><media xlink:href="Data_Sheet_1.DOCX"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>D.</given-names></name><name><surname>Wilcox-O&#x02019;Hearn</surname><given-names>L. A.</given-names></name><name><surname>Robinson</surname><given-names>J. A.</given-names></name><name><surname>Philips-Grant</surname><given-names>K.</given-names></name><name><surname>Francis</surname><given-names>L.</given-names></name><name><surname>Smith</surname><given-names>M.</given-names></name></person-group> (<year>2005</year>). <article-title>Fragment memories marks the end of childhood amnesia.</article-title>
<source><italic>Mem. Cogn.</italic></source>
<volume>33</volume>
<fpage>567</fpage>&#x02013;<lpage>576</lpage>. <pub-id pub-id-type="doi">10.3758/BF03195324</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cady</surname><given-names>E. T.</given-names></name><name><surname>Harris</surname><given-names>R. J.</given-names></name><name><surname>Knappenberger</surname><given-names>J. B.</given-names></name></person-group> (<year>2008</year>). <article-title>Using music to cue autobiographical memories of different lifetime periods.</article-title>
<source><italic>Psychol. Music</italic></source>
<volume>36</volume>
<fpage>157</fpage>&#x02013;<lpage>177</lpage>. <pub-id pub-id-type="doi">10.1177/0305735607085010</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>S.</given-names></name><name><surname>Downes</surname><given-names>J. J.</given-names></name></person-group> (<year>2000</year>). <article-title>Long live Proust: the odour-cued autobiographical memory bump.</article-title>
<source><italic>Cognition</italic></source>
<volume>75</volume>
<fpage>B41</fpage>&#x02013;<lpage>B50</lpage>. <pub-id pub-id-type="doi">10.1016/S0010-0277(00)00065-2</pub-id><pub-id pub-id-type="pmid">10771279</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>S.</given-names></name><name><surname>Downes</surname><given-names>J. J.</given-names></name></person-group> (<year>2002</year>). <article-title>Proust knows best: odors are better cues of autobiographical memory.</article-title>
<source><italic>Mem. Cogn.</italic></source>
<volume>30</volume>
<fpage>511</fpage>&#x02013;<lpage>518</lpage>. <pub-id pub-id-type="doi">10.3758/BF03194952</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colavita</surname><given-names>F. B.</given-names></name><name><surname>Weisberg</surname><given-names>D.</given-names></name></person-group> (<year>1979</year>). <article-title>A further investigation of visual dominance.</article-title>
<source><italic>Percept. Psychophys.</italic></source>
<volume>25</volume>
<fpage>345</fpage>&#x02013;<lpage>347</lpage>. <pub-id pub-id-type="doi">10.3758/BF03198814</pub-id><pub-id pub-id-type="pmid">461094</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conway</surname><given-names>M. A.</given-names></name><name><surname>Pleydell-Pearce</surname><given-names>C. W.</given-names></name></person-group> (<year>2000</year>). <article-title>The construction of autobiographical memories in the self-memory system.</article-title>
<source><italic>Psychol. Rev.</italic></source>
<volume>107</volume>
<fpage>261</fpage>&#x02013;<lpage>288</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295X.107.2.261</pub-id><pub-id pub-id-type="pmid">10789197</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driver</surname><given-names>J.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2000</year>). <article-title>Multisensory perception: beyond modularity and convergence.</article-title>
<source><italic>Curr. Biol.</italic></source>
<volume>10</volume>
<fpage>R731</fpage>&#x02013;<lpage>R735</lpage>. <pub-id pub-id-type="doi">10.1016/S0960-9822(00)00740-5</pub-id><pub-id pub-id-type="pmid">11069095</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ehrlichman</surname><given-names>H.</given-names></name><name><surname>Bastone</surname><given-names>L.</given-names></name></person-group> (<year>1992</year>). &#x0201c;<article-title>Olfaction and emotion</article-title>,&#x0201d; in <source><italic>Science of Olfaction</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Serby</surname><given-names>M.</given-names></name><name><surname>Chobor</surname><given-names>L.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer Verlag</publisher-name>), <fpage>410</fpage>&#x02013;<lpage>438</lpage>.</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goddard</surname><given-names>L.</given-names></name><name><surname>Pring</surname><given-names>L.</given-names></name><name><surname>Felmingham</surname><given-names>N.</given-names></name></person-group> (<year>2005</year>). <article-title>The effects of cue modality on the quality of personal memories retrieved.</article-title>
<source><italic>Memory</italic></source>
<volume>13</volume>
<fpage>79</fpage>&#x02013;<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1080/09658210344000594</pub-id><pub-id pub-id-type="pmid">15724909</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>D. L.</given-names></name><name><surname>Rubin</surname><given-names>D. C.</given-names></name></person-group> (<year>2003</year>). <article-title>The neuropsychology of autobiographical memory.</article-title>
<source><italic>Cortex</italic></source>
<volume>39</volume>
<fpage>678</fpage>&#x02013;<lpage>728</lpage>. <pub-id pub-id-type="doi">10.1016/S0010-9452(08)70860-8</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herz</surname><given-names>R. S.</given-names></name></person-group> (<year>2004</year>). <article-title>A naturalistic analysis of autobiographical memories triggered by olfactory visual and auditory stimuli.</article-title>
<source><italic>Chem. Senses</italic></source>
<volume>29</volume>
<fpage>217</fpage>&#x02013;<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1093/chemse/bjh025</pub-id><pub-id pub-id-type="pmid">15047596</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herz</surname><given-names>R. S.</given-names></name><name><surname>Schooler</surname><given-names>J. W.</given-names></name></person-group> (<year>2002</year>). <article-title>A naturalistic study of autobiographical memories evoked by olfactory and visual cues: testing the Proustian hypothesis.</article-title>
<source><italic>Am. J. Psychol.</italic></source>
<volume>115</volume>
<fpage>21</fpage>&#x02013;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.2307/1423672</pub-id><pub-id pub-id-type="pmid">11868193</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janssen</surname><given-names>S. M.</given-names></name><name><surname>Gralak</surname><given-names>A.</given-names></name><name><surname>Murre</surname><given-names>J. M.</given-names></name></person-group> (<year>2011</year>). <article-title>A model for removing the increased recall of recent events from the temporal distribution of autobiographical memory.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>43</volume>
<fpage>916</fpage>&#x02013;<lpage>930</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-011-0110-z</pub-id><pub-id pub-id-type="pmid">21614661</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlsson</surname><given-names>K.</given-names></name><name><surname>Sikstr&#x000f6;m</surname><given-names>S.</given-names></name><name><surname>Willander</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>The semantic representation of event information depends on the cue-modality: an instance of meaning based retrieval.</article-title>
<source><italic>PLoS ONE</italic></source>
<volume>8</volume>:<issue>e73378</issue>
<pub-id pub-id-type="doi">10.1371/journal.pone.0073378</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koppel</surname><given-names>J.</given-names></name><name><surname>Berntsen</surname><given-names>D.</given-names></name></person-group> (<year>2015</year>). <article-title>The peaks of life: the differential temporal locations of the reminiscence bump across disparate cueing methods.</article-title>
<source><italic>J. Appl. Res. Mem. Cogn.</italic></source>
<volume>4</volume>
<fpage>66</fpage>&#x02013;<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1016/j.jarmac.2014.11.004</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koppen</surname><given-names>C.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name></person-group> (<year>2007</year>). <article-title>Audiovisual asynchrony modulates the colavita visual dominance effect.</article-title>
<source><italic>Brain Res.</italic></source>
<volume>1186</volume>
<fpage>224</fpage>&#x02013;<lpage>232</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2007.09.076</pub-id><pub-id pub-id-type="pmid">18005944</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsson</surname><given-names>M.</given-names></name><name><surname>Willander</surname><given-names>J.</given-names></name><name><surname>Karlsson</surname><given-names>K.</given-names></name><name><surname>Arshamian</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>The olfactory LOVER model: behavioral and neural correlates in autobiographical memory.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>5</volume>:<issue>312</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2014.00312</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawless</surname><given-names>H.</given-names></name><name><surname>Engen</surname><given-names>T.</given-names></name></person-group> (<year>1977</year>). <article-title>Associations to odors: interference, mnemonics and verbal labeling.</article-title>
<source><italic>J. Exp. Psychol. Hum. Learn. Mem.</italic></source>
<volume>3</volume>
<fpage>52</fpage>&#x02013;<lpage>59</lpage>.</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>M. I.</given-names></name><name><surname>Nissen</surname><given-names>M. J.</given-names></name><name><surname>Klein</surname><given-names>R. M.</given-names></name></person-group> (<year>1976</year>). <article-title>Visual dominance: an information-processing account of its origins and significance.</article-title>
<source><italic>Psychol. Rev.</italic></source>
<volume>83</volume>
<fpage>157</fpage>&#x02013;<lpage>171</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295X.83.2.157</pub-id><pub-id pub-id-type="pmid">769017</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>D. C.</given-names></name><name><surname>Groth</surname><given-names>E.</given-names></name><name><surname>Goldsmith</surname><given-names>D. J.</given-names></name></person-group> (<year>1984</year>). <article-title>Olfactory cuing of autobiographical memory.</article-title>
<source><italic>Am. J. Psychol.</italic></source>
<volume>97</volume>
<fpage>493</fpage>&#x02013;<lpage>507</lpage>. <pub-id pub-id-type="doi">10.2307/1422158</pub-id><pub-id pub-id-type="pmid">6517162</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>D. C.</given-names></name><name><surname>Rahhal</surname><given-names>T. A.</given-names></name><name><surname>Poon</surname><given-names>L. W.</given-names></name></person-group> (<year>1998</year>). <article-title>Things learned in early adulthood are remembered best.</article-title>
<source><italic>Mem. Cogn.</italic></source>
<volume>26</volume>
<fpage>3</fpage>&#x02013;<lpage>19</lpage>. <pub-id pub-id-type="doi">10.3758/BF03211366</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>D. C.</given-names></name><name><surname>Schulkind</surname><given-names>M. D.</given-names></name></person-group> (<year>1997</year>). <article-title>The distribution of autobiographical memories across the lifespan.</article-title>
<source><italic>Mem. Cogn.</italic></source>
<volume>25</volume>
<fpage>859</fpage>&#x02013;<lpage>866</lpage>. <pub-id pub-id-type="doi">10.3758/BF03211330</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmid</surname><given-names>C.</given-names></name><name><surname>B&#x000fc;chel</surname><given-names>C.</given-names></name><name><surname>Rose</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>The neural basis of visual dominance in the context of audio-visual object processing.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>55</volume>
<fpage>304</fpage>&#x02013;<lpage>311</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.11.051</pub-id><pub-id pub-id-type="pmid">21112404</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulkind</surname><given-names>M. D.</given-names></name><name><surname>Hennis</surname><given-names>L. K.</given-names></name><name><surname>Rubin</surname><given-names>D. C.</given-names></name></person-group> (<year>1999</year>). <article-title>Music, emotion, and autobiographical memory: they&#x02019;re playing your song.</article-title>
<source><italic>Mem. Cogn.</italic></source>
<volume>27</volume>
<fpage>948</fpage>&#x02013;<lpage>955</lpage>. <pub-id pub-id-type="doi">10.3758/BF03201225</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulkind</surname><given-names>M. D.</given-names></name><name><surname>Woldorf</surname><given-names>G. M.</given-names></name></person-group> (<year>2005</year>). <article-title>Emotional organization of autobiographical memory.</article-title>
<source><italic>Mem. Cogn.</italic></source>
<volume>33</volume>
<fpage>1025</fpage>&#x02013;<lpage>1035</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193210</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname><given-names>H.-S.</given-names></name><name><surname>Roidl</surname><given-names>E.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>F.</given-names></name><name><surname>Negoias</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Odors enhance visual attention to congruent objects.</article-title>
<source><italic>Appetite</italic></source>
<volume>54</volume>
<fpage>544</fpage>&#x02013;<lpage>549</lpage>. <pub-id pub-id-type="doi">10.1016/j.appet.2010.02.011</pub-id><pub-id pub-id-type="pmid">20176065</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinnett</surname><given-names>S.</given-names></name><name><surname>Spence</surname><given-names>C.</given-names></name><name><surname>Soto-Faraco</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>Visual dominance and attention: the Colavita effect revisited.</article-title>
<source><italic>Percept. Psychophys.</italic></source>
<volume>69</volume>
<fpage>673</fpage>&#x02013;<lpage>686</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193770</pub-id><pub-id pub-id-type="pmid">17929691</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willander</surname><given-names>J.</given-names></name><name><surname>Larsson</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Smell your way back to childhood: autobiographical odor memory.</article-title>
<source><italic>Psychon. Bull. Rev.</italic></source>
<volume>13</volume>
<fpage>240</fpage>&#x02013;<lpage>244</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193837</pub-id><pub-id pub-id-type="pmid">16892988</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willander</surname><given-names>J.</given-names></name><name><surname>Larsson</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>Olfaction and emotion: the case of autobiographical memory.</article-title>
<source><italic>Mem. Cogn.</italic></source>
<volume>35</volume>
<fpage>1659</fpage>&#x02013;<lpage>1663</lpage>. <pub-id pub-id-type="doi">10.3758/BF03193499</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willander</surname><given-names>J.</given-names></name><name><surname>Larsson</surname><given-names>M.</given-names></name></person-group> (<year>2008</year>). <article-title>The mind&#x02019;s nose and autobiographical odor memory.</article-title>
<source><italic>Chemosens. Percept.</italic></source>
<volume>1</volume>
<fpage>210</fpage>&#x02013;<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1007/s12078-008-9026-0</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname><given-names>Y.</given-names></name><name><surname>Lapid</surname><given-names>H.</given-names></name><name><surname>Dudai</surname><given-names>Y.</given-names></name><name><surname>Sobel</surname><given-names>N.</given-names></name></person-group> (<year>2009</year>). <article-title>The privileged brain representation of first olfactory associations.</article-title>
<source><italic>Curr. Biol.</italic></source>
<volume>19</volume>
<fpage>1869</fpage>&#x02013;<lpage>1874</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2009.09.066</pub-id><pub-id pub-id-type="pmid">19896380</pub-id></mixed-citation></ref></ref-list></back></article>
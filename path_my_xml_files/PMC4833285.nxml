<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27082758</article-id><article-id pub-id-type="pmc">4833285</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0153596</article-id><article-id pub-id-type="publisher-id">PONE-D-15-41785</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Health Care</subject><subj-group><subject>Health Care Providers</subject><subj-group><subject>Medical Doctors</subject><subj-group><subject>Surgeons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Medical Doctors</subject><subj-group><subject>Surgeons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Health Care</subject><subj-group><subject>Health Care Providers</subject><subj-group><subject>Medical Doctors</subject><subj-group><subject>Physicians</subject><subj-group><subject>Surgeons</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Medical Doctors</subject><subj-group><subject>Physicians</subject><subj-group><subject>Surgeons</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Communications</subject><subj-group><subject>Semiotics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Health Care</subject><subj-group><subject>Health Care Providers</subject><subj-group><subject>Medical Doctors</subject><subj-group><subject>Physicians</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Medical Doctors</subject><subj-group><subject>Physicians</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Human Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Research Design</subject><subj-group><subject>Survey Research</subject><subj-group><subject>Questionnaires</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability Theory</subject><subj-group><subject>Probability Distribution</subject><subj-group><subject>Normal Distribution</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Clinical Medicine</subject><subj-group><subject>Clinical Trials</subject><subj-group><subject>Randomized Controlled Trials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Pharmacology</subject><subj-group><subject>Drug Research and Development</subject><subj-group><subject>Clinical Trials</subject><subj-group><subject>Randomized Controlled Trials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Clinical Trials</subject><subj-group><subject>Randomized Controlled Trials</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group></article-categories><title-group><article-title>Gesture-Controlled Image Management for Operating Room: A Randomized Crossover Study to Compare Interaction Using Gestures, Mouse, and Third Person Relaying</article-title><alt-title alt-title-type="running-head">Gesture-Controlled Image Management for Operating Room</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6339-5578</contrib-id><name><surname>Wipfli</surname><given-names>Rolf</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Dubois-Ferri&#x000e8;re</surname><given-names>Victor</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Budry</surname><given-names>Sylvain</given-names></name><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Hoffmeyer</surname><given-names>Pierre</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Lovis</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib></contrib-group><aff id="aff001"><label>1</label><addr-line>Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland</addr-line></aff><aff id="aff002"><label>2</label><addr-line>Division of Orthopaedics and Trauma Surgery, Geneva University Hospitals, Geneva, Switzerland</addr-line></aff><aff id="aff003"><label>3</label><addr-line>University of Geneva, Faculty of Medicine, Geneva, Switzerland</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>van Ooijen</surname><given-names>Peter M.A.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>University of Groningen, University Medical Center Groningen, NETHERLANDS</addr-line></aff><author-notes><fn fn-type="conflict" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con" id="contrib001"><p>Conceived and designed the experiments: RW VDF SB CL PH. Performed the experiments: RW SB. Analyzed the data: RW SB. Wrote the paper: RW VDF CL SB.</p></fn><corresp id="cor001">* E-mail: <email>victor.dubois-ferriere@hcuge.ch</email></corresp></author-notes><pub-date pub-type="epub"><day>15</day><month>4</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>11</volume><issue>4</issue><elocation-id>e0153596</elocation-id><history><date date-type="received"><day>21</day><month>9</month><year>2015</year></date><date date-type="accepted"><day>31</day><month>3</month><year>2016</year></date></history><permissions><copyright-statement>&#x000a9; 2016 Wipfli et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Wipfli et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0153596.pdf"/><abstract><sec id="sec001"><title>Objective</title><p>In this work, we aim at comparing formally three different interaction modes for image manipulation that are usable in a surgery setting: 1) A gesture-controlled approach using Kinect &#x000ae;; 2) oral instructions to a third part dedicated to manipulate the images; and 3) direct manipulation using a mouse.</p></sec><sec id="sec002"><title>Materials and Methods</title><p>Each participant used the radiology image viewer Weasis with the three interaction modes. In a crossover randomized controlled trial participants were attributed block wise to six experimental groups. For each group, the order for testing the three modes was randomly assigned. Nine standardized scenarios were used.</p></sec><sec id="sec003"><title>Results</title><p>30 physicians and senior medical students participated in the experiment. Efficiency, measured as time used to pass the scenario, was best when using the mouse (M = 109.10s, SD = 25.96), followed by gesture-controlled (M = 214.97s, SD = 46.29) and oral instructions (M = 246.33s, SD = 76.50). Satisfaction, measured by a questionnaire, was rated highest in the condition mouse (M = 6.63, SD = 0.56), followed by gesture-controlled (M = 5.77, SD = 0.93) and oral instructions (M = 4.40, SD = 1.71). Differences in efficiency and satisfaction rating were significant. No significant difference in effectiveness, measured with error rates, was found.</p></sec><sec id="sec004"><title>Discussion</title><p>The study shows with formal evaluation that the use of gestures is advantageous over instructions to a third person. In particular, the use of gestures is more efficient than verbalizing instructions. The given gestures could be learned easily and reliability of the tested gesture-control system is good.</p></sec><sec id="sec005"><title>Conclusion</title><p>Under the premise that mouse cannot be used directly during surgery, gesture-controlled approaches demonstrate to be superior to oral instructions for image manipulation.</p></sec></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="1"/><table-count count="1"/><page-count count="10"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All datasets used to make conclusions are available from the Zenodo database <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/31203">https://zenodo.org/record/31203</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All datasets used to make conclusions are available from the Zenodo database <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/31203">https://zenodo.org/record/31203</ext-link>.</p></notes></front><body><sec id="sec006"><title>Background and Significance</title><p>Operating rooms (OR) are filled with electronic equipment providing the intervention team control over instruments and information on patients&#x02019; health condition. Those devices are operated by surgeons, nurses, anesthesiologists, and technical assistants. A special requirement for these non-sterile devices is that they are only used by team members who are scrubbed. Accordingly, surgeons cannot directly manipulate the most common user interfaces, such as a mouse and a keyboard, during intervention without specific sterile packaging. These devices are known to be highly contaminated [<xref rid="pone.0153596.ref001" ref-type="bibr">1</xref>]. Thus, surgeons will often rely on workarounds to perform tasks such as manipulating images. A solution is to scrub out when interacting with a computer. However, this is time consuming and prolongs surgery time. It is also possible to put sterile covers over the mouse and the keyboard, but this considerably decreases the usability. Further, surgeons can use the inside of their gown to contact the keyboard, keeping the outside of their clothes sterile. This procedure can be risky and is not satisfying [<xref rid="pone.0153596.ref002" ref-type="bibr">2</xref>]. Finally, and the most often used solution is to rely on somebody who is present and executes required manipulations, guided by the surgeon. However, explaining the exact position of an area of interest and communicating the best viewing parameters is often difficult and time consuming [<xref rid="pone.0153596.ref003" ref-type="bibr">3</xref>].</p><p>Fortunately, there are alternatives to the classic mouse/keyboard interaction mode. Natural interfaces, which can interpret human action without direct contact, are on the rise. One of the obvious means of natural communication is voice control. They are used in OR, but problems are reported due to the noisy operating environment [<xref rid="pone.0153596.ref004" ref-type="bibr">4</xref>]. A controlled experiment was conducted to evaluate the effectiveness of an OR environment control system, comparing voice control interface, touch panel control, and giving order to a person. Voice control was significantly slower than touch panel control and relaying to a third person. In contrast to this objective finding, participants subjectively evaluated voice control as quickest. Finally, voice control suffers low reliability when non-native speakers use the recognition engine [<xref rid="pone.0153596.ref003" ref-type="bibr">3</xref>].</p><p>Gesture-controlled manipulation is another way of interacting with images, and it has received a lot of attention recently with the availability of low-cost devices used in the gaming landscape. Johnson et al. [<xref rid="pone.0153596.ref002" ref-type="bibr">2</xref>] evaluated the potential for gesture-controlled interactions in interventional radiology, illustrated the problems raised when guiding an assistant verbally, and proposed requirements for such systems. A gesture-controlled system has to discriminate the communication gestures between people in the OR and the ones for image control; it needs to capture movements of the hand relative to the body because physicians need to move freely around the table; and finally it should accommodate input from different team members. First tests with gesture control devoted for the OR were made in 2004 [<xref rid="pone.0153596.ref003" ref-type="bibr">3</xref>]. The system used stereo cameras to detect motion in a given three-dimensional work space. A mock-up interface was developed to test the interactions with the gesture-based menu control. Sixteen participants of varied backgrounds executed non-medical scenarios. It was shown that users quickly learned to use gestures to navigate and a menu point selection took less than 5 seconds. Whereas in first studies, gestures were used to move a mouse pointer, a more powerful interaction form consists of relating a specific gesture with a command. Studies showed the feasibility of such systems [<xref rid="pone.0153596.ref005" ref-type="bibr">5</xref>]. Wachs et al. [<xref rid="pone.0153596.ref006" ref-type="bibr">6</xref>] conducted an in vivo experiment for a biopsy planning task. The methods for evaluation included contextual interviews, individual interviews, and subjective satisfaction questionnaires. The surgeons were overall satisfied. In a second experiment with students, the same authors report a recognition accuracy of 96% and the efficiency of 22 seconds per task after 10 trials.</p><p>Starting 2010, the availability of the Kinect&#x000ae; device, developed by Microsoft for gamers, eased further research on gestures-controlled interactions. Kinect&#x000ae; is an inexpensive off-the-shelf motion tracker. The device can capture video data in 3D under any ambient light conditions using an infrared laser depth sensor combined with a monochrome CMOS sensor. It also features a multi-array microphone so that it can provide full-body 3D motion capture, facial recognition and voice recognition capabilities. Gallo et al. [<xref rid="pone.0153596.ref007" ref-type="bibr">7</xref>] describe how the sensor functionality can be used to control imaging data with body movements. Recently, finger movements could be tracked [<xref rid="pone.0153596.ref008" ref-type="bibr">8</xref>].</p><p>One of the challenges raised in gestures-control is the reproduction of switch actions, such as &#x0201c;click&#x0201d; or &#x0201c;enter&#x0201d;. Ruppert and colleagues [<xref rid="pone.0153596.ref009" ref-type="bibr">9</xref>] developed two prototypes for interaction with imaging data. In a first prototype, they used the hold still to click paradigm and, in a second, they used one hand for interactions and the other hand to activate virtual clicks.</p><p>There are already a few reports about Kinect &#x000ae; in the OR. Strickland et al. [<xref rid="pone.0153596.ref010" ref-type="bibr">10</xref>] tested it with surgeons during laparoscopic and open surgery procedures. According to the surgeons, the most useful functionality was the ability to intraoperatively animate CT and MRI scans and switch to different series within a study. They also reported that zooming, rotating, highlighting points of interest, and annotating are less useful. During the interventions it was observed that IR radiation of overhead lights may interfere with the capture qualities of Kinect &#x000ae;. Ebert et al. [<xref rid="pone.0153596.ref011" ref-type="bibr">11</xref>] evaluated a gesture recognition system for CT imaging visualization. They report that it took 1.4 times longer to complete the task with the Kinect&#x000ae; than with the mouse. Usability satisfaction with the gestures control was rated 3.4 out of 5. The rating 5 was given by the authors for the interaction with the mouse being a gold standard and communicated it so to participants. Tan et al. [<xref rid="pone.0153596.ref012" ref-type="bibr">12</xref>] conducted a usability study with 29 radiologists. They used a two hand manipulation paradigm. 5 specific tasks for abdominal CT inspection were solved. 69% of the participants found the solution useful and a 20% found it easy to use and another 58% reported that they found it moderately difficult.</p><p>User-defined gestures are generally easier to remember than pre-defined gestures [<xref rid="pone.0153596.ref013" ref-type="bibr">13</xref>]. However, there is a legacy bias when users define the gestures themselves: they choose the gestures that resemble what they are used to so far, for example mouse movements [<xref rid="pone.0153596.ref014" ref-type="bibr">14</xref>]. Jacob et al. [<xref rid="pone.0153596.ref015" ref-type="bibr">15</xref>] initiated their research with an ethnographic study including 10 surgeons. In a first step, surgeons suggested 21 gestures. In a second step, they agreed on 10 gestures out of the 21. The system took into account contextual information such as position of thorax and head, and position of arms. After training, a mean gesture recognition accuracy of 94% could be achieved. Bigdelou and colleagues [<xref rid="pone.0153596.ref016" ref-type="bibr">16</xref>] studied user-defined gestures using Kinect&#x000ae; with 12 participants from a general population. The test required to locate an aortic stent in CT and navigate to its bifurcation. They evaluated the pragmatic and hedonic quality of the system. However, the authors could not show that personalized gestures could significantly improve pragmatic and hedonic quality.</p><p>In research by H&#x000f6;tker et al. [<xref rid="pone.0153596.ref017" ref-type="bibr">17</xref>], ten radiological residents interacted with 10 CT guided punctures and 18 angiography using Kinect&#x000ae; with voice or gestures-control commands. The gesture control was evaluated with a score 7.7/10 for satisfaction and 6 out of 10 physicians would use it every day.</p></sec><sec id="sec007"><title>Objectives</title><p>It has been shown that -gesture-based interaction is technically feasible and that user satisfaction was generally good. The objective of this work is to compare experimentally the use of gestures-control of imaging data with the two common alternative methods: relaying to a third person and direct input with a mouse, the latter being the gold standard. This work applies the methodologies previously published by Ebert et al. [<xref rid="pone.0153596.ref011" ref-type="bibr">11</xref>] and Punt et al. [<xref rid="pone.0153596.ref004" ref-type="bibr">4</xref>] for comparing different interaction modes for systems destined for use in the OR.</p><p>Usability is defined as the &#x0201c;extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use&#x0201d; [<xref rid="pone.0153596.ref018" ref-type="bibr">18</xref>]. In our trial, we will measure effectiveness by the achievement of a goal with as few errors as possible, efficiency by the time to reach the goal, and satisfaction with a questionnaire.</p><p>There are thus three conditions: a) gesture control; b) a third person executing the orders given by a surgeon and c) using a non-sterile mouse, the gold standard. In the text, these three groups will be referred to as &#x0201c;<italic>Gesture&#x0201d;</italic>, <italic>&#x0201c;Mouse&#x0201d;</italic> and <italic>&#x0201c;Third&#x0201d;</italic>.</p><p>The study was approved by the human research ethics commission of the Geneva University Hospitals (HUG).</p></sec><sec sec-type="materials|methods" id="sec008"><title>Materials and Methods</title><sec id="sec009"><title>System details and system in use</title><p>At the Geneva University Hospitals (HUG), two systems are used routinely for image manipulation, namely OsiriX on the Mac platform and Weasis on all platforms. Weasis is a multiplatform Java-based system with basic imaging capabilities. Osirix and Weasis are both open source projects that have been initiated at HUG.</p><p>Currently, manipulation of medical imaging during surgery is done by a third person under the surgeon&#x02019;s instructions. Alternatively, the surgeon has to scrub out and do himself the manipulations.</p><p>KiOP (Kinect in the OPerating room) is non-commercial software developed at HUG that enables Weasis to be manipulated with Microsoft Kinect&#x000ae;. The system was developed based on surgeons&#x02019; needs by a team composed of two surgeons, two radiologists and three IT engineers.</p><p>The system is composed of three components:</p><list list-type="order"><list-item><p>Motion-sensing device (Kinect&#x000ae;, Microsoft);</p></list-item><list-item><p>DICOM viewer (Weasis, Open Source: <ext-link ext-link-type="uri" xlink:href="https://www.openhub.net/p/weasis">https://www.openhub.net/p/weasis</ext-link>);</p></list-item><list-item><p>KiOP software.</p></list-item></list><p>A screenshot of the system (Weasis + KiOP) can be found in <xref ref-type="supplementary-material" rid="pone.0153596.s001">S1 Fig</xref>. The Kinect&#x000ae; sensing device has been chosen because it has sufficient precision and response time, is low-cost, and is well-documented, and provides a programming interface.</p><p>The integration between the motion sensor and the DICOM viewer is achieved by messages transmitted by KiOP from the device to the viewer after interpretation. The messages of the device are extended to include zoom, window and level, pan, series scroll, layout and mouse control. The messages use the telnet network protocol, thus achieving maximum interoperability. The DICOM viewer receives the messages and converts them to actions.</p><p>For the purpose of this test we choose depersonalized X-rays and CT images used in the division of traumatology. They were chosen by VDF in order to have a sample of typical images.</p></sec><sec id="sec010"><title>Study Design</title><p>The study design was a crossover randomized controlled trial (RCT) with blocked randomization. By picking a paper slip, participants are allocated to one of the 6 experimental groups, each with a size of 5 participants.</p><p>Each block has a different order of interaction modes. Interaction modes are direct use of Weasis with the mouse (condition <italic>Mouse</italic>), use of Weasis by giving verbal instructions to a third person (condition <italic>Third</italic>), or using Weasis with the gestures-control system KiOP (condition <italic>Gesture</italic>).</p><p>The choice of a RCT is guided by the main research question, which is assessing and comparing efficiency, effectiveness, and user satisfaction of the three different interaction modes. We used a crossover design in order to let the participants compare the interaction modes in the subjective evaluation part of the study.</p></sec><sec id="sec011"><title>Measurement</title><p>For evaluation of user satisfaction the <italic>After-Scenario Questionnaire</italic> (ASQ) by Lewis was chosen [<xref rid="pone.0153596.ref019" ref-type="bibr">19</xref>]. ASQ score is a brief 3-item questionnaire that measures using a 7-step Likert response format (1 = strongly disagree, 7 = strongly agree) the components ease of task completion, time to complete a task, and adequacy of support information.</p><p>The quantitative approach was accompanied by a qualitative approach which is not reported here.</p></sec><sec id="sec012"><title>Participants</title><p>Physicians and medical students were recruited in the Division of Orthopedics and Trauma at HUG and in the Faculty of medicine at the University of Geneva. The inclusion criterion was that they have used Weasis more than three times in the past. The exclusion criteria was participants&#x02019; prior use of the gesture-based tool KiOP. Participation is voluntary without financial incentives.</p></sec><sec id="sec013"><title>Power Analysis</title><p>As the main outcome, the variable efficiency was used. A power analysis showed that we need a sample of 26 participants to show a difference. This calculation was made with a beta of 0.2 and an alpha of 0.05. Effect size was estimated using two distributions from a prior study [<xref rid="pone.0153596.ref020" ref-type="bibr">20</xref>] where two prototypes of a clinical information system were tested. We used means of those distributions and the geometric mean of corresponding standard deviations to calculate the required sample size. In order to have equal numbers of participants in each group, a total of 30 participants was targeted.</p></sec><sec id="sec014"><title>Study Flow</title><p>The study took place between 28 Feb 2014 and 22 May 2014. Physicians and medical students were asked to visit the usability laboratory (Evalab) at HUG. Participants were first introduced to the procedure and the goal of the study. After formal acceptance, they were attributed randomly to one of the experimental groups by drawing a slip (<xref ref-type="fig" rid="pone.0153596.g001">Fig 1</xref>).</p><fig id="pone.0153596.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0153596.g001</object-id><label>Fig 1</label><caption><title>Study flow.</title><p>Interaction modes: G = gesture, M = mouse, T = third.</p></caption><graphic xlink:href="pone.0153596.g001"/></fig><p>The slip was not put back so each block was filled with 5 participants. The 9 test scenarios were randomized using the random function in Microsoft Excel. The test scenarios were randomized across participants. Hence, in each experimental group, there was the same order of interaction modes but with a different order of test scenarios (<xref ref-type="fig" rid="pone.0153596.g001">Fig 1</xref>).</p><p>For each of the three interaction modes, <italic>Mouse</italic> and <italic>Third</italic>, and <italic>Gesture</italic>, there were three test scenarios compared to each other. With the interaction mode <italic>Gesture</italic> there was an additional teaching phase, a free learning phase, and 5 learning scenarios before the participants started with the test scenarios.</p><p>The teaching was scripted and consisted of a presentation of slides that explained the use of KiOP and a demonstration by the experimenter. Subsequently, the participants could freely explore the functionalities of KiOP for 2 minutes in the free learning phase. In both the teaching and the free learning phase, the participant could ask questions on the use of KiOP to manipulate images. In the following learning and test phase questions were allowed. In the test phase, participants&#x02019; time was limited to 30 seconds per task.</p><p>The order of the learning scenarios was kept constant, starting with a first simple scenario that required three tasks and then continuing with more complex scenarios (see scenarios in <xref ref-type="supplementary-material" rid="pone.0153596.s003">S1 Table</xref>).</p></sec><sec id="sec015"><title>Methods for data acquisition and measurement</title><p>The time spent on each interaction mode was measured using Techsmith Morae&#x02122;. During the experiment, time markers are set at the beginning and end of each interaction mode. At the end of data acquisition, the Morae software reports the time spent per interaction mode.</p><p>Effectiveness is the measure of the correctness of executing a scenario. The experimenter rated the performance. There was a maximum number of points per scenario, which were the number of tasks per scenario (see scenarios in <xref ref-type="supplementary-material" rid="pone.0153596.s003">S1 Table</xref>). The activation and deactivation of KiOP was not counted as a task in order to keep the same number of total tasks per scenario. Each task correctly executed is counted as one point. In the following cases the point was not counted: choosing the inappropriate tool, failing to execute the task within 30 seconds, or losing track by Kinect. The maximum score per interaction mode could change between 3 and 12 for learning scenarios, and between 7 and 9 for test scenarios. Therefore, effectiveness was calculated as the percentage of correct tasks (100 * correct/max points. Finally, we calculated the average percentage score for each interaction mode which could range from 0 to 100.</p><p>The user satisfaction was measured with an adapted version of the After Scenario Questionnaire (ASQ) [<xref rid="pone.0153596.ref019" ref-type="bibr">19</xref>]. The slightly modified questions were:</p><list list-type="order"><list-item><p>&#x0201c;Overall, I am satisfied with the ease of completing the tasks in this scenario.&#x0201d;</p></list-item><list-item><p>&#x0201c;Overall, I am satisfied with the amount of time it took to complete the tasks in this scenario.&#x0201d;</p></list-item><list-item><p>&#x0201c;Overall, I am satisfied with the way the system guided me to complete the tasks in this scenario.&#x0201d;</p></list-item></list><p>The third question was modified because there is no support information included in the system as the original ASQ implies. Also, this question was not asked after the interaction mode <italic>Third</italic> as there was no guidance by the system. Finally, the modified questionnaire was translated into French. The ASQ is measured on a Likert scale from 1 [strongly disagree] to 7 [strongly agree].</p><p>Following variables were gathered using a questionnaire given to the participants at the beginning of the introduction: age, sex, job title, months of experience with Weasis, and last use of Weasis.</p></sec><sec id="sec016"><title>Outcome measures</title><p>Our main outcome was efficiency. It was operationalized as the time spent in seconds solving the scenarios in each interaction mode. Further, we evaluated efficacy, operationalized by subtraction of points when errors were committed in solving the scenarios and finally the user&#x02019;s satisfaction reported by participants in each interaction mode.</p></sec><sec id="sec017"><title>Methods for Data Analysis</title><p>For all intra-subject comparisons, a Shapiro-Wilk test was applied in order to evaluate whether the distributions are normal. In the ASQ score, data was treated as an interval scale because the distances between the 7 steps of the response were equal [<xref rid="pone.0153596.ref021" ref-type="bibr">21</xref>].</p><p>Each analysis was paired between <italic>Gesture</italic> and <italic>Mouse</italic>, <italic>Gesture</italic> and <italic>Third</italic>, and <italic>Mouse</italic> and <italic>Third</italic>. For distributions that did not meet the criteria for parametric tests, the corresponding non-parametric tests were used.</p><p>Regarding the efficiency, all distributions were normal and a paired two-sided t-test was applied. In order to determine significant differences, we used a Bonferroni corrected alpha = 0.017.</p><p>The efficacy, measured by scores, showed a strong ceiling effect and the distribution was not normal. Therefore, the Wilcoxon signed rank test was used to compare the means.</p><p>All differences in ASQ scores between conditions were normally distributed, except: [ease] <italic>Gesture</italic>&#x02013;<italic>Mouse</italic> and [system guidance] <italic>Gesture</italic>&#x02013;<italic>Mouse</italic>. For the normal distributions, a paired two-sided t-test was applied. The question on system guidance was unclear to the participants and only descriptive statistics were applied.</p></sec></sec><sec sec-type="results" id="sec018"><title>Results</title><p>Thirty participants were recruited and participated in the experience; 18 were men and 12 were women. There were 13 advanced medical students (years 5 and 6), 14 residents, 1 attending physician, and 2 deputy heads of division. Participants had used the DICOM viewer Weasis in average for 17.9 months (SD = 20.38) Professional experience ranged from 0 years for medical students to 20 years for one of the deputy head of division.</p><p>Efficiency, meaning a short time passed on the test scenarios, was highest (M = 109.10s, SD = 25.96) when using <italic>Mouse</italic>, followed by <italic>Gesture</italic> control (M = 214.97s, SD = 46.29) and <italic>Third</italic> (M = 246.33s, SD = 76.50). A two-sided paired t-test showed that the differences between all pairs were significant (p = 0.014 for the difference between <italic>Gesture</italic>&#x02013;<italic>Third</italic> and p &#x0003c; 0.001 for <italic>Mouse</italic>&#x02013;<italic>Gesture</italic> and <italic>Mouse</italic>&#x02013;<italic>Third</italic>.</p><p>The mean relative test score revealed that the highest score was in the condition <italic>Mouse</italic> (M = 98.31, SD = 4.541) followed by <italic>Third</italic> (M = 96.81, SD = 5.81) and <italic>Gesture</italic> (M = 96.45, SD = 4.54), without reaching significance in a Wilcoxon test. There was a strong ceiling effect of scores (see <xref ref-type="table" rid="pone.0153596.t001">Table 1</xref>) and no further analysis on learning performance was conducted.</p><table-wrap id="pone.0153596.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0153596.t001</object-id><label>Table 1</label><caption><title>Effectiveness evaluated as a percentage of successful tasks with learning and test scenarios.</title></caption><alternatives><graphic id="pone.0153596.t001g" xlink:href="pone.0153596.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="5" rowspan="1">Learning scenarios</th><th align="left" colspan="9" rowspan="1">Test scenarios</th></tr><tr><th align="left" rowspan="1" colspan="1">Condition</th><th align="left" colspan="5" rowspan="1"><italic>Gesture</italic></th><th align="left" colspan="3" rowspan="1"><italic>Gesture</italic></th><th align="left" colspan="3" rowspan="1"><italic>Mouse</italic></th><th align="left" colspan="3" rowspan="1"><italic>Third</italic></th></tr><tr><th align="left" rowspan="1" colspan="1">Scenario</th><th align="left" rowspan="1" colspan="1">1</th><th align="left" rowspan="1" colspan="1">2</th><th align="left" rowspan="1" colspan="1">3</th><th align="left" rowspan="1" colspan="1">4</th><th align="left" rowspan="1" colspan="1">5</th><th align="left" rowspan="1" colspan="1">1</th><th align="left" rowspan="1" colspan="1">2</th><th align="left" rowspan="1" colspan="1">3</th><th align="left" rowspan="1" colspan="1">1</th><th align="left" rowspan="1" colspan="1">2</th><th align="left" rowspan="1" colspan="1">3</th><th align="left" rowspan="1" colspan="1">1</th><th align="left" rowspan="1" colspan="1">2</th><th align="left" rowspan="1" colspan="1">3</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">M</td><td align="left" rowspan="1" colspan="1">84.44</td><td align="left" rowspan="1" colspan="1">95.71</td><td align="left" rowspan="1" colspan="1">93.70</td><td align="left" rowspan="1" colspan="1">91.67</td><td align="left" rowspan="1" colspan="1">96.11</td><td align="left" rowspan="1" colspan="1">97.67</td><td align="left" rowspan="1" colspan="1">95.18</td><td align="left" rowspan="1" colspan="1">96.51</td><td align="left" rowspan="1" colspan="1">98.57</td><td align="left" rowspan="1" colspan="1">98.31</td><td align="left" rowspan="1" colspan="1">98.04</td><td align="left" rowspan="1" colspan="1">96.51</td><td align="left" rowspan="1" colspan="1">96.72</td><td align="left" rowspan="1" colspan="1">97.20</td></tr><tr><td align="left" rowspan="1" colspan="1">SD</td><td align="left" rowspan="1" colspan="1">20.96</td><td align="left" rowspan="1" colspan="1">7.64</td><td align="left" rowspan="1" colspan="1">8.60</td><td align="left" rowspan="1" colspan="1">10.26</td><td align="left" rowspan="1" colspan="1">6.83</td><td align="left" rowspan="1" colspan="1">5.59</td><td align="left" rowspan="1" colspan="1">9.08</td><td align="left" rowspan="1" colspan="1">7.03</td><td align="left" rowspan="1" colspan="1">4.36</td><td align="left" rowspan="1" colspan="1">4.43</td><td align="left" rowspan="1" colspan="1">4.48</td><td align="left" rowspan="1" colspan="1">8.66</td><td align="left" rowspan="1" colspan="1">8.42</td><td align="left" rowspan="1" colspan="1">5.98</td></tr></tbody></table></alternatives></table-wrap><p>Usability satisfaction was evaluated with questions about ease of use, efficiency, and system guidance on a scale from 1 [strongly disagree] to 7 [strongly agree]. Ease of use was rated highest in the condition <italic>Mouse</italic> (M = 6.63, SD = 0.56), followed by <italic>Gesture</italic> M = 5.77, SD = 0.93), and <italic>Third</italic> (M = 4.40, SD = 1.71). Paired Wilcoxon tests showed significant differences in the following pairs: <italic>Mouse-Gesture</italic>, <italic>Gesture</italic>-<italic>Third</italic>, and <italic>Mouse</italic>-<italic>Third</italic> (all pairs p &#x0003c; 0.001).</p><p>Regarding efficiency, participants subjectively rated that they were most efficient with <italic>Mouse</italic> (M = 6.77, SD = 0.43) followed by <italic>Gesture</italic> (M = 5.47, SD = 1.14) and <italic>Third</italic> (M = 4.00, SD = 1.762). The difference between all pairs was significant with a paired two-sided t-test.</p><p>Finally, system guidance was evaluated slightly higher in the condition <italic>Mouse</italic> (M = 6.33, SD = 0.92) than <italic>Gesture</italic> (M = 6.13, SD = 1.04), but this difference was not significant with a paired Wilcoxon test. Also, this question was often not understood by participants.</p><p>Regarding the demographic variables, no correlation of work experience with satisfaction or task performance could be found.</p></sec><sec sec-type="conclusions" id="sec019"><title>Discussion</title><p>The study showed that the use of gestures is advantageous over instructions to a third person. In particular, the use of gestures was more efficient than verbalizing instructions. Also, participants were more satisfied with it and made only insignificantly more errors than when relaying to a third person. A control condition was used where participants have been using a mouse without sterile cover. As expected the results showed that participants are more efficient with the mouse than with gestures-control. In our study the participants were 2.2 times faster using the mouse than using KiOP.</p><p>Regarding the effectiveness, we could not determine any significant difference between the three conditions. A ceiling effect was present in scores in the learning scenarios. Only few errors were made and the only notable difference was between the first and second learning scenario. Even if the experiment had been constructed in order to measure a learning effect, it failed to do so. The study supports previous studies that showed that pre-defined gestures with Kinect&#x000ae; can be learned easily [<xref rid="pone.0153596.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0153596.ref017" ref-type="bibr">17</xref>] and that reliability of gesture-control systems is good [<xref rid="pone.0153596.ref006" ref-type="bibr">6</xref>]. However, in a next study difficulty of scenarios should be increased.</p><p>Regarding user satisfaction, participants were most satisfied with the ease of use and efficiency with <italic>Mouse</italic>, rather satisfied with <italic>Gesture</italic>, and least satisfied in <italic>Third</italic>. Our results are in line with previous studies with gesture-control systems showing good evaluations in terms of ease of use [<xref rid="pone.0153596.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0153596.ref016" ref-type="bibr">16</xref>], although lower than using a mouse [<xref rid="pone.0153596.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0153596.ref017" ref-type="bibr">17</xref>]. The fact that giving order to a person was the least preferred interaction mode was a surprise. We would have expected that it is simpler to explain to somebody than using a new interaction mode, namely KiOP. A positive subjective evaluation of efficiency when using gestures could be shown. This finding is in line with the measured efficiency per scenario. The third question of ASQ on system guidance was not clear to participants, so no interpretation of the result is possible.</p><p>The strength of our study is the methodology that enables to test objectively with a significant sample of the target population. A gesture-control system is compared to the current way of using DICOM viewers in OR.</p><p>The limitations of our experiment are that it has not been made during surgery, but using a simulated environment. Another limitation is that participants were not blind about the study goals, which could induce a complacency bias.</p></sec><sec sec-type="conclusions" id="sec020"><title>Conclusion</title><p>Our work compared gesture-control, use of mouse and oral instructions given to a person to use a DICOM viewer in an OR setting. Efficiency, effectiveness, and user satisfaction have been evaluated. The study demonstrated that gesture-control worked better than oral instructions to a person, but not better than mouse usage. We believe that the difficult conditions of using a mouse in a sterile will further promote the use of gesture interfaces. The study can inform decision makers whether to integrate a gesture-control system in an OR using evidences rather than enthusiasm. We hope this work will help improving the work conditions of physicians and, as a consequence, patient safety.</p></sec><sec sec-type="supplementary-material" id="sec021"><title>Supporting Information</title><supplementary-material content-type="local-data" id="pone.0153596.s001"><label>S1 Fig</label><caption><title>Screenshot of user interface (Weasis + KiOP).</title><p>Tools in the lower part are from left to right: reset image, split window, move, change contrast, zoom, scroll through slices, point, select, and quit.</p><p>(TIF)</p></caption><media xlink:href="pone.0153596.s001.tif"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0153596.s002"><label>S1 File</label><caption><title>Permission to publish <xref ref-type="supplementary-material" rid="pone.0153596.s001">S1 Fig</xref>.</title><p>(PDF)</p></caption><media xlink:href="pone.0153596.s002.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0153596.s003"><label>S1 Table</label><caption><title>Scenarios.</title><p>(PDF)</p></caption><media xlink:href="pone.0153596.s003.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0153596.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Hartmann</surname><given-names>B</given-names></name>, <name><surname>Benson</surname><given-names>M</given-names></name>, <name><surname>Junger</surname><given-names>A</given-names></name>, <name><surname>Quinzo</surname><given-names>L</given-names></name>, <name><surname>R&#x000f6;hrig</surname><given-names>R</given-names></name>, <name><surname>Fengler</surname><given-names>B</given-names></name>, <etal>et al</etal>
<article-title>Computer keyboard and mouse as a reservoir of pathogens in an intensive care unit</article-title>. <source>J Clin Monit Comput</source>
<year>2004</year>;<volume>18</volume>(<issue>1</issue>):<fpage>7</fpage>&#x02013;<lpage>12</lpage>. <pub-id pub-id-type="pmid">15139578</pub-id></mixed-citation></ref><ref id="pone.0153596.ref002"><label>2</label><mixed-citation publication-type="other">Johnson R, O'Hara K, Sellen A, Cousins C, Criminisi A. Exploring the Potential for Touchless Interaction in Image-guided Interventional Radiology. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. New York, NY: ACM 2011: 3323&#x02013;32.</mixed-citation></ref><ref id="pone.0153596.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Graetzel</surname><given-names>C</given-names></name>, <name><surname>Fong</surname><given-names>T</given-names></name>, <name><surname>Grange</surname><given-names>S</given-names></name>, <name><surname>Baur</surname><given-names>C</given-names></name>. <article-title>A Non-Contact Mouse for Surgeon-Computer Interaction</article-title>. <source>Technology and HealthCare</source>
<year>2014</year>:<volume>12</volume>.</mixed-citation></ref><ref id="pone.0153596.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Punt</surname><given-names>MM</given-names></name>, <name><surname>Stefels</surname><given-names>CN</given-names></name>, <name><surname>Grimbergen</surname><given-names>CA</given-names></name>, <name><surname>Dankelman</surname><given-names>J</given-names></name>. (2005). <article-title>Evaluation of voice control, touch panel control and assistant control during steering of an endoscope</article-title>. <source>Minim Invasive Ther Allied Technol</source>
<year>2005</year>;<volume>14</volume>(<issue>3</issue>):<fpage>181</fpage>&#x02013;<lpage>187</lpage>. <pub-id pub-id-type="pmid">16754161</pub-id></mixed-citation></ref><ref id="pone.0153596.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Kipshagen</surname><given-names>T</given-names></name>, <name><surname>Graw</surname><given-names>M</given-names></name>, <name><surname>Tronnier</surname><given-names>V</given-names></name>,<name><surname>Bonsanto</surname><given-names>M</given-names></name>, <name><surname>Hofmann</surname><given-names>UG</given-names></name>, <name><surname>D&#x000f6;ssel</surname><given-names>O</given-names></name>, <name><surname>Schlegel</surname><given-names>WC</given-names></name>. <article-title>Touch- and marker-free interaction with medical software</article-title>. In: <name><surname>D&#x000f6;ssel</surname><given-names>O</given-names></name>, <name><surname>Schlegel</surname><given-names>W</given-names></name>, eds. <source>World Congress on Medical Physics and Biomedical Engineering</source>, September 7&#x02013;12, 2009. Munich, Germany: Springer <year>2009</year>;<volume>25</volume>(<issue>6</issue>):<fpage>75</fpage>&#x02013;<lpage>78</lpage>.</mixed-citation></ref><ref id="pone.0153596.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Wachs</surname><given-names>JP</given-names></name>, <name><surname>Stern</surname><given-names>HI</given-names></name>, <name><surname>Edan</surname><given-names>YG</given-names></name>,<name><surname>Handler</surname><given-names>M</given-names></name>, <name><surname>Feied</surname><given-names>J</given-names></name>, <name><surname>Smith</surname><given-names>MG</given-names></name>. <article-title>A gesture-based tool for sterile browsing of radiology images</article-title>. <source>J Am Med Inform Assoc</source>
<year>2008</year>;<volume>15</volume>(<issue>3</issue>):<fpage>321</fpage>&#x02013;<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1197/jamia.M241">10.1197/jamia.M241</ext-link></comment>
<pub-id pub-id-type="pmid">18451034</pub-id></mixed-citation></ref><ref id="pone.0153596.ref007"><label>7</label><mixed-citation publication-type="other">Gallo L, Placitelli AP, Ciampi M. Controller-free exploration of medical image data: Experiencing the Kinect. Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems. Washington, DC: IEEE Computer Society 2011:1&#x02013;6.</mixed-citation></ref><ref id="pone.0153596.ref008"><label>8</label><mixed-citation publication-type="other">Tuntakurn A, Thongvigitmanee S, Sa-ing V, Makhanov SS, Hasegawa S. Natural interaction on 3D medical image viewer software. Macau, China: Biomedical Engineering International Conference:2012:1&#x02013;5.</mixed-citation></ref><ref id="pone.0153596.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Ruppert</surname><given-names>GC</given-names></name>, <name><surname>Reis</surname><given-names>LO</given-names></name>, <name><surname>Amorim</surname><given-names>PH</given-names></name>, <name><surname>De Moraes</surname><given-names>TF</given-names></name>, <name><surname>Da Silva</surname><given-names>JV</given-names></name>. <article-title>Touchless gesture user interface for interactive image visualization in urological surgery</article-title>. <source>World J Urol</source>
<year>2012</year>;<volume>30</volume>(<issue>5</issue>):<fpage>687</fpage>&#x02013;<lpage>691</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00345-012-0879-0">10.1007/s00345-012-0879-0</ext-link></comment>
<pub-id pub-id-type="pmid">22580994</pub-id></mixed-citation></ref><ref id="pone.0153596.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Strickland</surname><given-names>M</given-names></name>, <name><surname>Tremaine</surname><given-names>J</given-names></name>, <name><surname>Brigley</surname><given-names>G</given-names></name>, <name><surname>Law</surname><given-names>C</given-names></name>. <article-title>Using a depth-sensing infrared camera system to access and manipulate medical imaging from within the sterile operating field</article-title>. <source>Can J Surg</source>
<year>2013</year>;<volume>56</volume>(<issue>3</issue>):<fpage>E1</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1503/cjs.035311">10.1503/cjs.035311</ext-link></comment>
<pub-id pub-id-type="pmid">23706851</pub-id></mixed-citation></ref><ref id="pone.0153596.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Ebert</surname><given-names>LC</given-names></name>, <name><surname>Hatch</surname><given-names>G</given-names></name>, <name><surname>Ampanozi</surname><given-names>G</given-names></name>, <name><surname>Thali</surname><given-names>MJ</given-names></name>, <name><surname>Ross</surname><given-names>S</given-names></name>. <article-title>You can't touch this: touch-free navigation through radiological images</article-title>. <source>Surg Innov</source>
<year>2012</year>;<volume>19</volume>(<issue>3</issue>):<fpage>301</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1553350611425508">10.1177/1553350611425508</ext-link></comment>
<pub-id pub-id-type="pmid">22064490</pub-id></mixed-citation></ref><ref id="pone.0153596.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Tan</surname><given-names>JH</given-names></name>, <name><surname>Chao</surname><given-names>C</given-names></name>, <name><surname>Zawaideh</surname><given-names>M</given-names></name>,<name><surname>Roberts</surname><given-names>AC</given-names></name>, <name><surname>Kinney</surname><given-names>TB</given-names></name>. <article-title>Informatics in Radiology: developing a touchless user interface for intraoperative image control during interventional radiology procedures</article-title>. <source>Radiographics</source>
<year>2013</year>;<volume>33</volume>(<issue>2</issue>):<fpage>E61</fpage>&#x02013;<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1148/rg.332125101">10.1148/rg.332125101</ext-link></comment>
<pub-id pub-id-type="pmid">23264282</pub-id></mixed-citation></ref><ref id="pone.0153596.ref013"><label>13</label><mixed-citation publication-type="other">Nacenta MA, Kamber Y, Qiang Y, Kristensson PO. Memorability of pre-designed and userdefined gesture sets. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. New York, NY: ACM 2013:1099&#x02013;108.</mixed-citation></ref><ref id="pone.0153596.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Morris</surname><given-names>MR</given-names></name>, <name><surname>Danielescu</surname><given-names>A</given-names></name>, <name><surname>Drucker</surname><given-names>S</given-names></name>, <name><surname>Fisher</surname><given-names>D</given-names></name>, <name><surname>Lee</surname><given-names>B</given-names></name>, <name><surname>Wobbrock</surname><given-names>JO</given-names></name>, <etal>et al</etal>
<article-title>Reducing legacy bias in gesture elicitation studies</article-title>. <source>interactions</source>
<year>2014</year>;<volume>21</volume>(<issue>3</issue>):<fpage>40</fpage>&#x02013;<lpage>5</lpage>.</mixed-citation></ref><ref id="pone.0153596.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Jacob</surname><given-names>MG</given-names></name>, <name><surname>Wachs</surname><given-names>JP</given-names></name>, <name><surname>Packer</surname><given-names>RA</given-names></name>. <article-title>Hand-gesture-based sterile interface for the operating room using contextual cues for the navigation of radiological images</article-title>. <source>J Am Med Inform Assoc</source>
<year>2012</year>;<volume>20</volume>:<fpage>183</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="pone.0153596.ref016"><label>16</label><mixed-citation publication-type="other">Bigdelou A, Benz T, Schwarz L, Navab N. Simultaneous categorical and spatio-temporal 3d gestures using kinect. IEEE Symposium on 3D User Interfaces (3DUI). New York, NY: IEEE 2012:53&#x02013;60.</mixed-citation></ref><ref id="pone.0153596.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>H&#x000f6;tker</surname><given-names>AM</given-names></name>, <name><surname>Pitton</surname><given-names>MB</given-names></name>, <name><surname>Mildenberger</surname><given-names>P</given-names></name>, <name><surname>D&#x000fc;ber</surname><given-names>C</given-names></name>. <article-title>Speech and motion control for interventional radiology: requirements and feasibility</article-title>. <source>Int J Comput Assist Radiol Surg</source>
<year>2013</year>;<volume>8</volume>(<issue>6</issue>):<fpage>997</fpage>&#x02013;<lpage>1002</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11548-013-0841-7">10.1007/s11548-013-0841-7</ext-link></comment>
<pub-id pub-id-type="pmid">23580026</pub-id></mixed-citation></ref><ref id="pone.0153596.ref018"><label>18</label><mixed-citation publication-type="book"><collab>ISO</collab>. <chapter-title>EN 9241: Ergonomic requirements for office work with visual display terminals (VDTs)&#x02014;Part 11</chapter-title>
<publisher-loc>Geneva, Switzerland</publisher-loc>: <publisher-name>International Organization for Standardization</publisher-name>
<year>1998</year>.</mixed-citation></ref><ref id="pone.0153596.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Lewis</surname><given-names>JR</given-names></name>. <article-title>IBM computer usability satisfaction questionnaires: Psychometric evaluation and instructions for use</article-title>. <source>International Journal of Human-Computer Interaction</source>
<year>1995</year>;<volume>7</volume>(<issue>1</issue>):<fpage>57</fpage>&#x02013;<lpage>78</lpage>.</mixed-citation></ref><ref id="pone.0153596.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Wipfli</surname><given-names>R</given-names></name>, <name><surname>Ehrler</surname><given-names>F</given-names></name>, <name><surname>Bediang</surname><given-names>G</given-names></name>, <name><surname>B&#x000e9;trancourt</surname><given-names>M</given-names></name>, <name><surname>Lovis</surname><given-names>C</given-names></name>. <article-title>How regrouping alerts in CPOE layout influences physicians&#x02019; prescription behavior: results of a crossover randomized trial</article-title>. <source>JMIR Human Factors</source>. Submitted.</mixed-citation></ref><ref id="pone.0153596.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Carifio</surname><given-names>J</given-names></name>, <name><surname>Perla</surname><given-names>RJ</given-names></name>. <article-title>Ten common misunderstandings, misconceptions, persistent myths and urban legends about Likert scales and Likert response formats and their antidotes</article-title>. <source>Journal of Social Sciences</source>
<year>2007</year>;<volume>3</volume>:<fpage>106</fpage>.</mixed-citation></ref></ref-list></back></article>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Psychol</journal-id><journal-id journal-id-type="iso-abbrev">Front Psychol</journal-id><journal-id journal-id-type="publisher-id">Front. Psychol.</journal-id><journal-title-group><journal-title>Frontiers in Psychology</journal-title></journal-title-group><issn pub-type="epub">1664-1078</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">27252671</article-id><article-id pub-id-type="pmc">4877370</article-id><article-id pub-id-type="doi">10.3389/fpsyg.2016.00745</article-id><article-categories><subj-group subj-group-type="heading"><subject>Psychology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Monitoring Alpha Oscillations and Pupil Dilation across a Performance-Intensity Function</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>McMahon</surname><given-names>Catherine M.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="author-notes" rid="fn001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/62900/overview"/></contrib><contrib contrib-type="author"><name><surname>Boisvert</surname><given-names>Isabelle</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/215721/overview"/></contrib><contrib contrib-type="author"><name><surname>de Lissa</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/348166/overview"/></contrib><contrib contrib-type="author"><name><surname>Granger</surname><given-names>Louise</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Ibrahim</surname><given-names>Ronny</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Lo</surname><given-names>Chi Yhun</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/245155/overview"/></contrib><contrib contrib-type="author"><name><surname>Miles</surname><given-names>Kelly</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/308885/overview"/></contrib><contrib contrib-type="author"><name><surname>Graham</surname><given-names>Petra L.</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/339485/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Linguistics, Macquarie University, Sydney, NSW</institution><country>Australia</country></aff><aff id="aff2"><sup>2</sup><institution>The HEARing CRC, Melbourne, VIC</institution><country>Australia</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Psychology, Macquarie University, Sydney, NSW</institution><country>Australia</country></aff><aff id="aff4"><sup>4</sup><institution>Department of Statistics, Macquarie University, Sydney, NSW</institution><country>Australia</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: <italic>Adriana A. Zekveld, VU University Medical Center, Netherlands</italic></p></fn><fn fn-type="edited-by"><p>Reviewed by: <italic>Stefanie E. Kuchinsky, University of Maryland, USA; Antje Strau&#x000df;, Centre National de la Recherche Scientifique, France</italic></p></fn><corresp id="fn001">*Correspondence: <italic>Catherine M. McMahon, <email xlink:type="simple">cath.mcmahon@mq.edu.au</email></italic></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Auditory Cognitive Neuroscience, a section of the journal Frontiers in Psychology</p></fn></author-notes><pub-date pub-type="epub"><day>24</day><month>5</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>7</volume><elocation-id>745</elocation-id><history><date date-type="received"><day>11</day><month>12</month><year>2015</year></date><date date-type="accepted"><day>05</day><month>5</month><year>2016</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2016 McMahon, Boisvert, de Lissa, Granger, Ibrahim, Lo, Miles and Graham.</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>McMahon, Boisvert, de Lissa, Granger, Ibrahim, Lo, Miles and Graham</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Listening to degraded speech can be challenging and requires a continuous investment of cognitive resources, which is more challenging for those with hearing loss. However, while alpha power (8&#x02013;12 Hz) and pupil dilation have been suggested as objective correlates of listening effort, it is not clear whether they assess the same cognitive processes involved, or other sensory and/or neurophysiological mechanisms that are associated with the task. Therefore, the aim of this study is to compare alpha power and pupil dilation during a sentence recognition task in 15 randomized levels of noise (-7 to +7 dB SNR) using highly intelligible (16 channel vocoded) and moderately intelligible (6 channel vocoded) speech. Twenty young normal-hearing adults participated in the study, however, due to extraneous noise, data from only 16 (10 females, 6 males; aged 19&#x02013;28 years) was used in the Electroencephalography (EEG) analysis and 10 in the pupil analysis. Behavioral testing of perceived effort and speech performance was assessed at 3 fixed SNRs <italic>per</italic> participant and was comparable to sentence recognition performance assessed in the physiological test session for both 16- and 6-channel vocoded sentences. Results showed a significant interaction between channel vocoding for both the alpha power and the pupil size changes. While both measures significantly decreased with more positive SNRs for the 16-channel vocoding, this was not observed with the 6-channel vocoding. The results of this study suggest that these measures may encode different processes involved in speech perception, which show similar trends for highly intelligible speech, but diverge for more spectrally degraded speech. The results to date suggest that these objective correlates of listening effort, and the cognitive processes involved in listening effort, are not yet sufficiently well understood to be used within a clinical setting.</p></abstract><kwd-group><kwd>alpha power</kwd><kwd>pupil dilation</kwd><kwd>listening effort</kwd><kwd>listening in noise</kwd><kwd>speech perception</kwd><kwd>perceived effort</kwd><kwd>mental exertion</kwd></kwd-group><counts><fig-count count="5"/><table-count count="1"/><equation-count count="0"/><ref-count count="79"/><page-count count="12"/><word-count count="0"/></counts></article-meta></front><body><sec><title>Introduction</title><p>Listening to degraded speech, either in adverse acoustic environments or with hearing loss, is challenging (<xref rid="B40" ref-type="bibr">McCoy et al., 2005</xref>; <xref rid="B65" ref-type="bibr">Stenfelt and R&#x000f6;nnberg, 2009</xref>), and it is assumed that the increased cognitive load required to understand a conversation is associated with self-reported effort (<xref rid="B36" ref-type="bibr">Lunner et al., 2009</xref>; <xref rid="B55" ref-type="bibr">Rudner et al., 2012</xref>). Adults with hearing loss report listening to be greatly taxing (<xref rid="B31" ref-type="bibr">Kramer et al., 2006</xref>), which may cause increased stress and fatigue (<xref rid="B23" ref-type="bibr">H&#x000e9;tu et al., 1988</xref>), contribute to early retirement (<xref rid="B13" ref-type="bibr">Danermark and Gellerstedt, 2004</xref>), social withdrawal (<xref rid="B72" ref-type="bibr">Weinstein and Ventry, 1982</xref>), and negatively affect relationships (<xref rid="B22" ref-type="bibr">H&#x000e9;tu et al., 1993</xref>). Current speech perception tests, which measure performance on a word or sentence recognition task, provide only a gross indication of the activity limitations caused by hearing loss, and do not consider the top&#x02013;down effects related to increased concentration and attention, as well as effort (<xref rid="B73" ref-type="bibr">Wingfield et al., 2005</xref>; <xref rid="B49" ref-type="bibr">Pichora-Fuller and Singh, 2006</xref>; <xref rid="B61" ref-type="bibr">Schneider et al., 2010</xref>). Therefore, concurrently measuring the cognitive load or listening effort needed to undertake a speech perception task could increase its sensitivity, enabling a more holistic understanding of the challenges faced by adults with hearing loss in communicative settings.</p><p>Listening effort, defined as &#x0201c;the mental exertion required to attend to, and understand, an auditory message&#x0201d; (<xref rid="B41" ref-type="bibr">McGarrigle et al., 2014</xref>), is influenced by both the clarity of the auditory signal and the cognitive resources available. As hearing loss and cognitive decline are highly associated with age (<xref rid="B56" ref-type="bibr">Salthouse, 2004</xref>; <xref rid="B34" ref-type="bibr">Lin et al., 2013</xref>), there is a recognized need to understand the contribution of cognition and effort to listening to everyday speech within a clinical environment to better direct rehabilitation strategies towards and/or improve device fitting, particularly for older adults. Certainly it has been shown that greater cognitive resources are required to perceive a speech signal that becomes more degraded and this is more challenging for older adults (<xref rid="B51" ref-type="bibr">Rabbitt, 1991</xref>; <xref rid="B54" ref-type="bibr">R&#x000f6;nnberg et al., 2010</xref>, <xref rid="B53" ref-type="bibr">2013</xref>). However, importantly, several studies have also highlighted the advantages that individuals with greater cognitive resources have to understand speech in noise (<xref rid="B35" ref-type="bibr">Lunner, 2003</xref>), utilize fast signal processing strategies in hearing aids (<xref rid="B37" ref-type="bibr">Lunner and Sundewall-Thor&#x000e9;n, 2007</xref>), and compensate when mismatches occur between what is heard and the brain&#x02019;s phonological representations of speech (<xref rid="B3" ref-type="bibr">Avivi-Reich et al., 2014</xref>).</p><p>Recently, there has been an increased interest in understanding and measuring listening effort, so that future clinical measures may ensue. Many studies have attempted to estimate listening effort, using behavioral, subjective or objective approaches (see <xref rid="B41" ref-type="bibr">McGarrigle et al., 2014</xref> for a review). While subjective measures have high face-validity, they have several inherent limitations; including whether participants are indeed rating perceived effort, or rating their ability to discriminate between different signal-to-noise ratios (SNRs; <xref rid="B55" ref-type="bibr">Rudner et al., 2012</xref>). Additionally, subjective measures poorly correlate with other behavioral and objective measures of listening effort (<xref rid="B77" ref-type="bibr">Zekveld et al., 2010</xref>; <xref rid="B17" ref-type="bibr">Gosselin and Gagn&#x000e9;, 2011</xref>; <xref rid="B24" ref-type="bibr">Hornsby, 2013</xref>), possibly because these measures relate to specific components of the goal-directed cognitive processes underpinning mental effort (<xref rid="B59" ref-type="bibr">Sarter et al., 2006</xref>), therefore each should be investigated. An effective and consistent objective correlate of listening effort has not yet been found (<xref rid="B6" ref-type="bibr">Bernarding et al., 2013</xref>), although pupil dilation and oscillations in the alpha frequency band (8&#x02013;12 Hz) have independently been shown to be associated with changes in speech intelligibility (<xref rid="B43" ref-type="bibr">Obleser et al., 2012</xref>; <xref rid="B4" ref-type="bibr">Becker et al., 2013</xref>; <xref rid="B76" ref-type="bibr">Zekveld and Kramer, 2014</xref>; <xref rid="B47" ref-type="bibr">Petersen et al., 2015</xref>) and seem to be sensitive to hearing loss during a speech recognition or digit recall task in noise (<xref rid="B30" ref-type="bibr">Kramer et al., 1997</xref>; <xref rid="B78" ref-type="bibr">Zekveld et al., 2011</xref>; <xref rid="B47" ref-type="bibr">Petersen et al., 2015</xref>). It is, however, not yet known whether these two objective measures assess the same processes, whether sensory (e.g., phonological mapping of degraded speech), cognitive (e.g., cognitive load, inhibition of task irrelevant activity, or working memory), or neurophysiological (e.g., acute stress associated with the investment of attentional resources). These physiological responses may also reflect the extent of brain regions that are recruited to achieve a specific performance (e.g., to increase cognitive processing or provide inhibitory control; see <xref rid="B52" ref-type="bibr">Radulescu et al., 2014</xref>). Further, while there is an extensive literature on the neurophysiological mechanisms governing pupil dilation (<xref rid="B33" ref-type="bibr">Laeng et al., 2012</xref>), less is understood about those which underpin oscillatory cortical activity or the neuromodulators which influence it (<xref rid="B28" ref-type="bibr">Klimesch et al., 2007</xref>).</p><p>There appear to be general trends observed between task difficulty and changes in pupil dilation or in alpha power, however, these are not consistent across all studies (see <xref rid="B76" ref-type="bibr">Zekveld and Kramer, 2014</xref>; <xref rid="B75" ref-type="bibr">W&#x000f6;stmann et al., 2015</xref>). This may in part depend on the type of task (i.e., listening to randomized or fixed speech tokens), the period when the physiological response is measured (during listening to degraded speech or during the retention period of a memory recall task), or the population characteristics (younger versus older adults, or normal hearing versus those with hearing loss). Alternatively, cognitive load/listening effort may be inherently non-linear and a function of the availability of processing resources coupled with the intentional motivation to allocate such resources to the task (<xref rid="B59" ref-type="bibr">Sarter et al., 2006</xref>). That is, when the task is too difficult and the processing demands exceed the available cognitive resources, or when the task is too easy and requires minimal cognitive resources (i.e., is automatic or passive), then effort may not be required or allocated to the task (<xref rid="B19" ref-type="bibr">Granholm et al., 1996</xref>; <xref rid="B76" ref-type="bibr">Zekveld and Kramer, 2014</xref>). As such, the greatest change in objective measures related to effort may be observable at medium levels of performance, rather than at the extreme ends of performance. Similar non-linear associations between performance and stress (<xref rid="B1" ref-type="bibr">Anderson, 1976</xref>) and performance and mental effort have been previously reported (<xref rid="B52" ref-type="bibr">Radulescu et al., 2014</xref>).</p><p>The current study aims to compare both alpha activity and pupil dilation measured simultaneously over a complete performance-intensity function while listening to sentences with high intelligibility (16-channel vocoded) or moderate intelligibility (6-channel vocoded). Specifically, it aims to identify whether these measures show similar patterns of behavior across the 15 SNRs and with the two levels of vocoding, suggesting that they may encode similar sensory, cognitive or neurophysiological processes involved in listening effort (that currently remain unclear; <xref rid="B41" ref-type="bibr">McGarrigle et al., 2014</xref>). A further reason to manipulate both the SNRs and the channel vocoding to degrade speech was to investigate the behavior of these measures on what could be approximated to a simulation of listening with a cochlear implant (<xref rid="B16" ref-type="bibr">Friesen et al., 2001</xref>). If these measures are to be applicable in clinical settings, their pattern of behavior should be predictable in a clinical population.</p></sec><sec sec-type="materials|methods" id="s1"><title>Materials and Methods</title><sec><title>Participants</title><p>Twenty young adults were recruited to participate in this study. Amongst this group, two did not attend all testing sessions. Invalid recordings led to the exclusion of two more participants for the Electroencephalography (EEG) measures and an additional six for the pupil measures. The main reason for excluding the data related to participants looking away from the visual target or closing their eyes when listening became difficult. Participants (10 females, 6 males) were aged from 19 to 28 years (mean = 23 years, <italic>SD</italic> = 2.6). All participants were native Australian English speakers and were right-handed. Participants&#x02019; hearing was screened using distortion product otoacoustic emissions. All participants had present emissions bilaterally between 1&#x02013;4 kHz, which ruled out a moderate or greater hearing loss. All participants reported normal or corrected-to-normal vision. Informed consent was obtained from all participants.</p></sec><sec><title>Speech Perception Material</title><p>Recorded Bamford-Kowal-Bench/Australia (BKB/A) sentences spoken by a native Australian-English female were presented as targets in the presence of four-talker babble noise. The sentences and background noise were vocoded by dividing the frequency range from 50 to 6000 Hz into 6 or 16 logarithmically spaced channels. The amplitude envelope was then extracted from each channel and used to modulate the noise with the same frequency band. Each band of noise was then recombined to produce the noise vocoded sentences and background noise. See <xref rid="B63" ref-type="bibr">Shannon et al. (1995)</xref> for more information about speech recognition with vocoded material.</p></sec><sec><title>Physiological Measures</title><p>Electroencephalography activity and pupil dilation were measured simultaneously during the speech recognition task conducted in a sound-treated and magnetically shielded room. With their forehead resting on an eye-tracker support, participants were asked to maintain their gaze on a small cross presented in the middle of the computer screen. The following presentation protocol was used: 1 s of quiet, variable length of noise (&#x0003e;1 s), sentence in noise, 1 s of noise. Physiological testing was conducted across two sessions: session one used the 16-channel vocoded material and session two used the 6-channel vocoded material. Each session presented 240 target sentences at 65 dB with the noise randomized between 58 and 72 dB (-7 to +7 dB SNR, a total of 15 levels). Pilot data indicated these SNRs provided the full range (0&#x02013;100%) of speech recognition scores (SRS). The randomization was programmed for sentences of the same BKB/A list to be presented at the same SNR to allow off-line scoring of performance as per the original lists.</p><p>After each presentation, a response period of 4 s was given, and indicated by a starting and a finishing tone. Participants were asked to repeat the sentences they heard between the two tone signals, and to guess when unsure. Oral responses were recorded using a voice recorder and video-camera setup directly in front of them, to allow more accurate marking of their responses at a later time. The sentence recognition in noise task was scored at a word level (using the standard BKB/A scoring criteria) and performance was scored for each SNR condition.</p><sec><title>EEG</title><p>A soft-cap was used to facilitate the spatial separation of the electrodes. EEG data were recorded from 32 Ag-AgCl sintered electrodes using the 10&#x02013;20 montage with a Synamps II amplifier. The ground electrode was located between the Fz and FPz electrodes. Electrode impedances were kept below 5 k&#x003a9;. Ocular movement was recorded with bipolar electrodes placed at the outer canthi, and above and below the left eye. Data was recorded at a sampling rate of 1000 Hz, an online band-pass filter of 0.01 to 100 Hz, and a notch filter at 50 Hz.</p><p>Post-acquisition, all cortical recordings were analyzed using Fieldtrip, an analysis toolbox in MATLAB developed by <xref rid="B44" ref-type="bibr">Oostenveld et al. (2011)</xref>. The raw EEG data were first epoched between -2 and 6 s relative to the stimulus onset at 0 s which were then re-referenced to the combined mastoids. The re-referenced epochs were then bandpass filtered with the cut-off frequencies of 0.5 to 45 Hz. Eyeblink artifacts were rejected by transforming the sensor space data into independent components space data using independent component analysis (&#x02018;runica&#x02019;). The eyeblink artifacts were visually inspected and rejected by transforming the components data back into sensor space by excluding the identified eyeblink component(s). Movement related artifacts and noisy trials were rejected by visual inspections. The accepted trials were bandpass filtered again with cut-off frequencies between 8 and 12 Hz to extract alpha oscillations. Alpha band activity was extracted from the parietal electrodes (P3, P4, and Pz) during the encoding period (1 s duration finishing 200 ms before the end of the sentence) and was subtracted from the baseline in noise (300&#x02013;800 ms after the noise onset) on a trial by trial basis, then averaged to obtain mean alpha power for each SNR. As no significant time-frequency electrode clusters were identified across the scalp during the sentence processing time period, alpha power in the parietal region was used in the current study. A time-frequency representation of the average EEG data collapsed across all of the signal-to-noise levels (<bold>Figure <xref ref-type="fig" rid="F1">1</xref></bold>) illustrated the increased activity occurring in the alpha frequency-band averaged during the sentence presentations for both 16-channel and 6-channel noise vocoded sentences.</p><fig id="F1" position="float"><label>FIGURE 1</label><caption><p><bold>Time-frequency representation of the EEG activity averaged across all participants, in the frontal and parietal region, for 16- and 6-channel vocoding.</bold> The time-frequency representations are relative to the activity occurring during the 1 s of noise beginning at the 1 s time-point. On this graph, all sentences finished at the 4.5 s time-point.</p></caption><graphic xlink:href="fpsyg-07-00745-g001"/></fig></sec><sec><title>Pupillometry</title><p>Pupil size was measured with a monocular (right eye) Eyelink 1000 eye-tracker sampling at 1000 Hz. Single-trial pupil data was processed through Dataviewer software (version 1.11.1), and compiled into single-trial pupil-diameter waveforms (0 s baseline to 6 s) for further o&#x0fb04;ine processing and analyses performed using MATLAB. Data were smoothed using a 5-points moving average.</p><p>Blinks were identified in each trial as pupil sample sizes that were smaller than three standard deviations below the mean pupil diameter. Trials where more than 15% of the trial samples were detected as in a blink (which also occurred when the participants were looking away from target) were rejected. In accepted trials, samples within blinks were interpolated from between 66 ms preceding the onset of a blink to 132 ms following the end of a blink. Accepted trials were averaged to form condition-specific pupil size waveforms to represent change of pupil dilation across the trial. For each participant a threshold of 135 or more accepted trials in both the 6- and the 16- channel blocks had to be met to not be excluded, so that a meaningful condition average may be formed. The average of accepted trials for each participant was 193, or 13 trials per SNR.</p><p>For each trial, the mean pupil size measured between 0 and 2 s was subtracted from the peak pupil size identified between 2 and 6 s (see <bold>Figure <xref ref-type="fig" rid="F2">2</xref></bold> for an example of the pupil response during the experiment).</p><fig id="F2" position="float"><label>FIGURE 2</label><caption><p><bold>Averaged pupil size over time for all trials and participants, for 16- and 6-channel vocoding.</bold> The 1 s time-point refers to the beginning of noise. On this graph, all sentences finished at the 4.5 s time-point.</p></caption><graphic xlink:href="fpsyg-07-00745-g002"/></fig></sec></sec><sec><title>Behavioral Measures</title><p>A behavioral test session was conducted with each participant to obtain a self-reported measure of effort during the sentence recognition task, which could be later compared to the physiological measures. This measure could not readily be obtained during the physiological test session because of the randomization of SNRs at each trial. The behavioral testing was performed in an acoustically treated room, with the equipment calibrated prior to each participant&#x02019;s session. The speaker was positioned one meter from the participant at 0&#x000b0; azimuth. An adaptive procedure was chosen to obtain effort ratings at three SNRs around the mid-range of each participant&#x02019;s performance-intensity function. The speech-in-noise algorithm and software used were developed by the National Acoustic Laboratories to obtain speech reception thresholds (SRT, the signal to noise ratio at which 50% of words were correctly perceived; see <xref rid="B26" ref-type="bibr">Keidser et al., 2013</xref> for a comprehensive review). Target sentences were presented at 65 dB and the background noise was modulated using an adaptive procedure. The participant&#x02019;s SRT was calculated when the standard error was less than 0.8 dB. The noise was then presented at a fixed level based on the participant&#x02019;s SRT with 1 list (16 sentences), to validate the accuracy of the initial SRT calculation. Finally, the noise was fixed at -3 and +3 dB relative to their SRT and two lists per condition were presented, so that performance could be measured in easier and more difficult conditions. Thus, the conditions presented were: 50%SRT, 50%SRT(-3 dB), and 50%SRT(+3 dB) in the 16- and 6-channel vocoded conditions. All presentations were counterbalanced across participants for level and vocoding. After each presentation, participants were asked to rate the perceived effort invested in each SRT condition on a Borg CR10 scale (<xref rid="B7" ref-type="bibr">Borg, 1998</xref>).</p></sec><sec><title>Statistical Methods</title><p>Linear mixed-effects models with a random intercept for individual were used for all analyses to control for repeated-measures over different levels of SNR on individuals. While random slopes were also of interest, these models failed to converge and were therefore not utilized.</p><p>Models for SRS were built by comparing a model with SNR, presentation mode and channel vocoding to a model containing SNR, presentation mode, channel vocoding and the interaction between SNR and presentation mode. The terms were fitted in the order described although no result difference was found if they were added to the model in a different order. Likelihood ratio tests were used to compare fixed effects of the simpler and more complex models after fitting the model using maximum likelihood. Where an interaction was not significant, the main effects model results were reported. All categorical variables used treatment contrasts (whereby all levels were compared with a reference level). <italic>P</italic>-values less than 0.05 were considered significant for all analyses.</p><p>Models for perceived effort, pupil size and alpha power were built by comparing a model with SNR and channel vocoding as main effects to a model with an interaction between SNR and channel vocoding. Because visual inspection of the change in pupil size and alpha power over SNRs suggested non-linear changes for one or both channels, models sequentially including a quadratic term for SNR (i.e., SNR<sup>2</sup>) and then a cubic term for SNR (i.e., SNR<sup>3</sup>) with an interaction between each term and vocoding channel were used to determine if the effects were similar for both channels. Again, likelihood ratio tests were used to compare models. These models are reported separately by channel vocoding (6 and 16) to aid interpretation. Models with a quadratic term are used to describe a simple curvilinear change while cubic terms are used to explain more complicated curvature with more than one change in the direction of the curve.</p><p>To account for the use of repeated measures on individuals, correlations presented in the results section are the average of the correlations calculated for each individual. Analyses were performed in R version using the nlme Package. This study was conducted under the ethical oversight of the Human Research Ethics Committee at Macquarie University (Ref: 5201100426).</p></sec></sec><sec><title>Results</title><sec><title>Performance-Intensity Functions and Effort Ratings</title><p>Performance-intensity functions were measured during the behavioral test session (using 3 fixed SNRs per participant) and the physiological test session (using randomized SNRs across the 15 levels of noise). As seen in <bold>Figure <xref ref-type="fig" rid="F3">3A</xref></bold>, SRSs measured during the physiological test session increased with SNR (<italic>p</italic> &#x0003c; 0.001) for both vocoding levels [16 ch: <italic>r</italic> = 0.93 (95% CI: 0.92 to 0.94); 6 ch: <italic>r</italic> = 0.92 (95% CI: 0.91 to 0.94)]. As expected, SRSs were significantly greater with the 16-channel material compared to the 6-channel (mean difference 26.72%, 95% CI: 22.12 to 31.31%, <italic>p</italic> &#x0003c; 0.001, <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>). <bold>Figure <xref ref-type="fig" rid="F3">3B</xref></bold> displays the performance-intensity functions where the three SNR levels presented in the behavioral session (fixed presentation) were matched to the same three SNRs measured during the objective session (randomized presentation). There was no evidence for a difference in the pattern of change in SRS between the fixed and random modes of presentation across the SNR levels, after adjusting for channel vocoding (<italic>p</italic> = 0.50, <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>). For the 16-channel vocoding, for every unit increase in SNR, SRS increased by 6.44% (95% CI: 5.07 to 7.82%) for the fixed versus 6.47% (95% CI: 5.12 to 7.82%) for the randomized presentation, showing that the slopes by mode of presentation overlap considerably. Similarly, for the 6-channel vocoding, for every unit increase in SNR, SRS increased by 5.47 (95% CI: 4.29 to 6.64%) for the fixed versus 6.65% (95% CI: 5.13 to 8.18%) for the randomized presentation.</p><fig id="F3" position="float"><label>FIGURE 3</label><caption><p><bold>(A)</bold> Performance-intensity functions (mean plus 95% confidence intervals) are shown for the 16-channel (open circles) and 6-channel (closed circles) vocoded sentences measured during the physiological test session where SNRs were randomized. <bold>(B)</bold> Performance-intensity functions across the behavioral (squares) and physiological (circles) test sessions are very similar. <bold>(C)</bold> Mean effort ratings for 16-channel and 6-channel vocoded material measured in the behavioral test session.</p></caption><graphic xlink:href="fpsyg-07-00745-g003"/></fig><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Results for the linear mixed-effects models.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Coefficient</th><th valign="top" align="left" rowspan="1" colspan="1">SRS</th><th valign="top" align="left" rowspan="1" colspan="1">Perceived effort</th><th valign="top" align="left" rowspan="1" colspan="1">Alpha power</th><th valign="top" align="left" rowspan="1" colspan="1">Pupil size Linear</th><th valign="top" align="left" rowspan="1" colspan="1">Pupil size Cubic</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Intercept<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>38.325 (2.110)<break/>18.160<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>7.506 (0.367)<break/>20.414<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>118.248 (16.382)<break/>7.218<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.401 (0.065)<break/>6.174<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.374 (0.067)<break/>5.577<break/>&#x0003c;0.001</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SNR<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>6.486 (0.453)<break/>14.304<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.497 (0.068)<break/>-7.334<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.0129 (1.242)<break/>0.010<break/>0.992</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.007 (0.003)<break/>2.297<break/>0.022</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.025 (0.008)<break/>3.111<break/>0.002</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SNR<sup>2</sup><break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.001 (0.001)<break/>1.739<break/>0.083</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SNR<sup>3</sup><break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.001 (0.000)<break/>-2.390<break/>0.018</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Presentation<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>1.509 (1.993)<break/>0.757<break/>0.450</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;<break/></td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013; <break/></td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Channel<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>26.715 (2.327)<break/>11.483<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-2.064 (0.321)<break/>6.433<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>21.719 (7.586)<break/>2.863<break/>0.004</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.086 (0.019)<break/>-4.441<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.045 (0.029)<break/>-1.545<break/>0.124</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SNR*Presentation <break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.383 (0.571)<break/>-0.672<break/>0.503</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SNR*Channel <break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.105 (0.093)<break/>-1.128<break/>0.263</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-4.352 (1.756)<break/>-2.478<break/>0.014</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.016 (0.004)<break/>-3.470<break/>&#x0003c;0.001</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.039 (0.011)<break/>-3.430<break/>&#x0003c;0.001</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SNR<sup>2</sup>*Channel <break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>-0.002 (0.001)<break/>-1.924<break/>0.055</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SNR<sup>3</sup>*Channel<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;Slope (SE)<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>t</italic>-value<break/>&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>p</italic>-value</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>&#x02013;</td><td valign="top" align="left" rowspan="1" colspan="1"><break/>0.001 (0.000)<break/>2.229<break/>0.027</td></tr></tbody></table><table-wrap-foot><attrib><italic>Channel 6 was the reference level for channel; Random was the reference level for presentation mode. SRS, Speech Recognition Score; SNR, Signal-to-Noise-Ratio; SNR<sup><italic>2</italic></sup>, quadratic model; SNR<sup><italic>3</italic></sup>, cubic model.</italic></attrib></table-wrap-foot></table-wrap><p><bold>Figure <xref ref-type="fig" rid="F3">3C</xref></bold> shows the mean effort ratings measured after each of the fixed SNR sentence blocks. There was no interaction between SNR and channel vocoding (<italic>p</italic> = 0.26, <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>) indicating no evidence of a different pattern of effort over SNR between the two channels. Excluding the interaction term, LME regression confirmed that perceived effort averaged over channels significantly decreased (<italic>p</italic> &#x0003c; 0.001) with increasing SNR (-0.55, 95% CI: -0.65 to -0.45). SRS with 6-channel vocoding required on average 2.10 units more effort than 16-channel vocoding (95% CI: 1.47 to 2.74; <italic>p</italic> &#x0003c; 0.001).</p></sec><sec><title>EEG Analyses</title><sec><title>Effect of Vocoding on Baseline Alpha</title><p>A LME regression was used to examine the effect of vocoding (conducted during different test sessions) on alpha power during baseline. No significant difference was found between16- and 6-channel vocoding (mean difference = 0.69 mcV<sup>2</sup>, 95% CI: -1.47 to 2.85, <italic>p</italic> = 0.53). This suggests that overall, participants had similar alpha power baselines on both test sessions.</p></sec><sec><title>Alpha Power Change and SNR</title><p>Alpha power was processed as a relative change from baseline in noise, for each trial. A LME regression model suggested a significant interaction effect between SNR and channel vocoding on alpha power change (<italic>p</italic> = 0.01, <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>). Specifically, for the 6-channel vocoding, there was no evidence of a change in alpha power over the different SNRs (0.01%, 95% CI: -2.38 to 2.41%); <italic>p</italic> = 0.99) while for the 16-channel vocoding, for every unit increase in SNR, alpha power decreased by 4.34% (95% CI: 1.94 to 6.73% decrease; <italic>p</italic> &#x0003c; 0.001). Non-linear models using a quadratic or cubic term for both channel vocoding did not improve model fit compared to a linear model (log likelihood -2632.18 vs. -2632.57, <italic>p</italic> = 0.68 and -2631.87 vs. -2632.57, p = 0.84, respectively). As seen in <bold>Figure <xref ref-type="fig" rid="F4">4</xref></bold>, the largest separation between 16- and 6-channel vocoding was in the most challenging (lower) SNRs.</p><fig id="F4" position="float"><label>FIGURE 4</label><caption><p><bold>Alpha power change relative to baseline during 16- and 6-channel vocoded sentence recognition at SNRs between -7 and +7 dB.</bold> Dashed bars indicate 95% CI. The trend lines shown correspond to the best model fit, respectively, for the 16- and the 6-channel conditions.</p></caption><graphic xlink:href="fpsyg-07-00745-g004"/></fig></sec></sec><sec><title>Pupil Analyses</title><sec><title>Pupil Size Change from Baseline</title><p>For the pupil size, a LME model was conducted to verify the effect of vocoding (conducted during different test sessions) on baseline, while controlling for repeated measures. The pupil size during baseline was found to be significantly larger during the second session [6-channel (harder condition); mean difference = 0.56 mm, 95% CI: 0.47 to 0.64 mm, <italic>p</italic> &#x0003c; 0.001].</p></sec><sec><title>Pupil Size Change and SNR</title><p>Looking at the pupil size change relative to baseline (<bold>Figure <xref ref-type="fig" rid="F5">5</xref></bold>), A LME regression model with only a linear term in SNR indicated a significant interaction effect between vocoding and SNR (<italic>p</italic> &#x0003c; 0.001, <bold>Table <xref ref-type="table" rid="T1">1</xref></bold>). For every unit increase in SNR, pupil size significantly increased by 0.007 mm (95% CI: 0.001 to 0.014 mm; <italic>p</italic> = 0.02) for the 6-channel vocoding while it significantly decreased for the 16-channel (mean change -0.008 mm, 95% CI: -0.015 to -0.002 mm; <italic>p</italic> = 0.01). Visual inspection of the relationship between pupil size and SNR indicated a potential non-linear relationship. As such a mixed effects model for pupil diameter containing a cubic term for SNR (<bold>Table <xref ref-type="table" rid="T1">1</xref></bold>) had significantly better fit compared to a linear model (log likelihood 97.6 versus 92.4, <italic>p</italic> = 0.04) or quadratic model (log likelihood 97.6 versus 94.4, <italic>p</italic> = 0.04). An interaction between the cubic term and channel was significant (<italic>p</italic> = 0.03). Examination of the relationship between pupil size and SNR within each channel indicated that with 16-channel vocoding, there was no significant effect of a quadratic term (<italic>p</italic> = 0.34) or cubic term in SNR (<italic>p</italic> = 0.46), while there was strong evidence of a cubic relationship (<italic>p</italic> = 0.01) for the 6-channel vocoding.</p><fig id="F5" position="float"><label>FIGURE 5</label><caption><p><bold>Pupil size change relative to baseline during 16-and 6-channel vocoded sentence recognition at SNRs between -7 and +7 dB.</bold> Dashed bars indicate 95% CI. The trend lines shown correspond to the best model fit, respectively, for the 16- and the 6-channel conditions.</p></caption><graphic xlink:href="fpsyg-07-00745-g005"/></fig></sec></sec><sec><title>Individual Alpha Power versus Pupil Size Change Comparisons</title><p>At the individual level, alpha power change was not found to be significantly correlated (<italic>p</italic> &#x0003e; 0.05) with pupil size change for either the 16-channel (mean <italic>r</italic> = 0.05, 95% CI: -0.16 to 0.26) or the 6-channel vocoding (mean <italic>r</italic> = -0.10, 95% CI: -0.35 to 0.16).</p></sec></sec><sec><title>Discussion</title><p>The results of this study suggest that, while there was a significant and expected difference in speech recognition performance and effort rating between the 6- and 16-channel vocoded material across the 15 SNRs, the mean changes observed in the physiological measures (alpha power and pupil size) were less predictable. Significant relationships were found between mean pupil dilation and SNR, and mean alpha power and SNR for 16-channel vocoded sentences, showing a similar trajectory of change; i.e., larger pupil responses and larger alpha power change were measured for less intelligible speech. For the pupil response only, there was also a significant non-linear relationship with SNR with the 6-channel vocoded sentences, whereby pupil dilation was larger in the hardest and easier conditions. This is perhaps consistent with the non-linear change in pupil dilation with changes in task difficulty that have been shown previously (<xref rid="B19" ref-type="bibr">Granholm et al., 1996</xref>; <xref rid="B76" ref-type="bibr">Zekveld and Kramer, 2014</xref>). Further, significant interactions between SNR and vocoding were seen in both physiological measures, although the largest difference between alpha power change was observed in the least intelligible conditions (more negative SNRs) whereas the largest difference in the pupil dilation was observed in the most intelligible conditions (more positive SNRs).</p><p>The linear association between SNR and pupil dilation for the 16-channel vocoded sentences, and the comparatively larger pupil dilation for the 6-channel compared with the 16-channel vocoded sentences at more positive SNRs (&#x02265;+2 dB), is similar to that observed in previous studies, i.e., larger pupil size is observed with greater cognitive load (<xref rid="B25" ref-type="bibr">Kahneman and Beatty, 1966</xref>; <xref rid="B19" ref-type="bibr">Granholm et al., 1996</xref>; <xref rid="B74" ref-type="bibr">Winn et al., 2015</xref>). Larger pupil dilation relative to baseline is typically measured during more cognitively demanding speech processing tasks. For example, poorer SNRs (<xref rid="B77" ref-type="bibr">Zekveld et al., 2010</xref>), greater spectral degradation with channel vocoding (<xref rid="B74" ref-type="bibr">Winn et al., 2015</xref>), single-talker compared with noise maskers (<xref rid="B29" ref-type="bibr">Koelewijn et al., 2012</xref>), randomized SNRs compared with fixed SNRs (<xref rid="B76" ref-type="bibr">Zekveld and Kramer, 2014</xref>), grammatical complexity (<xref rid="B60" ref-type="bibr">Schluroff, 1982</xref>) or perceptual effort with hearing loss (<xref rid="B30" ref-type="bibr">Kramer et al., 1997</xref>). Certainly the results of the current study support an increase in pupil dilation for the most challenging SNRs with the 16-channel vocoded sentences. However, the relationship between pupil dilation and SNR for the 6-channel vocoded sentences in the current study was not simple, where the mean pupil dilation across subjects plateaued for moderately negative SNRs and showed an increase with increasing speech intelligibility. It is possible that the changes in the pupil size across the 15 SNRs for the 6-channel vocoded sentences could reflect the non-linear behavior of the pupil size that has been observed when task difficulty exceeds capacity (<xref rid="B46" ref-type="bibr">Peavler, 1974</xref>; <xref rid="B19" ref-type="bibr">Granholm et al., 1996</xref>; <xref rid="B76" ref-type="bibr">Zekveld and Kramer, 2014</xref>). For example, it has been demonstrated that pupil dilation systematically increases with task difficulty (such as with a digit recall task), until it reaches or exceeds the limits of available cognitive resources, whereby it either asymptotes (<xref rid="B46" ref-type="bibr">Peavler, 1974</xref>), declines (<xref rid="B19" ref-type="bibr">Granholm et al., 1996</xref>), or shows both a decline followed by an asymptote for the most challenging intelligibility conditions (<xref rid="B76" ref-type="bibr">Zekveld and Kramer, 2014</xref>). An alternative explanation is that the noise levels <italic>per se</italic> could have influenced pupil dilation at the more negative SNRs (noise levels reached a maximum of 72 dB), where mean pupil dilation for both 16- and 6-channel vocoded sentences was similar. While <xref rid="B76" ref-type="bibr">Zekveld and Kramer (2014)</xref> attempted to reduce the likelihood of noise affecting pupil dilation by controlling the overall signal level while changing the SNR, in the current study, a fixed signal level was used with modulated levels of noise. Pupil dilation has been shown to be modulated by acute stress (<xref rid="B69" ref-type="bibr">Valentino and Van Bockstaele, 2008</xref>; <xref rid="B33" ref-type="bibr">Laeng et al., 2012</xref>) and animal studies have demonstrated that long-term effects of non-traumatic noise is associated with increased cortisol levels, hypertension and reduced cardiovascular function (see <xref rid="B18" ref-type="bibr">Gour&#x000e9;vitch et al., 2014</xref> for a review). A recent study looking at physiological measures of stress during listening in noise found that adults with hearing loss, who are constantly exposed to degraded speech, had higher autonomic system reactivity compared to adults with normal hearing, at similar performance levels (<xref rid="B38" ref-type="bibr">Mackersie et al., 2015</xref>). Therefore, while the noise levels in the current study were short-term, this may have caused a phasic stress reaction which could have influenced pupil dilation. This hypothesis, however, is not supported by studies suggesting that the pupil dilates with negative affect (<xref rid="B45" ref-type="bibr">Partala and Surakka, 2003</xref>).</p><p>The change in mean alpha power, relative to baseline, showed an enhancement of alpha activity in both 16-channel and 6-channel vocoding conditions, consistent with the inhibition hypothesis, where activity that is not related to the goal-directed task is actively inhibited (<xref rid="B28" ref-type="bibr">Klimesch et al., 2007</xref>). Therefore, it has been suggested that alpha enhancement which occurs during a speech-in-noise task results from the enhancement of auditory attention through the active suppression of noise (<xref rid="B66" ref-type="bibr">Strau&#x000df; et al., 2014</xref>). However, most studies assessing alpha power change with vocoded speech material (<xref rid="B42" ref-type="bibr">Obleser and Weisz, 2012</xref>; <xref rid="B4" ref-type="bibr">Becker et al., 2013</xref>; <xref rid="B66" ref-type="bibr">Strau&#x000df; et al., 2014</xref>) or during the processing of semantic information (<xref rid="B27" ref-type="bibr">Klimesch et al., 1997</xref>) have shown a reduction of alpha power, which is consistent with active cognitive processing of speech information. Specifically, the results of the current study appear contradictory to those reported by <xref rid="B42" ref-type="bibr">Obleser and Weisz (2012)</xref> using noise vocoded (2-, 4-, 8-, and 16-channels) mono- bi- and tri-syllabic words. They showed less alpha power suppression, of posterior-central alpha power with decreasing intelligibility measured between 800 and 900 ms post word onset. However, the task across the two studies was not the same. In the current study, participants were required to repeat the vocoded sentences, whereas in the <xref rid="B42" ref-type="bibr">Obleser and Weisz (2012)</xref> study, participants were asked to rank the comprehension of vocoded words without attending to the linguistic or acoustic aspects of the speech materials. While previous studies have shown a very high correlation between SRSs and rating scores, it is unclear whether the pattern of event-related oscillatory cortical activity measured during these different tasks is the same. Further, the types of analyses conducted across studies are not the same. For example, while <xref rid="B4" ref-type="bibr">Becker et al. (2013)</xref> demonstrated that mean alpha power during the region of interest (ROI) between 480 and 620 ms is reduced as speech intelligibility is increased (using monosyllabic French words), this was an absolute measure of alpha power rather than a change relative to the baseline. Variability of whether alpha power was increased or decreased was observed within studies. For example, <xref rid="B4" ref-type="bibr">Becker et al. (2013)</xref> showed the mean trajectory of change in alpha power during noise-vocoded monosyllabic words and demonstrated that alpha power is enhanced in the less intelligible conditions (similar to our results) but is suppressed in the most intelligible conditions (similar to the results shown by <xref rid="B42" ref-type="bibr">Obleser and Weisz, 2012</xref>). Further, using an auditory lexical decision task, <xref rid="B66" ref-type="bibr">Strau&#x000df; et al. (2014)</xref> demonstrated mean increases of alpha power occurred for clear pseudo-words but a reduction was observed for ambiguous and real-words, which parametrically changed as the clarity of the words increased. Finally, using 18 younger and 20 older healthy adults, <xref rid="B75" ref-type="bibr">W&#x000f6;stmann et al. (2015)</xref> demonstrated that decreases in mean alpha power which occurred as speech intelligibility increased (using four syllable digits masked by a single speaker) appeared to be driven by the older adults rather than an effect across the entire population. Given the differences in the types of speech stimuli used across the different studies, the task required, as well as the ROI used to assess alpha power changes (i.e., during or after the speech tokens), and the different populations assessed (older versus younger adults), further investigation of alpha power is needed to better understand the changes observed and how this might be used as an objective measure of attentional effort and/or cognitive load for the individual.</p><p>Within the current study, while a significant interaction was found between 6- and 16-channel vocoding for both alpha power and pupil size change, the trend patterns differed. The magnitude of the difference between both vocoding levels was greater in the most challenging SNRs for alpha power, but in the least challenging SNRs for the pupil size. This could suggest that these physiological responses are driven by different neurophysiological or attentional networks (<xref rid="B11" ref-type="bibr">Corbetta and Shulman, 2002</xref>; <xref rid="B10" ref-type="bibr">Corbetta et al., 2008</xref>; <xref rid="B48" ref-type="bibr">Petersen and Posner, 2012</xref>). There is a vast literature on attentional effort which suggests that discrete neuroanatomical areas encode specific cognitive operations (&#x0201c;processors&#x0201d;) that are involved in attention, which are modified by &#x0201c;controllers&#x0201d; depending on the type of attentional tasks required (see <xref rid="B50" ref-type="bibr">Power and Petersen, 2013</xref>). While the majority of the literature in this field focuses on the visual modality, there is evidence to suggest that similar processes should be evident when listening to degraded speech, such as listening in noise (<xref rid="B64" ref-type="bibr">Spagna et al., 2015</xref>). The main determinants of attentional allocation would then be; the identification of the appropriate processing strategy needed to undertake the speech perception task, the maintenance of attention during the task, and the processing of errors to increase (or, at least, reduce declines in) performance. Further, these processes may work synergistically under less cognitively demanding conditions but diverge under more challenging conditions, or conditions which have different types of attentional requirements (<xref rid="B70" ref-type="bibr">Vossel et al., 2014</xref>). It is also possible that different processors and controllers are used by different individuals to undertake these cognitively demanding task, which may have led to a lack of correlation between alpha power change and pupil dilation change within individuals. <xref rid="B11" ref-type="bibr">Corbetta and Shulman (2002)</xref> proposed the existence of two anatomically distinct attention networks; the dorsal fronto-parietal network, which is involved in the top&#x02013;down voluntary or goal-directed allocation of attention (which includes preparatory attention and orienting within memory), and the ventral fronto-parietal network, which is involved in the involuntary shifts in attention. It is proposed that under normal circumstances, the ventral network is suppressed but is activated by unexpected, novel, salient, or behaviorally relevant events. Where this occurs, it is assumed that a &#x0201c;circuit-breaking&#x0201d; signal is sent to the dorsal attention network, resulting in reorienting, or shifting in attention toward this new event (<xref rid="B10" ref-type="bibr">Corbetta et al., 2008</xref>). It has been proposed that the locus coeruleus-norepinephrine (LC-NE) system modulates the functional integration of the entire cortical attentional system (<xref rid="B10" ref-type="bibr">Corbetta et al., 2008</xref>; <xref rid="B57" ref-type="bibr">Sara, 2009</xref>), whereby NE released by the LC triggers the ventral network to interrupt the dorsal attention network (<xref rid="B8" ref-type="bibr">Bouret and Sara, 2005</xref>) and reset attention. This ensures a coordinated rapid and adaptive neurophysiological response to spontaneous or conditioned behavioral imperatives (<xref rid="B58" ref-type="bibr">Sara and Bouret, 2012</xref>).</p><p>Pupil dilation is under the control of the LC-NE system, therefore it may be reasonable to assume that indirect attention tasks may be associated with the changes in pupil dilation observed in the current study. It has been proposed that pupil dilation is modulated by both staying on task and choosing between alternatives (exploration; <xref rid="B2" ref-type="bibr">Aston-Jones and Cohen, 2005</xref>). Therefore, a complex task, such as the perception and comprehension of a moderately intelligible (vocoded) speech signal, may result in changes in pupil dilation that reflect the interaction between different processing strategies. Alpha power changes have been associated with top&#x02013;down inhibition of task irrelevant brain regions, and it has been suggested that alpha power is under the control of the dorsal attention network (<xref rid="B79" ref-type="bibr">Zumer et al., 2014</xref>). Further, increases in alpha power may inhibit the ventral attention network, preventing reorienting to irrelevant stimuli during goal-directed cognitive behavior (<xref rid="B5" ref-type="bibr">Benedek et al., 2014</xref>). While other models of attention exist (<xref rid="B62" ref-type="bibr">Seeley et al., 2007</xref>; <xref rid="B48" ref-type="bibr">Petersen and Posner, 2012</xref>), it is clear that a simple association between a physiological measure of attentional effort and task difficulty (e.g., changes in speech intelligibility) fails to consider the multiple autonomic cognitive operations as well as the voluntary control of attention that reflects effortful cognitive control (see <xref rid="B59" ref-type="bibr">Sarter et al., 2006</xref>). It is recognized that there is a dynamic interplay between the bottom&#x02013;up sensory information and the top&#x02013;down cognitively controlled factors (which may be either under automatic or voluntary control), such as knowledge, expectations and goals, that can be modulated by motivational factors, such as payment for participations (<xref rid="B67" ref-type="bibr">Tomporowski and Tinsley, 1996</xref>) and genetic influencers (<xref rid="B15" ref-type="bibr">Fan et al., 2003</xref>). Therefore, it is reasonable to assume that considerable variability in attentional allocation could exist between individuals undertaking a highly complex task.</p><p>An alternative explanation is that the within-subject variability of sustaining on-task attention toward sentences with unpredictable levels of intelligibility, was greater under the more challenging noise vocoding conditions (6-channel) where the effort-reward balance was not as high compared with the 16-channel vocoded materials. Sustaining attention on a complex task is challenging (<xref rid="B71" ref-type="bibr">Warm et al., 2008</xref>) and requires suppression of internal tendencies of mind-wandering, a default network activation that typically occurs during low task demands (<xref rid="B9" ref-type="bibr">Christoff et al., 2009</xref>; <xref rid="B20" ref-type="bibr">Gruberger et al., 2011</xref>), with concomitant activation of the goal-directed dorsal fronto-parietal attentional network (<xref rid="B11" ref-type="bibr">Corbetta and Shulman, 2002</xref>). Fluctuations in sustained attention can occur with stress, distraction with competing stimuli, fatigue, or lack of motivation toward the task, and are commonly associated with a decline in performance (<xref rid="B21" ref-type="bibr">Hancock, 1989</xref>; <xref rid="B14" ref-type="bibr">Esterman et al., 2012</xref>). As stated by <xref rid="B14" ref-type="bibr">Esterman et al. (2012)</xref> &#x0201c;as the neural systems supporting task performance appear to shift with one&#x02019;s attentional state, failure to account for attentional fluctuations may obscure meaningful information about underlying mechanisms&#x0201d;. Certainly, some people have a preponderance to mind-wandering (<xref rid="B39" ref-type="bibr">Mason et al., 2007</xref>). This may be a confounder to the results of the current study comparing physiological responses to a range of SNRs, despite the ecological validity that this may have to their ability to follow conversations within multi-talker environments. That is, the variability in the physiological measures may, in fact, provide important information about the individual&#x02019;s processing of degraded speech that is not captured within more common behavioral measures of speech perception. For example, a recent study by <xref rid="B32" ref-type="bibr">Kuchinsky et al. (2016)</xref>, suggests that individual differences in the pupillary response of older adults with hearing loss during a monosyllabic word recognition task was related to task vigilance (less variability in response time) and to the extent of primary auditory cortical activity. Therefore, pupil dilation may index the magnitude of the engagement between bottom&#x02013;up sensory and top&#x02013;down cortical processing which is increased with greater degradation of the speech signal (influenced by poorer SNR, reduced spectral information, or hearing loss).</p><p>Significant differences in the baseline data were also observed between the 6- and 16-channel vocoding for pupil size, but not for alpha power. These two levels of vocoding were assessed during different sessions for all subjects, therefore this could be due either to a session effect, or to a difference in the level of cognitive effort that was maintained throughout the session. Given that the results are consistent with an increase in cognitive load during the 6-channel vocoded session, it is likely that the difference in the tonic pupillary response across the two physiological measures sessions (16- versus 6-channel vocoded-sentence tasks) resulted from differences in vigilance or the awareness of errors in performance during the more cognitively challenging task (<xref rid="B12" ref-type="bibr">Critchley, 2005</xref>; <xref rid="B68" ref-type="bibr">Ullsperger et al., 2010</xref>).</p><p>Limitations of the study include the relatively small number of participants included in the final data analysis (particularly for pupillary measures), and that only 16 sentences were presented for each SNR level (scored as 50 words across the set of 16 sentences) in each condition, reducing statistical power. Further, the test set-up restricted people from responding normally to an effortful task (i.e., a number of participants tended to close their eyes during the stimuli presentation but were instructed to keep their eyes opened). Explicitly investing effort in trying to keep their eyes opened despite the natural tendency to want to close them may have in itself created changes in pupil size and alpha oscillations. This may also have added an additional stressful component to the task.</p></sec><sec><title>Conclusion</title><p>The results of this study suggest that the relationship between task difficulty and both pupil dilation and alpha power change was similar for the 16-channel vocoded sentences (high intelligibility), which might suggest that the attentional networks are operating with high concordance, or in a consistent and predictable manner across the SNRs. However, further degradations in the speech intelligibility, using the 6-channel vocoded materials, could have produced a discordant relationship between the attention networks, or different processors (such as linguistic strategies) may have been used to comprehend the speech signal. Importantly, however, given the considerable interest in assessing listening effort within clinical settings (see <xref rid="B41" ref-type="bibr">McGarrigle et al., 2014</xref>), it is important to ensure that we have a solid understanding of what these physiological measures are assessing, and how to interpret the responses for the individual. Certainly, the results of this study do not currently support the clinical use of these physiological techniques as sufficiently sensitive to provide complementary information about listening effort to existing measures of speech perception performance. To be clinically viable in a hearing rehabilitation setting, such objective indices of effort should be more sensitive to changes in auditory input than existing measures of speech perception performance or subjective ratings of effort. The behavior of these indices should also be predictable across a range of performances and speech degradation to be applicable to the range of hearing loss and devices available, including hearing aids, and cochlear implants.</p></sec><sec><title>Author Contributions</title><p>Original idea: PL, CM, IB. Protocol development: PL, IB, CM, RI. Data collection: CYL, LG, PL, IB. Writing of manuscript: CM, IB, LG, KM, CYL. Data processing and analyses: RI, PL, PG, IB, CM, KM.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><fn-group><fn fn-type="financial-disclosure"><p><bold>Funding.</bold> This study was supported by the HEARing CRC, established and supported by the Cooperative Research Centres Programme &#x02013; Business Australia.</p></fn></fn-group><ack><p>The authors thank J&#x000f6;rg Buchholz for support with protocol development, Mike Jones for support with statistical analyses, and Gabrielle Martinez for help with data collection. Contributions- Original idea: PL, CM, IB. Protocol development: PL, IB, CM, RI. Data collection: CYL, LG, PL, IB. Writing of manuscript: CM, IB, LG, KM, CYL. Data processing and analyses: RI, PL, PG, IB, CM, KM.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>C. R.</given-names></name></person-group> (<year>1976</year>). <article-title>Coping behaviors as intervening mechanisms in the inverted-U stress-performance relationship.</article-title>
<source><italic>J. Appl. Psychol.</italic></source>
<volume>61</volume>
<issue>30</issue>
<pub-id pub-id-type="doi">10.1037/0021-9010.61.1.30</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aston-Jones</surname><given-names>G.</given-names></name><name><surname>Cohen</surname><given-names>J. D.</given-names></name></person-group> (<year>2005</year>). <article-title>An integrative theory of locus coeruleus-norepinephrine function: adaptive gain and optimal performance.</article-title>
<source><italic>Annu. Rev. Neurosci.</italic></source>
<volume>28</volume>
<fpage>403</fpage>&#x02013;<lpage>450</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.28.061604.135709</pub-id><pub-id pub-id-type="pmid">16022602</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avivi-Reich</surname><given-names>M.</given-names></name><name><surname>Daneman</surname><given-names>M.</given-names></name><name><surname>Schneider</surname><given-names>B. A.</given-names></name></person-group> (<year>2014</year>). <article-title>How age and linguistic competence alter the interplay of perceptual and cognitive factors when listening to conversations in a noisy environment.</article-title>
<source><italic>Front. Syst. Neurosci.</italic></source>
<volume>8</volume>:<issue>21</issue>
<pub-id pub-id-type="doi">10.3389/fnsys.2014.00021</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>R.</given-names></name><name><surname>Pefkou</surname><given-names>M.</given-names></name><name><surname>Michel</surname><given-names>C. M.</given-names></name><name><surname>Hervais-Adelman</surname><given-names>A. G.</given-names></name></person-group> (<year>2013</year>). <article-title>Left temporal alpha-band activity reflects single word intelligibility.</article-title>
<source><italic>Front. Syst. Neurosci.</italic></source>
<volume>7</volume>:<issue>121</issue>
<pub-id pub-id-type="doi">10.3389/fnsys.2013.00121</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benedek</surname><given-names>M.</given-names></name><name><surname>Schickel</surname><given-names>R. J.</given-names></name><name><surname>Jauk</surname><given-names>E.</given-names></name><name><surname>Fink</surname><given-names>A.</given-names></name><name><surname>Neubauer</surname><given-names>A. C.</given-names></name></person-group> (<year>2014</year>). <article-title>Alpha power increases in right parietal cortex reflects focused internal attention.</article-title>
<source><italic>Neuropsychologia</italic></source>
<volume>56</volume>
<fpage>393</fpage>&#x02013;<lpage>400</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.02.010</pub-id><pub-id pub-id-type="pmid">24561034</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernarding</surname><given-names>C.</given-names></name><name><surname>Strauss</surname><given-names>D. J.</given-names></name><name><surname>Hannemann</surname><given-names>R.</given-names></name><name><surname>Seidler</surname><given-names>H.</given-names></name><name><surname>Corona-Strauss</surname><given-names>F. I.</given-names></name></person-group> (<year>2013</year>). <article-title>Neural correlates of listening effort related factors: influence of age and hearing impairment.</article-title>
<source><italic>Brain Res. Bull.</italic></source>
<volume>91</volume>
<fpage>21</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainresbull.2012.11.005</pub-id><pub-id pub-id-type="pmid">23201299</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Borg</surname><given-names>G.</given-names></name></person-group> (<year>1998</year>). <source><italic>Borg&#x02019;s Perceived Exertion and Pain Scales.</italic></source>
<publisher-loc>Champaign</publisher-loc>: <publisher-name>Human Kinetics</publisher-name>.</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouret</surname><given-names>S.</given-names></name><name><surname>Sara</surname><given-names>S. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Network reset: a simplified overarching theory of locus coeruleus noradrenaline function.</article-title>
<source><italic>Trends Neurosci.</italic></source>
<volume>28</volume>
<fpage>574</fpage>&#x02013;<lpage>582</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2005.09.002</pub-id><pub-id pub-id-type="pmid">16165227</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christoff</surname><given-names>K.</given-names></name><name><surname>Gordon</surname><given-names>A. M.</given-names></name><name><surname>Smallwood</surname><given-names>J.</given-names></name><name><surname>Smith</surname><given-names>R.</given-names></name><name><surname>Schooler</surname><given-names>J. W.</given-names></name></person-group> (<year>2009</year>). <article-title>Experience sampling during fMRI reveals default network and executive system contributions to mind wandering.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U.S.A.</italic></source>
<volume>106</volume>
<fpage>8719</fpage>&#x02013;<lpage>8724</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0900234106</pub-id><pub-id pub-id-type="pmid">19433790</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M.</given-names></name><name><surname>Patel</surname><given-names>G.</given-names></name><name><surname>Shulman</surname><given-names>G. L.</given-names></name></person-group> (<year>2008</year>). <article-title>The reorienting system of the human brain: from environment to theory of mind.</article-title>
<source><italic>Neuron</italic></source>
<volume>58</volume>
<fpage>306</fpage>&#x02013;<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2008.04.017</pub-id><pub-id pub-id-type="pmid">18466742</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M.</given-names></name><name><surname>Shulman</surname><given-names>G. L.</given-names></name></person-group> (<year>2002</year>). <article-title>Control of goal-directed and stimulus-driven attention in the brain.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>3</volume>
<fpage>201</fpage>&#x02013;<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1038/nrn755</pub-id><pub-id pub-id-type="pmid">11994752</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Critchley</surname><given-names>H. D.</given-names></name></person-group> (<year>2005</year>). <article-title>Neural mechanisms of autonomic, affective, and cognitive integration.</article-title>
<source><italic>J. Comp. Neurol.</italic></source>
<volume>493</volume>
<fpage>154</fpage>&#x02013;<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1002/cne.20749</pub-id><pub-id pub-id-type="pmid">16254997</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danermark</surname><given-names>B.</given-names></name><name><surname>Gellerstedt</surname><given-names>L. C.</given-names></name></person-group> (<year>2004</year>). <article-title>Psychosocial work environment, hearing impairment and health.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>43</volume>
<fpage>383</fpage>&#x02013;<lpage>389</lpage>. <pub-id pub-id-type="doi">10.1080/14992020400050049</pub-id><pub-id pub-id-type="pmid">15515637</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esterman</surname><given-names>M.</given-names></name><name><surname>Noonan</surname><given-names>S.K.</given-names></name><name><surname>Rosenberg</surname><given-names>M.</given-names></name><name><surname>DeGutis</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>In the zone or zoning out? Tracking behavioral and neural fluctuations during sustained attention.</article-title>
<source><italic>Cereb Cortex.</italic></source>
<volume>23</volume>
<fpage>2712</fpage>&#x02013;<lpage>2723</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhs261</pub-id><pub-id pub-id-type="pmid">22941724</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>J.</given-names></name><name><surname>Fossella</surname><given-names>J.</given-names></name><name><surname>Sommer</surname><given-names>T.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Posner</surname><given-names>M. I.</given-names></name></person-group> (<year>2003</year>). <article-title>Mapping the genetic variation of executive attention onto brain activity.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U.S.A.</italic></source>
<volume>100</volume>
<fpage>7406</fpage>&#x02013;<lpage>7411</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0732088100</pub-id><pub-id pub-id-type="pmid">12773616</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friesen</surname><given-names>L. M.</given-names></name><name><surname>Shannon</surname><given-names>R. V.</given-names></name><name><surname>Baskent</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name></person-group> (<year>2001</year>). <article-title>Speech recognition in noise as a function of the number of spectral channels: comparison of acoustic hearing and cochlear implants.</article-title>
<source><italic>J. Acoust. Soc. Am.</italic></source>
<volume>110</volume>
<fpage>1150</fpage>&#x02013;<lpage>1163</lpage>. <pub-id pub-id-type="doi">10.1121/1.1381538</pub-id><pub-id pub-id-type="pmid">11519582</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosselin</surname><given-names>P. A.</given-names></name><name><surname>Gagn&#x000e9;</surname><given-names>J. P.</given-names></name></person-group> (<year>2011</year>). <article-title>Older adults expend more listening effort than young adults recognizing speech in noise.</article-title>
<source><italic>J. Speech Lang. Hear. Res.</italic></source>
<volume>54</volume>
<fpage>944</fpage>&#x02013;<lpage>958</lpage>. <pub-id pub-id-type="doi">10.1044/1092-4388(2010/10-0069)</pub-id><pub-id pub-id-type="pmid">21060138</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gour&#x000e9;vitch</surname><given-names>B.</given-names></name><name><surname>Edeline</surname><given-names>J.-M.</given-names></name><name><surname>Occelli</surname><given-names>F.</given-names></name><name><surname>Eggermont</surname><given-names>J. J.</given-names></name></person-group> (<year>2014</year>). <article-title>Is the din really harmless? Long-term effects of non-traumatic noise on the adult auditory system.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>15</volume>
<fpage>483</fpage>&#x02013;<lpage>491</lpage>. <pub-id pub-id-type="doi">10.1038/nrn3744</pub-id><pub-id pub-id-type="pmid">24946762</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granholm</surname><given-names>E.</given-names></name><name><surname>Asarnow</surname><given-names>R. F.</given-names></name><name><surname>Sarkin</surname><given-names>A. J.</given-names></name><name><surname>Dykes</surname><given-names>K. L.</given-names></name></person-group> (<year>1996</year>). <article-title>Pupillary responses index cognitive resource limitations.</article-title>
<source><italic>Psychophysiology</italic></source>
<volume>33</volume>
<fpage>457</fpage>&#x02013;<lpage>461</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.1996.tb01071.x</pub-id><pub-id pub-id-type="pmid">8753946</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruberger</surname><given-names>M.</given-names></name><name><surname>Simon</surname><given-names>E. B.</given-names></name><name><surname>Levkovitz</surname><given-names>Y.</given-names></name><name><surname>Zangen</surname><given-names>A.</given-names></name><name><surname>Hendler</surname><given-names>T.</given-names></name></person-group> (<year>2011</year>). <article-title>Towards a neuroscience of mind-wandering.</article-title>
<source><italic>Front. Hum. Neurosci.</italic></source>
<volume>5</volume>:<issue>56</issue>
<pub-id pub-id-type="doi">10.3389/fnhum.2011.00056</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hancock</surname><given-names>P. A.</given-names></name></person-group> (<year>1989</year>). <article-title>A dynamic model of stress and sustained attention.</article-title>
<source><italic>Hum. Fact.</italic></source>
<volume>31</volume>
<fpage>519</fpage>&#x02013;<lpage>537</lpage>.</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000e9;tu</surname><given-names>R.</given-names></name><name><surname>Jones</surname><given-names>L.</given-names></name><name><surname>Getty</surname><given-names>L.</given-names></name></person-group> (<year>1993</year>). <article-title>The impact of acquired hearing impairment on intimate relationships: implications for rehabilitation.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>32</volume>
<fpage>363</fpage>&#x02013;<lpage>380</lpage>. <pub-id pub-id-type="doi">10.3109/00206099309071867</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000e9;tu</surname><given-names>R.</given-names></name><name><surname>Riverin</surname><given-names>L.</given-names></name><name><surname>Lalande</surname><given-names>N.</given-names></name><name><surname>Getty</surname><given-names>L.</given-names></name><name><surname>St-Cyr</surname><given-names>C.</given-names></name></person-group> (<year>1988</year>). <article-title>Qualitative analysis of the handicap associated with occupational hearing loss.</article-title>
<source><italic>Br. J. Audiol.</italic></source>
<volume>22</volume>
<fpage>251</fpage>&#x02013;<lpage>264</lpage>. <pub-id pub-id-type="doi">10.3109/03005368809076462</pub-id><pub-id pub-id-type="pmid">3242715</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hornsby</surname><given-names>B. W.</given-names></name></person-group> (<year>2013</year>). <article-title>The effects of hearing aid use on listening effort and mental fatigue associated with sustained speech processing demands.</article-title>
<source><italic>Ear Hear.</italic></source>
<volume>34</volume>
<fpage>523</fpage>&#x02013;<lpage>534</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e31828003d8</pub-id><pub-id pub-id-type="pmid">23426091</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D.</given-names></name><name><surname>Beatty</surname><given-names>J.</given-names></name></person-group> (<year>1966</year>). <article-title>Pupil diameter and load on memory.</article-title>
<source><italic>Science</italic></source>
<volume>154</volume>
<fpage>1583</fpage>&#x02013;<lpage>1585</lpage>. <pub-id pub-id-type="doi">10.1126/science.154.3756.1583</pub-id><pub-id pub-id-type="pmid">5924930</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keidser</surname><given-names>G.</given-names></name><name><surname>Dillon</surname><given-names>H.</given-names></name><name><surname>Mejia</surname><given-names>J.</given-names></name><name><surname>Nguyen</surname><given-names>C.-V.</given-names></name></person-group> (<year>2013</year>). <article-title>An algorithm that administers adaptive speech-in-noise testing to a specified reliability at selectable points on the psychometric function.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>52</volume>
<fpage>795</fpage>&#x02013;<lpage>800</lpage>. <pub-id pub-id-type="doi">10.3109/14992027.2013.817688</pub-id><pub-id pub-id-type="pmid">23957444</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W.</given-names></name><name><surname>Doppelmayr</surname><given-names>M.</given-names></name><name><surname>Pachinger</surname><given-names>T.</given-names></name><name><surname>Russegger</surname><given-names>H.</given-names></name></person-group> (<year>1997</year>). <article-title>Event-related desynchronization in the alpha band and the processing of semantic information.</article-title>
<source><italic>Cogn. Brain. Res.</italic></source>
<volume>6</volume>
<fpage>83</fpage>&#x02013;<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(97)00018-9</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W.</given-names></name><name><surname>Sauseng</surname><given-names>P.</given-names></name><name><surname>Hanslmayr</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>EEG alpha oscillations: the inhibition&#x02013;timing hypothesis.</article-title>
<source><italic>Brain Res. Rev.</italic></source>
<volume>53</volume>
<fpage>63</fpage>&#x02013;<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainresrev.2006.06.003</pub-id><pub-id pub-id-type="pmid">16887192</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelewijn</surname><given-names>T.</given-names></name><name><surname>Zekveld</surname><given-names>A. A.</given-names></name><name><surname>Festen</surname><given-names>J. M.</given-names></name><name><surname>Kramer</surname><given-names>S. E.</given-names></name></person-group> (<year>2012</year>). <article-title>Pupil dilation uncovers extra listening effort in the presence of a single-talker masker.</article-title>
<source><italic>Ear. Hear.</italic></source>
<volume>33</volume>
<fpage>291</fpage>&#x02013;<lpage>300</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e3182310019</pub-id><pub-id pub-id-type="pmid">21921797</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>S. E.</given-names></name><name><surname>Kapteyn</surname><given-names>T. S.</given-names></name><name><surname>Festen</surname><given-names>J. M.</given-names></name><name><surname>Kuik</surname><given-names>D. J.</given-names></name></person-group> (<year>1997</year>). <article-title>Assessing aspects of auditory handicap by means of pupil dilatation.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>36</volume>
<fpage>155</fpage>&#x02013;<lpage>164</lpage>. <pub-id pub-id-type="doi">10.3109/00206099709071969</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>S. E.</given-names></name><name><surname>Kapteyn</surname><given-names>T. S.</given-names></name><name><surname>Houtgast</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>). <article-title>Occupational performance: comparing normally-hearing and hearing-impaired employees using the Amsterdam Checklist for Hearing and Work.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>45</volume>
<fpage>503</fpage>&#x02013;<lpage>512</lpage>. <pub-id pub-id-type="doi">10.1080/14992020600754583</pub-id><pub-id pub-id-type="pmid">17005493</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuchinsky</surname><given-names>S. E.</given-names></name><name><surname>Vaden</surname><given-names>K. I.</given-names><suffix>Jr.</suffix></name><name><surname>Ahlstrom</surname><given-names>J. B.</given-names></name><name><surname>Cute</surname><given-names>S. L.</given-names></name><name><surname>Humes</surname><given-names>L. E.</given-names></name><name><surname>Dubno</surname><given-names>J. R.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Task-related vigilance during word recognition in noise for older adults with hearing loss.</article-title>
<source><italic>Exp. Aging Res.</italic></source>
<volume>42</volume>
<fpage>64</fpage>&#x02013;<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1080/0361073X.2016.1108712</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laeng</surname><given-names>B.</given-names></name><name><surname>Sirois</surname><given-names>S.</given-names></name><name><surname>Gredeb&#x000e4;ck</surname><given-names>G.</given-names></name></person-group> (<year>2012</year>). <article-title>Pupillometry a window to the preconscious?</article-title>
<source><italic>Perspect. Psychol. Sci.</italic></source>
<volume>7</volume>
<fpage>18</fpage>&#x02013;<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1177/1745691611427305</pub-id><pub-id pub-id-type="pmid">26168419</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>F. R.</given-names></name><name><surname>Yaffe</surname><given-names>K.</given-names></name><name><surname>Xia</surname><given-names>J.</given-names></name><name><surname>Xue</surname><given-names>Q.-L.</given-names></name><name><surname>Harris</surname><given-names>T. B.</given-names></name><name><surname>Purchase-Helzner</surname><given-names>E.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Hearing loss and cognitive decline in older adults.</article-title>
<source><italic>JAMA Int. Med.</italic></source>
<volume>173</volume>
<fpage>293</fpage>&#x02013;<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1001/jamainternmed.2013.1868</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lunner</surname><given-names>T.</given-names></name></person-group> (<year>2003</year>). <article-title>Cognitive function in relation to hearing aid use.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>42</volume>:<issue>S49</issue>-S58. <pub-id pub-id-type="doi">10.3109/14992020309074624</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lunner</surname><given-names>T.</given-names></name><name><surname>Rudner</surname><given-names>M.</given-names></name><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <article-title>Cognition and hearing aids.</article-title>
<source><italic>Scand J Psychol.</italic></source>
<volume>50</volume>
<fpage>395</fpage>&#x02013;<lpage>403</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9450.2009.00742.x</pub-id><pub-id pub-id-type="pmid">19778387</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lunner</surname><given-names>T.</given-names></name><name><surname>Sundewall-Thor&#x000e9;n</surname><given-names>E.</given-names></name></person-group> (<year>2007</year>). <article-title>Interactions between cognition, compression, and listening conditions: effects on speech-in-noise performance in a two-channel hearing aid.</article-title>
<source><italic>J. Am. Acad. Audiol.</italic></source>
<volume>18</volume>
<fpage>604</fpage>&#x02013;<lpage>617</lpage>. <pub-id pub-id-type="doi">10.3766/jaaa.18.7.7</pub-id><pub-id pub-id-type="pmid">18236647</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackersie</surname><given-names>C. L.</given-names></name><name><surname>MacPhee</surname><given-names>I. X.</given-names></name><name><surname>Heldt</surname><given-names>E. W.</given-names></name></person-group> (<year>2015</year>). <article-title>Effects of hearing loss on heart rate variability and skin conductance measured during sentence recognition in noise.</article-title>
<source><italic>Ear. Hear.</italic></source>
<volume>36</volume>
<fpage>145</fpage>&#x02013;<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0000000000000091</pub-id><pub-id pub-id-type="pmid">25170782</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mason</surname><given-names>M. F.</given-names></name><name><surname>Norton</surname><given-names>M. I.</given-names></name><name><surname>Van Horn</surname><given-names>J. D.</given-names></name><name><surname>Wegner</surname><given-names>D. M.</given-names></name><name><surname>Grafton</surname><given-names>S. T.</given-names></name><name><surname>Macrae</surname><given-names>C. N.</given-names></name></person-group> (<year>2007</year>). <article-title>Wandering minds: the default network and stimulus-independent thought.</article-title>
<source><italic>Science</italic></source>
<volume>315</volume>
<fpage>393</fpage>&#x02013;<lpage>395</lpage>. <pub-id pub-id-type="doi">10.1126/science.1131295</pub-id><pub-id pub-id-type="pmid">17234951</pub-id></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCoy</surname><given-names>S. L.</given-names></name><name><surname>Tun</surname><given-names>P. A.</given-names></name><name><surname>Cox</surname><given-names>L. C.</given-names></name><name><surname>Colangelo</surname><given-names>M.</given-names></name><name><surname>Stewart</surname><given-names>R. A.</given-names></name><name><surname>Wingfield</surname><given-names>A.</given-names></name></person-group> (<year>2005</year>). <article-title>Hearing loss and perceptual effort: downstream effects on older adults&#x02019; memory for speech.</article-title>
<source><italic>Q. J. Exp. Psychol. A.</italic></source>
<volume>58</volume>
<fpage>22</fpage>&#x02013;<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1080/02724980443000151</pub-id><pub-id pub-id-type="pmid">15881289</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGarrigle</surname><given-names>R.</given-names></name><name><surname>Munro</surname><given-names>K. J.</given-names></name><name><surname>Dawes</surname><given-names>P.</given-names></name><name><surname>Stewart</surname><given-names>A. J.</given-names></name><name><surname>Moore</surname><given-names>D. R.</given-names></name><name><surname>Barry</surname><given-names>J. G.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Listening effort and fatigue: what exactly are we measuring? A british society of audiology cognition in hearing special interest group &#x02018;white paper&#x02019;.</article-title>
<source><italic>Int. J. Audiol.</italic></source>
<volume>53</volume>
<fpage>433</fpage>&#x02013;<lpage>440</lpage>. <pub-id pub-id-type="doi">10.3109/14992027.2014.890296</pub-id><pub-id pub-id-type="pmid">24673660</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J.</given-names></name><name><surname>Weisz</surname><given-names>N.</given-names></name></person-group> (<year>2012</year>). <article-title>Suppressed alpha oscillations predict intelligibility of speech and its acoustic details.</article-title>
<source><italic>Cereb. Cortex</italic></source>
<volume>22</volume>
<fpage>2466</fpage>&#x02013;<lpage>2477</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr325</pub-id><pub-id pub-id-type="pmid">22100354</pub-id></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J.</given-names></name><name><surname>W&#x000f6;stmann</surname><given-names>M.</given-names></name><name><surname>Hellbernd</surname><given-names>N.</given-names></name><name><surname>Wilsch</surname><given-names>A.</given-names></name><name><surname>Maess</surname><given-names>B.</given-names></name></person-group> (<year>2012</year>). <article-title>Adverse listening conditions and memory load drive a common alpha oscillatory network.</article-title>
<source><italic>J. Neurosci.</italic></source>
<volume>32</volume>
<fpage>12376</fpage>&#x02013;<lpage>12383</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4908-11.2012</pub-id><pub-id pub-id-type="pmid">22956828</pub-id></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R.</given-names></name><name><surname>Fries</surname><given-names>P.</given-names></name><name><surname>Maris</surname><given-names>E.</given-names></name><name><surname>Schoffelen</surname><given-names>J.-M.</given-names></name></person-group> (<year>2011</year>). <article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data.</article-title>
<source><italic>Comput. Intel. Neurosci.</italic></source>
<volume>2011</volume>:<issue>156869</issue>
<pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Partala</surname><given-names>T.</given-names></name><name><surname>Surakka</surname><given-names>V.</given-names></name></person-group> (<year>2003</year>). <article-title>Pupil size variation as an indication of affective processing.</article-title>
<source><italic>Int. J. Hum. Comput. Stud.</italic></source>
<volume>59</volume>
<fpage>185</fpage>&#x02013;<lpage>198</lpage>. <pub-id pub-id-type="doi">10.1016/S1071-5819(03)00017-X</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peavler</surname><given-names>W. S.</given-names></name></person-group> (<year>1974</year>). <article-title>Pupil size, information overload, and performance differences.</article-title>
<source><italic>Psychophysiology</italic></source>
<volume>11</volume>
<fpage>559</fpage>&#x02013;<lpage>566</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.1974.tb01114.x</pub-id><pub-id pub-id-type="pmid">4415394</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>E. B.</given-names></name><name><surname>W&#x000f6;stmann</surname><given-names>M.</given-names></name><name><surname>Obleser</surname><given-names>J.</given-names></name><name><surname>Stenfelt</surname><given-names>S.</given-names></name><name><surname>Lunner</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>Hearing loss impacts neural alpha oscillations under adverse listening conditions.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>6</volume>:<issue>177</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2015.00177</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>S. E.</given-names></name><name><surname>Posner</surname><given-names>M. I.</given-names></name></person-group> (<year>2012</year>). <article-title>The attention system of the human brain: 20 years after.</article-title>
<source><italic>Annu. Rev. Neurosci.</italic></source>
<volume>35</volume>:<issue>73</issue>
<pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150525</pub-id></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>M. K.</given-names></name><name><surname>Singh</surname><given-names>G.</given-names></name></person-group> (<year>2006</year>). <article-title>Effects of age on auditory and cognitive processing: implications for hearing aid fitting and audiologic rehabilitation.</article-title>
<source><italic>Trends Amplif.</italic></source>
<volume>10</volume>
<fpage>29</fpage>&#x02013;<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1177/108471380601000103</pub-id><pub-id pub-id-type="pmid">16528429</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>J. D.</given-names></name><name><surname>Petersen</surname><given-names>S. E.</given-names></name></person-group> (<year>2013</year>). <article-title>Control-related systems in the human brain.</article-title>
<source><italic>Curr. Opin. Neurobiol.</italic></source>
<volume>23</volume>
<fpage>223</fpage>&#x02013;<lpage>228</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2012.12.009</pub-id><pub-id pub-id-type="pmid">23347645</pub-id></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabbitt</surname><given-names>P.</given-names></name></person-group> (<year>1991</year>). <article-title>Mild hearing loss can cause apparent memory failures which increase with age and reduce with IQ.</article-title>
<source><italic>Acta Otolaryngol.</italic></source>
<volume>111</volume>
<fpage>167</fpage>&#x02013;<lpage>176</lpage>. <pub-id pub-id-type="doi">10.3109/00016489109127274</pub-id></mixed-citation></ref><ref id="B52"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Radulescu</surname><given-names>E.</given-names></name><name><surname>Nagai</surname><given-names>Y.</given-names></name><name><surname>Critchley</surname><given-names>H.</given-names></name></person-group> (<year>2014</year>). <article-title>&#x0201c;Mental Effort: brain and Autonomic Correlates,&#x0201d; in</article-title>
<source><italic>Handbook of Biobehavioral Approaches to Self-Regulation</italic>,</source>
<role>eds</role>
<person-group person-group-type="editor"><name><surname>Gendolla</surname><given-names>G. H. E.</given-names></name><name><surname>Tops</surname><given-names>M</given-names></name><name><surname>Koole</surname><given-names>S. L.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>237</fpage>&#x02013;<lpage>253</lpage>.</mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name><name><surname>Lunner</surname><given-names>T.</given-names></name><name><surname>Zekveld</surname><given-names>A.</given-names></name><name><surname>S&#x000f6;rqvist</surname><given-names>P.</given-names></name><name><surname>Danielsson</surname><given-names>H.</given-names></name><name><surname>Lyxell</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>The Ease of Language Understanding (ELU) model: theoretical, empirical, and clinical advances.</article-title>
<source><italic>Front. Syst. Neurosci.</italic></source>
<volume>7</volume>:<issue>31</issue>
<pub-id pub-id-type="doi">10.3389/fnsys.2013.00031.</pub-id></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name><name><surname>Rudner</surname><given-names>M.</given-names></name><name><surname>Lunner</surname><given-names>T.</given-names></name><name><surname>Zekveld</surname><given-names>A. A.</given-names></name></person-group> (<year>2010</year>). <article-title>When cognition kicks in: working memory and speech understanding in noise.</article-title>
<source><italic>Noise Health</italic></source>
<volume>12</volume>
<fpage>263</fpage>&#x02013;<lpage>269</lpage>. <pub-id pub-id-type="doi">10.4103/1463-1741.70505</pub-id><pub-id pub-id-type="pmid">20871181</pub-id></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudner</surname><given-names>M.</given-names></name><name><surname>Lunner</surname><given-names>T.</given-names></name><name><surname>Behrens</surname><given-names>T.</given-names></name><name><surname>Thor&#x000e9;n</surname><given-names>E. S.</given-names></name><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>Working memory capacity may influence perceived effort during aided speech recognition in noise.</article-title>
<source><italic>J. Am. Acad. Audiol.</italic></source>
<volume>23</volume>
<fpage>577</fpage>&#x02013;<lpage>589</lpage>. <pub-id pub-id-type="doi">10.3766/jaaa.23.7.7</pub-id><pub-id pub-id-type="pmid">22967733</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salthouse</surname><given-names>T. A.</given-names></name></person-group> (<year>2004</year>). <article-title>What and when of cognitive aging.</article-title>
<source><italic>Curr. Dir. Psychol.</italic></source>
<volume>13</volume>
<fpage>140</fpage>&#x02013;<lpage>144</lpage>. <pub-id pub-id-type="doi">10.1111/j.0963-7214.2004.00293.x</pub-id></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sara</surname><given-names>S. J.</given-names></name></person-group> (<year>2009</year>). <article-title>The locus coeruleus and noradrenergic modulation of cognition.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>10</volume>
<fpage>211</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2573</pub-id><pub-id pub-id-type="pmid">19190638</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sara</surname><given-names>S. J.</given-names></name><name><surname>Bouret</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>Orienting and reorienting: the locus coeruleus mediates cognition through arousal.</article-title>
<source><italic>Neuron</italic></source>
<volume>76</volume>
<fpage>130</fpage>&#x02013;<lpage>141</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.09.011</pub-id><pub-id pub-id-type="pmid">23040811</pub-id></mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarter</surname><given-names>M.</given-names></name><name><surname>Gehring</surname><given-names>W. J.</given-names></name><name><surname>Kozak</surname><given-names>R.</given-names></name></person-group> (<year>2006</year>). <article-title>More attention must be paid: the neurobiology of attentional effort.</article-title>
<source><italic>Brain Res. Rev.</italic></source>
<volume>51</volume>
<fpage>145</fpage>&#x02013;<lpage>160</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainresrev.2005.11.002</pub-id><pub-id pub-id-type="pmid">16530842</pub-id></mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schluroff</surname><given-names>M.</given-names></name></person-group> (<year>1982</year>). <article-title>Pupil responses to grammatical complexity of sentences.</article-title>
<source><italic>Brain Lang.</italic></source>
<volume>17</volume>
<fpage>133</fpage>&#x02013;<lpage>145</lpage>. <pub-id pub-id-type="doi">10.1016/0093-934X(82)90010-4</pub-id><pub-id pub-id-type="pmid">7139265</pub-id></mixed-citation></ref><ref id="B61"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>B.</given-names></name><name><surname>Pichora-Fuller</surname><given-names>K.</given-names></name><name><surname>Daneman</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>&#x0201c;Effects of senescent changes in audition and cognition on spoken language comprehension,&#x0201d; in</article-title>
<source><italic>The Aging Auditory System</italic>,</source>
<role>eds</role>
<person-group person-group-type="editor"><name><surname>Gordon-Salant</surname><given-names>S.</given-names></name><name><surname>Frisina</surname><given-names>R. D.</given-names></name><name><surname>Fay</surname><given-names>R. R.</given-names></name><name><surname>Popper</surname><given-names>A. N.</given-names></name></person-group> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>167</fpage>&#x02013;<lpage>210</lpage>.</mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeley</surname><given-names>W. W.</given-names></name><name><surname>Menon</surname><given-names>V.</given-names></name><name><surname>Schatzberg</surname><given-names>A. F.</given-names></name><name><surname>Keller</surname><given-names>J.</given-names></name><name><surname>Glover</surname><given-names>G. H.</given-names></name><name><surname>Kenna</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2007</year>). <article-title>Dissociable intrinsic connectivity networks for salience processing and executive control.</article-title>
<source><italic>J. Neurosci.</italic></source>
<volume>27</volume>
<fpage>2349</fpage>&#x02013;<lpage>2356</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.5587-06.2007</pub-id><pub-id pub-id-type="pmid">17329432</pub-id></mixed-citation></ref><ref id="B63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>R. V.</given-names></name><name><surname>Zeng</surname><given-names>F.-G.</given-names></name><name><surname>Kamath</surname><given-names>V.</given-names></name><name><surname>Wygonski</surname><given-names>J.</given-names></name><name><surname>Ekelid</surname><given-names>M.</given-names></name></person-group> (<year>1995</year>). <article-title>Speech recognition with primarily temporal cues.</article-title>
<source><italic>Science</italic></source>
<volume>270</volume>
<fpage>303</fpage>&#x02013;<lpage>304</lpage>. <pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spagna</surname><given-names>A.</given-names></name><name><surname>Mackie</surname><given-names>M.-A.</given-names></name><name><surname>Fan</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Supramodal executive control of attention.</article-title>
<source><italic>Front. Psychol.</italic></source>
<volume>6</volume>:<issue>65</issue>
<pub-id pub-id-type="doi">10.3389/fpsyg.2015.00065</pub-id></mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stenfelt</surname><given-names>S.</given-names></name><name><surname>R&#x000f6;nnberg</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <article-title>The Signal-Cognition interface: interactions between degraded auditory signals and cognitive processes.</article-title>
<source><italic>Scand. J. Psychol.</italic></source>
<volume>50</volume>
<fpage>385</fpage>&#x02013;<lpage>393</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9450.2009.00748.x</pub-id><pub-id pub-id-type="pmid">19778386</pub-id></mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strau&#x000df;</surname><given-names>A.</given-names></name><name><surname>W&#x000f6;stmann</surname><given-names>M.</given-names></name><name><surname>Obleser</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Cortical alpha oscillations as a tool for auditory selective inhibition.</article-title>
<source><italic>Front. Hum. Neurosci.</italic></source>
<volume>8</volume>:<issue>350</issue>
<pub-id pub-id-type="doi">10.3389/fnhum.2014.00350.</pub-id></mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomporowski</surname><given-names>P. D.</given-names></name><name><surname>Tinsley</surname><given-names>V. F.</given-names></name></person-group> (<year>1996</year>). <article-title>Effects of memory demand and motivation on sustained attention in young and older adults.</article-title>
<source><italic>Am. J. Psychol.</italic></source>
<volume>109</volume>
<fpage>187</fpage>&#x02013;<lpage>204</lpage>. <pub-id pub-id-type="doi">10.2307/1423272</pub-id><pub-id pub-id-type="pmid">8644884</pub-id></mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullsperger</surname><given-names>M.</given-names></name><name><surname>Harsay</surname><given-names>H. A.</given-names></name><name><surname>Wessel</surname><given-names>J. R.</given-names></name><name><surname>Ridderinkhof</surname><given-names>K. R.</given-names></name></person-group> (<year>2010</year>). <article-title>Conscious perception of errors and its relation to the anterior insula.</article-title>
<source><italic>Brain Struct. Funct.</italic></source>
<volume>214</volume>
<fpage>629</fpage>&#x02013;<lpage>643</lpage>. <pub-id pub-id-type="doi">10.1007/s00429-010-0261-1</pub-id><pub-id pub-id-type="pmid">20512371</pub-id></mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentino</surname><given-names>R. J.</given-names></name><name><surname>Van Bockstaele</surname><given-names>E.</given-names></name></person-group> (<year>2008</year>). <article-title>Convergent regulation of locus coeruleus activity as an adaptive response to stress.</article-title>
<source><italic>Eur. J. Pharmacol.</italic></source>
<volume>583</volume>
<fpage>194</fpage>&#x02013;<lpage>203</lpage>. <pub-id pub-id-type="doi">10.1016/j.ejphar.2007.11.062</pub-id><pub-id pub-id-type="pmid">18255055</pub-id></mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vossel</surname><given-names>S.</given-names></name><name><surname>Geng</surname><given-names>J. J.</given-names></name><name><surname>Fink</surname><given-names>G. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Dorsal and ventral attention systems distinct neural circuits but collaborative roles.</article-title>
<source><italic>Neuroscientist</italic></source>
<volume>20</volume>
<fpage>150</fpage>&#x02013;<lpage>159</lpage>. <pub-id pub-id-type="doi">10.1177/1073858413494269</pub-id><pub-id pub-id-type="pmid">23835449</pub-id></mixed-citation></ref><ref id="B71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warm</surname><given-names>J. S.</given-names></name><name><surname>Parasuraman</surname><given-names>R.</given-names></name><name><surname>Matthews</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Vigilance requires hard mental work and is stressful.</article-title>
<source><italic>Hum. Factors</italic></source>
<volume>50</volume>
<fpage>433</fpage>&#x02013;<lpage>441</lpage>. <pub-id pub-id-type="doi">10.1518/001872008X312152</pub-id><pub-id pub-id-type="pmid">18689050</pub-id></mixed-citation></ref><ref id="B72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinstein</surname><given-names>B. E.</given-names></name><name><surname>Ventry</surname><given-names>I. M.</given-names></name></person-group> (<year>1982</year>). <article-title>Hearing impairment and social isolation in the elderly.</article-title>
<source><italic>J. Speech Lang. Hear. Res.</italic></source>
<volume>25</volume>
<fpage>593</fpage>&#x02013;<lpage>599</lpage>. <pub-id pub-id-type="doi">10.1044/jshr.2504.593</pub-id></mixed-citation></ref><ref id="B73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wingfield</surname><given-names>A.</given-names></name><name><surname>Tun</surname><given-names>P. A.</given-names></name><name><surname>McCoy</surname><given-names>S. L.</given-names></name></person-group> (<year>2005</year>). <article-title>Hearing loss in older adulthood what it is and how it interacts with cognitive performance.</article-title>
<source><italic>Curr. Dir. Psychol.</italic></source>
<volume>14</volume>
<fpage>144</fpage>&#x02013;<lpage>148</lpage>. <pub-id pub-id-type="doi">10.1111/j.0963-7214.2005.00356.x</pub-id></mixed-citation></ref><ref id="B74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winn</surname><given-names>M. B.</given-names></name><name><surname>Edwards</surname><given-names>J. R.</given-names></name><name><surname>Litovsky</surname><given-names>R. Y.</given-names></name></person-group> (<year>2015</year>). <article-title>The impact of auditory spectral resolution on listening effort revealed by pupil dilation.</article-title>
<source><italic>Ear. Hear.</italic></source>
<volume>36</volume>
<fpage>153</fpage>&#x02013;<lpage>165</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0000000000000145</pub-id></mixed-citation></ref><ref id="B75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>W&#x000f6;stmann</surname><given-names>M.</given-names></name><name><surname>Herrmann</surname><given-names>B.</given-names></name><name><surname>Wilsch</surname><given-names>A.</given-names></name><name><surname>Obleser</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Neural alpha dynamics in younger and older listeners reflect acoustic challenges and predictive benefits.</article-title>
<source><italic>J. Neurosci.</italic></source>
<volume>35</volume>
<fpage>1458</fpage>&#x02013;<lpage>1467</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3250-14.2015</pub-id><pub-id pub-id-type="pmid">25632123</pub-id></mixed-citation></ref><ref id="B76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>A. A.</given-names></name><name><surname>Kramer</surname><given-names>S. E.</given-names></name></person-group> (<year>2014</year>). <article-title>Cognitive processing load across a wide range of listening conditions: insights from pupillometry.</article-title>
<source><italic>Psychophysiology</italic></source>
<volume>51</volume>
<fpage>277</fpage>&#x02013;<lpage>284</lpage>. <pub-id pub-id-type="doi">10.1111/psyp.12151</pub-id><pub-id pub-id-type="pmid">24506437</pub-id></mixed-citation></ref><ref id="B77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>A. A.</given-names></name><name><surname>Kramer</surname><given-names>S. E.</given-names></name><name><surname>Festen</surname><given-names>J. M.</given-names></name></person-group> (<year>2010</year>). <article-title>Pupil response as an indication of effortful listening: the influence of sentence intelligibility.</article-title>
<source><italic>Ear. Hear.</italic></source>
<volume>31</volume>
<fpage>480</fpage>&#x02013;<lpage>490</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e3181d4f251</pub-id><pub-id pub-id-type="pmid">20588118</pub-id></mixed-citation></ref><ref id="B78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>A. A.</given-names></name><name><surname>Kramer</surname><given-names>S. E.</given-names></name><name><surname>Festen</surname><given-names>J. M.</given-names></name></person-group> (<year>2011</year>). <article-title>Cognitive load during speech perception in noise: the influence of age, hearing loss, and cognition on the pupil response.</article-title>
<source><italic>Ear. Hear.</italic></source>
<volume>32</volume>
<fpage>498</fpage>&#x02013;<lpage>510</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e31820512bb</pub-id><pub-id pub-id-type="pmid">21233711</pub-id></mixed-citation></ref><ref id="B79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zumer</surname><given-names>J. M.</given-names></name><name><surname>Scheeringa</surname><given-names>R.</given-names></name><name><surname>Schoffelen</surname><given-names>J.-M.</given-names></name><name><surname>Norris</surname><given-names>D. G.</given-names></name><name><surname>Jensen</surname><given-names>O.</given-names></name></person-group> (<year>2014</year>). <article-title>Occipital alpha activity during stimulus processing gates the information flow to object-selective cortex.</article-title>
<source><italic>PLoS Biol.</italic></source>
<volume>12</volume>:<issue>e1001965</issue>
<pub-id pub-id-type="doi">10.1371/journal.pbio.1001965</pub-id></mixed-citation></ref></ref-list></back></article>
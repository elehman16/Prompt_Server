<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Transl Vis Sci Technol</journal-id><journal-id journal-id-type="iso-abbrev">Transl Vis Sci Technol</journal-id><journal-id journal-id-type="hwp">tvst</journal-id><journal-id journal-id-type="pmc">tvst</journal-id><journal-id journal-id-type="publisher-id">TVST</journal-id><journal-title-group><journal-title>Translational Vision Science &#x00026; Technology</journal-title></journal-title-group><issn pub-type="epub">2164-2591</issn><publisher><publisher-name>The Association for Research in Vision and Ophthalmology</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28573074</article-id><article-id pub-id-type="pmc">5450922</article-id><article-id pub-id-type="doi">10.1167/tvst.6.3.10</article-id><article-id pub-id-type="sici">tvst-06-03-10</article-id><article-id pub-id-type="publisher-id">TVST-16-0357</article-id><article-categories><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group></article-categories><title-group><article-title>Living with Ultra-Low Vision: An Inventory of Self-Reported Visually Guided Activities by Individuals with Profound Visual Impairment</article-title><alt-title alt-title-type="runhead">Adeyemo et al.</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Adeyemo</surname><given-names>Olukemi</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Jeter</surname><given-names>Pamela E.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Rozanski</surname><given-names>Collin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Arnold</surname><given-names>Ellen</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author"><name><surname>Dalvin</surname><given-names>Lauren A.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Swenor</surname><given-names>Bonnielin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Dagnelie</surname><given-names>Gislin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><collab>PLoVR Study Group</collab><xref ref-type="author-notes" rid="n101">*</xref></contrib><aff id="aff1"><label>1</label>Lions Vision Center, Wilmer Eye Institute, Johns Hopkins University School of Medicine, Baltimore, MD, USA
</aff><aff id="aff2"><label>2</label>Department of Ophthalmology, Mayo Clinic, Rochester, MN, USA
</aff><aff id="aff3"><label>3</label>Katzen Eye Group, Baltimore, MD, USA
</aff></contrib-group><author-notes><corresp id="cor1"><bold>Correspondence:</bold> Olukemi Adeyemo, Wilmer Eye Institute, Johns Hopkins School of Medicine, Woods 355, 600 N. Wolfe St., Baltimore, MD 21287, USA. e-mail: <email>oadeyem2@jhmi.edu</email></corresp><fn id="n101" fn-type="con"><label>*</label><p>PLoVR Study Group: Judy Goldstein, Jim Deremeik, Duane Geruschat, Olukemi Adeyemo, Am&#x000e9;lie-Fran&#x000e7;oise Nkodo, Pamela E. Jeter, Robert Massof, and Gislin Dagnelie</p></fn></author-notes><pub-date pub-type="epub"><day>31</day><month>5</month><year>2017</year></pub-date><pub-date pub-type="collection"><month>5</month><year>2017</year></pub-date><volume>6</volume><issue>3</issue><elocation-id>10</elocation-id><history><date date-type="received"><day>21</day><month>3</month><year>2016</year></date><date date-type="accepted"><day>12</day><month>4</month><year>2017</year></date></history><permissions><copyright-statement>Copyright 2017 The Authors</copyright-statement><copyright-year>2017</copyright-year><license license-type="cc-by-nc-nd" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><license-p>This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.</license-p></license></permissions><self-uri xlink:title="pdf" xlink:href="i2164-2591-6-3-10.pdf"/><abstract><sec id="st1"><title>Purpose</title><p>To understand how individuals with profound visual impairment (ultra-low vision, ULV) use their remaining vision.</p></sec><sec id="st2"><title>Methods</title><p>Forty-six participants with ULV (visual acuity &#x02264; 200/500 in the better seeing eye) were divided into nine focus groups (4&#x02013;6 individuals per group) and met either in person (<italic>n</italic> = 2) or over the phone (<italic>n</italic> = 7). Discussions were guided by the Massof Activity Inventory. Audio recordings were transcribed and analyzed for visual activities that were then classified along two visual categorizations &#x02013; functional domains and visual aspects. The latter was based on a Grounded Theory classification of participants' descriptions.</p></sec><sec id="st3"><title>Results</title><p>Seven hundred sixty activities were reported. By functional domain they were classified as reading/shape recognition (10%), mobility (17%), visual motor (24%), and visual information gathering (49%). By visual aspects, they were classified as contrast (43%), luminance (17%), environmental lighting (9%), familiarity (3%), motion perception (5%), distance (7%), size (9%), eccentricity (5%), depth perception (1%), and other/miscellaneous (1%). More than one visual aspect may be critical for an activity: participants reported that contrast plays a role in 68% of visual activities, followed by luminance (27%), environmental lighting (14%), and size (14%).</p></sec><sec id="st4"><title>Conclusions</title><p>Visual aspects, primarily contrast, were found to be critical factors enabling ULV individuals to perform visual activities.</p></sec><sec id="st5"><title>Translational Relevance</title><p>This inventory, part of the Prosthetic Low Vision Rehabilitation (PLoVR) curriculum development study, provides a unique perspective into the visual world of the nearly blind, and can be used in the development of a Visual Functioning Questionnaire (VFQ) and visual performance measures suited for ULV populations.</p></sec></abstract><kwd-group><title>Keywords</title><kwd>low vision rehabilitation</kwd><kwd>patient-reported outcomes</kwd><kwd>ultra-low vision</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Visual impairment is defined functionally as the inability to perform customary visual activities, and depends on the visual demands one encounters in daily life.<sup><xref rid="i2164-2591-6-3-10-b01" ref-type="bibr">1</xref></sup> Chronic impairment of vision (i.e., low vision or blindness) is one of the leading causes of disability in the United States.<sup><xref rid="i2164-2591-6-3-10-b02" ref-type="bibr">2</xref></sup> Visual acuity (VA) in low vision ranges from near-normal vision to near blindness (i.e., bare light perception [BLP]).<sup><xref rid="i2164-2591-6-3-10-b03" ref-type="bibr">3</xref>,<xref rid="i2164-2591-6-3-10-b04" ref-type="bibr">4</xref></sup></p><p>The term ultra-low vision (ULV) was coined to describe visual impairment that limits the individual to minimal functional vision, especially in activities of daily living that involve visual shape recognition.<sup><xref rid="i2164-2591-6-3-10-b05" ref-type="bibr">5</xref></sup> In a broad sense, this level of vision thus corresponds with the profound low vision (VA 20/500&#x02013;20/1000) and near total blindness (&#x0003c;20/1000&#x02013;bare LP) categories of visual impairment in the International Classification of Diseases, Ninth Revision, Clinical Modification (ICD-9-CM) system. If absence of form vision is used as a strict criterion, ULV corresponds with VA less than 20/1600 or 6/480,<sup><xref rid="i2164-2591-6-3-10-b05" ref-type="bibr">5</xref></sup> the inability to detect the largest letter on the Early Treatment of Diabetic Retinopathy Study (ETDRS) chart at 0.5 m distance.<sup><xref rid="i2164-2591-6-3-10-b06" ref-type="bibr">6</xref></sup> Therefore, ULV differs from less severe forms of visual impairment. In this group of individuals, vision is very limited and often unreliable.<sup><xref rid="i2164-2591-6-3-10-b05" ref-type="bibr">5</xref></sup> Individuals with ULV rely more on nonvisual skills (also called &#x0201c;blindness skills&#x0201d;), (i.e., the use of other senses as an adjunct to vision) and on vision substitution devices.<sup><xref rid="i2164-2591-6-3-10-b01" ref-type="bibr">1</xref></sup> Some perform very limited shape recognition (spot reading) with low vision aids, and most have difficulty with orientation and mobility in unfamiliar locations, unless they use a cane or guide dog.<sup><xref rid="i2164-2591-6-3-10-b07" ref-type="bibr">7</xref></sup> Notably, there is a preference to use blindness skills in most situations.<sup><xref rid="i2164-2591-6-3-10-b08" ref-type="bibr">8</xref></sup> This may be due to the task or activity being too visually demanding to be enjoyable, vision being too unreliable, or the task being accomplished faster without vision.<sup><xref rid="i2164-2591-6-3-10-b01" ref-type="bibr">1</xref>,<xref rid="i2164-2591-6-3-10-b08" ref-type="bibr">8</xref></sup> The true range of activities individuals with ULV can perform, aided by their remaining vision, has not been explored systematically. We therefore aimed to develop an understanding of ULV based on the experiences of individuals in this population.</p><p>Gaining a better understanding of ULV is of more than academic interest. As advances have been made in vision restoration therapies such as the commercially available Food and Drug Administration (FDA)-approved Argus II retinal implant, and with the advent of clinical trials in gene and stem cell therapy, individuals with ULV have been among the first to volunteer for such new treatments, and will continue to fulfill this role in phase 1 trials for the foreseeable future. Yet, the benefits of phase 1 trials for these novel therapies are by no means guaranteed, and even if they materialize they may not raise the recipients' vision out of the ULV range. Therefore, to measure potential benefits of such trials, it will be critically important to understand and assess ULV, and to develop tools for rehabilitation in the ULV range. This was our principal reason for seeking funding from the National Eye Institute (NEI), in 2010, to develop a Prosthetic Low Vision Rehabilitation (PLoVR) curriculum, which has supported the work presented here.</p><p>In the case of less severe visual impairment, the physical aspect of vision is assessed through traditional visual function measures such as visual acuity, contrast sensitivity, visual field maps, reading speed, and so on, while the functional aspect can be assessed through visual functioning questionnaires (VFQs) or performance of standardized activities. Typically, such performance measures distinguish four functional domains: reading, mobility, visual information gathering, and visual motor activities. In the case of the ULV population, the content area remains largely undefined; this prompted us to examine the visual activities of the ULV population, in preparation for the development of targeted assessment instruments and rehabilitation tools.</p><p>Using VFQs to measure visual outcomes is a crucial component of clinical trials, especially because the participant's vision related quality of life (VRQoL) is being given an increasingly important role in the assessment of treatment benefit. VRQoL also plays an increasingly important role in visual rehabilitation. It describes how well a person functions in vision-related activities<sup><xref rid="i2164-2591-6-3-10-b06" ref-type="bibr">6</xref></sup> and comprises four main areas &#x02013; physical, functional, social, and psychological.<sup><xref rid="i2164-2591-6-3-10-b07" ref-type="bibr">7</xref></sup></p><p>Most of the currently available VFQs probe the individual's visual ability to perform tasks that require at least form vision, such as reading, face recognition, and fine hand&#x02013;eye coordination.<sup><xref rid="i2164-2591-6-3-10-b08" ref-type="bibr">8</xref></sup> Thus, these VFQs are not suitable for the ULV group. Even patient-reported outcomes (PRO) instruments specifically designed for individuals with severely reduced vision such as the Impact of Vision Impairment-Very Low Vision (IVI-VLV) questionnaire<sup><xref rid="i2164-2591-6-3-10-b09" ref-type="bibr">9</xref></sup> contain many items that require form vision, and therefore target a broader range of vision than strict ULV.</p><p>According to Pesudovs et al,<sup><xref rid="i2164-2591-6-3-10-b10" ref-type="bibr">10</xref></sup> phases in the development of a VFQ involve (1) specifying a prestudy aim and intended population to be studied, (2) determining the extent to which the instrument has been studied in the intended population, and (3) determining to what degree the content is relevant to the intended population. The current study identifies the intended population as individuals with ULV. In this case, however, one cannot determine whether the content of any VFQ is relevant to the ULV population, because it is unknown how they use their vision. Thus, the sequence sketched by Pesudovs is turned on its head, and the first step is an inventory of visual activities by conducting focus groups to determine ULV-relevant content for the development of an instrument.</p><p>Covering all facets in the daily lives of ULV individuals with the purpose of developing a complete set of visual activities may not be possible, but a systematic approach called the Activity Breakdown Structure (ABS) developed in rehabilitation medicine has previously been applied to low vision. The ABS divides the landscape of day-to-day activities into objectives, goals, and tasks/activities, and charts in a systematic manner the full range of activities a person undertakes in daily life. As a framework for our discussions within these groups we used the ABS as implemented in the Massof Activity Inventory (AI).<sup><xref rid="i2164-2591-6-3-10-b11" ref-type="bibr">11</xref>,<xref rid="i2164-2591-6-3-10-b12" ref-type="bibr">12</xref></sup></p><p>In recent years, Grounded Theory<sup><xref rid="i2164-2591-6-3-10-b13" ref-type="bibr">13</xref>,<xref rid="i2164-2591-6-3-10-b14" ref-type="bibr">14</xref></sup> has emerged as a powerful tool in exploratory research, assisting investigators in organizing their findings and developing hypotheses about underlying structures and frameworks. Grounded Theory groups findings into an initial classification and then systematically reorganizes these groups until a classification of more or less independent categories emerges; this appeared to be an appropriate approach in the classification of the experiences of individuals with ULV.</p><p>In the case of ULV, this approach seemed particularly appropriate, because there has been no detailed inventory of the way individuals with rudimentary vision use their remaining sight. Powerful first-person accounts, such as that by Mike May,<sup><xref rid="i2164-2591-6-3-10-b15" ref-type="bibr">15</xref></sup> have given us descriptions for the experiences of a few individuals, but it is not clear how generalizable such accounts are. For example, the case of Mike May is unique because he had his vision restored after many years of functional blindness, whereas most ULV occurs after a slow process of vision loss or as a congenital condition.</p><p>The present paper uses the AI to guide the questioning of our focus groups to gain insight into the visual activities of the ULV population, as no prior data exist in this group. We first characterize the focus group findings in the context of traditional vision functional domains. We then identify to what extent the content is relevant to the intended population by defining a new visual classification specific to the ULV population and evaluating its relationship to the traditional functional visual domains.</p></sec><sec sec-type="methods" id="s2"><title>Methods</title><sec id="s2a"><title>Participants</title><p>Focus group recruitment and data collection were conducted between February and August 2012. All ultra-low vision participants enrolled in the focus groups were 14 years and older and met our inclusion criteria for having native (as opposed to prosthetic or restored) best-corrected vision in the hand-motions/light perception range or a visual prosthesis that provides some level of functional vision following functional blindness. A small number of participants (&#x0003c;10) had visual acuities in the 20/500 to count fingers range; thus, approximately 80% of the study participants did not have any form perception, and their vision ranged from movement perception to light projection or BLP. Our search strategy recruited ULV individuals across the United States to participate by phone, so we were limited to using medical records only to determine eligibility. This explains why some participants did not meet the strict ULV criterion: eye care providers frequently underestimate how much a person with profound vision loss can still see. The inclusion of a few individuals with slightly better vision had the advantage, however, that some of the more demanding activities can be compared with items in existing self-report instruments such as the IVI-VLV. The eligibility criteria were also kept broad to capture a wide range of participants and the greatest possible diversity of activities. Participants were recruited through the Lions Low Vision Service of the Johns Hopkins Wilmer Eye Institute, organizations for the visually impaired, such as The Foundation Fighting Blindness (FFB) and National Federation of the Blind (NFB), other low vision clinics, mailing lists, listservs, and word of mouth. A small number of Argus II users was purposefully included, even though one may hypothesize that they might use their vision differently from individuals with native ULV. We think their inclusion was justified for two reasons: (1) they had experience with native ULV from the time before they lost all useful vision and qualified to receive a retinal implant, and (2) the quality of vision with the Argus II may differ from native ULV. However, because this study aims to garner the broadest possible inventory of ULV the distinction between native and artificial ULV was of no immediate concern. We reason that only a well-developed VFQ and data analysis examining differential item functioning can answer the question whether there is a meaningful functional difference between these forms of ULV. For this reason, Argus II users were encouraged to report on visual experiences from their previous sight with end-stage retinitis pigmentosa as well as from their Argus II use. Participants had no cognitive or physical limitations affecting their full participation in the focus group sessions. The study protocol was reviewed and approved by the Johns Hopkins institutional review board. All participants provided written informed consent in accordance to the Declaration of Helsinki, after explanation of the nature and possible consequences of study.</p></sec><sec id="s2b"><title>Focus Groups</title><p>Participants enrolled in the study were spread out across 23 states and the District of Columbia. They were divided into nine focus groups with four to six individuals per group. The groups met in person (local, <italic>n</italic> = 2) or over the phone via conference call (remote, <italic>n</italic> = 7). Local focus groups met in Arlington, VA and Baltimore, MD. Each focus group met four to seven times. Every session was recorded. Session frequency was determined by group members' availability, and duration of each session depended on the involvement group members in the goals and activities of the Massof AI<sup><xref rid="i2164-2591-6-3-10-b11" ref-type="bibr">11</xref>,<xref rid="i2164-2591-6-3-10-b12" ref-type="bibr">12</xref></sup> and varied from 60 to 90 minutes. Each session was moderated by the same research team members (PJ, GD, EA) for consistency. Moderators were trained in the conduct of qualitative research and adhered to the general guidelines of conducting focus group sessions.<sup><xref rid="i2164-2591-6-3-10-b16" ref-type="bibr">16</xref>,<xref rid="i2164-2591-6-3-10-b17" ref-type="bibr">17</xref></sup> Moderators helped facilitate the discussions, encouraged participation by each group member, and ensured equal opportunity for contribution from each participant.</p><p>The AI groups the universe of daily activities into goals such as entertaining guests (i.e., the reason why a visual task is performed) and does so for all activities of daily living. This framework was used to guide the discussion, assuring that all representative facets of life were explored for activities requiring even a small amount of vision. Every session covered multiple goals, beginning each one with a few open-ended questions. All 50 goals in the AI were covered in this fashion. Sample goals discussed include daily meal preparation, dining out, dressing, childcare, entertainment, computer use, and so on. Thus, using the AI as a guide, the participants were able to discuss all different areas of their daily lives and report on activities that required the use of their rudimentary vision.</p></sec><sec id="s2c"><title>Data Extraction, Item Categorization, and Data Analysis</title><p>The focus group sessions generated a total of 73 hours of audio recordings. Transcripts from recorded sessions generated 760 activities in which participants reported to benefit from their remaining vision.</p><p>Four team members transcribed the audio recordings. Transcribed text was collated for each group and for each goal in the AI. To ensure a consistent set of categorizations, every activity identified in the transcripts was then rated independently by two team members. The study team members went over the activities and looked for possible classifications, based on established categorizations as well as the comments provided during the focus group sessions. The established categorization divided the items into four traditional functional domains: reading (or more generally, shape recognition), mobility, visual motor, and visual information gathering. The second categorization was a reflection of what focus group described as most relevant to their ability to interpret the visual scene. Each activity was assigned visual aspects that were critical for the participant's ability to achieve the task, as described in the focus group transcripts. For example, if a participant was able to discern an object only when the foreground was in high contrast with the background (e.g., black coffee in a white mug), the item was categorized under the visual aspect &#x02018;contrast'. Other aspects voiced by the participants were luminance, lighting, familiarity, movement, distance, size, eccentricity, depth, and other/miscellaneous. Categorizations for each item were finalized after consensus by all four raters. Disagreements by the raters on activities were rare (&#x0003c;5%) and were resolved by discussion and consensus among the entire research team.</p><p>Each activity was assigned to a single functional domain (i.e., reading, visual information, visual motor, or mobility). Subsequently, in a meeting of the entire research team, approximately 36 representative activities were discussed, using the focus group participants' characterization of the conditions that would allow them to perform these activities. This yielded an initial classification of properties for which the term &#x0201c;visual aspects&#x0201d; was coined &#x02013; approximately 12 descriptors of what made activities manageable to individuals with ULV. Once this set of visual aspects had been defined, each of the 760 activities was assigned one or more aspects listed as important by participants. If more than one aspect was listed for an activity, the one mentioned most frequently in the focus group discussion was designated the primary visual aspect of that activity.</p><p>While the intent of this work was to qualitatively describe how those with ULV use their remaining vision, post-hoc analyses used &#x003c7;<sup>2</sup> tests to compare the proportions of each aspect across functional domains. These analyses tested if there were significant differences across domains, even with this small sample size. These statistical analyses were performed using Stata Statistical Software, release 14.1 (StataCorp LP, College Station, TX).</p></sec></sec><sec id="s3"><title>Results</title><sec id="s3a"><title>Participant Characteristics</title><p>Our study population was comprised of 46 participants with ultra-low vision, 48% were female and 52% male. Unique in our population, was the participation of six retinal implant wearers with vision in the range of ULV. The average age of the total sample was 59 &#x000b1; 18-year-old (mean &#x000b1; SD; range, 14&#x02013;103 years). <xref ref-type="table" rid="i2164-2591-6-3-10-t01">Table 1</xref> shows a detailed breakdown of participant demographics.</p><table-wrap id="i2164-2591-6-3-10-t01" content-type="1col" orientation="portrait" position="float"><label>Table 1</label><caption><p>Participants' Demographics as Mean (&#x000b1;SD) or <italic>n</italic> (%)</p></caption><graphic xlink:href="i2164-2591-6-3-10-t01"/></table-wrap></sec><sec id="s3b"><title>Sample Data Collected</title><p><xref ref-type="table" rid="i2164-2591-6-3-10-t02">Table 2</xref> shows examples of the activities our focus group participants reported they could still do with their vision, in their own words, and the enabling factors that helped them complete these tasks. Each example lists the AI goal and task, the focus group participant's quote, the functional domain, and the visual aspect(s). As noted above, more than one visual aspect can be important in performing a given activity. It demonstrates the interrelationship between goals, tasks, activities, functional domains, and visual aspects. One can appreciate how each functional domain encompasses many activities, each of which can be governed by more than one visual aspect. A complete list of activities discussed and their classification along the dimensions of visual domains and aspects can be found on-line as <xref ref-type="supplementary-material" rid="tvst-06-03-10_s01">Supplementary Material</xref>.</p><table-wrap id="i2164-2591-6-3-10-t02" content-type="2col" orientation="portrait" position="float"><label>Table 2</label><caption><p>Categorization of Sample Activities, as Reported by Participants</p></caption><graphic xlink:href="i2164-2591-6-3-10-t02"/></table-wrap></sec><sec id="s3c"><title>Categorization by Functional Domain</title><p>The 760 activities generated by our focus groups were categorized under the four functional domains (<xref ref-type="fig" rid="i2164-2591-6-3-10-f01">Fig. 1</xref>). Visual information gathering (49%) is particularly important in our ULV population, whereas very few of their remaining visual activities can be classified as reading, or more generally, recognizing shapes (10%).</p><fig id="i2164-2591-6-3-10-f01" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Assignment of visual activities reported by individuals with ULV to functional vision domains &#x02013; reading/shape recognition, mobility, visual motor, and visual information gathering.</p></caption><graphic xlink:href="i2164-2591-6-3-10-f01"/></fig></sec><sec id="s3d"><title>Categorization by Visual Aspects</title><p><xref ref-type="fig" rid="i2164-2591-6-3-10-f02">Figure 2</xref> shows activities for each primary visual aspect. This characterization was based on a Grounded Theory approach, where themes from participant's transcripts are extracted and categorized to identify common themes.<sup><xref rid="i2164-2591-6-3-10-b13" ref-type="bibr">13</xref>,<xref rid="i2164-2591-6-3-10-b14" ref-type="bibr">14</xref></sup> Contrast (43%) accounts for the largest percentage in determining use of vision in the ULV population, while depth perception (1%) accounts for the smallest percentage. Luminance (17%) and environmental lighting (9%) together form the second most frequently reported attribute in our ULV population, while the related concepts of size (10%) and distance (6%), when taken jointly, are the third most frequently reported aspects.</p><fig id="i2164-2591-6-3-10-f02" orientation="portrait" position="float"><label>Figure 2</label><caption><p>The distribution of primary visual aspects &#x02013; contrast, luminance, environmental lighting, familiarity, motion perception, distance, size, eccentricity, depth perception, and others/misc &#x02013; across visual activities in ULV.</p></caption><graphic xlink:href="i2164-2591-6-3-10-f02"/></fig></sec><sec id="s3e"><title>Distribution of Visual Aspects across Functional Domains</title><p><xref ref-type="table" rid="i2164-2591-6-3-10-t03">Table 3</xref> shows the distribution of visual aspects (rows) across activities reported by focus group members, for each functional domain (columns). Multiple aspects can play a role in a given activity; thus, an activity can be categorized under more than one visual aspect. This explains why the table contains 1199 aspects for 760 activities. The two percentages given in the rightmost column refer to the total number of times an aspect was referred to, as a fraction of the total number of aspects (1199) and total number of activities (760), respectively; thus, contrast is the most commonly reported aspect, with 43% of the total number of aspects reported, but it actually plays a role in 68% of all reported activities. The percentages in the top row refer to the number of aspects in a functional domain as a fraction of the total number of aspects (1199), and the percentage in other cells refers to number of aspects in that cell as a fraction of the total number of aspects across all activities in that visual domain.</p><table-wrap id="i2164-2591-6-3-10-t03" content-type="2col" orientation="portrait" position="float"><label>Table 3</label><caption><p>Item Categorization</p></caption><graphic xlink:href="i2164-2591-6-3-10-t03"/></table-wrap><p>Pairwise <italic>t</italic>-tests and Fisher's exact tests were used to compare the percentage of each aspect across domains (analyzing 2 domains at a time). Categorization of all activities reported by our ULV population shows that contrast is by far the most frequently reported aspect across all domains, playing a major role in 68% of all reported activities (<xref ref-type="fig" rid="i2164-2591-6-3-10-f03">Fig. 3</xref>). Contrast is crucial in visual motor activities (54%), and accounts for a significantly greater percentage of reported visual aspects than in the visual information gathering (43%, <italic>P</italic> = 0.035), and mobility activities (31%, <italic>P</italic> = 0.002), but not significantly greater than in the reading functional domain (44%, <italic>P</italic> = 0.231). The mobility domain had a significantly higher percentage of references to luminance and lighting combined (41%) than reading (17%, <italic>P</italic> = 0.048), visual-motor (23%, <italic>P</italic> = 0.021), and visual information (24%, <italic>P</italic> = 0.006). Size accounted for a large proportion in the reading domain (22%), as did familiarity in the mobility domain (11%); however, due to smaller sample sizes these comparisons did not reach statistical significance.</p><fig id="i2164-2591-6-3-10-f03" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Distribution of the visual aspects across the functional vision domains.</p></caption><graphic xlink:href="i2164-2591-6-3-10-f03"/></fig><p><xref ref-type="fig" rid="i2164-2591-6-3-10-f03">Figure 3</xref> shows the aspect distributions within each of the domains, as a function of the total number of aspects, reemphasizing the importance of contrast and lighting (i.e., luminance and illumination).</p></sec></sec><sec id="s4"><title>Discussion</title><p>Individuals with ultra-low vision (i.e., profound visual impairment or near-total blindness) are a unique subset of the low vision population. Very little has been reported about the activities they are still able to do with their remaining vision. Existing assessment instruments (e.g., VFQs) are not suited for this population and have not been validated in ULV individuals. Our study uncovered a large number of activities for which people with ULV still use their vision.</p><p>From the content, we found that all four functional domains (visual information gathering, mobility, visual motor, and reading) are represented in these activities. Not all of them are represented equally, however, nor are the activities typical of those normally found in each domain. Reading is reduced to crude shape recognition, and activities enabled by ULV are more heavily concentrated in the visual information-gathering domain.</p><p>A Grounded Theory approach was used to categorize<sup><xref rid="i2164-2591-6-3-10-b13" ref-type="bibr">13</xref>,<xref rid="i2164-2591-6-3-10-b14" ref-type="bibr">14</xref></sup> the descriptions offered by focus group members, and demonstrated that basic visual aspects (contrast, luminance and lighting, size and distance, familiarity, movement, eccentricity, and depth) are good descriptors of the critical factors that play a role in visual activities performed by the ULV population. The descriptions also showed that the ability to perform an activity could depend on more than one visual aspect.</p><p>From our study results, we learned that contrast accounts for the largest percentage of reported visual aspects in all domains, with lighting (natural and environmental) and size/distance as additional important aspects. This explains a frequent observation by the participants in our focus groups: as their vision diminishes, there is greater need for clearer/sharper demarcations between objects or sections in a scene. This has also been shown to be very important in some of the new vision restoration therapies like the Argus II retinal implant. Users report that contrast and clear segmentation of the image play a crucial role in what they are able to see.</p><p>Visual aspects behave in many ways as would be expected from more familiar activities. For example, even though &#x0201c;reading&#x0201d; in the ULV population is reduced to crude shape recognition, size still plays a critical role in the activities falling under this domain. Similarly, luminance, lighting, and familiarity together account for more than half of the critical visual aspects contributing to activities in the mobility domain.</p><p>Our study population shows a diverse representation of diagnoses, age, ethnic representation, education, employment status, and living conditions. One may object that retinitis pigmentosa was highly prevalent in our sample, but this is not surprising as major eye diseases such as age-related macular degeneration, diabetic retinopathy, and glaucoma rarely lead to loss of form vision, which leaves congenital conditions and inherited retinal degenerations as the primary contributors to the ULV population. Another point of concern may be the ethnic representation in our sample, but this is similar to the reported distribution in the legally blind population: according to the NEI,<sup><xref rid="i2164-2591-6-3-10-b18" ref-type="bibr">18</xref></sup> the ethnicity of legally blind individuals across all ages in 2010 was 83% white, 3% Hispanic, and 11% black, which is similar to our sample. In terms of sex, our sample was predominantly male (52%) in contrast with the NEI data,<sup><xref rid="i2164-2591-6-3-10-b14" ref-type="bibr">14</xref></sup> where the sample included 66% female versus 34% male. This may be due to the fact that age-related macular degeneration and proliferative diabetic retinopathy (two major categories contributing to legal blindness, but not to ULV) tend to be more prevalent among women. Another concern about our sample may be the low representation of younger age groups, but this too is representative of the distribution in the profoundly visually impaired population. Furthermore, the NEI legal blindness figures include ages of 40 and older, whereas we included a wider range of ages. Age is a predominant factor for prevalence of legal blindness in the US population.</p><p>Finally, one may wonder whether a random sample from the overall ULV population would have yielded substantially different activities from the ones we collected in what, despite our best efforts to recruit as widely as possible, may be considered by some a sample of convenience. We strongly feel that this would not be the case. In fact, even among those we included with prosthetic vision (Argus II), we did not find substantive differences in reported activities when compared with those with native ultra-low vision. Therefore, we are convinced that the activities we collected fairly represent those activities in which ULV individuals still derive benefit from their rudimentary vision.</p><p>The nature of this research was patient-driven rather than hypothesis-driven. The research team wanted to create a framework for ULV that is based on the way the participants in the focus groups use their vision, because so little was known about this: ULV individuals are commonly considered functionally blind, and no systematic effort has been made to understand what role vision plays in their lives. An important outcome of this work is that it allows us to develop new hypotheses about visual ability, based on the experiences of individuals with ULV. While we used statistical tests to compare the visual attributes comprising each domain, this qualitative study was not intended or powered to reach significant differences across domains. Also, our study population did not include individuals with better levels of vision that would have allowed us to study the relative importance of different visual aspects across the spectrum from normal vision to BLP. Instead, this study aims to develop testable hypotheses based on the experiences of individuals with ULV. Our results from this qualitative study indicate that contrast accounts for the largest percentage (i.e., is the most important aspect) of attributes within all four domains, and this is the primary hypothesis that should be the focus of subsequent quantitative research in this area. Additionally, luminance and lighting together account for more than 17% to 41% of each domain, and size and distance together account for more than 25% of the reading/shape domain, suggesting secondary hypotheses that these aspects account for the second and third largest percentages across domains, after contrast. The accomplishment of this focus group study is that it has given us a large set of carefully categorized activities. They represent the visual experience of the ULV population and can be used to develop tools for assessment and rehabilitation: a VFQ that can probe the rudimentary visual abilities of individuals with ULV;<sup><xref rid="i2164-2591-6-3-10-b19" ref-type="bibr">19</xref></sup> standardized activities that can be used as performance measures;<sup><xref rid="i2164-2591-6-3-10-b20" ref-type="bibr">20</xref></sup> and a curriculum of training activities to be used in the native ULV population as well as in individuals undergoing vision-restoring interventions. Together, these components will form what we set out to create: a PLoVR curriculum.</p><sec id="s4a"><title>Supplementary Material</title><p>The <xref ref-type="supplementary-material" rid="tvst-06-03-10_s01">Supplementary Material</xref> shows a detailed listing of each goal discussed, the number activities generated, and their categorization by functional domain and by visual aspect.</p></sec></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="tvst-06-03-10_s01"><label>Supplement 1</label><media xlink:href="tvst-06-03-10_s01.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>This study was funded in full by R01 EY021220 (GD) from the National Eye Institute and an administrative supplement to PEJ.</p><p>Disclosure: <bold>O. Adeyemo</bold>, None; <bold>P.E. Jeter</bold>, None; <bold>C. Rozanski</bold>, None; <bold>E. Arnold</bold>, None; <bold>L.A. Dalvin</bold>, None; <bold>B. Swenor</bold>, None; <bold>G. Dagnelie</bold>, None</p></ack><ref-list><title>References</title><ref id="i2164-2591-6-3-10-b01"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Colenbrander</surname><given-names>A,</given-names></name><name><surname>Fletcher</surname><given-names>DC.</given-names></name></person-group>
<article-title>Basic concepts and terms for low vision rehabilitation</article-title>.
<source><italic>Am J Occup Ther</italic></source>.
<year>1995</year>;
<volume>49</volume>:
<fpage>865</fpage>&#x02013;<lpage>869</lpage>.
<pub-id pub-id-type="pmid">8572044</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b02"><label>2.</label><mixed-citation publication-type="journal">
<collab>Center for Disease Control (CDC)</collab>.
<article-title>Available at: <ext-link ext-link-type="uri" xlink:href="http://www.cdc.gov/nchs/fastats/disability.htm">http://www.cdc.gov/nchs/fastats/disability.htm</ext-link></article-title>.
<source>Accessed June 24</source>,
<year>2016</year>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b03"><label>3.</label><mixed-citation publication-type="journal">
<collab>American Academy of Ophthalmology</collab>.
<article-title>Guidelines: SmartSight/Low Vision. Available at: <ext-link ext-link-type="uri" xlink:href="http://aao.org/smart-sight-low-vision">http://aao.org/smart-sight-low-vision</ext-link></article-title>.
<source>Accessed February 21</source>,
<year>2016</year>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b04"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Jackson</surname><given-names>ML.</given-names></name></person-group>
<article-title>Vision rehabilitation for Canadians with less than 20/40 acuity: the SmartSight model</article-title>.
<source><italic>Can J Ophthalmol</italic></source>.
<year>2006</year>;
<volume>41</volume>:
<fpage>355</fpage>&#x02013;<lpage>361</lpage>.
<pub-id pub-id-type="pmid">16767192</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b05"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Nau</surname><given-names>A,</given-names></name><name><surname>Bach</surname><given-names>M,</given-names></name><name><surname>Fisher</surname><given-names>C.</given-names></name></person-group>
<article-title>Clinical tests of ultra-low vision used to evaluate rudimentary visual perceptions enabled by the BrainPort vision device</article-title>.
<source><italic>Transl Vis Sci Technol</italic></source>.
<year>2013</year>;
<volume>2</volume>
<issue>3</issue>:
<fpage>1</fpage>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b06"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Geruschat</surname><given-names>DR,</given-names></name><name><surname>Dagnelie</surname><given-names>G.</given-names></name></person-group>
<article-title>Restoration of vision following long-term blindness: considerations or providing rehabilitation</article-title>.
<source><italic>J Vis Impair Blind (Online)</italic></source>.
<year>2016</year>;
<volume>110</volume>:
<fpage>5</fpage>
<comment>Available at <ext-link ext-link-type="uri" xlink:href="http://www.afb.org/jvib/jvibabstractNew.asp?articleid=jvib100102">http://www.afb.org/jvib/jvibabstractNew.asp?articleid=jvib100102</ext-link>. Accessed March 7, 2016</comment>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b07"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Colenbrander</surname><given-names>A.</given-names></name></person-group>
<article-title>Visual functions and functional vision</article-title>.
<source><italic>Int Congr Ser</italic></source>.
<year>2005</year>;
<volume>1282</volume>:
<fpage>482</fpage>&#x02013;<lpage>486</lpage>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b08"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Geruschat</surname><given-names>DR,</given-names></name><name><surname>Flax</surname><given-names>M,</given-names></name><name><surname>Tanna</surname><given-names>N,</given-names></name><etal/></person-group>
<article-title>FLORA&#x02122;: phase I development of a functional vision assessment for prosthetic vision users</article-title>.
<source><italic>Clin Exp Optom</italic></source>.
<year>2015</year>;
<volume>98</volume>:
<fpage>342</fpage>&#x02013;<lpage>347</lpage>.
<pub-id pub-id-type="pmid">25675964</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b09"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Finger</surname><given-names>RP,</given-names></name><name><surname>Tellis</surname><given-names>B,</given-names></name><name><surname>Crewe</surname><given-names>J,</given-names></name><etal/></person-group>
<article-title>Developing the impact of vision impairment&#x02013;very low vision (IVI-VLV) questionnaire as part of the LoVADA protocol developing the IVI-VLV</article-title>.
<source><italic>Invest Ophthalmol Vis Sci</italic></source>.
<year>2014</year>;
<volume>55</volume>:
<fpage>6150</fpage>&#x02013;<lpage>6158</lpage>.
<pub-id pub-id-type="pmid">25190656</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Pesudovs</surname><given-names>K,</given-names></name><name><surname>Burr</surname><given-names>JM,</given-names></name><name><surname>Harley</surname><given-names>C,</given-names></name><etal/></person-group>
<article-title>The development, assessment, and selection of questionnaires</article-title>.
<source><italic>Optom Vis Sci</italic></source>.
<year>2007</year>;
<volume>84</volume>:
<fpage>663</fpage>&#x02013;<lpage>674</lpage>.
<pub-id pub-id-type="pmid">17700331</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b11"><label>11.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Massof</surname><given-names>RW,</given-names></name><name><surname>Ahmadian</surname><given-names>L,</given-names></name><name><surname>Grover</surname><given-names>LL,</given-names></name><etal/></person-group>
<article-title>The activity inventory: an adaptive visual function questionnaire</article-title>.
<source><italic>Optom Vis Sci</italic></source>.
<year>2007</year>;
<volume>84</volume>:
<fpage>763</fpage>&#x02013;<lpage>774</lpage>.
<pub-id pub-id-type="pmid">17700339</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Massof</surname><given-names>RW.</given-names></name></person-group>
<article-title>A systems model for low vision rehabilitation. I. Basic concepts</article-title>.
<source><italic>Optom Vis Sci</italic></source>.
<year>1995</year>;
<volume>72</volume>:
<fpage>725</fpage>&#x02013;<lpage>736</lpage>.
<pub-id pub-id-type="pmid">8570162</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b13"><label>13.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Bernard</surname><given-names>HR.</given-names></name></person-group>
<source><italic>Research Methods in Anthropology: Qualitative and Quantitative Approaches</italic>. 5th ed</source>.
<publisher-loc>Lanhan</publisher-loc>:
<publisher-name>Rowman Altamira;</publisher-name>
<year>2011</year>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b14"><label>14.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Strauss</surname><given-names>A,</given-names></name><name><surname>Corbin</surname><given-names>J.</given-names></name></person-group>
<source><italic>Basics of Qualitative Research</italic>. 4th ed</source>.
<publisher-loc>Thousand Oaks</publisher-loc>:
<publisher-name>Sage;</publisher-name>
<year>2015</year>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b15"><label>15.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Kurson</surname><given-names>R.</given-names></name></person-group>
<source><italic>Crashing Through: A True Story of Risk, Adventure, and the Man Who Dared to See</italic></source>.
<publisher-loc>New York</publisher-loc>:
<publisher-name>Random House;</publisher-name>
<year>2006</year>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b16"><label>16.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name><surname>Fowler</surname><given-names>FJ</given-names><suffix>Jr.</suffix></name></person-group>
<source><italic>Survey Research Methods</italic>. 5th ed</source>.
<publisher-loc>Thousand Oaks</publisher-loc>:
<publisher-name>Sage Publications;</publisher-name>
<year>2013</year>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Sim</surname><given-names>J.</given-names></name></person-group>
<article-title>Collecting and analysing qualitative data: issues raised by the focus group</article-title>.
<source><italic>J Adv Nurs</italic></source>.
<year>1998</year>;
<volume>28</volume>:
<fpage>345</fpage>&#x02013;<lpage>352</lpage>.
<pub-id pub-id-type="pmid">9725732</pub-id></mixed-citation></ref><ref id="i2164-2591-6-3-10-b18"><label>18.</label><mixed-citation publication-type="journal">
<collab>National Eye Institute (NEI)</collab>.
<article-title>Available at: <ext-link ext-link-type="uri" xlink:href="https://nei.nih.gov/eyedata/blind">https://nei.nih.gov/eyedata/blind</ext-link></article-title>.
<source>Accessed June 24</source>,
<year>2016</year>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Jeter</surname><given-names>PE,</given-names></name><name><surname>Rosanski</surname><given-names>C,</given-names></name><name><surname>Massof</surname><given-names>R,</given-names></name><etal/></person-group>
<article-title>Development of the Ultra-Low Vision Visual Functioning Questionnaire (ULV-VFQ)</article-title>.
<source><italic>Trans Vis Sci Technol</italic></source>.
<year>2017</year>;
<volume>6</volume>
<issue>3</issue>:
<fpage>XXX</fpage>&#x02013;<lpage>XXX</lpage>.
</mixed-citation></ref><ref id="i2164-2591-6-3-10-b20"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name><surname>Dagnelie</surname><given-names>G,</given-names></name><name><surname>Geruschat</surname><given-names>D,</given-names></name><name><surname>Massof</surname><given-names>RW,</given-names></name><etal/></person-group>
<article-title>Developing a calibrated ultra-low vision (ULV) assessment toolkit</article-title>.
<source><italic>Optom Vis Sci</italic></source>.
<year>2015</year>;
<volume>92</volume>:
<comment>AAO E-abstract #155252</comment>.
</mixed-citation></ref></ref-list></back></article>
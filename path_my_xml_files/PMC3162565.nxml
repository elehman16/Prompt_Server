<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Orthop Surg Res</journal-id><journal-title-group><journal-title>Journal of Orthopaedic Surgery and Research</journal-title></journal-title-group><issn pub-type="epub">1749-799X</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">21801370</article-id><article-id pub-id-type="pmc">3162565</article-id><article-id pub-id-type="publisher-id">1749-799X-6-38</article-id><article-id pub-id-type="doi">10.1186/1749-799X-6-38</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Classification and treatment of proximal humerus fractures: inter-observer reliability and agreement across imaging modalities and experience</article-title></title-group><contrib-group><contrib contrib-type="author" id="A1"><name><surname>Foroohar</surname><given-names>Abtin</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>aforoohar@gmail.com</email></contrib><contrib contrib-type="author" id="A2"><name><surname>Tosti</surname><given-names>Rick</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>rtosti@temple.edu</email></contrib><contrib contrib-type="author" id="A3"><name><surname>Richmond</surname><given-names>John M</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>john.richmond@temple.edu</email></contrib><contrib contrib-type="author" id="A4"><name><surname>Gaughan</surname><given-names>John P</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>john.gaughan@temple.edu</email></contrib><contrib contrib-type="author" corresp="yes" id="A5"><name><surname>Ilyas</surname><given-names>Asif M</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>aimd2001@yahoo.com</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Orthopaedic Surgery and Sports Medicine, Temple University School of Medicine, 3401 N. Broad Street, Philadelphia, PA 1914, USA</aff><aff id="I2"><label>2</label>Department Of Physiology, Temple University School of Medicine, 3500 N. Broad Street, Philadelphia, PA 19141, USA</aff><aff id="I3"><label>3</label>Rothman Institute, Thomas Jefferson University Hospital, 925 Chestnut Street, Philadelphia, PA 19107, USA</aff><pub-date pub-type="collection"><year>2011</year></pub-date><pub-date pub-type="epub"><day>29</day><month>7</month><year>2011</year></pub-date><volume>6</volume><fpage>38</fpage><lpage>38</lpage><history><date date-type="received"><day>14</day><month>9</month><year>2010</year></date><date date-type="accepted"><day>29</day><month>7</month><year>2011</year></date></history><permissions><copyright-statement>Copyright &#x000a9;2011 Foroohar et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2011</copyright-year><copyright-holder>Foroohar et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="http://www.josr-online.com/content/6/1/38"/><abstract><sec><title>Summary</title><p>Proximal humerus fractures (PHF) are common injuries, but previous studies have documented poor inter-observer reliability in fracture classification. This disparity has been attributed to multiple variables including poor imaging studies and inadequate surgeon experience. The purpose of this study is to evaluate whether inter-observer agreement can be improved with the application of multiple imaging modalities including X-ray, CT, and 3D CT reconstructions, stratified by physician experience, for both classification and treatment of PHFs.</p></sec><sec><title>Methods</title><p>Inter-observer agreement was measured for classification and treatment of PHFs. A total of sixteen fractures were imaged by plain X-ray (scapular AP and lateral), CT scan, and 3D CT reconstruction, yielding 48 randomized image sets. The observers consisted of 16 orthopaedic surgeons (4 upper extremity specialists, 4 general orthopedists, 4 senior residents, 4 junior residents), who were asked to classify each image set using the Neer system, and recommend treatment from four pre-selected choices. The results were evaluated by kappa reliability coefficients for inter-observer agreement between all imaging modalities and sub-divided by: fracture type and observer experience.</p></sec><sec><title>Results</title><p>All kappa values ranged from "slight" to "moderate" (k = .03 to .57) agreement. For overall classification and treatment, no advanced imaging modality had significantly higher scores than X-ray. However, when sub-divided by experience, 3D reconstruction and CT scan both had significantly higher agreement on classification than X-ray, among upper extremity specialists. Agreement on treatment among upper extremity specialists was best with CT scan. No other experience sub-division had significantly different kappa scores. When sub-divided by fracture type, CT scan and 3D reconstruction had higher scores than X-ray for classification only in 4-part fractures. Agreement on treatment of 4 part fractures was best with CT scan. No other fracture type sub-division had significantly different kappa scores.</p></sec><sec><title>Conclusions</title><p>Although 3D reconstruction showed a slight improvement in the inter-observer agreement for fracture classification among specialized upper extremity surgeons compared to all imaging modalities, fracture types, and surgeon experience; overall all imaging modalities continue to yield low inter-observer agreement for both classification and treatment regardless of physician experience.</p></sec></abstract></article-meta></front><body><sec><title>Introduction</title><p>Proximal humeral fractures (PHFs) comprise 5% of all fractures in adults and are the third most common fracture in adults over 65 years old [<xref ref-type="bibr" rid="B1">1</xref>]. In 1970, Charles Neer II created a classification system for fractures of the proximal humerus, which is widely utilized [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B3">3</xref>]. However, over the past 2 decades the reliability of Neer's system has been challenged, as multiple studies have reported low inter-observer agreement when attempting to classify PHFs using Neer's system [<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B17">17</xref>] or recommending subsequent treatment [<xref ref-type="bibr" rid="B18">18</xref>]. Neer's classification is not alone in this quandary, as many studies have similarly reported disagreement in classification schemes for other types of fractures [<xref ref-type="bibr" rid="B19">19</xref>-<xref ref-type="bibr" rid="B21">21</xref>]. It has been postulated that the low levels of agreement is not a limitation of the classification systems itself but rather the surgeons' inability to accurately interpret the images. In fact, Neer himself has rebutted that experience and suboptimal imaging are likely responsible for the lack of agreement in his system [<xref ref-type="bibr" rid="B22">22</xref>].</p><p>Although some authors have evaluated the effect on inter-observer agreement by adding advanced imaging such as CT scans and three-dimensional (3D) reconstructions, [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B23">23</xref>-<xref ref-type="bibr" rid="B25">25</xref>] the results have been inconclusive, and none have addressed all of these modalities in terms of both classification and treatment recommendations as a function of physician experience. Thus, to the best of our knowledge, this is the first study to evaluate the inter-observer agreement of multiple imaging modalities: X-ray, CT, and 3D reconstructions on both the classification as well as treatment of proximal humerus fractures in a single study. The secondary study goal was to observe the effect of stratifying agreement based on fracture severity and surgeon experience.</p></sec><sec sec-type="methods"><title>Patients and methods</title><p>Sixteen proximal humerus fractures were selected and classified by the senior author as four 2-part fractures, eight 3-part fractures, and four 4-part fractures. Each of the 16 fractures had an X-ray (anteroposterior and a scapular-Y lateral), a CT scan, and a 3D CT reconstruction, which resulted in a total of 48 standardized image sets. All images were taken between 2003-2008 at the same institution and drawn from the same PACS system.</p><p>After providing a brief review of the Neer classification system, each observer was presented the same 48 image-sets by PowerPoint in random order. They were blinded to any patient demographic information, mechanism of injury, or associated morbidities. Each observer was asked only two questions per set of images: (1) to classify the fracture using the Neer classification, and (2) to determine their treatment of choice. Treatment options were standardized to four choices: non-operative, open reduction internal fixation, hemiarthroplasty or total shoulder arthroplasty. No case demographics were provided.</p><p>The observers included orthopedists of varying experience: 8 board-certified attending surgeons (consisting of 4 general orthopedists and 4 upper extremity specialists), 4 senior residents, and 4 junior residents. A general orthopedist was defined as a surgeon practicing all aspects of orthopaedic surgery including the surgical management of PHFs. An upper extremity specialist was defined as a surgeon with fellowship training and a practice focus on the upper extremity whose practice includes the surgical management of PHFs.</p><sec><title>Statistical Analysis</title><p>Inter-observer agreement was assessed via computer-calculated kappa statistics based on the works of Cohen and Fleiss [<xref ref-type="bibr" rid="B26">26</xref>,<xref ref-type="bibr" rid="B27">27</xref>]. Calculating agreement by this method adjusts the proportion of observed agreement between observers to correct for the proportion of agreement between observers due to chance. Thus, kappa values are always lower than absolute agreement except when 100% agreement is achieved. The kappa coefficients range from +1 (total agreement) to 0 (chance agreement). Although kappa values ranging 0 to -1 are possible, these seldom are encountered, as it represents an agreement less than that which would occur by random chance. The strength of agreement of kappa coefficients was guided by the boundaries suggested by Landis and Koch [<xref ref-type="bibr" rid="B28">28</xref>]. Values less than 0.00 indicate "poor" reliability, 0.00-0.20 is "slight" reliability, 0.21-0.40 is "fair" reliability, 0.41-0.60 is "moderate" reliability, 0.61-0.80 is "substantial" agreement, 0.81-1.00 "excellent" or "almost perfect" agreement. Although these categories are arbitrary, they have been well recognized in the orthopedic literature. Statistical differences between individual kappa values were considered significant when the upper and lower boundaries of 95% confidence intervals did not overlap.</p></sec></sec><sec><title>Results</title><sec><title>Overall inter-observer agreement (table <xref ref-type="table" rid="T1">1</xref>, figure <xref ref-type="fig" rid="F1">1</xref>)</title><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Overall inter-observer agreement</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="left">Classification</th><th align="left">Classification</th><th/><th align="left">Treatment</th><th/><th/></tr></thead><tbody><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td colspan="7"><hr/></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.1416</td><td align="left">(0.1177-0.1655)</td><td align="left">slight</td><td align="left">0.2852</td><td align="left">(0.2600-0.3104)</td><td align="left">fair</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.0690</td><td align="left">(0.0000-0.0920)</td><td align="left">slight</td><td align="left">0.3285</td><td align="left">(0.3028-0.3543)</td><td align="left">fair</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.0947</td><td align="left">(0.0710-0.1185)</td><td align="left">slight</td><td align="left">0.3082</td><td align="left">(0.2817-0.3346)</td><td align="left">fair</td></tr></tbody></table></table-wrap><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Graph showing confidence intervals of overall kappa scores for classification and treatment of proximal humerus fractures</bold>. Inter-observer agreement was considered significantly different in non-overlapping intervals. Strength of agreement based on guidelines recommended by Landis and Koch [<xref ref-type="bibr" rid="B19">19</xref>].</p></caption><graphic xlink:href="1749-799X-6-38-1"/></fig><p>Agreement of classification across all modalities was only "slight," and agreement of treatment across all modalities was "fair." For classification: X-ray &#x0003e; 3D CT reconstruction &#x0003e; 2D CT scan with the kappa values being 0.14, 0.09, 0.07 respectively; 3D reconstruction was not statistically different than either X-ray or CT scan, but X-ray was significantly stronger than CT. For treatment recommendation, the inter-observer agreement ranged from 0.29-0.33, and no statistically significant difference was detected between the modalities.</p></sec><sec><title>Inter-observer agreement subdivided by fracture type (table <xref ref-type="table" rid="T2">2</xref>, figure <xref ref-type="fig" rid="F2">2</xref>)</title><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Inter-observer agreement subdivided by fracture type</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">2 Part Neer Fractures</th><th/><th/><th/><th/><th/><th/></tr></thead><tbody><tr><td/><td align="left"><bold>Classification</bold></td><td/><td align="left"><bold>Treatment</bold></td><td/><td/><td/></tr><tr><td colspan="7"><hr/></td></tr><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td colspan="7"><hr/></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.0358</td><td align="left">(0.0000-0.0907)</td><td align="left">slight</td><td align="left">0.1494</td><td align="left">(0.0959-0.2030)</td><td align="left">slight</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.0448</td><td align="left">(0.0000-0.0977)</td><td align="left">slight</td><td align="left">0.2446</td><td align="left">(0.1912-0.2980)</td><td align="left">fair</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.0770</td><td align="left">(0.0251-0.1290)</td><td align="left">slight</td><td align="left">0.1793</td><td align="left">(0.1241-0.2344)</td><td align="left">slight</td></tr><tr><td/><td/><td/><td/><td/><td/><td/></tr><tr><td align="left"><bold>3 Part Neer Fractures</bold></td><td/><td/><td/><td/><td/><td/></tr><tr><td/><td align="left"><bold>Classification</bold></td><td/><td align="left"><bold>Treatment</bold></td><td/><td/><td/></tr><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.0877</td><td align="left">(0.0556-0.1198)</td><td align="left">slight</td><td align="left">0.3430</td><td align="left">(0.3057-0.3802)</td><td align="left">fair</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.0524</td><td align="left">(0.0000-0.0850)</td><td align="left">slight</td><td align="left">0.2860</td><td align="left">(0.2492-0.3229)</td><td align="left">fair</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.0960</td><td align="left">(0.0640-0.1284)</td><td align="left">slight</td><td align="left">0.3579</td><td align="left">(0.3222-0.3935)</td><td align="left">fair</td></tr><tr><td/><td/><td/><td/><td/><td/><td/></tr><tr><td align="left"><bold>4 Part Neer Fractures</bold></td><td/><td/><td/><td/><td/><td/></tr><tr><td/><td align="left"><bold>Classification</bold></td><td/><td align="left"><bold>Treatment</bold></td><td/><td/><td/></tr><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.2600</td><td align="left">(0.2105-0.3090)</td><td align="left">fair</td><td align="left">0.1697</td><td align="left">(0.1481-0.2454)</td><td align="left">slight</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.4467</td><td align="left">(0.3989-0.4946)</td><td align="left">moderate</td><td align="left">0.3368</td><td align="left">(0.2857-0.3880)</td><td align="left">fair</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.5743</td><td align="left">(0.5225-0.6260)</td><td align="left">moderate</td><td align="left">0.0893</td><td align="left">(0.0344-0.1441)</td><td align="left">slight</td></tr></tbody></table></table-wrap><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Graph showing confidence intervals of kappa scores sub-divided by fracture type</bold>. Advanced imaging seemed to only improve agreement of classification in 4 part fractures.</p></caption><graphic xlink:href="1749-799X-6-38-2"/></fig><p>We selected four fractures for each of the four major types of Neer classification schemes yielding a total of sixteen fractures. For classification of 2 part fractures, the kappa values ranged from 0.03-0.07 (achieving "slight" agreement) with no statistically significant differences between the modalities. For treatment of 2 part fractures, the kappa values ranged from 0.15-.24; CT scan was the only modality to reach "fair" agreement, but none of the agreement scores were statistically different from each other. For classification of 3 part fractures, all of the modalities reached only "slight" agreement, and none were statistically different from one another. For treatment of 3 part fractures, all of the modalities reached "fair" agreement, and none were statistically different from one another. For classification of 4 part fractures: 3D reconstruction &#x0003e; CT scan &#x0003e; X-ray, and both 3D reconstruction and CT scan reached the "moderate" level. All kappa values from the 4-part classification subdivision were significantly different. Noteworthy, the highest individual kappa value achieved in this study was agreement on 3D reconstructed 4 part fractures. For treatment of 4 part fractures, CT scan had the highest agreement with a "fair" score of 0.34. This kappa score was statistically different than both of the "slight" scores yielded by X-ray and 3D reconstruction.</p></sec><sec><title>Inter-observer agreement subdivided by experience (table <xref ref-type="table" rid="T3">3</xref>, figure <xref ref-type="fig" rid="F3">3</xref>)</title><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>Inter-observer agreement subdivided by experience</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2">Upper Extremity Specialists</th><th/><th/><th/><th/><th/></tr></thead><tbody><tr><td/><td align="left" colspan="2"><bold>Classification</bold></td><td/><td align="left" colspan="2"><bold>Treatment</bold></td><td/></tr><tr><td colspan="7"><hr/></td></tr><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td colspan="7"><hr/></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.0315</td><td align="left">(0.0000-0.0917)</td><td align="left">slight</td><td align="left">0.1605</td><td align="left">(0.0190-0.3020)</td><td align="left">slight</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.233</td><td align="left">(0.1096-0.3339)</td><td align="left">fair</td><td align="left">0.4673</td><td align="left">(0.3294-0.6052)</td><td align="left">moderate</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.3246</td><td align="left">(0.1946-0.4546)</td><td align="left">fair</td><td align="left">0.1832</td><td align="left">(0.0333-0.3330)</td><td align="left">slight</td></tr><tr><td/><td/><td/><td/><td/><td/><td/></tr><tr><td align="left" colspan="2"><bold>General Orthopedists</bold></td><td/><td/><td/><td/><td/></tr><tr><td/><td align="left" colspan="2"><bold>Classification</bold></td><td/><td align="left" colspan="2"><bold>Treatment</bold></td><td/></tr><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.1079</td><td align="left">(0.0467-.01691)</td><td align="left">slight</td><td align="left">0.3883</td><td align="left">(0.3213-0.4553)</td><td align="left">fair</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.0351</td><td align="left">(0.0000-0.0914)</td><td align="left">slight</td><td align="left">0.46</td><td align="left">(0.3945-0.5255)</td><td align="left">moderate</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.036</td><td align="left">(0.0000-0.0980)</td><td align="left">slight</td><td align="left">0.4069</td><td align="left">(0.3405-0.4734)</td><td align="left">moderate</td></tr><tr><td/><td/><td/><td/><td/><td/><td/></tr><tr><td align="left" colspan="2"><bold>Senior Residents</bold></td><td/><td/><td/><td/><td/></tr><tr><td/><td align="left" colspan="2"><bold>Classification</bold></td><td/><td align="left" colspan="2"><bold>Treatment</bold></td><td/></tr><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.2184</td><td align="left">(0.0760-0.3608)</td><td align="left">fair</td><td align="left">0.4273</td><td align="left">(0.2849-0.5697)</td><td align="left">moderate</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.0597</td><td align="left">(0.0000-0.2230)</td><td align="left">slight</td><td align="left">0.2613</td><td align="left">(0.1133-0.4094)</td><td align="left">fair</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.0364</td><td align="left">(0.0000-0.1670)</td><td align="left">slight</td><td align="left">0.368</td><td align="left">(0.2154-0.5210)</td><td align="left">fair</td></tr><tr><td/><td/><td/><td/><td/><td/><td/></tr><tr><td align="left" colspan="2"><bold>Junior Residents</bold></td><td/><td/><td/><td/><td/></tr><tr><td/><td align="left" colspan="2"><bold>Classification</bold></td><td/><td align="left" colspan="2"><bold>Treatment</bold></td><td/></tr><tr><td align="left"><bold>Modality</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td><td align="left"><bold>Kappa Score</bold></td><td align="left"><bold>95% Confidence Interval</bold></td><td align="left"><bold>Strength of Agreement</bold></td></tr><tr><td align="left">Plain film X-ray</td><td align="left">0.0295</td><td align="left">(0.0000-0.1807)</td><td align="left">slight</td><td align="left">0.029</td><td align="left">(0.0000-0.1701)</td><td align="left">slight</td></tr><tr><td align="left">2D CT scan</td><td align="left">0.1111</td><td align="left">(0.0388-0.2610)</td><td align="left">slight</td><td align="left">0.1288</td><td align="left">(0.0397-0.2973)</td><td align="left">slight</td></tr><tr><td align="left">3D reconstruction</td><td align="left">0.1438</td><td align="left">(0.0390-0.2915)</td><td align="left">slight</td><td align="left">0.2284</td><td align="left">(0.0738-0.3831)</td><td align="left">fair</td></tr></tbody></table></table-wrap><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Graph showing confidence intervals of kappa scores sub-divided by surgeon experience</bold>. Statistically significant differences were only observed among upper extremity surgeons.</p></caption><graphic xlink:href="1749-799X-6-38-3"/></fig><p>We divided our orthopedic observers into upper extremity specialists, general orthopedists, senior residents, and junior residents. Among the upper extremity surgeons, both 3D reconstruction and CT scan yielded "fair" agreement for classification and were both significantly stronger than X-ray. Their agreement trended: 3D reconstruction &#x0003e; CT scan &#x0003e; X-ray. However, CT scan had the highest kappa score for treatment recommendation with a "moderate" score of 0.47. Although CT scan was significantly higher than X-ray in the treatment category, it was not significantly higher than 3D reconstruction. Among general orthopedists, all modalities achieved a "slight" agreement rating (0.04-0.11) for classification, and they ranged from "fair" to "moderate" (0.39-0.46) for the treatment recommendations. No statistically significant differences between any kappa values were observed for the general orthopedists within respective classification or treatment categories. Among senior residents, the kappa scores ranged from "slight" to "fare" (0.03-0.21) for classification and from "fair" to "moderate" (0.26-0.43) for treatment recommendation. No statistically significant differences between any kappa values were observed for the senior residents within respective classification or treatment categories. Among junior residents, all imaging modalities yielded only "slight agreement" for both classification and treatment except one "fair" agreement was observed for treatment recommendations after 3D reconstruction. No statistically significant differences were detected between any kappa scores for the junior residents.</p></sec></sec><sec><title>Discussion</title><p>In the past two decades, the validity and reproducibility of fracture classification systems has come under greater scrutiny, which has sparked much debate in the orthopedic literature [<xref ref-type="bibr" rid="B29">29</xref>-<xref ref-type="bibr" rid="B34">34</xref>]. As a result, subsequent studies have examined the utility of advanced imaging techniques in improving inter-observer agreement, but the results have been inconclusive [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B22">22</xref>,<xref ref-type="bibr" rid="B25">25</xref>]. In the beginning of this debate, a few studies have concluded that CT scan or three-dimensional reconstruction add very little in pre-operative assessment [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B22">22</xref>]; however, a study published recently by Brunner et al. has challenged this assertion by showing a consistent increase in inter-observer agreement through the use of stereo-visualization and real 3D imaging [<xref ref-type="bibr" rid="B25">25</xref>].</p><p>In our series, we examined the effect of advanced imaging, including 3D reconstructions, on fracture classification and treatment and found that inter-observer agreement was less than ideal for both classification and treatment among orthopedic surgeons, which is consistent with the majority of reports in the literature [<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B20">20</xref>,<xref ref-type="bibr" rid="B22">22</xref>]. In a recent review, "slight" to "moderate" agreement has been reported in almost all major studies regarding inter-observer reliability in PHFs [<xref ref-type="bibr" rid="B8">8</xref>], and our results indicate the same despite the addition of advanced imaging in the form of 3D reconstructions. However, it should be noted that the comparison of kappa coefficients across studies should be done with caution, as factors such as bias, prevalence, and marginal distributions influence kappa values and can vary at different institutions [<xref ref-type="bibr" rid="B35">35</xref>]. Thus, our second goal was to compare overall inter-observer agreement only within our institution and to observe the effect sub-dividing our results by fracture type and observer experience. In doing this, we observed two major trends in our data: 1) the only significant improvement in agreement with advanced imaging was among upper extremity surgeons and 2) the only benefit of advanced imaging was among all users in attempting to classify 4 part fractures. No benefit was witnessed with advanced imaging in order to improve inter-observer agreement on treatment.</p><p>Our study showed that the greatest inter-observer agreement was among upper extremity surgeons with 3D reconstruction. Furthermore, none of the other groups of observers had significantly improved kappa scores with the addition of advanced imaging, which may suggest that experience enhances inter-observer agreement in our study. Reports in the literature are nearly split regarding the role of experience. Kristiansen et al. was the first to suggest that low experience accounted for low agreement [<xref ref-type="bibr" rid="B17">17</xref>]. Then, Sidor et al. argued against experience by concluding that the three attending physicians had the same agreement as the residents; however, the group of attending physicians was heterogeneous and not all were orthopedic surgeons [<xref ref-type="bibr" rid="B11">11</xref>]. Siebenrock et al. studied only shoulder specialists and found that inter-observer agreement with plain films still landed in the "fair" to "moderate" range; they suggested that experience did not improve the kappa score when compared to other studies, but they did not compare the specialists to a control group [<xref ref-type="bibr" rid="B13">13</xref>]. Sallay et al. was the first article to refute experience by sorting observers into groups. They measured agreement with both X-ray and 3D reconstructions, but their technology for 3D reconstruction was an earlier version and had lower resolution than in the present study (Figure <xref ref-type="fig" rid="F4">4</xref>) [<xref ref-type="bibr" rid="B10">10</xref>]. On the other hand, a few studies have supported the role of experience. Brorson et al. showed significantly higher confidence intervals in specialists when compared to residents and fellows, and although not explicitly stated as a study aim, Bernstein et al. showed higher absolute kappa values among attending surgeons when compared to residents [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B8">8</xref>]. As a response to the challenge of the 4-type classification system, Neer commented that inter-observer variability is likely the combination of "suboptimal quality of current imaging and inexperienced interpreters [<xref ref-type="bibr" rid="B22">22</xref>]." Our study agrees with Neer's interpretation, as our highest and most significant agreement was observed in both our most experienced observers and most advanced imaging modality. This may suggest that the greatest benefit of advanced imaging is to the upper extremity surgeon; however, despite the improved trend, overall agreement is still less than ideal and therefore not recommended.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>An example of a (A) 3D reconstruction showing AP, lateral, and PA views of a right shoulder</bold>. Most popular answers: 50% of raters classified this image as a 3-part fracture (37.5% classified 4-part) and 75% recommended ORIF. From the same patient are (B) coronal and axial views of the CT scan with (C) AP and lateral X-rays.</p></caption><graphic xlink:href="1749-799X-6-38-4"/></fig><p>4-part fractures showed the greatest inter-observer agreement among all observers for both classification and treatment. The data in this category also trended significantly, as 3D reconstruction was stronger than CT scan, which was stronger than X-ray. However, treatment of 4 part-fractures was most agreeable with CT scan. All other subdivisions of fracture type did not show significant improvement with advanced imaging. These data may suggest that complex 4-part fracture classification could be improved by 3D reconstruction. A few studies have corroborated this assertion: Mora-Guix et al. showed that despite the little overall value of CT imaging, it did improve identification of number of fragments [<xref ref-type="bibr" rid="B23">23</xref>]. Additionally, Brien et al. cited that the largest point of contention in their inter-observer study was agreeing upon 4-part fractures, and the surgeons would benefit from CT scans in that regard [<xref ref-type="bibr" rid="B5">5</xref>].</p><p>The literature regarding inter-observer agreement of fracture classifications appears to converge on the following paradigm: low inter-observer agreement is largely caused by compromised interpretation of the imaging, which is caused by imprecise measurements of the pathoanatomy. Moreover, Neer described patient, procedural, and clinical variability as causes of these imprecise measurements [<xref ref-type="bibr" rid="B22">22</xref>], and a few studies have improved precision through education [<xref ref-type="bibr" rid="B6">6</xref>,<xref ref-type="bibr" rid="B8">8</xref>,<xref ref-type="bibr" rid="B16">16</xref>]. Important to remember is that "the 4-segment classification is not a radiographic system but is a pathoanatomic classification of fracture displacement [<xref ref-type="bibr" rid="B22">22</xref>]." Our study and others have underscored the difficulty in categorizing a 3D concept with 3D images displayed on a 2D screen. Perhaps further studies with experienced users of advanced technology or stereo-visualization need to be evaluated for observer agreement possibly with correlation to intra-operative findings.</p><p>There were several study limitations. First, it should be understood that these conclusions are based on an experimental model; thus the distribution of Neer Fractures is not reflective of that which would be experienced in a clinical setting. Furthermore, the observers were not privileged to any patient demographic information, which certainly influences a treating surgeon's decision algorithm. Further studies evaluating agreement of treatment based on a more complete clinical picture would have a broader application. The number of cases (sixteen) presented to the observers was a limitation, and a power analysis was not performed in the selection of this number; however, it was chosen to provide an adequate breadth of cases without resulting in observer fatigue, which might have confounded the results. Additionally, the observers were not able to combine or manipulate images, as they might in a clinical setting. Gonimeters or rulers were also not provided but have been shown to be ignored in clinical setting even when available [<xref ref-type="bibr" rid="B11">11</xref>]. The image sets were pre-selected, which imparts a selection bias. Also, treatment comparisons are inherently biased by the observer's comfort level with a procedure and by their experience with the fracture classification, which also may have changed if they were given the opportunity to combine modalities. Observers may have also been limited by their specific experience with 3D technology.</p></sec><sec><title>Summary</title><p>In examining the inter-observer agreement with kappa values for X-ray, CT scan, and 3D reconstruction for fracture classification and treatment, we conclude that although 3D reconstruction showed a slight improvement in the inter-observer agreement for fracture classification among specialized upper extremity surgeons, overall all imaging modalities yielded low inter-observer agreement for both classification and treatment.</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>AF conceived the study design and participated in data collection. RT wrote the manuscript, constructed the tables and graphs, edited the imaging, revised the statistical methods, and performed the literature search. JR participated in data collection. JG performed the statistical analysis. AI revised the final manuscript, revised the study design, and oversaw all aspects pertaining to the current study. All authors read and approved the final manuscript.</p></sec></body><back><ref-list><ref id="B1"><mixed-citation publication-type="journal"><name><surname>Baron</surname><given-names>JA</given-names></name><name><surname>Barrett</surname><given-names>JA</given-names></name><name><surname>Karagas</surname><given-names>MR</given-names></name><article-title>The epidemiology of peripheral fractures</article-title><source>Bone</source><year>1996</year><volume>18</volume><issue>3 suppl</issue><fpage>209</fpage><lpage>13S</lpage></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><name><surname>Neer</surname><given-names>CS</given-names></name><article-title>Displaced proximal humeral fractures. I. Classification and evaluation</article-title><source>J Bone Joint Surg Am</source><year>1970</year><volume>52</volume><fpage>1077</fpage><lpage>89</lpage><pub-id pub-id-type="pmid">5455339</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><name><surname>Neer</surname><given-names>CS</given-names></name><article-title>Displaced proximal humeral fractures. II. Treatment of three-part and four-part displacement</article-title><source>J Bone Joint Surg Am</source><year>1970</year><volume>52</volume><fpage>1090</fpage><lpage>103</lpage><pub-id pub-id-type="pmid">5455340</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><name><surname>Bernstein</surname><given-names>J</given-names></name><name><surname>Adler</surname><given-names>LM</given-names></name><name><surname>Blank</surname><given-names>JE</given-names></name><name><surname>Dalsey</surname><given-names>RM</given-names></name><name><surname>Williams</surname><given-names>GR</given-names></name><name><surname>Iannotti</surname><given-names>JP</given-names></name><article-title>Evaluation of the Neer system of classification of proximal humeral fractures with computerized tomographic scans and plain radiographs</article-title><source>J Bone Joint Surg Am</source><year>1996</year><volume>78</volume><fpage>1371</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">8816653</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><name><surname>Brien</surname><given-names>H</given-names></name><name><surname>Noftall</surname><given-names>F</given-names></name><name><surname>MacMaster</surname><given-names>S</given-names></name><name><surname>Cummings</surname><given-names>T</given-names></name><name><surname>Landells</surname><given-names>C</given-names></name><name><surname>Rockwood</surname><given-names>P</given-names></name><article-title>Neer's classification system: a critical appraisal</article-title><source>J Trauma</source><year>1995</year><volume>38</volume><fpage>257</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1097/00005373-199502000-00022</pub-id><pub-id pub-id-type="pmid">7869449</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><name><surname>Brorson</surname><given-names>S</given-names></name><name><surname>Bagger</surname><given-names>J</given-names></name><name><surname>Sylvest</surname><given-names>A</given-names></name><name><surname>Hrobjartsson</surname><given-names>A</given-names></name><article-title>Improved interobserver variation after training of doctors in the Neer system</article-title><source>A randomised trial J Bone Joint Surg Br</source><year>2002</year><volume>84</volume><fpage>950</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1302/0301-620X.84B7.13010</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><name><surname>Brorson</surname><given-names>S</given-names></name><name><surname>Bagger</surname><given-names>J</given-names></name><name><surname>Sylvest</surname><given-names>A</given-names></name><name><surname>Hrobjartsson</surname><given-names>A</given-names></name><article-title>Low agreement among 24 doctors using the Neer-classification; only moderate agreement on displacement, even between specialists</article-title><source>Int Orthop</source><year>2002</year><volume>26</volume><fpage>271</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1007/s00264-002-0369-x</pub-id><pub-id pub-id-type="pmid">12378351</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><name><surname>Brorson</surname><given-names>S</given-names></name><name><surname>Hr&#x000f3;bjartsson</surname><given-names>A</given-names></name><article-title>Training improves agreement among doctors using the Neer system for proximal humeral fractures in a systematic review</article-title><source>J Clin Epidemiol</source><year>2008</year><volume>61</volume><issue>1</issue><fpage>7</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/j.jclinepi.2007.04.014</pub-id><pub-id pub-id-type="pmid">18083458</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><name><surname>Brorson</surname><given-names>S</given-names></name><name><surname>Bagger</surname><given-names>J</given-names></name><name><surname>Sylvest</surname><given-names>A</given-names></name><name><surname>Hrobjartsson</surname><given-names>A</given-names></name><article-title>Diagnosing displaced four-part fractures of the proximal humerus: a review of observer studies</article-title><source>Int Orthop</source><year>2009</year><volume>33</volume><issue>2</issue><fpage>323</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1007/s00264-008-0591-2</pub-id><pub-id pub-id-type="pmid">18536918</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><name><surname>Sallay</surname><given-names>PI</given-names></name><name><surname>Pedowitz</surname><given-names>RA</given-names></name><name><surname>Mallon</surname><given-names>WJ</given-names></name><name><surname>Vandemark</surname><given-names>RM</given-names></name><name><surname>Dalton</surname><given-names>JD</given-names></name><name><surname>Speer</surname><given-names>KP</given-names></name><article-title>Reliability and reproducibility of radiographic interpretation of proximal humeral fracture pathoanatomy</article-title><source>J Shoulder Elbow Surg</source><year>1997</year><volume>6</volume><fpage>60</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/S1058-2746(97)90072-0</pub-id><pub-id pub-id-type="pmid">9071684</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><name><surname>Sidor</surname><given-names>ML</given-names></name><name><surname>Zuckerman</surname><given-names>JD</given-names></name><name><surname>Lyon</surname><given-names>T</given-names></name><name><surname>Koval</surname><given-names>K</given-names></name><name><surname>Cuomo</surname><given-names>F</given-names></name><name><surname>Schoenberg</surname><given-names>N</given-names></name><article-title>The Neer classification system for proximal humeral fractures. An assessment of interobserver reliability and intraobserver reproducibility</article-title><source>J Bone Joint Surg Am</source><year>1993</year><volume>75</volume><fpage>1745</fpage><lpage>50</lpage><pub-id pub-id-type="pmid">8258543</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><name><surname>Sidor</surname><given-names>ML</given-names></name><name><surname>Zuckerman</surname><given-names>JD</given-names></name><name><surname>Lyon</surname><given-names>T</given-names></name><name><surname>Koval</surname><given-names>K</given-names></name><name><surname>Schoenberg</surname><given-names>N</given-names></name><article-title>Classification of proximal humerus fractures: the contribution of the scapular and axillary radiographs</article-title><source>J Shoulder Elbow Surg</source><year>1994</year><volume>3</volume><fpage>24</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/S1058-2746(09)80004-9</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><name><surname>Siebenrock</surname><given-names>KA</given-names></name><name><surname>Gerber</surname><given-names>C</given-names></name><article-title>The reproducibility of classification of fractures of the proximal end of the humerus</article-title><source>J Bone Joint Surg Am</source><year>1993</year><volume>75</volume><fpage>1751</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">8258544</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><name><surname>Sjoden</surname><given-names>GOJ</given-names></name><name><surname>Movin</surname><given-names>T</given-names></name><name><surname>Guntner</surname><given-names>P</given-names></name><name><surname>Aspelin</surname><given-names>P</given-names></name><name><surname>Ahrengart</surname><given-names>L</given-names></name><name><surname>Ersmark</surname><given-names>H</given-names></name><etal/><article-title>Poor reproducibility of classification of proximal humeral fractures. Additional CT of minor value</article-title><source>Acta Orthop Scand</source><year>1997</year><volume>68</volume><fpage>239</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.3109/17453679708996692</pub-id><pub-id pub-id-type="pmid">9246984</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><name><surname>Sjoden</surname><given-names>GOJ</given-names></name><name><surname>Movin</surname><given-names>T</given-names></name><name><surname>Aspelin</surname><given-names>P</given-names></name><name><surname>Guntner</surname><given-names>P</given-names></name><name><surname>Shalabi</surname><given-names>A</given-names></name><article-title>3D-radiographic analysis does not improve the Neer and AO classifications of proximal humeral fractures</article-title><source>Acta Orthop Scand</source><year>1999</year><volume>70</volume><fpage>325</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.3109/17453679908997818</pub-id><pub-id pub-id-type="pmid">10569259</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><name><surname>Shrader</surname><given-names>MW</given-names></name><name><surname>Sanchez-Sotelo</surname><given-names>J</given-names></name><name><surname>Sperling</surname><given-names>JW</given-names></name><name><surname>Rowland</surname><given-names>CM</given-names></name><name><surname>Cofield</surname><given-names>RH</given-names></name><article-title>Understanding proximal humerus fractures: image analysis, classification, and treatment</article-title><source>J Shoulder Elbow Surg</source><year>2005</year><volume>14</volume><fpage>497</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1016/j.jse.2005.02.014</pub-id><pub-id pub-id-type="pmid">16194741</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><name><surname>Kristansen</surname><given-names>B</given-names></name><name><surname>Anderson</surname><given-names>UL</given-names></name><name><surname>Olsen</surname><given-names>CA</given-names></name><etal/><article-title>The Neer classification of fractures of the proximal humerus. An assessment of interobserver variation</article-title><source>Skeletal Radiol</source><year>1988</year><volume>17</volume><fpage>420</fpage><lpage>2</lpage><pub-id pub-id-type="doi">10.1007/BF00361661</pub-id><pub-id pub-id-type="pmid">3238441</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><name><surname>Petit</surname><given-names>CJ</given-names></name><name><surname>Millett</surname><given-names>PJ</given-names></name><name><surname>Endres</surname><given-names>NK</given-names></name><name><surname>Diller</surname><given-names>D</given-names></name><name><surname>Harris</surname><given-names>MB</given-names></name><name><surname>Warner</surname><given-names>JP</given-names></name><article-title>Management of proximal humeral fractures: Surgeons don't agree</article-title><source>J Shoulder Elbow Surg</source><year>2010</year><volume>19</volume><fpage>446</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1016/j.jse.2009.06.012</pub-id><pub-id pub-id-type="pmid">19800259</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><name><surname>Frandsen</surname><given-names>PA</given-names></name><name><surname>Andersen</surname><given-names>E</given-names></name><name><surname>Madsen</surname><given-names>F</given-names></name><name><surname>Skjadt</surname><given-names>T</given-names></name><article-title>Garden's classification of femoral neck fractures. An assessment of interobserver variation</article-title><source>J Bone and Joint Surg</source><year>1988</year><volume>70-B</volume><issue>4</issue><fpage>588</fpage><lpage>590</lpage></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><name><surname>Nielsen</surname><given-names>J0</given-names></name><name><surname>Dons-Jensen</surname><given-names>H</given-names></name><name><surname>Sorensen</surname><given-names>HT</given-names></name><article-title>Lauge-Hansen classification of malleolar fractures. An assessment of the reproducibility in 118 cases</article-title><source>Acta Orthop Scandinavica</source><year>1990</year><volume>61</volume><fpage>385</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.3109/17453679008993545</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><name><surname>Thomsen</surname><given-names>NOB</given-names></name><name><surname>Olsen</surname><given-names>LH</given-names></name><name><surname>Nielsen</surname><given-names>ST</given-names></name><article-title>Kappa statistics in the assessment of observer variation: the significance of multiple observers classifying ankle fractures</article-title><source>J Orthop Sci</source><year>2002</year><volume>7</volume><fpage>163</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1007/s007760200028</pub-id><pub-id pub-id-type="pmid">11956974</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><name><surname>Neer</surname><given-names>CS</given-names></name><article-title>Four-segment classification of proximal humeral fractures: purpose and reliable use</article-title><source>J Shoulder Elbow Surg</source><year>2002</year><volume>11</volume><fpage>389</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1067/mse.2002.124346</pub-id><pub-id pub-id-type="pmid">12195260</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><name><surname>Mora-Guix</surname><given-names>JM</given-names></name><name><surname>Gonzalez</surname><given-names>AS</given-names></name><name><surname>Brugalla</surname><given-names>JV</given-names></name><name><surname>Carril</surname><given-names>EC</given-names></name><name><surname>Ba&#x000f1;os</surname><given-names>FG</given-names></name><article-title>Proposed protocol for reading images of humeral head fractures</article-title><source>Clin Orthop Relat Res</source><year>2006</year><volume>448</volume><comment>225 33</comment></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><name><surname>Castagno</surname><given-names>AA</given-names></name><name><surname>Shuma</surname><given-names>WP</given-names></name><name><surname>Kilcoyne</surname><given-names>RF</given-names></name><etal/><article-title>Complex fractures of the proximal humerus: role of CT in treatment</article-title><source>Radiology</source><year>1987</year><volume>165</volume><fpage>759</fpage><lpage>62</lpage><pub-id pub-id-type="pmid">3685356</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><name><surname>Brunner</surname><given-names>A</given-names></name><name><surname>Honigmann</surname><given-names>P</given-names></name><name><surname>Treumann</surname><given-names>T</given-names></name><name><surname>Babst</surname><given-names>R</given-names></name><article-title>The impact of stereo-visualisation of three-dimensional CT datasets on the inter- and intraobserver reliability of the AO/OTA and Neer classifications in the assessment of fractures of the proximal humerus</article-title><source>J Bone Joint Surg Br</source><year>2009</year><volume>91</volume><issue>6</issue><fpage>766</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1302/0301-620X.91B6.22109</pub-id><pub-id pub-id-type="pmid">19483230</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>J</given-names></name><article-title>A coefficient of agreement for nominal scales</article-title><source>Educ Psychol Meas</source><year>1960</year><volume>20</volume><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><name><surname>Fleiss</surname><given-names>J</given-names></name><article-title>Measuring nominal scale agreement among many raters</article-title><source>Pyschol Bull</source><year>1971</year><volume>76</volume><issue>5</issue><fpage>378</fpage><lpage>382</lpage></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><name><surname>Landis</surname><given-names>JR</given-names></name><name><surname>Koch</surname><given-names>GG</given-names></name><article-title>Measurement of observer agreement for categorical data</article-title><source>Biometrics</source><year>1977</year><volume>33</volume><fpage>159</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.2307/2529310</pub-id><pub-id pub-id-type="pmid">843571</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><name><surname>Burstein</surname><given-names>AH</given-names></name><article-title>Fracture classification systems: do they work and are they useful?</article-title><source>J Bone Joint Surg Am</source><year>1993</year><volume>75</volume><fpage>1743</fpage><lpage>1744</lpage><pub-id pub-id-type="pmid">8258542</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><name><surname>Neer</surname><given-names>CS</given-names><suffix>II</suffix></name><article-title>Fracture classification systems. Do they work and are they useful? [correspondence]</article-title><source>J Bone Joint Surg Am</source><year>1994</year><volume>76</volume><fpage>789</fpage><pub-id pub-id-type="pmid">8175826</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><name><surname>Rockwood</surname><given-names>CA</given-names></name><article-title>Fracture classification systems. Do they work and are they useful? [correspondence]</article-title><source>J Bone Joint Surg Am</source><year>1994</year><volume>76</volume><fpage>790</fpage><pub-id pub-id-type="pmid">8018176</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><name><surname>Bigliani</surname><given-names>LU</given-names></name><name><surname>Flatow</surname><given-names>EL</given-names></name><name><surname>Pollock</surname><given-names>RG</given-names></name><article-title>Fracture classification systems. Do they work and are they useful? [correspondence]</article-title><source>J Bone Joint Surg Am</source><year>1994</year><volume>76</volume><fpage>790</fpage><lpage>1</lpage><pub-id pub-id-type="pmid">8175827</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><name><surname>Cuomo</surname><given-names>F</given-names></name><article-title>Fracture classifications systems. Do they work and are they useful? [correspondence]</article-title><source>J Bone Joint Surg Am</source><year>1994</year><volume>76</volume><fpage>792</fpage><pub-id pub-id-type="pmid">8175828</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><name><surname>Bernstein</surname><given-names>J</given-names></name><article-title>Fracture classification systems. Do they work and are they useful? [correspondence]</article-title><source>J Bone Joint Surg Am</source><year>1994</year><volume>76</volume><fpage>792</fpage><lpage>3</lpage><comment>299-301</comment><pub-id pub-id-type="pmid">8175829</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><name><surname>Brennan</surname><given-names>R</given-names></name><name><surname>Prediger</surname><given-names>D</given-names></name><article-title>Coefficient kappa: some uses, misuses, and alternatives</article-title><source>Educational and Psychological Measurement</source><year>1981</year><volume>41</volume><fpage>687</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1177/001316448104100307</pub-id></mixed-citation></ref></ref-list></back></article>
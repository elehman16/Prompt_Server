<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Iperception</journal-id><journal-id journal-id-type="iso-abbrev">Iperception</journal-id><journal-id journal-id-type="publisher-id">IPE</journal-id><journal-id journal-id-type="hwp">spipe</journal-id><journal-title-group><journal-title>i-Perception</journal-title></journal-title-group><issn pub-type="epub">2041-6695</issn><publisher><publisher-name>SAGE Publications</publisher-name><publisher-loc>Sage UK: London, England</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">29774139</article-id><article-id pub-id-type="pmc">5950935</article-id><article-id pub-id-type="doi">10.1177/2041669518755806</article-id><article-id pub-id-type="publisher-id">10.1177_2041669518755806</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Line-Drawn Scenes Provide Sufficient Information for Discrimination of Threat and Mere Negativity</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Boshyan</surname><given-names>Jasmine</given-names></name><xref ref-type="corresp" rid="corresp1-2041669518755806"/></contrib><aff id="aff1-2041669518755806">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA, USA; Department of Radiology, Harvard Medical School, Boston, MA, USA</aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Feldman Barrett</surname><given-names>Lisa</given-names></name></contrib><aff id="aff2-2041669518755806">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA, USA; Department of Psychology, Northeastern University, Boston, MA, USA; Department of Psychiatry, Massachusetts General Hospital, Charlestown, MA, USA</aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Betz</surname><given-names>Nicole</given-names></name></contrib><aff id="aff3-2041669518755806">Department of Psychology, Northeastern University, Boston, MA, USA</aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Adams</surname><given-names>Reginald B.</given-names><suffix>Jr.</suffix></name></contrib><aff id="aff4-2041669518755806">Department of Psychology, The <institution-wrap><institution-id institution-id-type="Ringgold">8082</institution-id><institution content-type="university">Pennsylvania State University</institution></institution-wrap>, University Park, PA, USA</aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Kveraga</surname><given-names>Kestutis</given-names></name></contrib><aff id="aff5-2041669518755806">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA, USA; Department of Radiology, Harvard Medical School, Boston, MA, USA</aff></contrib-group><author-notes><corresp id="corresp1-2041669518755806">Jasmine Boshyan, Department of Radiology, Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, MA 02129, USA. Email: <email>jboshyan@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2018</year></pub-date><pub-date pub-type="collection"><season>Jan-Feb</season><year>2018</year></pub-date><volume>9</volume><issue>1</issue><elocation-id>2041669518755806</elocation-id><history><date date-type="received"><day>5</day><month>10</month><year>2017</year></date><date date-type="accepted"><day>2</day><month>1</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><copyright-year>2018</copyright-year><copyright-holder content-type="society">SAGE Publications Ltd. Manuscript content on this site is licensed under Creative Commons Licenses</copyright-holder><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>Creative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License (<ext-link ext-link-type="uri" xlink:href="http://www.creativecommons.org/licenses/by/4.0/">http://www.creativecommons.org/licenses/by/4.0/</ext-link>) which permits any use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages (<ext-link ext-link-type="uri" xlink:href="https://us.sagepub.com/en-us/nam/open-access-at-sage">https://us.sagepub.com/en-us/nam/open-access-at-sage</ext-link>).</license-p></license></permissions><abstract><p>Previous work using color photographic scenes has shown that human observers are keenly sensitive to different types of threatening and negative stimuli and reliably classify them by the presence, and spatial and temporal directions of threat. To test whether such distinctions can be extracted from impoverished visual information, we used 500&#x02009;line drawings made by hand-tracing the original set of photographic scenes. Sixty participants rated the scenes on spatial and temporal dimensions of threat. Based on these ratings, trend analysis revealed five scene categories that were comparable to those identified for the matching color photographic scenes. Another 61 participants were randomly assigned to rate the valence or arousal evoked by the line drawings. The line drawings perceived to be the most negative were also perceived to be the most arousing, replicating the finding for color photographic scenes. We demonstrate here that humans are very sensitive to the spatial and temporal directions of threat even when they must extract this information from simple line drawings, and rate the line drawings very similarly to matched color photographs. The set of 500 hand-traced line-drawing scenes has been made freely available to the research community: <ext-link ext-link-type="uri" xlink:href="http://www.kveragalab.org/threat.html">http://www.kveragalab.org/threat.html</ext-link>.</p></abstract><kwd-group><kwd>threat perception</kwd><kwd>line drawings</kwd><kwd>affect</kwd><kwd>emotion</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>cover-date</meta-name><meta-value>January-February 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1-2041669518755806"><title>Introduction</title><p>Our visual system has been honed by the need for survival. From a continuous flood of visual information, our eyes and brain quickly extract meaning and scan for signs of danger, as rapid recognition of threat promotes survival (<xref rid="bibr36-2041669518755806" ref-type="bibr">LeDoux, 2012</xref>). Previous studies have shown that stimuli are automatically represented in terms of their affective valence (<xref rid="bibr2-2041669518755806" ref-type="bibr">Barrett, 2006</xref>; <xref rid="bibr16-2041669518755806" ref-type="bibr">Duckworth, Bargh, Garcia, &#x00026; Chaiken, 2002</xref>). But dimensional approaches to affective perception that map the valence and arousal of stimuli do not distinguish between unpleasant images that are perceived as aversive and threatening and those that are not (for a review, see <xref rid="bibr3-2041669518755806" ref-type="bibr">Barrett &#x00026; Bliss-Moreau, 2009</xref>; <xref rid="bibr4-2041669518755806" ref-type="bibr">Barrett &#x00026; Russell, 1999</xref>; <xref rid="bibr32-2041669518755806" ref-type="bibr">Larsen &#x00026; Diener, 1992</xref>; <xref rid="bibr56-2041669518755806" ref-type="bibr">Russell, 1980</xref>; <xref rid="bibr63-2041669518755806" ref-type="bibr">Watson &#x00026; Tellegen, 1985</xref>). We have shown that not all negative stimuli are threatening, and that merely negative, but not threatening, stimuli evoke qualitatively different brain activation patterns (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>). For example, imminent threat and accident scenes are both unpleasant situations, but responses to these types of images are quite different. While detecting imminent danger is critical for immediate survival, identifying appetitive stimuli and learning to avoid potential future threats are essential for long-term wellness and survival. Indeed, a recent study has confirmed that human observers exhibit &#x0201c;morbid curiosity&#x0201d; for socially negative, but not threatening, images (<xref rid="bibr52-2041669518755806" ref-type="bibr">Oosterwijk, 2017</xref>).</p><p>It has been proposed that responses to some types of threat stimuli such as spiders, snakes, and angry faces may be innate and shaped by evolution (<xref rid="bibr10-2041669518755806" ref-type="bibr">Brosch, Pourtois, &#x00026; Sander, 2010</xref>; <xref rid="bibr24-2041669518755806" ref-type="bibr">Isbell, 2006</xref>; <xref rid="bibr35-2041669518755806" ref-type="bibr">Le et&#x000a0;al., 2013</xref>; <xref rid="bibr47-2041669518755806" ref-type="bibr">New, Cosmides, &#x00026; Tooby, 2007</xref>; <xref rid="bibr58-2041669518755806" ref-type="bibr">Seligman, 1971</xref>). For example, it has been demonstrated that spiders and snakes are detected more rapidly than mushrooms and flowers (<xref rid="bibr49-2041669518755806" ref-type="bibr">&#x000d6;hman, Flykt, &#x00026; Esteves, 2001</xref>) and angry faces are detected faster than neutral faces and happy faces (<xref rid="bibr18-2041669518755806" ref-type="bibr">Eastwood, Smilek, &#x00026; Merikle, 2001</xref>; <xref rid="bibr21-2041669518755806" ref-type="bibr">Hansen &#x00026; Hansen, 1988</xref>; <xref rid="bibr22-2041669518755806" ref-type="bibr">Horstmann, 2007</xref>; <xref rid="bibr61-2041669518755806" ref-type="bibr">Tipples, Atkinson, &#x00026; Young, 2002</xref>). However, the latter effect is clearly seen in schematic, line-drawn faces (<xref rid="bibr51-2041669518755806" ref-type="bibr">&#x000d6;hman, Soares, Juth, Lindstr&#x000f6;m, &#x00026; Esteves, 2012</xref>), but not necessarily in photographic faces because of the higher salience, or &#x0201c;vividness&#x0201d;, of happy, smiling faces (e.g., <xref rid="bibr5-2041669518755806" ref-type="bibr">Becker, Anderson, Mortensen, Neufeld, &#x00026; Neel, 2011</xref>; <xref rid="bibr6-2041669518755806" ref-type="bibr">Becker &#x00026; Srinivasan, 2014</xref>). Similar results were reported for children (<xref rid="bibr37-2041669518755806" ref-type="bibr">LoBue, 2009</xref>, <xref rid="bibr38-2041669518755806" ref-type="bibr">2010a</xref>; <xref rid="bibr40-2041669518755806" ref-type="bibr">LoBue &#x00026; DeLoache, 2008</xref>) and infants (<xref rid="bibr41-2041669518755806" ref-type="bibr">LoBue &#x00026; DeLoache, 2010</xref>). Threatening faces are processed faster than are other facial expressions (<xref rid="bibr57-2041669518755806" ref-type="bibr">Schupp et&#x000a0;al., 2004</xref>). In addition, saccadic eye movements orient more quickly to images of threatening compared to neutral faces and body postures (<xref rid="bibr1-2041669518755806" ref-type="bibr">Bannerman, Milders, de Gelder, &#x00026; Sahraie, 2009</xref>) as well as towards emotional compared to neutral scenes (<xref rid="bibr48-2041669518755806" ref-type="bibr">Nummenmaa, Hyona, &#x00026; Calvo, 2009</xref>). While these studies support the view that spiders, snakes, and angry faces belong to a special class of stimuli that are perceptually prioritized due to their importance for survival, adults have been also shown to quickly detect modern threats, such as guns, knives, and syringes (e.g., <xref rid="bibr9-2041669518755806" ref-type="bibr">Blanchette, 2006</xref>). Using the same stimuli, <xref rid="bibr39-2041669518755806" ref-type="bibr">LoBue (2010b)</xref> reported that children only detected the syringes particularly quickly. These findings suggest that humans learn to detect novel threatening stimuli efficiently as a result of negative experiences. Together the research suggests that learning plays a vital role in threat perception. Humans and other primates not only have biases for the rapid detection of evolutionarily ancient threats such as snakes and spiders (<xref rid="bibr24-2041669518755806" ref-type="bibr">Isbell, 2006</xref>) but also have the flexibility to learn to efficiently detect new threats that are specific to our environments (<xref rid="bibr41-2041669518755806" ref-type="bibr">LoBue, Rakison, &#x00026; DeLoache, 2010</xref>).</p><p>In a recent study focusing on how humans perceive and process negative stimuli (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>), we presented color photographic scenes, including both natural threats (e.g., snakes, spiders, and carnivorans) as well as manmade threats (e.g., humans pointing guns and knives) classified a priori into four categories: Direct Threat, Indirect Threat, Threat Aftermath, and Low Threat situations (<xref ref-type="fig" rid="fig1-2041669518755806">Figure 1(a)</xref>). We demonstrated that humans discriminate these four types of stimuli quickly and accurately. The images containing direct, immediate threats were recognized most quickly, while Threat Aftermath (merely negative) scene images without imminent threats were processed much more slowly, with responses lagging behind even Low Threat images. In addition, we found that threatening and merely negative scene images activated different, though somewhat overlapping, networks of brain regions. Specifically, the scene images depicting threat situations differentially activated the amygdalae, periaqueductal gray, and orbitofrontal cortex, while merely negative, Threat Aftermath scenes evoked stronger activity in the parahippocampal, retrosplenial, medial and lateral prefrontal, and lateral temporal cortices (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>; <xref ref-type="fig" rid="fig2-2041669518755806">Figure 2</xref>).
<fig id="fig1-2041669518755806" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>(a) Examples of color photographic scene stimuli used in <xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al. (2015)</xref> depicting human and animal Direct Threat, Indirect Threat, Threat Aftermath, and Low Threat situations. (b) Line-drawing scene stimuli used in the current study.</p></caption><graphic xlink:href="10.1177_2041669518755806-fig1"/></fig>
<fig id="fig2-2041669518755806" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Eight unique patterns capturing 500 images in our stimulus set based on trend analysis. Patterns 1 (<italic>n</italic>&#x02009;=&#x02009;85), 2 (<italic>n</italic>&#x02009;=&#x02009;34), and 3 (<italic>n</italic>&#x02009;=&#x02009;51) all captured images that depicted scenes posing high threat to the observer, receiving high ratings from <italic>Harm to you</italic> group. Both Patterns 4 (<italic>n</italic>&#x02009;=&#x02009;80) and 5 (<italic>n</italic>&#x02009;=&#x02009;97) captured images depicting scenes posing high threat to someone else, receiving highest ratings from <italic>Harm to other</italic> group. Patterns 6 (<italic>n</italic>&#x02009;=&#x02009;59) and 7 (<italic>n</italic>&#x02009;=&#x02009;23) both captured images depicting scenes where harm (physical, psychological, or both) has already happened, receiving highest ratings from <italic>Past harm</italic> group. Finally, Pattern 8 (<italic>n</italic>&#x02009;=&#x02009;71) captured images depicting low level of threat either to the observer or someone else that were rated equally low by all three groups.</p></caption><graphic xlink:href="10.1177_2041669518755806-fig2"/></fig></p><p>Line drawings have been long used as a tool for studying object and scene perception because they are much easier to systematically manipulate (e.g., altering eye gaze, body poses, presence or absence of a certain object and so on) compared to photographic images (<xref rid="bibr8-2041669518755806" ref-type="bibr">Biederman, Mezzanotte, &#x00026; Rabinowitz, 1982</xref>; <xref rid="bibr29-2041669518755806" ref-type="bibr">Kveraga, Boshyan, &#x00026; Bar, 2007</xref>; <xref rid="bibr53-2041669518755806" ref-type="bibr">Palmer, 1975</xref>; <xref rid="bibr62-2041669518755806" ref-type="bibr">Walther, Chai, Caddigan, Beck, &#x00026; Li, 2011</xref>). For example, in their seminal studies, <xref rid="bibr8-2041669518755806" ref-type="bibr">Biederman et&#x000a0;al. (1982)</xref> and <xref rid="bibr53-2041669518755806" ref-type="bibr">Palmer (1975)</xref> demonstrated the effects of context on object recognition by manipulating line-drawing scenes. In another study, <xref rid="bibr29-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al. (2007)</xref> used line drawings to investigate the contribution of the magnocellular and parvocellular visual pathways to object recognition. More recently, <xref rid="bibr62-2041669518755806" ref-type="bibr">Walther et&#x000a0;al. (2011)</xref> reported that despite the marked difference in nonaffective scene statistics, they were able to decode scene category from fMRI data for nonaffective line dawings just as well as from activity for color photographs, in primary visual cortex through parahippocampal place area and retrosplenial cortex. Even more remarkably, in these regions, error patterns for decoding from line drawings were very similar to those obtained using color photographs.</p><p>Several general and specialized visual stimulus sets have been developed to facilitate behavioral research on emotion, such as the International Affective Picture System (IAPS; <xref rid="bibr31-2041669518755806" ref-type="bibr">Lang, Bradley, &#x00026; Cuthbert, 2005</xref>), the Geneva Affective Picture Database (GAPED; <xref rid="bibr13-2041669518755806" ref-type="bibr">Dan-Glauser &#x00026; Scherer, 2011</xref>), and the Open Affective Standardized Image Set (OASIS; <xref rid="bibr26-2041669518755806" ref-type="bibr">Kurdi, Lozano, &#x00026; Banaji, 2017</xref>). IAPS images, for example, have been used in several thousand research studies since they were first made available to the research community. However, IAPS images are subject to copyright restrictions that prohibit their usage in online research studies. Because of this, researchers conducting online studies were forced to create visual stimuli in an ad hoc manner, which was time-consuming, inefficient, and limited the comparability and generalizability of research findings. The OASIS database was specifically developed to address this problem by providing an open-access standardized stimulus set containing affective images with corresponding normative affective ratings, which similarly to the IAPS included a broad spectrum of themes.</p><p>Even though it has been previously demonstrated that line-drawing images are a useful tool for studying objects and scenes, no line-drawn stimuli set of affective scenes exists to date. The goals of the present study were twofold: (a) to test whether distinctions in affective scene context that allow humans to discriminate different dimensions of threat and negativity can be extracted from very basic, impoverished visual information such as line drawings and (b) to compile a set of open-access images varying in their threat value that differ little in low-level features and are easier to manipulate compared to photographic images to facilitate studies of visual threat perception in neurotypical, as well as clinical, populations.</p></sec><sec sec-type="methods" id="sec2-2041669518755806"><title>Method</title><sec id="sec3-2041669518755806" sec-type="subjects"><title>Participants</title><sec id="sec4-2041669518755806"><title>Study 1</title><p>Sixty participants were recruited from Northeastern University and were rewarded with course credit for their participation. Eight participants were removed from the analyses due to poor performance (see Supplemental Tables 1 and 2 for details). The remaining 52 participants (27 males and 25 females) had a mean age of 19.49 (<italic>SD</italic>&#x02009;=&#x02009;1.50). Participants reported their ethnicity as European American (<italic>n</italic>&#x02009;=&#x02009;31), Asian (<italic>n</italic>&#x02009;=&#x02009;15), and African American (<italic>n</italic>&#x02009;=&#x02009;1). Five participants chose not to report their race.</p></sec><sec id="sec5-2041669518755806"><title>Study 2</title><p>Sixty-one participants were recruited from the Pennsylvania State University and were rewarded with course credit for their participation. One participant was removed from the analyses due to poor performance. The remaining 60 participants (29 males and 31 females) had a mean age of 19.28 (<italic>SD</italic>&#x02009;=&#x02009;1.22). Participants reported their ethnicity as European American (<italic>n</italic>&#x02009;=&#x02009;34), Asian (<italic>n</italic>&#x02009;=&#x02009;13), African American (<italic>n</italic>&#x02009;=&#x02009;9), and Hispanic American (<italic>n</italic>&#x02009;=&#x02009;2). Two participants chose not to report their race.</p></sec></sec><sec id="sec6-2041669518755806"><title>Images</title><p>We previously collected a set of 509 color photographic images depicting negative and neutral situations from Internet searches. As reported in <xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al. (2015)</xref>, prior to collecting any data, we had classified these images into four categories based on our initial impressions: (a) negative, Direct Threat (from the viewpoint of the observer); (b) negative, Indirect Threat (predominantly to others); (c) negative, Threat Aftermath (a possible past threat, but not a threat any longer); and (d) neutral, Low Threat. We selected 500 out of these images to create line drawings by hand-tracing the main contours of agents (humans and animals), objects, and backgrounds. The line drawings were 700&#x02009;&#x000d7;&#x02009;700 pixels, with a black foreground and white background (e.g., see <xref ref-type="fig" rid="fig1-2041669518755806">Figure 1b</xref>).</p></sec><sec id="sec7-2041669518755806" sec-type="methods"><title>Procedure</title><sec id="sec8-2041669518755806"><title>Study 1</title><p>Participants were invited to the laboratory where they first read and signed the consent form. They were then randomly assigned to one of three groups. Each group of participants only responded to one of the questions on a scale from 1 to 6 (from <italic>none</italic> to <italic>extreme</italic>). Five hundred line-drawing scene images were presented in a random order, and one of the following three questions was presented below each picture: (a) &#x0201c;How much harm might you be about to suffer if this was your view in this scene?&#x0201d; (<italic>Harm to you</italic> group), (b) &#x0201c;How much harm might someone else (not you) be about to suffer in this scene?&#x0201d; (<italic>Harm to other</italic> group), and (c) &#x0201c;How much harm might someone else (not you) have already suffered in this scene?&#x0201d; (<italic>Past harm</italic> group). Participants could view each stimulus as long as they wanted and enter their rating using keys 1 to 6 on the keyboard. The rating scale was placed below the image and the points of the scale were labeled as <italic>None</italic>, <italic>Little</italic>, <italic>Some</italic>, <italic>Quite a lot</italic>, <italic>Even more</italic>, and <italic>Extreme</italic>. Entering the rating replaced the current stimulus with the next one. In order to discourage participants from rushing through the ratings, the program would not move to the next image if the response was made sooner than 500&#x02009;ms after stimulus presentation. Instead, the image would stay on the screen for at least 500&#x02009;ms, and participants would have to repeat their response in order to proceed to the next image. Even though a 500&#x02009;ms threshold was employed to discourage participants from speeding, we also collected reaction times (RT) to screen for participants who nevertheless speeded through the task or took an extraordinarily long time to respond, as compared to other participants. After rating all the images, participants were asked to fill out a demographic information form, debriefed, and thanked for their time. All procedures were approved by the Northeastern University Institutional Review Board (IRB#11-03-35).</p></sec><sec id="sec9-2041669518755806"><title>Study 2</title><p>Participants were invited to the laboratory where they first read and signed the consent form. They were then randomly assigned to rate five hundred line-drawing scene images (presented in an individually randomized order) on only the valence dimension or only the arousal dimension using a 7-point Likert scale. We used image-focused instructions for valence and arousal ratings following the procedure described in <xref rid="bibr26-2041669518755806" ref-type="bibr">Kurdi et&#x000a0;al. (2017)</xref>. Participants could view each stimulus as long as they wanted and enter their rating using keys 1 to 7 on the keyboard. The rating scale was placed below the image. For the valence dimension, the word &#x0201c;Valence&#x0201d; was displayed above the rating scale and the points of the scale were labeled as <italic>Very positive</italic>, <italic>Moderately positive</italic>, <italic>Somewhat positive</italic>, <italic>Neutral</italic>, <italic>Somewhat negative</italic>, <italic>Moderately negative</italic>, and <italic>Very negative</italic>. For the arousal dimension, the word &#x0201c;Arousal&#x0201d; was displayed above the rating scale and the points of the scale were labeled as <italic>Very low</italic>, <italic>Moderately low</italic>, <italic>Somewhat low</italic>, <italic>Neither low nor high</italic>, <italic>Somewhat high</italic>, <italic>Moderately high</italic>, and <italic>Very high</italic>. Entering the rating replaced the current stimulus with the next one. After rating all the images, participants were asked to fill out a demographic information form, debriefed, and thanked for their time. All procedures were approved by the Pennsylvania State University Institutional Review Board (IRB#00005514).</p></sec></sec></sec><sec sec-type="results" id="sec10-2041669518755806"><title>Results</title><sec id="sec11-2041669518755806"><title>Study 1</title><p>The RT data were first examined to screen for evidence of speeding through the task. One-sample <italic>t</italic> tests comparing mean RT of each participant to the mean RT of their respective groups revealed that two participants were significantly faster as compared to other participants in their corresponding groups (both <italic>p</italic>s&#x02009;=&#x02009;.0001). Thus, we excluded their data from further analyses. In addition, we examined the RT distribution of each participant, removing any responses that were 2 standard deviations above the mean RT of corresponding participant (<xref rid="bibr55-2041669518755806" ref-type="bibr">Ratcliff, 1993</xref>). This procedure resulted in the removal of 3.73% of the data. Next, we examined interrater reliability for each group. This analysis identified six additional participants whose responses were significantly different from the responses of other participants in their respective groups. The resulting mean interreliability across all three rating groups was .979 (<italic>Harm to you</italic> group: <italic>n</italic>&#x02009;=&#x02009;18, .986; <italic>Harm to other</italic> group: <italic>n</italic>&#x02009;=&#x02009;17, .983; <italic>Past harm</italic> group: <italic>n</italic>&#x02009;=&#x02009;17, .969).<sup><xref ref-type="fn" rid="fn1-2041669518755806">1</xref></sup></p><sec id="sec12-2041669518755806"><title>Patterns</title><p>Means and standard deviations were calculated for each image as rated by each group. We then ran one-way ANOVAs on ratings from the three groups for each image to find the best fitting pattern for each image by fitting eight contrasts capturing different patterns (<xref ref-type="fig" rid="fig2-2041669518755806">Figure 2</xref>).<sup><xref ref-type="fn" rid="fn2-2041669518755806">2</xref></sup> Based on the ratings across the three groups, we identified three patterns of images depicting scenes posing high threat to the observer (Direct Threat). Pattern 1 (<italic>n</italic>&#x02009;=&#x02009;85) included images depicting scenes where the observer was the target of potential harm, but where potential harm to someone else was less, and already occurring harm was even less. Pattern 2 (<italic>n</italic>&#x02009;=&#x02009;34) included images depicting scenes where the observer was the target of potential harm with lower but equal potential harm to someone else and already occurring harm. Like Patterns 1 and 2, images captured by Pattern 3 (<italic>n</italic>&#x02009;=&#x02009;51) received high ratings by <italic>Harm to you</italic> group. However, unlike the other two patterns, these images were rated equally high by <italic>Harm to other</italic> group, followed by lower ratings by <italic>Past harm</italic> group. While examining the content of the scenes in the Direct Threat category we did not see any obvious differences between the images fitting Patterns 1 and 2, whereas stimuli fitting Pattern 3 depicted scenes with deadly weapons pointing directly at the observer. It is worth noting that none of the images fitting Pattern 3 included any animals. In addition, participants in <italic>Harm to you</italic> group rated stimuli fitting Pattern 3 (<italic>M</italic>&#x02009;=&#x02009;5.36, <italic>SD</italic>&#x02009;=&#x02009;.06) significantly higher as compared to images fitting both Pattern 1 (<italic>M</italic>&#x02009;=&#x02009;4.67, <italic>SD</italic>&#x02009;=&#x02009;.05), <italic>t</italic>&#x02009;=&#x02009;8.41, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001 and Pattern 2 (<italic>M</italic>&#x02009;=&#x02009;4.78, <italic>SD</italic>&#x02009;=&#x02009;.06), <italic>t</italic>&#x02009;=&#x02009;7.73, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, with no difference between the latter two, <italic>t</italic>&#x02009;=&#x02009;1.12, <italic>p</italic>&#x02009;=&#x02009;.265. Thus, we combined the stimuli fitting Patterns 1 and 2 into one Direct Threat category (<italic>n</italic>&#x02009;=&#x02009;119), but separated images fitting Pattern 3 into a category we called Deadly Threat (<italic>n</italic>&#x02009;=&#x02009;51).</p><p>Patterns 4 (<italic>n</italic>&#x02009;=&#x02009;80) and 5 (<italic>n</italic>&#x02009;=&#x02009;97) both captured images depicting scenes posing high threat to someone else (Indirect Threat). However, images captured by Pattern 4 received highest ratings by <italic>Harm to other</italic> group with equally lower ratings by <italic>Harm to you</italic> and <italic>Past harm</italic> groups, while images captured by Pattern 5 received equally high ratings by <italic>Harm to other</italic> and <italic>Past harm</italic> groups with lower ratings by <italic>Harm to you</italic> group. No differences in content were found between images fitting Patterns 4 and 5 and participants in <italic>Harm to other</italic> group rated stimuli fitting Pattern 4 (<italic>M</italic>&#x02009;=&#x02009;4.39, <italic>SD</italic>&#x02009;=&#x02009;.73) and Pattern 5 (<italic>M</italic>&#x02009;=&#x02009;4.29, <italic>SD</italic>&#x02009;=&#x02009;.61) equally, <italic>t</italic>&#x02009;=&#x02009;.97, <italic>p</italic>&#x02009;=&#x02009;.334. Hence, we combined the stimuli of these two patterns into one Indirect Threat category (<italic>n</italic>&#x02009;=&#x02009;177).</p><p>Patterns 6 (<italic>n</italic>&#x02009;=&#x02009;59) and 7 (<italic>n</italic>&#x02009;=&#x02009;23) both captured images depicting scenes where harm (physical, psychological, or both) has already happened (Threat Aftermath). Images captured by Patterns 6 and 7 received highest ratings from <italic>Past harm</italic> group and lowest ratings from <italic>Harm to you</italic> group. However, <italic>Harm to other</italic> group rated images characterized by Pattern 6 higher than <italic>Harm to you</italic> group but lower than <italic>Past harm</italic> group, while they rated images captured by Pattern 7 equally low as <italic>Harm to you</italic> group. Visual examination of image content of scenes captured by Patterns 6 and 7 revealed that while both depicted images of obvious past harm, all images characterized by Pattern 7 depicted dead (or mortally injured) animals. However, because no statistically significant difference was found between the ratings of participants from <italic>Past harm</italic> group for images in Pattern 6 (<italic>M</italic>&#x02009;=&#x02009;4.34, <italic>SD</italic>&#x02009;=&#x02009;.95) and Pattern 7 (<italic>M</italic>&#x02009;=&#x02009;4.25, <italic>SD</italic>&#x02009;=&#x02009;.66), <italic>t</italic>&#x02009;=&#x02009;.42, <italic>p</italic>&#x02009;=&#x02009;.680, we combined these images into one Threat Aftermath category (<italic>n</italic>&#x02009;=&#x02009;82).</p><p>Finally, Pattern 8 (<italic>n</italic>&#x02009;=&#x02009;71) captured images depicting low level of threat either to the observer or someone else (Low Threat; e.g., an image of a woman walking a dog on a leash) that were rated equally low by all three groups.</p></sec><sec id="sec13-2041669518755806"><title>Image categories</title><p>Based on trend analyses we created five image categories (<xref ref-type="fig" rid="fig3-2041669518755806">Figure 3</xref>). <italic>Deadly Threat</italic> category (<italic>n</italic>&#x02009;=&#x02009;51) included images depicting scenes with deadly weapons pointing directly at the observer with equal potential harm to someone else, but lower possibility of harm already occurring. It is worth noting that none of the images in this category included any animals. <italic>Direct Threat</italic> category (<italic>n</italic>&#x02009;=&#x02009;119) included images depicting scenes where the observer was the target of potential harm, but where potential harm to someone else was less, and already occurring harm was even less. <italic>Indirect Threat</italic> category (<italic>n</italic>&#x02009;=&#x02009;177) included images depicting scenes with highest harm to someone else, lower possibility of harm already occurring, and the lowest potential harm to the observer. <italic>Threat Aftermath</italic> category (<italic>n</italic>&#x02009;=&#x02009;82) included images depicting scenes where past harm (physical, psychological, or both) has already occurred, with lower potential harm to someone else, and lowest potential harm to the observer. Finally, <italic>Low Threat</italic> category (<italic>n</italic>&#x02009;=&#x02009;71) included images depicting low level of harm to the observer or someone else, as well as low possibility of past harm already occurring.
<fig id="fig3-2041669518755806" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>Five distinct image categories with corresponding prototypical examples.</p></caption><graphic xlink:href="10.1177_2041669518755806-fig3"/></fig></p></sec><sec id="sec14-2041669518755806"><title>Mean ratings</title><p>To assess the statistical significance of the differences in the ratings, we ran a 3 (rating group)&#x02009;&#x000d7;&#x02009;5 (image category) repeated-measures ANOVA. In order to account for eventual biases due to unbalanced numbers of stimuli in each image category, we used Type II calculation for obtaining sums of squares.The analysis revealed a significant main effect of rating group (<italic>F</italic>(2,990)&#x02009;=&#x02009;122.48, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;</italic><sup>2</sup><sub><italic>p</italic></sub>&#x02009;=&#x02009;.198) with <italic>Harm to other</italic> group having the highest overall ratings as compared to <italic>Harm to you</italic> and <italic>Past harm</italic> groups (both <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001), with no significant difference between the latter two (<italic>Harm to you</italic> group: <italic>M</italic><sub>difference</sub>&#x02009;=&#x02009;3.31, <italic>SE</italic>&#x02009;=&#x02009;.03; <italic>Past harm</italic> group: <italic>M</italic><sub>difference</sub>&#x02009;=&#x02009;3.38, <italic>SE</italic>&#x02009;=&#x02009;.04, <italic>t</italic>(998)&#x02009;=&#x02009;2.27, <italic>p</italic>&#x02009;=&#x02009;.066). There was also a significant main effect of image category (<italic>F</italic>(4,495)&#x02009;=&#x02009;253.31, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;</italic><sup>2</sup><sub><italic>p</italic></sub>&#x02009;=&#x02009;.672). Except for a nonsignificant difference between Direct (<italic>M</italic>&#x02009;=&#x02009;3.80, <italic>SE</italic>&#x02009;=&#x02009;.06) and Indirect Threat (<italic>M</italic>&#x02009;=&#x02009;3.70, <italic>SE</italic>&#x02009;=&#x02009;.08) categories (<italic>t</italic>(294)&#x02009;=&#x02009;1.29, <italic>p</italic>&#x02009;=&#x02009;.884), all image categories were significantly different from each other as indicated by post hoc pairwise comparisons with a Bonferroni correction (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001). This nonsignificant difference between Direct Threat and Indirect Threat image categories, collapsing across rating groups, was due to the difference in ratings for these images among all the three groups of participants. Specifically, while participants in <italic>Harm to you</italic> group rated Direct Threat images higher than Indirect Threat images, the opposite was true for the participants in <italic>Harm to other</italic> and <italic>Past harm</italic> groups. Thus, when collapsing across the three groups, the difference between Direct Threat and Indirect Threat image categories was nonsignificant. This is illustrated by a significant interaction between the image category and rating group (<italic>F</italic>(8,990)&#x02009;=&#x02009;445.96, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;</italic><sup>2</sup><sub>p</sub>&#x02009;=&#x02009;.783; <xref ref-type="fig" rid="fig4-2041669518755806">Figure 4</xref>). <italic>Harm to you</italic> group had the highest ratings for the Deadly Threat image category compared to all other categories (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001), followed by Direct Threat (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001), Indirect Threat (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001), Threat Aftermath (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.017), and Low Threat (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.017) as indicated by pairwise comparisons with a Bonferroni correction. Similarly for <italic>Harm to other</italic> group, all pairwise comparisons with Bonferroni correction were significant (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001). Participants rated Deadly Threat images the highest followed by Indirect Threat, Direct Threat, Threat Aftermath, and Low Threat categories. For <italic>Past harm</italic> group, all pairwise comparisons with Bonferroni correction were significant (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.043), except for the comparison between the Deadly Threat (<italic>M</italic>&#x02009;=&#x02009;4.15, <italic>SE</italic>&#x02009;=&#x02009;.10) and Threat Aftermath image categories (<italic>M</italic>&#x02009;=&#x02009;4.31, <italic>SE</italic>&#x02009;=&#x02009;.08; <italic>t</italic>(131)&#x02009;=&#x02009;1.24, <italic>p</italic>&#x02009;=&#x02009;.908). Participants rated Threat Aftermath and Deadly Threat images the highest, followed by Indirect Threat, Direct Threat, and Low Threat categories.
<fig id="fig4-2041669518755806" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>Study 1 results. (a) Results of the scene ratings. The three panels show the results from the three rating groups. Participants in Group 1 (left panel) were asked to rate all images in terms of potential harm to them personally; participants in Group 2 (middle panel) were asked to rate all images in terms of potential harm to someone else; participants in Group 3 (right panel) were asked to rate all images in terms of past harm, but no present harm. 500 images were presented in a randomized mixed sequence. (b) Results of the RT. The error bars indicate standard error of the mean.</p></caption><graphic xlink:href="10.1177_2041669518755806-fig4"/></fig></p></sec><sec id="sec15-2041669518755806"><title>Reaction times</title><p>By design, we enforced a 500-ms threshold to discourage participants from speeding through the images; thus, the shortest RT in our collected data was 501&#x02009;ms. Even though the interpretation of our RT data is limited, we still examined any possible differences in RT between the rating groups as well as image categories by performing a 3 (rating group)&#x02009;&#x000d7;&#x02009;5 (image category) repeated-measures ANOVA using Type II calculation for obtaining sums of squares. The analysis revealed a significant main effect of image category (<italic>F</italic>(4,495)&#x02009;=&#x02009;50.39, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;</italic><sup>2</sup><sub><italic>p</italic></sub>&#x02009;=&#x02009;.289). Except for no statistically significant difference between the Direct Threat (<italic>M</italic>&#x02009;=&#x02009;2.10, <italic>SE</italic>&#x02009;=&#x02009;.04) and Low Threat (<italic>M</italic>&#x02009;=&#x02009;2.26, <italic>SE</italic>&#x02009;=&#x02009;.05) categories (<italic>t</italic>(188)&#x02009;=&#x02009;2.72, <italic>p</italic>&#x02009;=&#x02009;.064), all image categories were significantly different from each other as indicated by post hoc pairwise comparisons with a Bonferroni correction (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.006). Deadly Threat images were rated most quickly, followed by Direct Threat and Low Threat images, Indirect Threat, and finally Threat Aftermath images, which had the longest RT. There was also a significant main effect of rating group (<italic>F</italic>(2,990)&#x02009;=&#x02009;14.78, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;</italic><sup>2</sup><sub><italic>p</italic></sub>&#x02009;=&#x02009;.029), with <italic>Harm to other</italic> group having the longest overall RTs as compared to <italic>Harm to you</italic> and <italic>Past harm</italic> groups (both <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001), with no significant difference between the latter two (<italic>Harm to you</italic> group: <italic>M</italic><sub>difference</sub>&#x02009;=&#x02009;2.20, <italic>SE</italic>&#x02009;=&#x02009;.02; <italic>Past harm</italic> group: <italic>M</italic><sub>difference</sub>&#x02009;=&#x02009;2.25, <italic>SE</italic>&#x02009;=&#x02009;.03; <italic>t</italic>(998)&#x02009;=&#x02009;1.88, <italic>p</italic>&#x02009;=&#x02009;.182). Finally, there was a significant interaction between the image category and rating group (<italic>F</italic>(8,990)&#x02009;=&#x02009;3.04, <italic>p</italic>&#x02009;=&#x02009;.002, <italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;=&#x02009;.024; <xref ref-type="fig" rid="fig4-2041669518755806">Figure 4</xref>). <italic>Harm to you</italic> group provided fastest responses to the Deadly Threat image category (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001), followed by equally fast responses to Direct Threat and Low Threat images (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001, except for no statistically significant difference between Direct Threat (<italic>M</italic>&#x02009;=&#x02009;2.07, <italic>SE</italic>&#x02009;=&#x02009;.04) and Low Threat images (<italic>M</italic>&#x02009;=&#x02009;2.20, <italic>SE</italic>&#x02009;=&#x02009;.05), <italic>t</italic>(188)&#x02009;=&#x02009;1.94, <italic>p</italic>&#x02009;=&#x02009;.426), and longest RT for Indirect Threat and Threat Aftermath images&#x02014;all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001, except for no statistically significant difference between Indirect Threat (<italic>M</italic>&#x02009;=&#x02009;2.46, <italic>SE</italic>&#x02009;=&#x02009;.03) and Threat Aftermath (<italic>M</italic>&#x02009;=&#x02009;2.53, <italic>SE</italic>&#x02009;=&#x02009;.05), <italic>t</italic>(257)&#x02009;=&#x02009;1.05, <italic>p</italic>&#x02009;=&#x02009;.968&#x02014;as indicated by pairwise comparisons with a Bonferroni correction. For <italic>Harm to other</italic> group, all pairwise comparisons with Bonferroni correction were significant (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.023) except for no statistically significant difference between the Direct Threat (<italic>M</italic>&#x02009;=&#x02009;2.16, <italic>SE</italic>&#x02009;=&#x02009;.05) and Low Threat (<italic>M</italic>&#x02009;=&#x02009;2.27, <italic>SE</italic>&#x02009;=&#x02009;.06; <italic>t</italic>(188)&#x02009;=&#x02009;1.59, <italic>p</italic>&#x02009;=&#x02009;.695) categories. Participants rated Deadly Threat images the fastest, followed by equally fast Direct and Low Threat images, followed by Indirect Threat and finally Threat Aftermath images with longest RTs. Similarly for <italic>Past harm</italic> group, all pairwise comparisons with a Bonferroni correction were significant (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.020), except for the comparison between the Indirect Threat (<italic>M</italic>&#x02009;=&#x02009;2.39, <italic>SE</italic>&#x02009;=&#x02009;.04) and Low Threat (<italic>M</italic>&#x02009;=&#x02009;2.30, <italic>SE</italic>&#x02009;=&#x02009;.06) image categories <italic>t</italic>(246)&#x02009;=&#x02009;1.26, <italic>p</italic>&#x02009;=&#x02009;.907. Participants again rated the Deadly Threat images the fastest followed by Direct Threat, equally slower Indirect Threat and Low Threat images and finally the slowest Threat Aftermath images.</p></sec></sec><sec id="sec16-2041669518755806"><title>Study 2</title><p>Prior to analyses, valence ratings were reverse coded such that lower ratings would correspond to negative valence, while higher ratings would correspond to positive valence. Interrater reliability for both valence and arousal ratings was excellent (.943 and .959, respectively).<sup><xref ref-type="fn" rid="fn3-2041669518755806">3</xref></sup></p><p>The relationship between valence and arousal ratings is shown in <xref ref-type="fig" rid="fig5-2041669518755806">Figure 5</xref>. Overall, there was a strong negative linear relationship between valence and arousal ratings, Pearson&#x02019;s <italic>r</italic>&#x02009;=&#x02009;&#x02212;.816, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001. Low Threat images had the highest mean valence rating (<italic>M</italic>&#x02009;=&#x02009;4.21, <italic>SE</italic>&#x02009;=&#x02009;.07), followed by Threat Aftermath images (<italic>M</italic>&#x02009;=&#x02009;3.30, <italic>SE</italic>&#x02009;=&#x02009;.06), Direct Threat images (<italic>M</italic>&#x02009;=&#x02009;3.10, <italic>SE</italic>&#x02009;=&#x02009;.05), Indirect Threat images (<italic>M</italic>&#x02009;=&#x02009;2.76, <italic>SD</italic>&#x02009;=&#x02009;.04), and Deadly Threat images (<italic>M</italic>&#x02009;=&#x02009;2.14, <italic>SD</italic>&#x02009;=&#x02009;.08). On the arousal dimension, Deadly Threat images were rated as most highly arousing (<italic>M</italic>&#x02009;=&#x02009;5.57, <italic>SE</italic>&#x02009;=&#x02009;.08), followed by Indirect Threat images (<italic>M</italic>&#x02009;=&#x02009;4.99, <italic>SE</italic>&#x02009;=&#x02009;.04), Direct Threat images (<italic>M</italic>&#x02009;=&#x02009;4.57, <italic>SE</italic>&#x02009;=&#x02009;.05), Threat Aftermath images (<italic>M</italic>&#x02009;=&#x02009;4.28, <italic>SE</italic>&#x02009;=&#x02009;.06), and Low Threat images (<italic>M</italic>&#x02009;=&#x02009;3.48, <italic>SE</italic>&#x02009;=&#x02009;.07).
<fig id="fig5-2041669518755806" orientation="portrait" position="float"><label>Figure 5.</label><caption><p>Study 2 results. The relationship between valence and arousal ratings, with valence (measured on a 1&#x02013;7 Likert scale) on the <italic>x</italic>-axis and arousal (also measured on a 1&#x02013;7 Likert scale) on the <italic>y</italic>-axis. The colors correspond to image categories.</p></caption><graphic xlink:href="10.1177_2041669518755806-fig5"/></fig></p></sec><sec id="sec17-2041669518755806"><title>Comparison of Line-Drawn and Color Photographic Scenes</title><p>In order to validate our findings, we also reanalyzed the data collected and reported previously (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>) for the color photographic images using the five image categories described above. In the previous study, 27 participants were randomly assigned to rate all color photographic images answering one of three questions: (a) How much harm might you be about to suffer in this scene if this was your view of the scene?; (b) How much harm might someone (not you) be about to suffer in this scene?; and (c) How much harm might someone (not you) have already suffered in this scene? Another 33 participants were asked to rate all the color photographic images on valence and arousal dimensions using 6-point Likert scales (for details see <xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>).</p><sec id="sec18-2041669518755806"><title>Study 1</title><p>A 3 (rating group)&#x02009;&#x000d7;&#x02009;5 (image category) repeated-measures ANOVA (using Type II calculation for obtaining sums of squares) on color photographic scenes revealed similar pattern of results as reported for the line-drawing images (<xref ref-type="fig" rid="fig6-2041669518755806">Figure 6</xref>). Descriptive statistics and results summary are presented in <xref ref-type="table" rid="table1-2041669518755806">Table 1</xref>.
<fig id="fig6-2041669518755806" orientation="portrait" position="float"><label>Figure 6.</label><caption><p>Study 1 validation results. (a) Results of the color photographic scene ratings (left). The three panels show the results from the three rating groups. Participants in Group 1 (left panel) were asked to rate all images in terms of potential harm to them personally; participants in Group 2 (middle panel) were asked to rate all images in terms of potential harm to someone else; participants in Group 3 (right panel) were asked to rate all images in terms of past harm, but no present harm. 500 images were presented in a randomized mixed sequence. Results of the line drawings of the matching scenes (Study 1) are presented on the right for the comparison. (b) Results of the RT. The error bars indicate standard error of the mean.</p></caption><graphic xlink:href="10.1177_2041669518755806-fig6"/></fig>
<table-wrap id="table1-2041669518755806" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Results Summary Tables (a) and Descriptive Statistics (b) for the 3(Rating Group)&#x02009;&#x000d7;&#x02009;5(Image Category) Repeated Measures ANOVA for Line Drawings (Study 1) and Matching Color Photographic Images (Validation Analysis for Study 1 Ran on Data Previously Reported in <xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>).</p></caption><alternatives><graphic specific-use="table1-2041669518755806" xlink:href="10.1177_2041669518755806-table1"/><table frame="hsides" rules="groups"><thead align="left" valign="top"><tr><th rowspan="1" colspan="1">
<bold>(a)</bold>
</th><th colspan="6" rowspan="1">Line drawings<hr/></th><th colspan="5" rowspan="1">Color photographs<hr/></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">
<italic>df</italic>
</th><th rowspan="1" colspan="1">
<italic>F</italic>
</th><th rowspan="1" colspan="1">Sig.</th><th rowspan="1" colspan="1">&#x003b7;<sup>2</sup></th><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">
<italic>df</italic>
</th><th rowspan="1" colspan="1">
<italic>F</italic>
</th><th rowspan="1" colspan="1">Sig.</th><th rowspan="1" colspan="1">&#x003b7;<sup>2</sup></th></tr></thead><tbody align="left" valign="top"><tr><td rowspan="1" colspan="1">Rating</td><td rowspan="1" colspan="1">Group</td><td rowspan="1" colspan="1">2,990</td><td rowspan="1" colspan="1">122.48</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.198</td><td rowspan="1" colspan="1">Rating</td><td rowspan="1" colspan="1">Group</td><td rowspan="1" colspan="1">2,990</td><td rowspan="1" colspan="1">159.72</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.244</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Category</td><td rowspan="1" colspan="1">4,495</td><td rowspan="1" colspan="1">253.31</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.672</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Category</td><td rowspan="1" colspan="1">4,495</td><td rowspan="1" colspan="1">114.95</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.482</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Group &#x000d7; Category</td><td rowspan="1" colspan="1">8,990</td><td rowspan="1" colspan="1">445.96</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.783</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Group&#x02009;&#x000d7;&#x02009;Category</td><td rowspan="1" colspan="1">8,990</td><td rowspan="1" colspan="1">114.62</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.481</td></tr><tr><td rowspan="1" colspan="1">Reaction times</td><td rowspan="1" colspan="1">Group</td><td rowspan="1" colspan="1">2,990</td><td rowspan="1" colspan="1">14.82</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.029</td><td rowspan="1" colspan="1">Reaction times</td><td rowspan="1" colspan="1">Group</td><td rowspan="1" colspan="1">2,990</td><td rowspan="1" colspan="1">32.81</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.062</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Category</td><td rowspan="1" colspan="1">4,495</td><td rowspan="1" colspan="1">50.39</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.289</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Category</td><td rowspan="1" colspan="1">4,495</td><td rowspan="1" colspan="1">20.66</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.143</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Group &#x000d7; Category</td><td rowspan="1" colspan="1">8,990</td><td rowspan="1" colspan="1">3.04</td><td rowspan="1" colspan="1">.002</td><td rowspan="1" colspan="1">.024</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Group &#x000d7; Category</td><td rowspan="1" colspan="1">8,990</td><td rowspan="1" colspan="1">6.37</td><td rowspan="1" colspan="1">&#x0003c;.001</td><td rowspan="1" colspan="1">.049</td></tr></tbody></table><table frame="hsides" rules="groups"><thead align="left" valign="top"><tr><th colspan="12" rowspan="1">
<bold>(b)</bold>
<hr/>
</th></tr><tr><th rowspan="1" colspan="1"/><th colspan="6" rowspan="1">Line drawings<hr/></th><th colspan="5" rowspan="1">Color photographs<hr/></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th colspan="2" rowspan="1">Rating<hr/></th><th colspan="2" rowspan="1">Reaction times<hr/></th><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th colspan="2" rowspan="1">Ratin<hr/>g</th><th colspan="2" rowspan="1">Reaction times<hr/></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">Group</th><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SE</italic>
</th><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SE</italic>
</th><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">Group</th><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SE</italic>
</th><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SE</italic>
</th></tr></thead><tbody align="left" valign="top"><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">3.31</td><td rowspan="1" colspan="1">0.03</td><td rowspan="1" colspan="1">2.20</td><td rowspan="1" colspan="1">0.02</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">3.02</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1">2.24</td><td rowspan="1" colspan="1">0.02</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">3.56</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1">2.33</td><td rowspan="1" colspan="1">0.02</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">3.45</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1">2.41</td><td rowspan="1" colspan="1">0.03</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">3.38</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1">2.25</td><td rowspan="1" colspan="1">0.03</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">2.76</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1">2.22</td><td rowspan="1" colspan="1">0.03</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<bold>Category</bold>
</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<bold>Category</bold>
</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Deadly Threat</td><td rowspan="1" colspan="1">4.95</td><td rowspan="1" colspan="1">0.09</td><td rowspan="1" colspan="1">1.82</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Deadly Threat</td><td rowspan="1" colspan="1">4.17</td><td rowspan="1" colspan="1">0.10</td><td rowspan="1" colspan="1">2.14</td><td rowspan="1" colspan="1">0.06</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Direct Threat</td><td rowspan="1" colspan="1">3.80</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1">2.10</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Direct Threat</td><td rowspan="1" colspan="1">3.02</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1">2.18</td><td rowspan="1" colspan="1">0.04</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Indirect Threat</td><td rowspan="1" colspan="1">3.70</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1">2.45</td><td rowspan="1" colspan="1">0.03</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Indirect Threat</td><td rowspan="1" colspan="1">3.46</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1">2.50</td><td rowspan="1" colspan="1">0.03</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Threat Aftermath</td><td rowspan="1" colspan="1">3.05</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1">2.67</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Threat Aftermath</td><td rowspan="1" colspan="1">3.01</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.49</td><td rowspan="1" colspan="1">0.05</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Low Threat</td><td rowspan="1" colspan="1">1.58</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.26</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Low Threat</td><td rowspan="1" colspan="1">1.65</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.15</td><td rowspan="1" colspan="1">0.05</td></tr><tr><td rowspan="1" colspan="1">
<bold>Group</bold>
</td><td rowspan="1" colspan="1">
<bold>Category</bold>
</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">
<bold>Group</bold>
</td><td rowspan="1" colspan="1">
<bold>Category</bold>
</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">Deadly Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">5.36</td><td rowspan="1" colspan="1">0.10</td><td rowspan="1" colspan="1">1.76</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1">Deadly Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">4.68</td><td rowspan="1" colspan="1">0.11</td><td rowspan="1" colspan="1">2.10</td><td rowspan="1" colspan="1">0.07</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">5.33</td><td rowspan="1" colspan="1">0.10</td><td rowspan="1" colspan="1">1.91</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">4.78</td><td rowspan="1" colspan="1">0.12</td><td rowspan="1" colspan="1">2.10</td><td rowspan="1" colspan="1">0.09</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">4.15</td><td rowspan="1" colspan="1">0.10</td><td rowspan="1" colspan="1">1.80</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">3.05</td><td rowspan="1" colspan="1">0.13</td><td rowspan="1" colspan="1">2.22</td><td rowspan="1" colspan="1">0.07</td></tr><tr><td rowspan="1" colspan="1">Direct Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">4.70</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1">2.07</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1">Direct Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">3.42</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1">2.29</td><td rowspan="1" colspan="1">0.05</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">3.66</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1">2.16</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">3.56</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.24</td><td rowspan="1" colspan="1">0.06</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">3.04</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1">2.06</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">2.09</td><td rowspan="1" colspan="1">0.09</td><td rowspan="1" colspan="1">2.00</td><td rowspan="1" colspan="1">0.05</td></tr><tr><td rowspan="1" colspan="1">Indirect Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">2.96</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1">2.46</td><td rowspan="1" colspan="1">0.03</td><td rowspan="1" colspan="1">Indirect Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">3.10</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1">2.42</td><td rowspan="1" colspan="1">0.04</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">4.34</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1">2.51</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">3.96</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1">2.64</td><td rowspan="1" colspan="1">0.05</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">3.82</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1">2.39</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">3.32</td><td rowspan="1" colspan="1">0.07</td><td rowspan="1" colspan="1">2.42</td><td rowspan="1" colspan="1">0.04</td></tr><tr><td rowspan="1" colspan="1">Threat Aftermath</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">1.93</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.53</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1">Threat Aftermath</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">2.24</td><td rowspan="1" colspan="1">0.09</td><td rowspan="1" colspan="1">2.37</td><td rowspan="1" colspan="1">0.06</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">2.91</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.80</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">2.94</td><td rowspan="1" colspan="1">0.10</td><td rowspan="1" colspan="1">2.74</td><td rowspan="1" colspan="1">0.07</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">4.31</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.69</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">3.87</td><td rowspan="1" colspan="1">0.10</td><td rowspan="1" colspan="1">2.37</td><td rowspan="1" colspan="1">0.06</td></tr><tr><td rowspan="1" colspan="1">Low Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">1.58</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.20</td><td rowspan="1" colspan="1">0.05</td><td rowspan="1" colspan="1">Low Threat</td><td rowspan="1" colspan="1">Harm to you</td><td rowspan="1" colspan="1">1.67</td><td rowspan="1" colspan="1">0.09</td><td rowspan="1" colspan="1">2.03</td><td rowspan="1" colspan="1">0.06</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">1.58</td><td rowspan="1" colspan="1">0.08</td><td rowspan="1" colspan="1">2.27</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Harm to other</td><td rowspan="1" colspan="1">1.81</td><td rowspan="1" colspan="1">0.10</td><td rowspan="1" colspan="1">2.35</td><td rowspan="1" colspan="1">0.07</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">1.58</td><td rowspan="1" colspan="1">0.09</td><td rowspan="1" colspan="1">2.30</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Past harm</td><td rowspan="1" colspan="1">1.47</td><td rowspan="1" colspan="1">0.11</td><td rowspan="1" colspan="1">2.09</td><td rowspan="1" colspan="1">0.06</td></tr></tbody></table></alternatives></table-wrap></p></sec><sec id="sec19-2041669518755806"><title>Study 2</title><p>Because valence and arousal ratings for the color photographic images were collected on 6-point Likert scales as compared to 7-point Likert scales used in Study 2, all the ratings were <italic>Z</italic>-scored prior to analysis. <xref ref-type="fig" rid="fig7-2041669518755806">Figure 7(a)</xref> shows the relationship between valence and arousal ratings of color photographic images. Similarly to line drawings, there was a strong negative linear relationship between valence and arousal ratings of color photographs, Pearson&#x02019;s <italic>r</italic>&#x02009;=&#x02009;&#x02212;.894, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001. To directly compare the scenes in color photographic and line drawing form on valence and arousal dimensions, we ran two 2(image type)&#x02009;&#x000d7;&#x02009;5(image category) repeated measured ANOVAs using Type II calculation for obtaining sums of squares separately for each dimension (<xref ref-type="fig" rid="fig7-2041669518755806">Figure 7(b)</xref>).
<fig id="fig7-2041669518755806" orientation="portrait" position="float"><label>Figure 7.</label><caption><p>Study 2 validation results. (a) The relationship between valence and arousal ratings of color photographic scenes (left) with the relationship between valence and arousal ratings of line drawings of the matching scenes on the right for comparison. The colors correspond to image categories. (b) Means and <italic>SE</italic> of valence and arousal ratings for each image category for color photographic images and line-drawing images of matching scenes for comparison.</p></caption><graphic xlink:href="10.1177_2041669518755806-fig7"/></fig></p><p>On arousal dimension, the analyses revealed no statistically significant main effect of image type (<italic>F</italic>(1,495)&#x02009;&#x0003c;&#x02009;.001, <italic>p</italic>&#x02009;=&#x02009;1.000, <italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;&#x0003c;&#x02009;.001), but a significant main effect of image category (<italic>F</italic>(4,495)&#x02009;=&#x02009;179.78, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;=&#x02009;.592). All image categories were significantly different from each other as indicated by post hoc pairwise comparisons with a Bonferroni correction (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.003). Deadly Threat images were rated as most arousing (<italic>M</italic>&#x02009;=&#x02009;1.08, <italic>SE</italic>&#x02009;=&#x02009;.09), followed by Indirect Threat (<italic>M</italic>&#x02009;=&#x02009;.46, <italic>SE</italic>&#x02009;=&#x02009;.05), Direct Threat (<italic>M</italic>&#x02009;=&#x02009;&#x02212;.02, <italic>SE</italic>&#x02009;=&#x02009;.06), Threat Aftermath (<italic>M</italic>&#x02009;=&#x02009;&#x02212;.35, <italic>SE</italic>&#x02009;=&#x02009;.07), and Low Threat (<italic>M</italic>&#x02009;=&#x02009;&#x02212;1.50, <italic>SE</italic>&#x02009;=&#x02009;.07). There was also a significant interaction between the image category and image type (<italic>F</italic>(4,495)&#x02009;=&#x02009;3.56, <italic>p</italic>&#x02009;=&#x02009;.007, <italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;=&#x02009;.028). Pairwise comparisons with a Bonferroni correction revealed that Deadly Threat and Low Threat images were rated as more arousing in line drawing compared to color photographic form (<italic>t</italic>(100)&#x02009;=&#x02009;2.02, <italic>p</italic>&#x02009;=&#x02009;.044 and <italic>t</italic>(140)&#x02009;=&#x02009;2.15, <italic>p</italic>&#x02009;=&#x02009;.032, respectively) while the opposite was observed for the Threat Aftermath images (<italic>t</italic>(162)&#x02009;=&#x02009;2.00, <italic>p</italic>&#x02009;=&#x02009;.046). No statistically significant differences were found between line-drawing and color photographic images for Direct Threat and Indirect Threat images (both <italic>p</italic>s&#x02009;&#x0003e;&#x02009;.229, see <xref ref-type="table" rid="table2-2041669518755806">Table 2</xref> for descriptive statistics). It is worth noting that while the interaction effect was statistically significant (<italic>p</italic>&#x02009;=&#x02009;.007), the effect size observed was small (<italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;=&#x02009;.028). Thus, the results reported here should be interpreted with caution.
<table-wrap id="table2-2041669518755806" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Descriptive Statistics for Valence and Arousal Ratings for Line Drawings and Color Photographic Images (data previously reported in <xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>).</p></caption><alternatives><graphic specific-use="table2-2041669518755806" xlink:href="10.1177_2041669518755806-table2"/><table frame="hsides" rules="groups"><thead align="left" valign="top"><tr><th colspan="6" rowspan="1">Line drawings<hr/></th><th rowspan="1" colspan="1"/><th colspan="5" rowspan="1">Color photographs<hr/></th></tr><tr><th colspan="2" rowspan="2"/><th colspan="2" rowspan="1">Arousal<hr/></th><th colspan="2" rowspan="1">Valence<hr/></th><th rowspan="2" colspan="1"/><th rowspan="2" colspan="1"/><th colspan="2" rowspan="1">Arousal<hr/></th><th colspan="2" rowspan="1">Valence<hr/></th></tr><tr><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SD</italic>
</th><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SD</italic>
</th><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SD</italic>
</th><th rowspan="1" colspan="1">
<italic>M</italic>
</th><th rowspan="1" colspan="1">
<italic>SD</italic>
</th></tr></thead><tbody align="left" valign="top"><tr><td rowspan="5" colspan="1">
<bold>Category</bold>
</td><td rowspan="1" colspan="1">Deadly Threat</td><td rowspan="1" colspan="1">1.17</td><td rowspan="1" colspan="1">.33</td><td rowspan="1" colspan="1">&#x02212;1.16</td><td rowspan="1" colspan="1">.37</td><td rowspan="5" colspan="1">
<bold>Category</bold>
</td><td rowspan="1" colspan="1">Deadly Threat</td><td rowspan="1" colspan="1">.99</td><td rowspan="1" colspan="1">.25</td><td rowspan="1" colspan="1">&#x02212;1.01</td><td rowspan="1" colspan="1">.24</td></tr><tr><td rowspan="1" colspan="1">Direct Threat</td><td rowspan="1" colspan="1">&#x02212;.06</td><td rowspan="1" colspan="1">.60</td><td rowspan="1" colspan="1">.04</td><td rowspan="1" colspan="1">.59</td><td rowspan="1" colspan="1">Direct Threat</td><td rowspan="1" colspan="1">.01</td><td rowspan="1" colspan="1">.51</td><td rowspan="1" colspan="1">.35</td><td rowspan="1" colspan="1">.63</td></tr><tr><td rowspan="1" colspan="1">Indirect Threat</td><td rowspan="1" colspan="1">.46</td><td rowspan="1" colspan="1">.74</td><td rowspan="1" colspan="1">&#x02212;.38</td><td rowspan="1" colspan="1">.79</td><td rowspan="1" colspan="1">Indirect Threat</td><td rowspan="1" colspan="1">.46</td><td rowspan="1" colspan="1">.72</td><td rowspan="1" colspan="1">&#x02212;.44</td><td rowspan="1" colspan="1">.77</td></tr><tr><td rowspan="1" colspan="1">Threat Aftermath</td><td rowspan="1" colspan="1">&#x02212;.42</td><td rowspan="1" colspan="1">.88</td><td rowspan="1" colspan="1">.28</td><td rowspan="1" colspan="1">.82</td><td rowspan="1" colspan="1">Threat Aftermath</td><td rowspan="1" colspan="1">&#x02212;.28</td><td rowspan="1" colspan="1">.98</td><td rowspan="1" colspan="1">&#x02212;.12</td><td rowspan="1" colspan="1">.95</td></tr><tr><td rowspan="1" colspan="1">Low Threat</td><td rowspan="1" colspan="1">&#x02212;1.41</td><td rowspan="1" colspan="1">.64</td><td rowspan="1" colspan="1">1.42</td><td rowspan="1" colspan="1">.82</td><td rowspan="1" colspan="1">Low Threat</td><td rowspan="1" colspan="1">&#x02212;1.58</td><td rowspan="1" colspan="1">.62</td><td rowspan="1" colspan="1">1.45</td><td rowspan="1" colspan="1">.52</td></tr></tbody></table></alternatives></table-wrap></p><p>On valence dimension, the analyses revealed no statistically significant main effect of image type (<italic>F</italic>(1,495)&#x02009;&#x0003c;&#x02009;.001, <italic>p</italic>&#x02009;=&#x02009;1.000, <italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;&#x0003c;&#x02009;.001), but a significant main effect of image category (<italic>F</italic>(4,495)&#x02009;=&#x02009;145.40, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;=&#x02009;.540). All image categories were significantly different from each other (all <italic>p</italic>s&#x02009;&#x0003c;&#x02009;.001), except for no statistically significant difference between Direct Threat and Threat Aftermath images (<italic>t</italic>(199)&#x02009;=&#x02009;1.55, <italic>p</italic>&#x02009;=&#x02009;.727) as indicated by post hoc pairwise comparisons with a Bonferroni correction. Deadly Threat images were rated as most negative (<italic>M</italic>&#x02009;=&#x02009;&#x02212;1.08, <italic>SE</italic>&#x02009;=&#x02009;.09), followed by Indirect Threat (<italic>M</italic>&#x02009;=&#x02009;&#x02212;.41, <italic>SE</italic>&#x02009;=&#x02009;.05), Direct Threat (<italic>M</italic>&#x02009;=&#x02009;.19, <italic>SE</italic>&#x02009;=&#x02009;.06), Threat Aftermath (<italic>M</italic>&#x02009;=&#x02009;.05, <italic>SE</italic>&#x02009;=&#x02009;.07), and Low Threat (<italic>M</italic>&#x02009;=&#x02009;1.44, <italic>SE</italic>&#x02009;=&#x02009;.08). There was also a significant interaction between the image category and image type (<italic>F</italic>(4,495)&#x02009;=&#x02009;21.34, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001, <italic>&#x003b7;<sup>2</sup><sub>p</sub></italic>&#x02009;=&#x02009;.147). Pairwise comparisons with a Bonferroni correction revealed that Direct Threat images were rated more negatively in line-drawing compared to color photographic form (<italic>t</italic>(236)&#x02009;=&#x02009;5.69, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001). Similar but only marginally statistically significant effect was found for Deadly Threat images (<italic>t</italic>(100)&#x02009;=&#x02009;1.85, <italic>p</italic>&#x02009;=&#x02009;.066). The opposite was observed for the Threat Aftermath images (<italic>t</italic>(140)&#x02009;=&#x02009;6.94, <italic>p</italic>&#x02009;&#x0003c;&#x02009;.001). Specifically, Threat Aftermath images were rated less negatively in line-drawing compared to color photographic form. No statistically significant differences were found between line-drawing and color photographic images for Indirect Threat and Low Threat images (both <italic>p</italic>s&#x02009;&#x0003e;&#x02009;.214, see <xref ref-type="table" rid="table2-2041669518755806">Table 2</xref> for descriptive statistics).</p></sec></sec></sec><sec sec-type="discussion" id="sec20-2041669518755806"><title>Discussion</title><p>In the present study, we demonstrated that human observers are able to finely discriminate different types of threat or merely negative situations from line-drawn scenes, even though line drawings lack many of the defining characteristics seen in the real world (such as color, most texture, most shading, and many background details). Specifically, we found that even though all of the images (with the exception of the Low Threat category) were negative, these scenes were keenly discriminated by the observers based on the perceived target of the threat (spatially directed towards the observer vs. towards someone else in the scene) as well as the temporal proximity of threat (happening right now or about to happen vs. something that happened sometime in the past). Previously, we reported that human observers are sensitive to different types of negative stimuli, with their evaluation of the stimuli differing sharply depending on the spatial and temporal direction of the threat using color photographic images (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>). The present study replicated and extended our previous findings to line-drawn scenes.</p><p>The 500 open-access line-drawn images depicting scenes with different levels of threat created for the present study were rated by a sample of American young adults on three scales capturing spatial and temporal direction of the threat (how much harm there is to you, how much harm there is to someone else, and how much harm has already happened). Based on these ratings, 500 images were divided into five distinct categories: Deadly Threat (depicting humans with weapons pointing at the observer; <italic>n</italic>&#x02009;=&#x02009;51), Direct Threat (depicting scenes posing high threat to the observer, <italic>n</italic>&#x02009;=&#x02009;119), Indirect Threat (depicting scenes posing high threat to someone else; <italic>n</italic>&#x02009;=&#x02009;117), Threat Aftermath (depicting scenes where harm&#x02014;physical, psychological, or both&#x02014;has already happened; <italic>n</italic>&#x02009;=&#x02009;82), and Low Threat (depicting low level of threat either to the observer, or someone else; <italic>n</italic>&#x02009;=&#x02009;71).</p><p>Immediate threats of personal harm should evoke a fight-or-flight response, while threats where the immediate danger has passed, or the threat is to others, might instead evoke an approach response, possibly to render assistance or to gather information that would enable observers to avoid a similar fate in the future (<xref rid="bibr27-2041669518755806" ref-type="bibr">Kveraga, 2014</xref>; <xref rid="bibr52-2041669518755806" ref-type="bibr">Oosterwijk, 2017</xref>). The ratings in the three groups (<italic>Harm to you</italic>, <italic>Harm to other</italic>, and <italic>Past harm</italic>) reflected the task, in that the image that received the highest ratings clearly depicted these spatial and temporal threat dimensions, with one exception. All three groups rated the Deadly Threat images (images depicting scenes of humans pointing deadly weapons&#x02014;mostly guns&#x02014;at the observer) highly, implying that they pose direct threat to the observer, as well as to someone else, and may have also caused past harm. In addition, Deadly Threat images evoked the lowest RT across all three groups. An alternative explanation of shorter RT could also indicate that the images belonging to the Deadly Threat category are easier to judge than the others, because they are less complex and also because it is easier to evaluate the situation from our point of view than from the point of view of someone else.</p><p>The valence and arousal dimensions had a strong negative linear relationship with each other both for the line-drawing and color photographic form of the matched scenes. Because all the images, with the exception of Low Threat category, were negative with a varying level and type of threat, this relationship is reasonable. The slightly stronger relationship for color photographic images as compared to line drawings of the matching scenes may be due to the fact that while the same participants rated both valence and arousal of the color photographic images, different participants rated valence and arousal of the line-drawing images. Line-drawing images depict the essence of the scene, while color photographic images provide more detail and a richer context.</p><p>It has been demonstrated that humans possess a natural ability to recognize and interpret line drawings and line drawings can evoke similar brain activity as color photographs (<xref rid="bibr62-2041669518755806" ref-type="bibr">Walther et&#x000a0;al., 2011</xref>). Our findings further demonstrate that young human observers show similar sensitivity for threat information in an image, which they can extract quickly whether they are presented with a color photographic image (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>) or a simple line drawing. This indicates that the gist of the scene is extracted and used in categorizing threat and merely negative stimuli, in color photographs and line drawings alike, as the latter are missing much detail, color and background information available in the former. Our findings make sense in light of evolutionary pressures, as the human (and its mammalian predecessors&#x02019;) visual system had to be able to extract the form of threatening stimuli in widely varying visibility conditions quickly and automatically to promote survival. Among such stimuli, snakes, spiders, and angry faces are thought to be especially important cues of potential danger (<xref rid="bibr49-2041669518755806" ref-type="bibr">&#x000d6;hman et&#x000a0;al., 2001</xref>; <xref rid="bibr50-2041669518755806" ref-type="bibr">&#x000d6;hman &#x00026; Mineka, 2001</xref>), with snakes being the primary predator of humans&#x02019; primate predecessors (<xref rid="bibr24-2041669518755806" ref-type="bibr">Isbell, 2006</xref>; <xref rid="bibr34-2041669518755806" ref-type="bibr">Le et&#x000a0;al., 2016</xref>). However, our results revealed no natural predators in the Deadly Threat category, which was dominated by images of humans pointing weapons at the observer. While we did not explicitly set out to test whether these stimuli would be recognized faster than natural predators, or unarmed, angry humans, the ratings and the RTs clearly suggest that humans pointing weapons, including guns, are considered significantly more dangerous than natural predators. This may stem from most observers&#x02019; surroundings, upbringing and limited experience with natural threats. While they may not have had any personal experience with people pointing guns at them, societal exposure to such stimuli via the media may have been sufficient to encode such evolutionarily modern stimuli as more dangerous than natural predators.</p><sec id="sec21-2041669518755806"><title>Potential Uses of Threat-Depicting and Negative Line Drawings in Research</title><p>Compared to photographic images, line-drawing images are much more easily manipulated without disrupting the natural image statistics, making them a useful tool for studying objects and scenes (<xref rid="bibr7-2041669518755806" ref-type="bibr">Biederman &#x00026; Ju, 1988</xref>; <xref rid="bibr8-2041669518755806" ref-type="bibr">Biederman et&#x000a0;al., 1982</xref>; <xref rid="bibr29-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2007</xref>; <xref rid="bibr53-2041669518755806" ref-type="bibr">Palmer, 1975</xref>; <xref rid="bibr59-2041669518755806" ref-type="bibr">Snodgrass &#x00026; Vanderwart, 1980</xref>; <xref rid="bibr62-2041669518755806" ref-type="bibr">Walther et&#x000a0;al., 2011</xref>). In addition, unlike color photographic scenes, line-drawn scenes offer better control over image statistics across image categories. For example, a color photographic scene depicting a man with a gun pointed at the observer (Deadly Threat) differs from a scene depicting a gravely injured person (Threat Aftermath) not only in the threat context but also in presence (or absence) of color and textural visual cues, such as blood. On the other hand, line-drawn images provide only the gist of the scene across different image categories and thus offer better experimental control. This has been shown to be important in, for example, facial expression research, where anger expressions are detected more quickly when the facial stimuli are identical in their low-level components (e.g., schematic, line-drawn faces), but the evidence is mixed when face photographs are used (<xref rid="bibr51-2041669518755806" ref-type="bibr">&#x000d6;hman et&#x000a0;al., 2012</xref>). Thus, our line-drawing image set could be used to study threat and emotion perception, either in original or altered form, for example, through visual pathway biasing (<xref rid="bibr29-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2007</xref>; <xref rid="bibr60-2041669518755806" ref-type="bibr">Thomas, Kveraga, Huberle, Karnath, &#x00026; Bar, 2012</xref>), fragmenting (<xref rid="bibr7-2041669518755806" ref-type="bibr">Biederman &#x00026; Ju, 1988</xref>), grouping into global/local ensembles (<xref rid="bibr62-2041669518755806" ref-type="bibr">Walther et&#x000a0;al., 2011</xref>), manipulating object-context (<xref rid="bibr53-2041669518755806" ref-type="bibr">Palmer, 1975</xref>), object-agent (<xref rid="bibr12-2041669518755806" ref-type="bibr">Correll, Park, Judd, &#x00026; Wittenbrink, 2002</xref>), object-location (<xref rid="bibr8-2041669518755806" ref-type="bibr">Biederman et&#x000a0;al., 1982</xref>) congruency, or facial expressions (<xref rid="bibr51-2041669518755806" ref-type="bibr">&#x000d6;hman et&#x000a0;al., 2012</xref>).</p><p>Furthermore, given that our line-drawing image set contains distinct image categories with varying threat loadings, differentiating negative, threatening images (Deadly Threat, Direct Threat, and Indirect Threat) from merely negative images (Threat Aftermath), it may also be useful for studying clinical populations showing abnormal sensitivity to threat stimuli. For example, various studies demonstrated that anxious individuals exhibit selective attention for threat-related stimuli (<xref rid="bibr19-2041669518755806" ref-type="bibr">Eysenck, Derakshan, Santos, &#x00026; Calvo, 2007</xref>; <xref rid="bibr44-2041669518755806" ref-type="bibr">Mogg &#x00026; Bradley, 1999</xref>; <xref rid="bibr45-2041669518755806" ref-type="bibr">Mogg, Millar, &#x00026; Bradley, 2000</xref>), spend more time on such stimuli (<xref rid="bibr20-2041669518755806" ref-type="bibr">Fox, Russo, Bowles, &#x00026; Dutton, 2001</xref>), and have difficulty in shifting their attention away from threat (<xref rid="bibr25-2041669518755806" ref-type="bibr">Klumpp &#x00026; Amir, 2009</xref>; <xref rid="bibr64-2041669518755806" ref-type="bibr">Yiend &#x00026; Mathews, 2001</xref>). In a recent fMRI study (<xref rid="bibr23-2041669518755806" ref-type="bibr">Im et&#x000a0;al., 2017</xref>), we utilized visual pathway biasing manipulation (<xref rid="bibr29-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2007</xref>; <xref rid="bibr60-2041669518755806" ref-type="bibr">Thomas et&#x000a0;al., 2012</xref>) and found that trait anxiety differentially affected perception of clear and ambiguous threat cues from faces presented to the magnocellular or parvocellular visual pathways and correlated with amygdala activity in lateralized fashion. The reported abnormalities in threat processing by anxious individuals can be studied further by examining their responses to the different types of negative and threatening images (Deadly Threat, Direct Threat and Indirect Threat) compared to responses to merely negative images (Threat Aftermath).</p></sec><sec id="sec22-2041669518755806"><title>Limitations and Future Directions</title><p>Among the limitations of the study is the use of young adult U.S. student samples. Future work should investigate larger scale samples of young, as well as middle-aged and older adults in order to increase generalizability of the findings reported here. In addition, there is an unbalanced number of images per category in the image set described here. However, the set offers a fairly large pool of images to choose from, and a balanced design among the categories can be accomplished by selecting image subsets. Researchers will be able to select a sample of stimuli corresponding to their design needs from the entire image set, either by using the ratings reported here or by rerating the sample in their own population, with the same or new rating parameters relevant to their research question. Furthermore, even though a reasonably stable relationship has been observed between psychophysiological responses such as heart rate, skin conductance, facial EMG, and the startle reflex as compared to subjective affective ratings (<xref rid="bibr30-2041669518755806" ref-type="bibr">Lang, Bradley, &#x00026; Cuthbert, 1990</xref>), it is nevertheless necessary to acquire affective responses to our line-drawing images that go beyond self-reported subjective ratings using psychophysiological reactions (<xref rid="bibr11-2041669518755806" ref-type="bibr">Cacioppo, Berntson, Larsen, Poehlmann, &#x00026; Ito, 2000</xref>), fMRI (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>; <xref rid="bibr46-2041669518755806" ref-type="bibr">Moriguchi et&#x000a0;al., 2011</xref>), and magnetoencephalography (MEG: <xref rid="bibr54-2041669518755806" ref-type="bibr">Panichello, Kveraga, Chaumon, Bar, &#x00026; Barrett, 2017</xref>). Finally, using IAPS images, cross-cultural studies conducted around the world (<xref rid="bibr14-2041669518755806" ref-type="bibr">Deak, Csenki, &#x00026; Revesz, 2010</xref>; <xref rid="bibr15-2041669518755806" ref-type="bibr">Drace, Efendic, Kusturica, &#x00026; Landzo, 2013</xref>; <xref rid="bibr17-2041669518755806" ref-type="bibr">Dufey, Fernandez, &#x00026; Mayol, 2011</xref>; <xref rid="bibr33-2041669518755806" ref-type="bibr">Lasaitis, Ribeiro, &#x00026; Bueno, 2008</xref>; <xref rid="bibr43-2041669518755806" ref-type="bibr">Lohani, Gupta, &#x00026; Srinivasan, 2013</xref>) revealed major similarities as well as subtle cultural differences in terms of valence and arousal ratings. Thus, future studies will have to evaluate the cross-cultural validity of the current set of images.</p><p>Future work on the line-drawing image set presented here should investigate the following: (a) larger scale samples of young, middle-aged, and older adults that might reveal age-related differences in processing affectively relevant images in the set; (b) the validity and reliability of the set across clinical populations, such as populations suffering from anxiety disorders; (c) the validity and reliability of the set across cultures; and (d) affective responses to the set that go beyond self-reported subjective ratings, such as heart rate, skin conductance, and facial EMG.</p></sec></sec><sec sec-type="conclusions" id="sec23-2041669518755806"><title>Conclusions</title><p>We have previously demonstrated that young human observers are keenly sensitive to the spatial and temporal directions of threat in color photographic visual scenes (<xref rid="bibr28-2041669518755806" ref-type="bibr">Kveraga et&#x000a0;al., 2015</xref>). Here, we extended our previous work by demonstrating that young human observers are just as sensitive to spatial and temporal directions of threat, even when it is presented as hand-traced line drawings of the same visual scenes. This suggests that, even though line drawings are much more visually impoverished as compared to color photographic images, they nevertheless appear to capture essential features that allow human observers to differentiate the scenes according to their threat value. Because a line-drawing format of an image allows for easier manipulation of the image content, our line-drawing stimulus set offers a multitude of possibilities for future research on threat perception. A more nuanced understanding of threat perception is not only of considerable theoretical interest, but has practical implications for the safety and well-being of healthy and clinical populations.</p></sec><sec sec-type="supplementary-material" specific-use="figshare"><title>Supplemental Material</title><supplementary-material content-type="local-data" id="suppl1-2041669518755806"><caption><title>Supplementary Material table and figures</title></caption><media xlink:href="Supplemental_material_table_and_figures.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><title>Acknowledgements</title><p>The authors thank Joseph Brandenburg for his help with data collection.</p></ack><sec id="sec24-2041669518755806"><title>Author Note</title><p>This work was presented at the seventeenth annual meeting of Vision Sciences Society (VSS 2017) held in St. Pete Beach, FL, in May 2017. Institutional Review Board exemption was obtained before the beginning of the study.</p></sec><sec id="sec25-2041669518755806"><title>Declaration of Conflicting Interests</title><p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></sec><sec id="sec26-2041669518755806"><title>Funding</title><p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The research reported in this article was supported by an NIH K01-MH84011 award to KK and NIH R01-MH101194 grant to KK and RA.</p></sec><sec><title>Supplementary material</title><p>Supplementary material for this article is available online.</p></sec><notes><title>Notes</title><fn-group><fn id="fn1-2041669518755806"><label>1</label><p>The interrater reliability for each rating group before removing data of eight participants (<italic>n</italic>&#x02009;=&#x02009;20 each) was .972, .969, and .950 respectively.</p></fn><fn id="fn2-2041669518755806"><label>2</label><p>If an image fitted more than one pattern, the pattern producing a more significant fit was selected.</p></fn><fn id="fn3-2041669518755806"><label>3</label><p>For arousal ratings, one participant&#x02019;s responses were significantly different from the responses of other participants. Thus, data of this participant were removed from further analysis. Interreliability for arousal ratings before removing these data was .956.</p></fn></fn-group></notes><bio id="d35e2937"><title>Author Biographies</title><p>
<fig id="img1-2041669518755806" orientation="portrait" position="anchor"><graphic xlink:href="10.1177_2041669518755806-img1"/></fig>
</p><p><bold>Jasmine Boshyan</bold> holds a PhD in Social-Developmental Psychology from Brandeis University (Waltham, MA, US), a BS in Psychology from University of Wisconsin-Madison (Madison, WI, US), and a BA in Classical Piano Performance from Yerevan State Conservatory (Yerevan, Armenia). After a postdoctoral fellowship at Harvard Medical School and the Athinoula A. Martinos Center for Biomedical Imaging at the Massachusetts General Hospital she is now an independent researcher interested in human perception of threat from faces and scenes.</p><p>
<fig id="img2-2041669518755806" orientation="portrait" position="anchor"><graphic xlink:href="10.1177_2041669518755806-img2"/></fig>
</p><p><bold>Lisa Feldman Barrett</bold>, PhD, is University Distinguished Professor of Psychology and Director of the Interdisciplinary Affective Science Laboratory (IASLab) at Northeastern University, with research appointments at Harvard Medical School and Massachusetts General Hospital. Her research focuses on the nature of emotion from both psychological and neuroscience perspectives. In addition to publishing over 200 peer reviewed papers and 50 book chapters, Dr. Barrett has testified before US Congress in support of basic behavioral research funding and has edited five volumes, including the 4th edition of the <italic>Handbook of Emotion</italic>, published by Guilford Press. Her book, <italic>How emotions are made: The secret life of the brain</italic> is published by Houghton Mifflin Harcourt.</p><p>
<fig id="img3-2041669518755806" orientation="portrait" position="anchor"><graphic xlink:href="10.1177_2041669518755806-img3"/></fig>
</p><p><bold>Nicole Betz</bold> is a PhD candidate studying Cognitive Psychology at Northeastern University. Her current research focuses on intuitive theories of concepts ranging from emotions to climate change.</p><p>
<fig id="img4-2041669518755806" orientation="portrait" position="anchor"><graphic xlink:href="10.1177_2041669518755806-img4"/></fig>
</p><p><bold>Reginald B. Adams Jr.</bold> is Associate Professor of psychology at the Pennsylvania State University. Much of his work focuses on the combinatorial nature of social perception. Social cues, such as race, gender, age, appearance, eye gaze, and emotion have largely been studied independent of one another, even isolated within separate fields. His research examines how these social cues combine to form the unified representations that guide our impressions of and responses to others. Most recently he has been working on an NIMH R01 examining the neural dynamics of visual compound threat cue processing.</p><p>
<fig id="img5-2041669518755806" orientation="portrait" position="anchor"><graphic xlink:href="10.1177_2041669518755806-img5"/></fig>
</p><p><bold>Kestutis Kveraga</bold> is an Assistant Professor of Radiology at Harvard Medical School and the Athinoula A. Martinos Center for Biomedical Imaging at the Massachusetts General Hospital. He studies behavioral and neural dynamics underlying threat perception in face and scene images using fMRI, MEG and eye-tracking. He is co-editor of a volume on scene perception, <italic>Scene Vision: Making Sense of What We See.</italic></p></bio><ref-list><title>References</title><ref id="bibr1-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bannerman</surname><given-names>R. L.</given-names></name><name><surname>Milders</surname><given-names>M.</given-names></name><name><surname>de Gelder</surname><given-names>B.</given-names></name><name><surname>Sahraie</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>) <article-title>Orienting to threat: Faster localization of fearful facial expressions and body postures revealed by saccadic eye movements</article-title>. <source>Proceedings of the Royal Society of Sciences: B Biological Sciences</source>
<volume>276</volume>: <fpage>1635</fpage>&#x02013;<lpage>1641</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rspb.2008.1744">http://dx.doi.org/10.1098/rspb.2008.1744</ext-link></comment>.</mixed-citation></ref><ref id="bibr2-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>L. F.</given-names></name></person-group> (<year>2006</year>) <article-title>Valence as a basic building block of emotional life</article-title>. <source>Journal of Research in Personality</source>
<volume>40</volume>: <fpage>35</fpage>&#x02013;<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jrp.2005.08.006">http://dx.doi.org/10.1016/j.jrp.2005.08.006</ext-link></comment>.</mixed-citation></ref><ref id="bibr3-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>L. F.</given-names></name><name><surname>Bliss-Moreau</surname><given-names>E.</given-names></name></person-group> (<year>2009</year>) <article-title>Affect as a psychological primitive</article-title>. <source>Advances in Experimental Social Psychology</source>
<volume>41</volume>: <fpage>167</fpage>&#x02013;<lpage>218</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0065-2601">https://doi.org/10.1016/S0065-2601</ext-link>(08)00404-8</comment>.<pub-id pub-id-type="pmid">20552040</pub-id></mixed-citation></ref><ref id="bibr4-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>L. F.</given-names></name><name><surname>Russell</surname><given-names>J. A.</given-names></name></person-group> (<year>1999</year>) <article-title>Structure of current affect</article-title>. <source>Current Directions in Psychological Science</source>
<volume>8</volume>: <fpage>10</fpage>&#x02013;<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/1467-8721.00003">https://doi.org/10.1111/1467-8721.00003</ext-link></comment>.</mixed-citation></ref><ref id="bibr5-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>D. V.</given-names></name><name><surname>Anderson</surname><given-names>U. S.</given-names></name><name><surname>Mortensen</surname><given-names>C. R.</given-names></name><name><surname>Neufeld</surname><given-names>S. L.</given-names></name><name><surname>Neel</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>) <article-title>The face in the crowd effect unconfounded: Happy faces, not angry faces, are more efficiently detected in single- and multiple-target visual search tasks</article-title>. <source>Journal of Experimental Psychology: General</source>
<volume>140</volume>: <fpage>637</fpage>&#x02013;<lpage>659</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0024060">https://doi.org/10.1037/a0024060</ext-link></comment>.<pub-id pub-id-type="pmid">21744984</pub-id></mixed-citation></ref><ref id="bibr6-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>D. V.</given-names></name><name><surname>Srinivasan</surname><given-names>N.</given-names></name></person-group> (<year>2014</year>) <article-title>The vividness of the happy face</article-title>. <source>Current Directions in Psychological Science</source>
<volume>23</volume>: <fpage>189</fpage>&#x02013;<lpage>194</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0963721414533702">https://doi.org/10.1177/0963721414533702</ext-link></comment>.</mixed-citation></ref><ref id="bibr7-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I.</given-names></name><name><surname>Ju</surname><given-names>G.</given-names></name></person-group> (<year>1988</year>) <article-title>Surface versus edge-based determinants of visual recognition</article-title>. <source>Cognitive Psychology</source>
<volume>14</volume>: <fpage>143</fpage>&#x02013;<lpage>177</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0010-0285">https://doi.org/10.1016/0010-0285</ext-link>(82)90007-X</comment>.</mixed-citation></ref><ref id="bibr8-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I.</given-names></name><name><surname>Mezzanotte</surname><given-names>R. J.</given-names></name><name><surname>Rabinowitz</surname><given-names>J. C.</given-names></name></person-group> (<year>1982</year>) <article-title>Scene perception: Detecting and judging objects undergoing relational violations</article-title>. <source>Cognitive Psychology</source>
<volume>14</volume>: <fpage>143</fpage>&#x02013;<lpage>177</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0010-0285">https://doi.org/10.1016/0010-0285</ext-link>(82)90007-X</comment>.<pub-id pub-id-type="pmid">7083801</pub-id></mixed-citation></ref><ref id="bibr9-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchette</surname><given-names>I.</given-names></name></person-group> (<year>2006</year>) <article-title>Snakes, spiders, guns, and syringes: How specific are evolutionary constraints on the detection of threatening stimuli?</article-title>
<source>The Quarterly Journal of Experimental Psychology</source>
<volume>59</volume>: <fpage>1484</fpage>&#x02013;<lpage>1504</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02724980543000204">https://doi.org/10.1080/02724980543000204</ext-link></comment>.<pub-id pub-id-type="pmid">16846972</pub-id></mixed-citation></ref><ref id="bibr10-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>T.</given-names></name><name><surname>Pourtois</surname><given-names>G.</given-names></name><name><surname>Sander</surname><given-names>D.</given-names></name></person-group> (<year>2010</year>) <article-title>The perception and categorisation of emotional stimuli: A review</article-title>. <source>Cognition and Emotion</source>
<volume>24</volume>: <fpage>377</fpage>&#x02013;<lpage>400</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699930902975754">https://doi.org/10.1080/02699930902975754</ext-link></comment>.</mixed-citation></ref><ref id="bibr11-2041669518755806"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cacioppo</surname><given-names>J. T.</given-names></name><name><surname>Berntson</surname><given-names>G. G.</given-names></name><name><surname>Larsen</surname><given-names>J. T.</given-names></name><name><surname>Poehlmann</surname><given-names>K. M.</given-names></name><name><surname>Ito</surname><given-names>T. A.</given-names></name></person-group> (<year>2000</year>) <article-title>The psychophysiology of emotion</article-title>. In: <person-group person-group-type="editor"><name><surname>Lewis</surname><given-names>M.</given-names></name><name><surname>Haviland-Jones</surname><given-names>J. M.</given-names></name></person-group> (eds) <source>Handbook of emotions</source>, <edition>2nd ed</edition>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford Press</publisher-name>, pp. <fpage>173</fpage>&#x02013;<lpage>191</lpage>.</mixed-citation></ref><ref id="bibr12-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Correll</surname><given-names>J.</given-names></name><name><surname>Park</surname><given-names>B.</given-names></name><name><surname>Judd</surname><given-names>C. M.</given-names></name><name><surname>Wittenbrink</surname><given-names>B.</given-names></name></person-group> (<year>2002</year>) <article-title>The police officer's dilemma: Using ethnicity to disambiguate potentially threatening individuals</article-title>. <source>Journal of Personality and Social Psychology</source>
<volume>83</volume>: <fpage>1314</fpage>&#x02013;<lpage>1329</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0022-3514.83.6.1314">https://doi.org/10.1037/0022-3514.83.6.1314</ext-link></comment>.<pub-id pub-id-type="pmid">12500813</pub-id></mixed-citation></ref><ref id="bibr13-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dan-Glauser</surname><given-names>E. S.</given-names></name><name><surname>Scherer</surname><given-names>K. R.</given-names></name></person-group> (<year>2011</year>) <article-title>The Geneva affective picture database (GAPED): A new 730-picture database focusing on valence and normative significance</article-title>. <source>Behavioral Research</source>
<volume>43</volume>: <fpage>468</fpage>&#x02013;<lpage>477</lpage>.</mixed-citation></ref><ref id="bibr14-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deak</surname><given-names>A.</given-names></name><name><surname>Csenki</surname><given-names>L.</given-names></name><name><surname>Revesz</surname><given-names>G.</given-names></name></person-group> (<year>2010</year>) <article-title>Hungarian ratings for the International Affective Picture System (IAPS): A cross-cultural comparison</article-title>. <source>Empirical Text and Culture Research</source>
<volume>4</volume>: <fpage>90</fpage>&#x02013;<lpage>101</lpage>.</mixed-citation></ref><ref id="bibr15-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drace</surname><given-names>S.</given-names></name><name><surname>Efendic</surname><given-names>E.</given-names></name><name><surname>Kusturica</surname><given-names>M.</given-names></name><name><surname>Landzo</surname><given-names>L.</given-names></name></person-group> (<year>2013</year>) <article-title>Cross-cultural validation of the &#x0201c;International Affective Picture System&#x0201d; (IAPS) on a sample from Bosnia and Herzegovina</article-title>. <source>Psihologija</source>
<volume>46</volume>: <fpage>17</fpage>&#x02013;<lpage>26</lpage>.</mixed-citation></ref><ref id="bibr16-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duckworth</surname><given-names>K. L.</given-names></name><name><surname>Bargh</surname><given-names>J. A.</given-names></name><name><surname>Garcia</surname><given-names>M.</given-names></name><name><surname>Chaiken</surname><given-names>S.</given-names></name></person-group> (<year>2002</year>) <article-title>The automatic evaluation of novel stimuli</article-title>. <source>Psychological Science</source>
<volume>13</volume>: <fpage>513</fpage>&#x02013;<lpage>519</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/1467-9280.00490">https://doi.org/10.1111/1467-9280.00490</ext-link></comment>.<pub-id pub-id-type="pmid">12430834</pub-id></mixed-citation></ref><ref id="bibr17-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dufey</surname><given-names>M.</given-names></name><name><surname>Fernandez</surname><given-names>A. M.</given-names></name><name><surname>Mayol</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>) <article-title>Adding support to cross-cultural emotional assessment: Validation of the International Affective Picture System in a Chilean sample</article-title>. <source>Universitas Psychologica</source>
<volume>10</volume>: <fpage>521</fpage>&#x02013;<lpage>533</lpage>.</mixed-citation></ref><ref id="bibr18-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eastwood</surname><given-names>J. D.</given-names></name><name><surname>Smilek</surname><given-names>D.</given-names></name><name><surname>Merikle</surname><given-names>P. M.</given-names></name></person-group> (<year>2001</year>) <article-title>Differential attentional guidance by unattended faces expressing positive and negative emotion</article-title>. <source>Perception and Psychophysics</source>
<volume>63</volume>: <fpage>1004</fpage>&#x02013;<lpage>1013</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03194519">https://doi.org/10.3758/BF03194519</ext-link></comment>.<pub-id pub-id-type="pmid">11578045</pub-id></mixed-citation></ref><ref id="bibr19-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eysenck</surname><given-names>M. W.</given-names></name><name><surname>Derakshan</surname><given-names>N.</given-names></name><name><surname>Santos</surname><given-names>R.</given-names></name><name><surname>Calvo</surname><given-names>M. G.</given-names></name></person-group> (<year>2007</year>) <article-title>Anxiety and cognitive performance: Attentional control theory</article-title>. <source>Emotion</source>
<volume>7</volume>: <fpage>336</fpage>&#x02013;<lpage>353</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/1528-3542.7.2.336">https://doi.org/10.1037/1528-3542.7.2.336</ext-link></comment>.<pub-id pub-id-type="pmid">17516812</pub-id></mixed-citation></ref><ref id="bibr20-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>E.</given-names></name><name><surname>Russo</surname><given-names>R.</given-names></name><name><surname>Bowles</surname><given-names>R. J.</given-names></name><name><surname>Dutton</surname><given-names>K.</given-names></name></person-group> (<year>2001</year>) <article-title>Do threatening stimuli draw or hold visual attentional in subclinical anxiety?</article-title>
<source>Journal of Experimental Psychology: General</source>
<volume>130</volume>: <fpage>681</fpage>&#x02013;<lpage>700</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-3445.130.4.681">https://doi.org/10.1037/0096-3445.130.4.681</ext-link></comment>.<pub-id pub-id-type="pmid">11757875</pub-id></mixed-citation></ref><ref id="bibr21-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>C. H.</given-names></name><name><surname>Hansen</surname><given-names>R. D.</given-names></name></person-group> (<year>1988</year>) <article-title>Finding the face in the crowd &#x02013; An anger superiority effect</article-title>. <source>Journal of Personality and Social Pscyhology</source>
<volume>54</volume>: <fpage>917</fpage>&#x02013;<lpage>924</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0022-3514.54.6.917">https://doi.org/10.1037/0022-3514.54.6.917</ext-link></comment>.</mixed-citation></ref><ref id="bibr22-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horstmann</surname><given-names>G.</given-names></name></person-group> (<year>2007</year>) <article-title>Preattentive face processing: What do visual search experiments with schematic faces tell us?</article-title>
<source>Visual Cognition</source>
<volume>15</volume>: <fpage>799</fpage>&#x02013;<lpage>833</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/13506280600892798">https://doi.org/10.1080/13506280600892798</ext-link></comment>.</mixed-citation></ref><ref id="bibr23-2041669518755806"><mixed-citation publication-type="other"><comment>Im, H. Y., Adams R. B. Jr., Boshyan, J., Ward, N., Cushing, C. A., &#x00026; Kveraga, K. (2017). Observer's anxiety facilitates magnocellular processing of clear facial threat cues, but impairs parvocellular processing of ambiguous facial threat cues. <italic>Scientific Reports</italic>, <italic>7</italic>, 15151. doi: 10.1038/s41598-017-15495-2</comment>.</mixed-citation></ref><ref id="bibr24-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isbell</surname><given-names>L. A.</given-names></name></person-group> (<year>2006</year>) <article-title>Snakes as agents of evolutionary change in primate brains</article-title>. <source>Journal of Human Evolution</source>
<volume>51</volume>: <fpage>1</fpage>&#x02013;<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jhevol.2005.12.012">https://doi.org/10.1016/j.jhevol.2005.12.012</ext-link></comment>.<pub-id pub-id-type="pmid">16545427</pub-id></mixed-citation></ref><ref id="bibr25-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klumpp</surname><given-names>H.</given-names></name><name><surname>Amir</surname><given-names>N.</given-names></name></person-group> (<year>2009</year>) <article-title>Examination of vigilance and disengagement of threat in social anxiety with a probe detection task</article-title>. <source>Anxiety, Stress, and Coping</source>
<volume>22</volume>: <fpage>283</fpage>&#x02013;<lpage>296</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/10615800802449602">https://doi.org/10.1080/10615800802449602</ext-link></comment>.</mixed-citation></ref><ref id="bibr26-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurdi</surname><given-names>B.</given-names></name><name><surname>Lozano</surname><given-names>S.</given-names></name><name><surname>Banaji</surname><given-names>M. R.</given-names></name></person-group> (<year>2017</year>) <article-title>Introducing the Open Affective Standardized Image Set (OASIS)</article-title>. <source>Behavior Research Methods</source>
<volume>49</volume>: <fpage>457</fpage>&#x02013;<lpage>470</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3758/s13428-016-0715-3">http://dx.doi.org/10.3758/s13428-016-0715-3</ext-link></comment>.<pub-id pub-id-type="pmid">26907748</pub-id></mixed-citation></ref><ref id="bibr27-2041669518755806"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kveraga</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>) <article-title>Threat perception in visual scenes: Dimensions, action and neural dynamics</article-title>. In: <person-group person-group-type="editor"><name><surname>Kveraga</surname><given-names>K.</given-names></name><name><surname>Bar</surname><given-names>M.</given-names></name></person-group> (eds) <source>Scene Vision: Making sense of what we see</source>, <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, pp. <fpage>291</fpage>&#x02013;<lpage>307</lpage>.</mixed-citation></ref><ref id="bibr28-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kveraga</surname><given-names>K.</given-names></name><name><surname>Boshyan</surname><given-names>J.</given-names></name><name><surname>Adams</surname><given-names>R. B.</given-names></name><name><surname>Mote</surname><given-names>J.</given-names></name><name><surname>Betz</surname><given-names>N.</given-names></name><name><surname>Ward</surname><given-names>N.</given-names></name><name><surname>Barrett</surname><given-names>L. F.</given-names></name></person-group> (<year>2015</year>) <article-title>If it bleeds, it leads: Separating threat from mere negativity</article-title>. <source>Social Cognitive Affective Neuroscience</source>
<volume>10</volume>: <fpage>28</fpage>&#x02013;<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nsu007">https://doi.org/10.1093/scan/nsu007</ext-link></comment>.<pub-id pub-id-type="pmid">24493851</pub-id></mixed-citation></ref><ref id="bibr29-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kveraga</surname><given-names>K.</given-names></name><name><surname>Boshyan</surname><given-names>J.</given-names></name><name><surname>Bar</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>) <article-title>Magnocellular projections as the trigger of top-down facilitation in recognition</article-title>. <source>Journal of Neuroscience</source>
<volume>27</volume>: <fpage>13232</fpage>&#x02013;<lpage>13240</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3481-07.2007">https://doi.org/10.1523/JNEUROSCI.3481-07.2007</ext-link></comment>.<pub-id pub-id-type="pmid">18045917</pub-id></mixed-citation></ref><ref id="bibr30-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>P. J.</given-names></name><name><surname>Bradley</surname><given-names>M. M.</given-names></name><name><surname>Cuthbert</surname><given-names>B. N.</given-names></name></person-group> (<year>1990</year>) <article-title>Emotion, attention, and the startle reflex</article-title>. <source>Psychological Review</source>
<volume>97</volume>: <fpage>377</fpage>&#x02013;<lpage>395</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.97.3.377">https://doi.org/10.1037/0033-295X.97.3.377</ext-link></comment>.<pub-id pub-id-type="pmid">2200076</pub-id></mixed-citation></ref><ref id="bibr31-2041669518755806"><mixed-citation publication-type="other"><comment>Lang, P. J., Bradley, M. M, &#x00026; Cuthbert, B. N. (2005). <italic>International affective picture system (IAPS): Affective ratings of pictures and instruction manual.</italic> Technical Report No A-6. University of Florida. Gainesville, FL</comment>.</mixed-citation></ref><ref id="bibr32-2041669518755806"><mixed-citation publication-type="other"><comment>Larsen, R. J., &#x00026; Diener, E. (1992). Promises and problems with the circumplex model of emotion. In M. S. Clark (Ed.), <italic>Review of personality and social psychology: Emotion</italic> (Vol. 13, pp. 25&#x02013;59). Newbury Park, CA: Sage</comment>.</mixed-citation></ref><ref id="bibr33-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lasaitis</surname><given-names>C.</given-names></name><name><surname>Ribeiro</surname><given-names>R. L.</given-names></name><name><surname>Bueno</surname><given-names>O. F. A.</given-names></name></person-group> (<year>2008</year>) <article-title>Brazilian norms for the International Affective Picture System (IAPS): Comparison of the affective ratings for new stimuli between Brazilian and North-American subjects</article-title>. <source>Jornal Brasileiro De Psiquiatria</source>
<volume>57</volume>: <fpage>270</fpage>&#x02013;<lpage>275</lpage>.</mixed-citation></ref><ref id="bibr34-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>Q. V.</given-names></name><name><surname>Isbell</surname><given-names>L. A.</given-names></name><name><surname>Matsumoto</surname><given-names>J.</given-names></name><name><surname>Le</surname><given-names>V. Q.</given-names></name><name><surname>Nishimaru</surname><given-names>H.</given-names></name><name><surname>Hori</surname><given-names>E.</given-names></name><name><surname>Nishijo</surname><given-names>H.</given-names></name></person-group> (<year>2016</year>) <article-title>Snakes elicit earlier, and monkey faces, later, gamma oscillations in macaque pulvinar neurons</article-title>. <source>Scientific Reports</source>
<volume>6</volume>: <fpage>20595</fpage>&#x02013;<lpage>20604</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep20595">https://doi.org/10.1038/srep20595</ext-link></comment>.<pub-id pub-id-type="pmid">26854087</pub-id></mixed-citation></ref><ref id="bibr35-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>Q. V.</given-names></name><name><surname>Isbell</surname><given-names>L. A.</given-names></name><name><surname>Matsumoto</surname><given-names>J.</given-names></name><name><surname>Nguyen</surname><given-names>M.</given-names></name><name><surname>Hori</surname><given-names>E.</given-names></name><name><surname>Maior</surname><given-names>R. S.</given-names></name><name><surname>Nishijo</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>) <article-title>Pulvinar neurons reveal neurobiological evidence of past selection for rapid detection of snakes</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>
<volume>110</volume>: <fpage>19000</fpage>&#x02013;<lpage>19005</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1312648110">https://doi.org/10.1073/pnas.1312648110</ext-link></comment>.<pub-id pub-id-type="pmid">24167268</pub-id></mixed-citation></ref><ref id="bibr36-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeDoux</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>) <article-title>Rethinking the emotional brain</article-title>. <source>Neuron</source>
<volume>73</volume>: <fpage>653</fpage>&#x02013;<lpage>676</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2012.02.004">https://doi.org/10.1016/j.neuron.2012.02.004</ext-link></comment>.<pub-id pub-id-type="pmid">22365542</pub-id></mixed-citation></ref><ref id="bibr37-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LoBue</surname><given-names>V.</given-names></name></person-group> (<year>2009</year>) <article-title>More than just a face in the crowd: Detection of emotional facial expressions in young children and adults</article-title>. <source>Developmental Science</source>
<volume>12</volume>: <fpage>305</fpage>&#x02013;<lpage>313</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-7687.2008.00767.x">https://doi.org/10.1111/j.1467-7687.2008.00767.x</ext-link></comment>.<pub-id pub-id-type="pmid">19143803</pub-id></mixed-citation></ref><ref id="bibr38-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LoBue</surname><given-names>V.</given-names></name></person-group> (<year>2010</year>a) <article-title>And along came a spider: Superior detection of spiders in children and adults</article-title>. <source>Journal of Experimental Child Psychology</source>
<volume>107</volume>: <fpage>59</fpage>&#x02013;<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jecp.2010.04.005">https://doi.org/10.1016/j.jecp.2010.04.005</ext-link></comment>.<pub-id pub-id-type="pmid">20529694</pub-id></mixed-citation></ref><ref id="bibr39-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LoBue</surname><given-names>V.</given-names></name></person-group> (<year>2010</year>b) <article-title>What's so scary about needles and knives? Examining the role of experience in threat detection</article-title>. <source>Cognition and Emotion</source>
<volume>24</volume>: <fpage>80</fpage>&#x02013;<lpage>87</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699930802542308">https://doi.org/10.1080/02699930802542308</ext-link></comment>.</mixed-citation></ref><ref id="bibr40-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LoBue</surname><given-names>V.</given-names></name><name><surname>DeLoache</surname><given-names>J. S.</given-names></name></person-group> (<year>2008</year>) <article-title>Detectiong the snake in the grass: Attention to ffear-relevant stimuli by adults and young children</article-title>. <source>Psychological Science</source>
<volume>19</volume>: <fpage>284</fpage>&#x02013;<lpage>289</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2008.02081.x">https://doi.org/10.1111/j.1467-9280.2008.02081.x</ext-link></comment>.<pub-id pub-id-type="pmid">18315802</pub-id></mixed-citation></ref><ref id="bibr41-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LoBue</surname><given-names>V.</given-names></name><name><surname>DeLoache</surname><given-names>J. S.</given-names></name></person-group> (<year>2010</year>) <article-title>Superior detection of threat-relevant stimuli in infancy</article-title>. <source>Developmental Science</source>
<volume>13</volume>: <fpage>221</fpage>&#x02013;<lpage>228</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-7687.2009.00872.x">https://doi.org/10.1111/j.1467-7687.2009.00872.x</ext-link></comment>.<pub-id pub-id-type="pmid">20121878</pub-id></mixed-citation></ref><ref id="bibr42-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LoBue</surname><given-names>V.</given-names></name><name><surname>Rakison</surname><given-names>D. H.</given-names></name><name><surname>DeLoache</surname><given-names>J. S.</given-names></name></person-group> (<year>2010</year>) <article-title>Threat perception across the life span: Evidence for multiple converging pathways</article-title>. <source>Current Directions in Psychological Science</source>
<volume>19</volume>: <fpage>375</fpage>&#x02013;<lpage>379</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0963721410388801">https://doi.org/10.1177/0963721410388801</ext-link></comment>.</mixed-citation></ref><ref id="bibr43-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lohani</surname><given-names>M.</given-names></name><name><surname>Gupta</surname><given-names>R.</given-names></name><name><surname>Srinivasan</surname><given-names>N.</given-names></name></person-group> (<year>2013</year>) <article-title>Cross-culturalevaluation of the International Affective Picture System on an Indian sample</article-title>. <source>Psychological Studies</source>
<volume>58</volume>: <fpage>233</fpage>&#x02013;<lpage>241</lpage>.</mixed-citation></ref><ref id="bibr44-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mogg</surname><given-names>K.</given-names></name><name><surname>Bradley</surname><given-names>B. P.</given-names></name></person-group> (<year>1999</year>) <article-title>Orienting of attention to threatening facial expressions presented under conditions of restricted awarness</article-title>. <source>Cognition and Emotion</source>
<volume>13</volume>: <fpage>713</fpage>&#x02013;<lpage>740</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/026999399379050">https://doi.org/10.1080/026999399379050</ext-link></comment>.</mixed-citation></ref><ref id="bibr45-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mogg</surname><given-names>K.</given-names></name><name><surname>Millar</surname><given-names>N.</given-names></name><name><surname>Bradley</surname><given-names>B. P.</given-names></name></person-group> (<year>2000</year>) <article-title>Biases in eye movements to threatening facial expressions in feneralized anxiety disorder and depressive disorder</article-title>. <source>Journal of Abnormal Psychology</source>
<volume>109</volume>: <fpage>695</fpage>&#x02013;<lpage>704</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0021-843X.109.4.695">https://doi.org/10.1037/0021-843X.109.4.695</ext-link></comment>.<pub-id pub-id-type="pmid">11195993</pub-id></mixed-citation></ref><ref id="bibr46-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moriguchi</surname><given-names>Y.</given-names></name><name><surname>Negreira</surname><given-names>A.</given-names></name><name><surname>Weierich</surname><given-names>M.</given-names></name><name><surname>Dautoff</surname><given-names>R.</given-names></name><name><surname>Dickerson</surname><given-names>B. C.</given-names></name><name><surname>Wright</surname><given-names>C. I.</given-names></name><name><surname>Barrett</surname><given-names>L. F.</given-names></name></person-group> (<year>2011</year>) <article-title>Differential hemodynamic response in affective circuitry with aging: An fMRI study of novelty, valence, and arousal</article-title>. <source>Journal of Cognitive Neuroscience</source>
<volume>23</volume>: <fpage>1027</fpage>&#x02013;<lpage>1041</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2010.21527">https://doi.org/10.1162/jocn.2010.21527</ext-link></comment>.<pub-id pub-id-type="pmid">20521849</pub-id></mixed-citation></ref><ref id="bibr47-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>New</surname><given-names>J.</given-names></name><name><surname>Cosmides</surname><given-names>L.</given-names></name><name><surname>Tooby</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>) <article-title>Category-specific attention for animals reflects ancestral priorities, not expertise</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>
<volume>104</volume>: <fpage>16598</fpage>&#x02013;<lpage>16603</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0703913104">https://doi.org/10.1073/pnas.0703913104</ext-link></comment>.<pub-id pub-id-type="pmid">17909181</pub-id></mixed-citation></ref><ref id="bibr48-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L.</given-names></name><name><surname>Hyona</surname><given-names>J.</given-names></name><name><surname>Calvo</surname><given-names>M. G.</given-names></name></person-group> (<year>2009</year>) <article-title>Emotional scene content drives the saccade generation system reflecively</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>
<volume>35</volume>: <fpage>305</fpage>&#x02013;<lpage>323</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0013626">https://doi.org/10.1037/a0013626</ext-link></comment>.<pub-id pub-id-type="pmid">19331490</pub-id></mixed-citation></ref><ref id="bibr49-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>&#x000d6;hman</surname><given-names>A.</given-names></name><name><surname>Flykt</surname><given-names>A.</given-names></name><name><surname>Esteves</surname><given-names>F.</given-names></name></person-group> (<year>2001</year>) <article-title>Emotion drives attention: Detecting the snake in the grass</article-title>. <source>Journal of Experimental Psychology: General</source>
<volume>130</volume>: <fpage>466</fpage>&#x02013;<lpage>478</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-3445.130.3.466">https://doi.org/10.1037/0096-3445.130.3.466</ext-link></comment>.<pub-id pub-id-type="pmid">11561921</pub-id></mixed-citation></ref><ref id="bibr50-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>&#x000d6;hman</surname><given-names>A.</given-names></name><name><surname>Mineka</surname><given-names>S.</given-names></name></person-group> (<year>2001</year>) <article-title>Fears, phobias, and preparedness: Toward an evolved module of fear and fear learning</article-title>. <source>Psychological Review</source>
<volume>108</volume>: <fpage>483</fpage>&#x02013;<lpage>522</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.108.3.483">https://doi.org/10.1037/0033-295X.108.3.483</ext-link></comment>.<pub-id pub-id-type="pmid">11488376</pub-id></mixed-citation></ref><ref id="bibr51-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>&#x000d6;hman</surname><given-names>A.</given-names></name><name><surname>Soares</surname><given-names>S. C.</given-names></name><name><surname>Juth</surname><given-names>P.</given-names></name><name><surname>Lindstr&#x000f6;m</surname><given-names>B. R.</given-names></name><name><surname>Esteves</surname><given-names>F.</given-names></name></person-group> (<year>2012</year>) <article-title>Evolutionary derived modulations of attention to two common fear stimuli: Serpents and hostile humans</article-title>. <source>Journal of Cognitive Psychology</source>
<volume>24</volume>: <fpage>17</fpage>&#x02013;<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/20445911.2011.629603">https://doi.org/10.1080/20445911.2011.629603</ext-link></comment>.</mixed-citation></ref><ref id="bibr52-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterwijk</surname><given-names>S.</given-names></name></person-group> (<year>2017</year>) <article-title>Choosing the negative: A behavioral demonstration of morbid curiosity</article-title>. <source>PLoS One</source>
<volume>12</volume>: <fpage>e0178399</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0178399">https://doi.org/10.1371/journal.pone.0178399</ext-link></comment>.<pub-id pub-id-type="pmid">28683147</pub-id></mixed-citation></ref><ref id="bibr53-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>S. E.</given-names></name></person-group> (<year>1975</year>) <article-title>The effectsof contextual scenes on the identification of objects</article-title>. <source>Memory &#x00026; Cognition</source>
<volume>3</volume>: <fpage>519</fpage>&#x02013;<lpage>526</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03197524">https://doi.org/10.3758/BF03197524</ext-link></comment>.<pub-id pub-id-type="pmid">24203874</pub-id></mixed-citation></ref><ref id="bibr54-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>M. T.</given-names></name><name><surname>Kveraga</surname><given-names>K.</given-names></name><name><surname>Chaumon</surname><given-names>M.</given-names></name><name><surname>Bar</surname><given-names>M.</given-names></name><name><surname>Barrett</surname><given-names>L. F.</given-names></name></person-group> (<year>2017</year>) <article-title>Internal valence modulates the speed of object recognition</article-title>. <source>Scientific Reports</source>. <comment>Advance online publication. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-017-00385-4">https://doi.org/10.1038/s41598-017-00385-4</ext-link></comment>.</mixed-citation></ref><ref id="bibr55-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R.</given-names></name></person-group> (<year>1993</year>) <article-title>Methods for dealing with reaction time outliers</article-title>. <source>Psychological Bulletin</source>
<volume>114</volume>: <fpage>510</fpage>&#x02013;<lpage>532</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-2909.114.3.510">https://doi.org/10.1037/0033-2909.114.3.510</ext-link></comment>.<pub-id pub-id-type="pmid">8272468</pub-id></mixed-citation></ref><ref id="bibr56-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>J. A.</given-names></name></person-group> (<year>1980</year>) <article-title>A circumplex model of affect</article-title>. <source>Journal of Personality and Social Pscyhology</source>
<volume>39</volume>: <fpage>1161</fpage>&#x02013;<lpage>1178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0077714">https://doi.org/10.1037/h0077714</ext-link></comment>.</mixed-citation></ref><ref id="bibr57-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schupp</surname><given-names>H. T.</given-names></name><name><surname>&#x000d6;hman</surname><given-names>A.</given-names></name><name><surname>Junghofer</surname><given-names>M.</given-names></name><name><surname>Weike</surname><given-names>A. I.</given-names></name><name><surname>Stockburger</surname><given-names>J.</given-names></name><name><surname>Hamm</surname><given-names>A. O.</given-names></name></person-group> (<year>2004</year>) <article-title>The facilitated processing of threatening faces: An ERP analysis</article-title>. <source>Emotion</source>
<volume>4</volume>: <fpage>189</fpage>&#x02013;<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/1528-3542.4.2.189">https://doi.org/10.1037/1528-3542.4.2.189</ext-link></comment>.<pub-id pub-id-type="pmid">15222855</pub-id></mixed-citation></ref><ref id="bibr58-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seligman</surname><given-names>M. E.</given-names></name></person-group> (<year>1971</year>) <article-title>Phobias and preparedness</article-title>. <source>Behavior Therapy</source>
<volume>2</volume>: <fpage>307</fpage>&#x02013;<lpage>320</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0005-7894">https://doi.org/10.1016/S0005-7894</ext-link>(71)80064-3</comment>.</mixed-citation></ref><ref id="bibr59-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snodgrass</surname><given-names>J. G.</given-names></name><name><surname>Vanderwart</surname><given-names>M.</given-names></name></person-group> (<year>1980</year>) <article-title>A standardized set of 260 pictures: Norms for name agreement, image agreement, familiarity, and visual complexity</article-title>. <source>Journal of Experimental Psychology: Human Learning and Memory</source>
<volume>6</volume>: <fpage>174</fpage>&#x02013;<lpage>215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0278-7393.6.2.174">https://doi.org/10.1037/0278-7393.6.2.174</ext-link></comment>.<pub-id pub-id-type="pmid">7373248</pub-id></mixed-citation></ref><ref id="bibr60-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>C.</given-names></name><name><surname>Kveraga</surname><given-names>K.</given-names></name><name><surname>Huberle</surname><given-names>E.</given-names></name><name><surname>Karnath</surname><given-names>H.-O.</given-names></name><name><surname>Bar</surname><given-names>M.</given-names></name></person-group> (<year>2012</year>) <article-title>Enabling global processing in simultanagnosia by psychophysical biasing of visual pathways</article-title>. <source>Brain</source>
<volume>135</volume>: <fpage>1578</fpage>&#x02013;<lpage>1585</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/aws066">https://doi.org/10.1093/brain/aws066</ext-link></comment>.<pub-id pub-id-type="pmid">22418740</pub-id></mixed-citation></ref><ref id="bibr61-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tipples</surname><given-names>J.</given-names></name><name><surname>Atkinson</surname><given-names>A. P.</given-names></name><name><surname>Young</surname><given-names>A. W.</given-names></name></person-group> (<year>2002</year>) <article-title>The eyebrow frown: A salient social signal</article-title>. <source>Emotion</source>
<volume>2</volume>: <fpage>288</fpage>&#x02013;<lpage>296</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/1528-3542.2.3.288">https://doi.org/10.1037/1528-3542.2.3.288</ext-link></comment>.<pub-id pub-id-type="pmid">12899361</pub-id></mixed-citation></ref><ref id="bibr62-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>D. B.</given-names></name><name><surname>Chai</surname><given-names>B.</given-names></name><name><surname>Caddigan</surname><given-names>E.</given-names></name><name><surname>Beck</surname><given-names>D. M.</given-names></name><name><surname>Li</surname><given-names>F.-F.</given-names></name></person-group> (<year>2011</year>) <article-title>Simple line drawings suffice for functional MRI decoding of natural scene categories</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>
<volume>108</volume>: <fpage>9661</fpage>&#x02013;<lpage>9666</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1015666108">https://doi.org/10.1073/pnas.1015666108</ext-link></comment>.<pub-id pub-id-type="pmid">21593417</pub-id></mixed-citation></ref><ref id="bibr63-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>D.</given-names></name><name><surname>Tellegen</surname><given-names>A.</given-names></name></person-group> (<year>1985</year>) <article-title>Toward a consensual structure of mood</article-title>. <source>Psychological Bulletin</source>
<volume>98</volume>: <fpage>219</fpage>&#x02013;<lpage>223</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-2909.98.2.219">https://doi.org/10.1037/0033-2909.98.2.219</ext-link></comment>.<pub-id pub-id-type="pmid">3901060</pub-id></mixed-citation></ref><ref id="bibr64-2041669518755806"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yiend</surname><given-names>J.</given-names></name><name><surname>Mathews</surname><given-names>A.</given-names></name></person-group> (<year>2001</year>) <article-title>Anxiety and attention to threatening pictures</article-title>. <source>Quarterly Journal of Experimental Psychology: A Human Experimental Psychology</source>
<volume>54</volume>: <fpage>665</fpage>&#x02013;<lpage>681</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/713755991">https://doi.org/10.1080/713755991</ext-link></comment>.<pub-id pub-id-type="pmid">11548029</pub-id></mixed-citation></ref></ref-list></back></article>
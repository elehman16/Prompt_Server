<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Exp Brain Res</journal-id><journal-id journal-id-type="iso-abbrev">Exp Brain Res</journal-id><journal-title-group><journal-title>Experimental Brain Research</journal-title></journal-title-group><issn pub-type="ppub">0014-4819</issn><issn pub-type="epub">1432-1106</issn><publisher><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">26879767</article-id><article-id pub-id-type="pmc">4851710</article-id><article-id pub-id-type="publisher-id">4567</article-id><article-id pub-id-type="doi">10.1007/s00221-016-4567-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>An assessment of auditory-guided locomotion in an obstacle circumvention task</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kolarik</surname><given-names>Andrew J.</given-names></name><address><phone>08451962642</phone><email>andrewjoseph.kolarik@sas.ac.uk</email><email>andrew.kolarik@anglia.ac.uk</email><email>ak771@cam.ac.uk</email></address><xref ref-type="aff" rid="Aff1"/><xref ref-type="aff" rid="Aff2"/><xref ref-type="aff" rid="Aff3"/></contrib><contrib contrib-type="author"><name><surname>Scarfe</surname><given-names>Amy C.</given-names></name><address><email>amy.scarfe@anglia.ac.uk</email><email>amy.scarfe@sth.nhs.uk</email></address><xref ref-type="aff" rid="Aff2"/><xref ref-type="aff" rid="Aff4"/></contrib><contrib contrib-type="author"><name><surname>Moore</surname><given-names>Brian C. J.</given-names></name><address><email>bcjm@cam.ac.uk</email></address><xref ref-type="aff" rid="Aff3"/></contrib><contrib contrib-type="author"><name><surname>Pardhan</surname><given-names>Shahina</given-names></name><address><email>shahina.pardhan@anglia.ac.uk</email></address><xref ref-type="aff" rid="Aff2"/></contrib><aff id="Aff1"><label/>Centre for the Study of the Senses, Institute of Philosophy, University of London, Senate House, Malet Street, London, WC1E 7HU UK </aff><aff id="Aff2"><label/>Vision and Eye Research Unit (VERU), Postgraduate Medical Institute, Anglia Ruskin University, Eastings 204, East Road, Cambridge, CB1 1PT UK </aff><aff id="Aff3"><label/>Department of Psychology, University of Cambridge, Downing Street, Cambridge, CB2 3EB UK </aff><aff id="Aff4"><label/>Department of Clinical Engineering, Medical Imaging and Medical Physics Directorate, Sheffield Teaching Hospitals NHS Foundation Trust, Sheffield, UK </aff></contrib-group><pub-date pub-type="epub"><day>15</day><month>2</month><year>2016</year></pub-date><pub-date pub-type="pmc-release"><day>15</day><month>2</month><year>2016</year></pub-date><pub-date pub-type="ppub"><year>2016</year></pub-date><volume>234</volume><fpage>1725</fpage><lpage>1735</lpage><history><date date-type="received"><day>7</day><month>7</month><year>2015</year></date><date date-type="accepted"><day>30</day><month>11</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2016</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p>This study investigated how effectively audition can be used to guide navigation around an obstacle. Ten blindfolded normally sighted participants navigated around a 0.6&#x000a0;&#x000d7;&#x000a0;2&#x000a0;m obstacle while producing self-generated mouth click sounds. Objective movement performance was measured using a Vicon motion capture system. Performance with full vision without generating sound was used as a baseline for comparison. The obstacle&#x02019;s location was varied randomly from trial to trial: it was either straight ahead or 25&#x000a0;cm to the left or right relative to the participant. Although audition provided sufficient information to detect the obstacle and guide participants around it without collision in the majority of trials, buffer space (clearance between the shoulder and obstacle), overall movement times, and number of velocity corrections were significantly (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05) greater with auditory guidance than visual guidance. Collisions sometime occurred under auditory guidance, suggesting that audition did not always provide an accurate estimate of the space between the participant and obstacle. Unlike visual guidance, participants did not always walk around the side that afforded the most space during auditory guidance. Mean buffer space was 1.8 times higher under auditory than under visual guidance. Results suggest that sound can be used to generate buffer space when vision is unavailable, allowing navigation around an obstacle without collision in the majority of trials.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Obstacle avoidance</kwd><kwd>Navigation</kwd><kwd>Echolocation</kwd><kwd>Locomotion</kwd><kwd>Central nervous system</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer-Verlag Berlin Heidelberg 2016</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p>Vision provides important information for locomotion, allowing individuals to detect and walk around obstacles in the path of travel, and avoid collisions. Although vision loss can cause substantial reduction in spatial awareness, audition is capable of offering a rich source of spatial information when vision is degraded or lost (Zahorik et al. <xref ref-type="bibr" rid="CR59">2005</xref>). Relatively little is known regarding how sound informs dynamic obstacle clearance and how accurately sound can inform the locomotor system in order to avoid collisions (Kolarik et al. <xref ref-type="bibr" rid="CR25">2014a</xref>). This study investigated whether sound could be used in the absence of vision to circumvent an obstacle obstructing the path of travel, a task frequently encountered during daily locomotion (G&#x000e9;rin-Lajoie et al. <xref ref-type="bibr" rid="CR15">2008</xref>; Hackney et al. <xref ref-type="bibr" rid="CR19">2014</xref>).</p><p>Some blind humans echolocate by emitting sounds and use the returning echoes to obtain spatial information (Rosenblum <xref ref-type="bibr" rid="CR38">2011</xref>; Teng et al. <xref ref-type="bibr" rid="CR48">2012</xref>), and this skill can be used for navigation. Wallmeier and Wiegrebe (<xref ref-type="bibr" rid="CR54">2014b</xref>) showed that during echolocation sighted and blind participants orient the body and head towards a desired location. Fiehler et al. (<xref ref-type="bibr" rid="CR11">2015</xref>) showed that blind echolocation experts performed better than blind and sighted novices in a task involving listening to pre-recorded binaural echolocation clicks that were created while walking along a corridor, and reporting the direction of the corridor (left, right, or straight ahead). Strelow and Brabyn (<xref ref-type="bibr" rid="CR45">1982</xref>) reported that blind and normally sighted blindfolded participants could use sounds to travel parallel to a wall. Performance was poorer when a line of poles was used rather than a wall. Gordon and Rosenblum (<xref ref-type="bibr" rid="CR17">2004</xref>) showed that normally sighted blindfolded participants were able to use acoustic information to judge whether apertures of various width could afford passage without turning their body. Kolarik et al. (<xref ref-type="bibr" rid="CR26">2014b</xref>) investigated navigation using echoic sensory substitution devices (SSDs), which are electronic travel aids based on an echolocation principle that locate silent objects (these are distinct from &#x0201c;visual pattern&#x0201d; SSDs, that convert visual information into an auditory signal using a predetermined transformation algorithm). They reported that echoic information informed the degree of shoulder rotation required by blindfolded sighted participants to move through narrow apertures. When near a wall, low-frequency sound informed locomotion by blind individuals to allow them to walk parallel to the wall (Ashmead et al. <xref ref-type="bibr" rid="CR4">1998</xref>). Ashmead et al. (<xref ref-type="bibr" rid="CR3">1989</xref>) examined how congenitally blind children walked along a path that was sometimes obstructed by an obstacle. Children spent longer in front of the obstacle than behind, suggesting they perceived the presence of the obstacle, and spent more time in front in order to notice and navigate around it. However, they were not given specific instructions regarding whether they should make sounds or what they should do regarding the obstacle, and they contacted it in approximately half of the trials.</p><p>Several studies have investigated whether audition informs locomotion for blind (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>; Worchel et al. <xref ref-type="bibr" rid="CR58">1950</xref>; Worchel and Mauney <xref ref-type="bibr" rid="CR57">1951</xref>; Ammons et al. <xref ref-type="bibr" rid="CR1">1953</xref>) and blindfolded sighted participants (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>; Carlson-Smith and Weiner <xref ref-type="bibr" rid="CR6">1996</xref>; Rosenblum et al. <xref ref-type="bibr" rid="CR39">2000</xref>) using an obstacle approach and detection task. Although collisions did sometimes occur, participants were able to use sound to approach a large obstacle such as a wall, report when they first perceived it, approach it as close as possible without touching it, and to stop in front of it to face it. Various sounds could be used to perform the task, including shoe clicks on a hardwood floor (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>; Worchel and Dallenbach <xref ref-type="bibr" rid="CR56">1947</xref>) or linoleum (Carlson-Smith and Weiner <xref ref-type="bibr" rid="CR6">1996</xref>), sounds from shoes when walking along a concrete path (Worchel et al. <xref ref-type="bibr" rid="CR58">1950</xref>; Worchel and Mauney <xref ref-type="bibr" rid="CR57">1951</xref>; Ammons et al. <xref ref-type="bibr" rid="CR1">1953</xref>), or a sound chosen by the participant, such as mouth clicks or the words &#x0201c;hello&#x0201d; or &#x0201c;check&#x0201d; (Rosenblum et al. <xref ref-type="bibr" rid="CR39">2000</xref>). Detection was also possible with sounds from feet wearing stockings on a carpet, although performance was poorer than that with shoes on a hardwood floor (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>).</p><p>In real-life travel situations, individuals often have to detect and then circumvent an obstacle. An assessment of the ability to use sound for this is the focus of the present study. Obstacle circumvention is arguably a more difficult task than obstacle approach and detection, since the former requires the individual to detect the obstacle&#x02019;s position and edges, perform precise motor actions to move around it, and allow adequate space between the obstacle and the body at the point of moving past it to ensure safe navigation. Research in the visual domain has indicated that individuals maintain a protective envelope around themselves when moving around obstacles (G&#x000e9;rin-Lajoie et al. <xref ref-type="bibr" rid="CR15">2008</xref>; Hackney et al. <xref ref-type="bibr" rid="CR19">2014</xref>), constituting the personal space around the body when walking (G&#x000e9;rin-Lajoie et al. <xref ref-type="bibr" rid="CR14">2005</xref>). In addition, full vision allows passing on the side of the obstacle affording most space (Fajen and Warren <xref ref-type="bibr" rid="CR10">2003</xref>; Hackney et al. <xref ref-type="bibr" rid="CR19">2014</xref>). The term &#x0201c;buffer space&#x0201d;, used by Franchak et al. (<xref ref-type="bibr" rid="CR12">2012</xref>) to describe the margin between the body and sides of an aperture during locomotion, is used here to denote the space between the shoulders and the obstacle. It is currently unknown whether sound allows participants to generate a buffer space and to pass on the side affording the most space.</p><p>Locomotion under auditory guidance is often relatively slow (Strelow and Brabyn <xref ref-type="bibr" rid="CR45">1982</xref>). However, movement indices including buffer space, movement time, and number of velocity corrections have not previously been assessed for obstacle approach and circumvention using sound. In the current study, a Vicon motion capture system was utilized to formally quantify these indices to enable greater insight regarding how useful auditory-guided locomotion is for real-life travel situations, for example by quantifying how large a margin of safety is needed when avoiding an obstacle using sound, and to provide a performance baseline for rehabilitation training for auditory-guided locomotion following visual loss.</p><p>In summary, the aim of the current study was to build on and extend previous work investigating auditory-guided locomotion in indoor environments (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>; Strelow and Brabyn <xref ref-type="bibr" rid="CR45">1982</xref>; Carlson-Smith and Weiner <xref ref-type="bibr" rid="CR6">1996</xref>) by assessing the obstacle circumvention performance of blindfolded sighted echo-na&#x000ef;ve participants generating mouth click sounds. Visually guided locomotion was used as a baseline. We hypothesized that sound would provide spatial information allowing participants to circumvent an obstacle in their path accurately and safely. Compared to visual guidance, auditory guidance was predicted to result in greater space between the shoulders and the obstacle (buffer space), more velocity corrections, and longer movement times reflecting a more cautious approach, as well as an increased number of impacts. An additional &#x0201c;no-click&#x0201d; control experiment was conducted, to compare obstacle circumvention using mouth clicks to that using ambient sound or footfalls on the carpet.</p></sec><sec id="Sec2" sec-type="materials|methods"><title>Methods</title><sec id="Sec3"><title>Participants</title><p>There were 10 participants in the main experiment (seven males and three females, mean age 31 years, range 21&#x02013;42 years). For the &#x0201c;no-click&#x0201d; control experiment, there were nine participants (seven males and two females, mean age 34 years, range 26&#x02013;42 years, six of whom took part in the main experiment). All reported normal or corrected to normal vision, and had normal hearing, defined as better-ear average (BEA) hearing threshold across the frequencies 500, 1000, 2000 and 4000&#x000a0;Hz&#x000a0;&#x02264;&#x000a0;15&#x000a0;dB HL, as measured using an Interacoustics AS608 audiometer following the procedure recommended by the British Society of Audiology (<xref ref-type="bibr" rid="CR5">2011</xref>). None of the participants reported any prior experience using self-generated sounds to perceive objects. The experiments followed the tenets of the Declaration of Helsinki. Written informed consent was obtained from all individual participants, following an explanation of the nature and possible consequences of the study. The experiments were approved by the Anglia Ruskin University Ethics committee.</p></sec><sec id="Sec4"><title>Apparatus and data acquisition</title><p>Participants were tested in a quiet room measuring 5.7&#x000a0;&#x000d7;&#x000a0;3.5&#x000a0;m with a ceiling height of 2.8&#x000a0;m, and with an ambient sound level of approximately 36 dBA. The floor was carpeted, the walls were painted, and the ceiling was tiled. Obstacles were flat, rectangular, movable, and constructed of wood covered by smooth aluminium foil, following Arnott et al. (<xref ref-type="bibr" rid="CR2">2013</xref>). The aluminium foil provided high reflectivity, but also produced near-specular reflection of sounds, perhaps making the obstacle more difficult to localize than bare wood. A small practice obstacle was used for training (width 0.5&#x000a0;&#x000d7;&#x000a0;height 0.34&#x000a0;m), and a large obstacle was used for both training and testing (width 0.6&#x000a0;&#x000d7;&#x000a0;height 2&#x000a0;m). The obstacles were located close to the centre of the room. Silence was maintained during testing, except when participants produced self-generated mouth clicks.</p><p>Three-dimensional kinematic data were collected at 50&#x000a0;Hz using an 8-camera motion capture system (Vicon Bonita; Oxford Metrics Ltd). Retro-reflective spherical markers were attached to the participant, bilaterally, at the following anatomical locations: the most distal, superior aspect of the first toe, the most distal, superior aspect of the fifth toe, the posterior aspect of the calcanei, the acromioclavicular joint, and antero-lateral and postero-lateral aspects of the head. Single markers were placed on the sternum and the posterior aspect of the dominant hand. Three additional markers were attached to the front aspect of the obstacle to determine the width and location of the obstacle within the laboratory coordinate system; these remained attached throughout the experiment. Marker trajectory data were filtered using the cross-validatory quintic spline smoothing routine, with &#x0201c;smoothing&#x0201d; options set at a predicted mean squared error value of 10 and processed using the Plug-in gait software (Oxford Metrics Ltd). Due to the aluminium foil covering the obstacle, incorrect reflections were sometimes recorded by the Vicon system. However, individual markers were carefully labelled for each trial to exclude invalid reflections from the analyses.</p><p>Key variables relating to obstacle circumvention were assessed either using custom-written Visual Basic scripts or recorded by the experimenter. Definitions are presented in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table&#x000a0;1</label><caption><p>Assessed dependent variables and their descriptions</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2">Variable</th></tr></thead><tbody><tr><td align="left" colspan="2">
<italic>Obstacle present trials</italic>
</td></tr><tr><td align="left">Buffer space</td><td align="left">Medio-lateral distance between the shoulder marker and obstacle at the point of crossing the obstacle (defined as the point where the shoulder marker passes the marker attached to the front aspect of the obstacle)</td></tr><tr><td align="left">Movement time</td><td align="left">Time taken to complete movement measured from when the sternum marker was 1&#x000a0;m away from the obstacle in the anterior-posterior direction until the point of crossing the obstacle marker</td></tr><tr><td align="left">Velocity corrections</td><td align="left">Number of changes in forward velocity from when the sternum marker was 1&#x000a0;m away from the obstacle until the point of crossing the obstacle marker</td></tr><tr><td align="left">Obstacle detections</td><td align="left">The experimenter recorded trials where the participant raised their hand marker to indicate obstacle perception</td></tr><tr><td align="left">Obstacle detection range</td><td align="left">Anterior-posterior distance between the participant&#x02019;s sternum marker and obstacle marker, measured at the point at which the participant raised their hand marker to indicate obstacle perception</td></tr><tr><td align="left">Collisions</td><td align="left">The experimenter recorded trials where a collision occurred between any part of the participant&#x02019;s body and the obstacle</td></tr><tr><td align="left">Side of obstacle avoidance</td><td align="left">The side of avoidance of the obstacle by the participant was recorded by the experimenter</td></tr><tr><td align="left" colspan="2">
<italic>Obstacle absent trials</italic>
</td></tr><tr><td align="left">False perceptions</td><td align="left">Number of obstacle absent trials in which the participant raised their hand marker to falsely indicate obstacle perception</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec5"><title>Procedures</title><p>Participants were instructed that they would first take part in an auditory guidance condition (which included a training session) that involved using sound to perceive and navigate around an obstacle, followed by a visual guidance condition. Following Kolarik et al. (<xref ref-type="bibr" rid="CR26">2014b</xref>), the visual condition was presented last to avoid the participants becoming familiar with the range of obstacle positions and distances prior to the auditory guidance condition.</p><p>The auditory guidance condition began with static and dynamic training in order for the participants to become familiar with producing echolocation clicks and listening to them when navigating. During static training, participants were seated in the centre of the room with the small practice obstacle placed on a table in front of them at head height at a distance of 25&#x000a0;cm. The obstacle was randomly placed in front of them or was absent. Participants practised generating mouth clicks and using the sound echoes to detect the presence or absence of the practice obstacle. Although alternative sounds such as finger snaps, hand claps, or mechanical sounds such as those produced by tapping a cane against the floor can be used for echolocation, mouth clicks were chosen in the current study following Rojas et al. (<xref ref-type="bibr" rid="CR37">2009</xref>), who hypothesized that mouth clicks offer several advantages to alternative sounds, providing relatively accurate distance estimation and usability. Also, mouth clicks can be generated at high sustained repetition rates. The static training lasted for a minimum of 15&#x000a0;min; for the first 5&#x000a0;min, the participants were allowed to keep their eyes open, for the following 5&#x000a0;min they were encouraged to close their eyes, and for the last 5&#x000a0;min they were blindfolded (Kolarik et al. <xref ref-type="bibr" rid="CR26">2014b</xref>).</p><p>The dynamic training phase consisted of practising navigating around the large obstacle using mouth clicks from an approach distance of 1.75 m. Participants trained for a minimum of 15&#x000a0;min, split into three sessions (1. full vision, 2. eyes closed, 3. blindfolded) lasting 5&#x000a0;min each. The centre of the obstacle was located on the midline relative to the participant (straight ahead) for the first 10&#x000a0;min of training, and varied randomly in location relative to the participant, at the midline, or 25&#x000a0;cm to the right or left during the last 5&#x000a0;min of training when the participant was blindfolded.</p><p>Obstacle detection performance was assessed prior to testing with locomotion. Participants were positioned facing the location where the obstacle might be and at 25&#x000a0;cm from that location, and were asked to produce mouth clicks and to report whether the obstacle was present. Twenty trials were performed. In half of the trials (randomized), the large obstacle was present and in the other half it was absent. Mean performance was 90&#x000a0;% (SD&#x000a0;=&#x000a0;11&#x000a0;%). All participants scored 70&#x000a0;% or more, and four participants scored 100&#x000a0;%.</p><p>The large obstacle was used in the main testing phase. The layout is illustrated in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. Participants were instructed that they would be blindfolded, would need to walk forward while generating clicks, and should report if an obstacle was present in their path. They were asked to maintain a straight line of travel until the obstacle was first perceived, report when they first perceived the obstacle by raising their dominant hand (similar to the obstacle approach and detection task used by Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>; Ammons et al. <xref ref-type="bibr" rid="CR1">1953</xref>), and then to circumnavigate the obstacle without collision. They were informed that the obstacle would sometimes be absent. Trials were terminated when the participant moved past the obstacle or if a collision occurred. The position of the large obstacle was chosen randomly from three possibilities: on the midline, or 25&#x000a0;cm to the right or left. To avoid the participants anticipating the distance to the obstacle, the distance was randomly set to 1.5 or 2&#x000a0;m (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). Each participant performed three trials for each obstacle location. In total each participant completed 24 trials, including six no-obstacle &#x0201c;check&#x0201d; or &#x0201c;false&#x0201d; trials (Worchel et al. <xref ref-type="bibr" rid="CR58">1950</xref>; Worchel and Mauney <xref ref-type="bibr" rid="CR57">1951</xref>; Ashmead et al. <xref ref-type="bibr" rid="CR3">1989</xref>). Use of the hands to touch the obstacle was not allowed. Participants&#x02019; ears were occluded between trials using ear-defender constructed headphones. The start of the trial was signalled by a shoulder tap from the experimenter. At the start of each trial, a removable plastic box was used so that participants aligned their feet facing forward. Participants were taken back to the starting position by the experimenter, who stood in the same place during each trial, against the wall to the right of the starting point.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>Schematic diagram of the experimental setup. The obstacle was placed either on the midline or 25&#x000a0;cm to the <italic>left</italic> or <italic>right</italic> relative to the participant. The approach distance was either 1.5 or 2&#x000a0;m. The obstacle was 0.6&#x000a0;m wide</p></caption><graphic xlink:href="221_2016_4567_Fig1_HTML" id="MO1"/></fig></p><p>When the auditory guidance trials had been completed, participants took part in a full- vision condition, similar to that used by Hackney et al. (<xref ref-type="bibr" rid="CR19">2014</xref>). This was identical to the main testing phase of the auditory condition, except that participants did not produce mouth clicks and wore blindfolds only between trials. No feedback was provided during the testing phase, and data were obtained in a single session lasting approximately 1.5&#x000a0;h.</p><p>The &#x0201c;no-click&#x0201d; control experiment was identical to the testing phase of the auditory guidance condition of the main experiment, except that participants did not produce mouth clicks, and completed 7 trials (one for each obstacle position and 1 with the obstacle absent), obtained in a single session lasting approximately 20&#x000a0;min.</p></sec><sec id="Sec6"><title>Statistical analysis</title><p>Unless otherwise specified, data were analysed using repeated-measures ANOVAs to investigate how buffer space, side (right or left) of obstacle avoidance, movement time, and number of velocity corrections were affected by guidance condition (audition and vision), obstacle lateral location (left, midline, or right relative to the participant), and repetition (1&#x02013;3). The level of significance was chosen as <italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.05. A preliminary analysis showed that scores for all measures were not significantly different for the two approach distances (<italic>p</italic>&#x000a0;&#x0003e;&#x000a0;0.05), so the results for these were pooled. Proportional data for side of avoidance were subjected to arcsine transformation, as recommended by Howell (<xref ref-type="bibr" rid="CR22">1997</xref>). Post hoc analyses were performed using Bonferroni correction.</p></sec></sec><sec id="Sec7" sec-type="results"><title>Results</title><p>Overall, under auditory guidance for trials where the obstacle was present, participants detected the obstacle on 85&#x000a0;% (SD&#x000a0;=&#x000a0;11&#x000a0;%) of trials and circumvented the obstacle without collision on 67&#x000a0;% (SD&#x000a0;=&#x000a0;21&#x000a0;%) of trials. Participants made no collisions under visual guidance.</p><p>Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> shows the proportion of obstacle-present trials where collisions occurred under auditory guidance. For trials where a collision occurred, the proportion of trials on which the obstacle was detected did not differ significantly from the proportion of trials on which it was not [&#x003c7;<sup>2</sup> (2)&#x000a0;=&#x000a0;1.51, ns]. Two trials under auditory guidance were discarded from the analyses since the participant did not begin the trial facing directly forwards for these two. The percentage of false reports of obstacle presence in the auditory condition was relatively small at 10&#x000a0;% (SD&#x000a0;=&#x000a0;18&#x000a0;%). For the &#x0201c;no-click&#x0201d; control experiment, success was relatively low compared to that with mouth clicks; for obstacle-present trials, participants detected the obstacle on 33, 28, and 17&#x000a0;% of trials for obstacles positioned to the left, midline, and right, respectively, and circumvented the obstacle without collision on 22, 22, and 6&#x000a0;% of trials, respectively. Participants falsely perceived the obstacle to be present on 6&#x000a0;% of obstacle-absent trials. With mouth clicks, participants detected the obstacle on 87, 85, and 85&#x000a0;% of trials, respectively, and circumvented the obstacle without collision on 63, 73, and 65&#x000a0;% of trials, respectively. These results suggest that participants mainly relied on information from mouth click sounds to perform the task in the main experiment.<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>Proportion of obstacle-present trials for which collisions occurred for each obstacle location under auditory guidance. <italic>Open</italic> and <italic>grey</italic>
<italic>bars</italic> show the proportions of trials where the obstacle was either not detected or detected prior to collision, respectively. In this and subsequent figures, <italic>error bars</italic> represent &#x000b1;1 SE</p></caption><graphic xlink:href="221_2016_4567_Fig2_HTML" id="MO2"/></fig></p><p>For obstacle-present trials where collisions did not occur, the mean buffer space at the point of crossing the obstacle was larger with auditory guidance than for vision for all obstacle locations (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). Under visual guidance, the mean buffer space was 22, 19, and 23&#x000a0;cm for obstacles located to the left, midline, and right, respectively, while it was 42, 37, and 37&#x000a0;cm, respectively, for auditory guidance. There was a significant main effect of guidance condition [<italic>F</italic>(1, 9)&#x000a0;=&#x000a0;48.28, <italic>p</italic>&#x000a0;=&#x000a0;0.001] and an interaction between guidance condition and repetition [<italic>F</italic>(2, 18)&#x000a0;=&#x000a0;5.51, <italic>p</italic>&#x000a0;=&#x000a0;0.014]. However, with Bonferroni correction, post hoc comparisons indicated that there were no significant differences in buffer space in either guidance condition across trials. The percentage of right-side avoidances was 97, 37, and 2&#x000a0;% for obstacles located to the left, midline, or right, respectively, under visual guidance, while it was 65, 50, and 31&#x000a0;%, respectively, under auditory guidance (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). These results show that participants did not always pass the obstacle on the side affording most space under auditory guidance. Under visual guidance, participants almost always moved to the side of the obstacle that afforded the most space. An analysis of side of avoidance showed a main effect of obstacle location only [<italic>F</italic>(2, 18)&#x000a0;=&#x000a0;21.67, <italic>p</italic>&#x000a0;=&#x000a0;0.001] and a significant interaction between obstacle location and guidance condition [<italic>F</italic>(2,18)&#x000a0;=&#x000a0;8.48, <italic>p</italic>&#x000a0;=&#x000a0;0.003]. As expected, in the vision condition participants chose to circumvent the obstacle towards the right side significantly more often when the obstacle was on the left or on the midline than when it was on the right, and when the obstacle was on the midline compared to the right (<italic>p</italic>&#x000a0;&#x0003c;&#x000a0;0.001). However, these differences were not significant for the auditory condition.<fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>Mean buffer space at the time of crossing under auditory (<italic>open bars</italic>) and visual (<italic>grey bars</italic>) guidance for each obstacle location</p></caption><graphic xlink:href="221_2016_4567_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>Percentage of right-side avoidances under auditory (<italic>open bars</italic>) and visual (<italic>grey bars</italic>) guidance for each obstacle location</p></caption><graphic xlink:href="221_2016_4567_Fig4_HTML" id="MO4"/></fig></p><p>As would be expected, both the mean movement times to pass the obstacle (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, upper panel) and the mean number of velocity corrections (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, lower panel) were considerably greater under auditory guidance than under visual guidance. Under auditory guidance, the mean movement times were 30, 29, and 37&#x000a0;s for obstacles located to the left, midline, and right, respectively, while under visual guidance they were 1.1, 1.3, and 1.1&#x000a0;s, respectively. There was a main effect of guidance condition only [<italic>F</italic>(1, 9)&#x000a0;=&#x000a0;14.75, <italic>p</italic>&#x000a0;=&#x000a0;0.004]. Under auditory guidance, the mean numbers of velocity corrections were 55, 55, and 66 for obstacles located to the left, midline, and right, respectively, while under visual guidance they were 4, 5, and 4, respectively. There was a main effect of guidance condition only [<italic>F</italic>(1, 9)&#x000a0;=&#x000a0;22.20, <italic>p</italic>&#x000a0;=&#x000a0;0.001]. Participants raised their hand under auditory guidance to indicate they had perceived the obstacle, as they had to avoid generating sounds other than mouth clicks. This may have resulted in them slowing down in some trials, leading to a small increase in movement time and an increase in one or two velocity corrections compared to visual guidance, where no hand raise was required. However, this would not be sufficient to account for the substantial differences in movement time and velocity corrections between visual and auditory guidance. Increased movement times and velocity corrections confirm that participants took a more cautious approach under auditory guidance than when using vision.<fig id="Fig5"><label>Fig.&#x000a0;5</label><caption><p>Mean movement time to pass the obstacle (<italic>upper panel</italic>) and mean number of velocity corrections (<italic>lower panel</italic>) under auditory (<italic>open bars</italic>) or visual (<italic>grey bars</italic>) guidance for each obstacle location. The <italic>y</italic> axis is plotted on logarithmic coordinates</p></caption><graphic xlink:href="221_2016_4567_Fig5_HTML" id="MO5"/></fig></p><p>The mean obstacle detection range using sound was 55.2, 65.1, and 57.3&#x000a0;cm for obstacles located to the left, midline, and right, respectively (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>). The obstacle detection range was moderately greater for obstacles located on the midline, possibly as this location provided participants with more reflected sound during obstacle approach than when the obstacle was located either leftwards or rightwards. However, there were no significant differences in obstacle detection range for the different obstacle locations [<italic>F</italic>(2, 18)&#x000a0;=&#x000a0;0.51, <italic>p</italic>&#x000a0;=&#x000a0;0.61] or over repetitions [<italic>F</italic>(2, 18)&#x000a0;=&#x000a0;1.54, <italic>p</italic>&#x000a0;=&#x000a0;0.24], and no interaction [<italic>F</italic>(4,36)&#x000a0;=&#x000a0;0.52, <italic>p</italic>&#x000a0;=&#x000a0;0.72].<fig id="Fig6"><label>Fig.&#x000a0;6</label><caption><p>Mean obstacle detection range under auditory guidance for each obstacle location</p></caption><graphic xlink:href="221_2016_4567_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec8" sec-type="discussion"><title>Discussion</title><p>The main findings are as follows: (1) under auditory guidance, sighted blindfolded participants detected the presence of an obstacle in their path on 85&#x000a0;% of trials and circumvented it on 67&#x000a0;% of the trials, suggesting that sound can be used to provide spatial layout information for locomotion when avoiding obstacles, but not always with high efficiency. (2) Buffer space was larger by a factor of 1.8 under auditory guidance than under visual guidance. (3) Movement times were greater by a factor of 27 and velocity corrections were greater by a factor of 14 under auditory guidance than under vision.</p><p>We next discuss three possible approaches to understanding how obstacle circumvention might be performed: (1) representation or model-based control, (2) information-based control, or (3) use of time-to-contact information. The representation or model-based control approach (Frenz and Lappe <xref ref-type="bibr" rid="CR13">2005</xref>; Turano et al. <xref ref-type="bibr" rid="CR50">2005</xref>) proposes that sensory information allows participants to form an internal representation of their surroundings for navigation. Under visual guidance, an individual&#x02019;s surrounding space is accurately represented in relation to their action capabilities, and vision provides a constant stream of information allowing the central nervous system (CNS) to control locomotor actions in a feedforward manner (Higuchi et al. <xref ref-type="bibr" rid="CR21">2006</xref>). Using visual information accumulated over a series of saccades as the eye is directed to different regions of an obstacle, a comparatively rich internal representation can be built up using a process called transsaccadic integration (Prime et al. <xref ref-type="bibr" rid="CR35">2011</xref>). While substantial spatial layout information can be obtained using self-generated sounds (Stoffregen and Pittenger <xref ref-type="bibr" rid="CR44">1995</xref>; Teng et al. <xref ref-type="bibr" rid="CR48">2012</xref>), during locomotion internal spatial representations would need to be updated for each new self-generated sound over a relatively long time period. In contrast, transsaccadic integration under visual guidance can occur over a few hundred milliseconds, possibly resulting in more accurate internal representations during locomotion. Milne et al. (<xref ref-type="bibr" rid="CR33">2014</xref>) suggested that head movements made while echolocating could produce sound snapshots or &#x0201c;echo saccades&#x0201d; to provide perceptual representations that are likely coarser than for vision, possibly due to lower precision using echolocation, which is poor compared to foveal acuity (Teng et al. <xref ref-type="bibr" rid="CR48">2012</xref>). The increased buffer space, velocity corrections, and movement times shown under auditory guidance in this study might be due to coarser internal representations of space based on sound compared to vision.</p><p>The information-based control approach (Gibson <xref ref-type="bibr" rid="CR16">1958</xref>; Warren <xref ref-type="bibr" rid="CR55">1998</xref>; Fajen and Warren <xref ref-type="bibr" rid="CR10">2003</xref>) proposes that the perceptual system detects information from a relevant variable (such as optic flow field variables under visual guidance) to guide movement on a moment-by-moment basis, according to some law of control (Fajen <xref ref-type="bibr" rid="CR9">2007</xref>). In contrast to the general-purpose perceptual cues utilized in the internal representation approach, information-based perceptual variables are task-specific. A single variable informs the individual how to perform a given task, such as negotiating the obstacle, but does not provide spatial layout information, distinguishing this approach from the internal representation approach and rendering the need for path planning or internal models unnecessary. Participants may have adapted their actions &#x0201c;online&#x0201d; (Warren <xref ref-type="bibr" rid="CR55">1998</xref>) to avoid the obstacle, guided by information in sound echoes during locomotion. The increased buffer space, velocity corrections, and movement times seen in our study may have been due to participants being less attuned to the acoustic information specifying passability.</p><p>A third approach involves the internal generation of &#x0201c;echoic tau&#x0201d; (Lee et al. <xref ref-type="bibr" rid="CR29">1992</xref>), also called &#x0201c;echoic time-to-contact&#x0201d; (Stoffregen and Pittenger <xref ref-type="bibr" rid="CR44">1995</xref>), to guide locomotion under auditory guidance. Echoic tau can be derived from monitoring the rate of change of an acoustic parameter such as spectral information (Stoffregen and Pittenger <xref ref-type="bibr" rid="CR44">1995</xref>), or the intensity difference between the emitted sound and returning echo, the echo delay, or the changing acoustic angle of the echo. This process might or might not depend on internal representations (Schiff and Oldak <xref ref-type="bibr" rid="CR42">1990</xref>). Lee et al. (<xref ref-type="bibr" rid="CR29">1992</xref>) reported that echoic tau might be used by echolocating bats to govern their braking behaviour. Rosenblum et al. (<xref ref-type="bibr" rid="CR39">2000</xref>) reported that moving blindfolded sighted echolocating participants were more accurate than stationary participants when locating the position of a removable wall, possibly due to echoic tau information being available in the moving condition.</p><p>Buffer space was larger by a factor of 1.8 under auditory guidance than when using vision. Personal space may reflect a safety margin (Graziano and Cooke <xref ref-type="bibr" rid="CR18">2006</xref>), and our findings suggest that while personal space can be generated using auditory information, a larger buffer space and greater caution are required than for vision. Greater buffer space under auditory guidance may also be due to increases in the variability of the walking path or postural sway while blindfolded. Franchak et al. (<xref ref-type="bibr" rid="CR12">2012</xref>) found that under visual guidance, participants allowed larger buffers when moving through horizontal apertures compared to vertical apertures. This may be due to greater lateral sway of the body compared to vertical bounce. Paulus et al. (<xref ref-type="bibr" rid="CR34">1984</xref>) showed that decreasing visual acuity in sighted participants using semitransparent plastic foils resulted in a proportional increase in postural sway.</p><p>Increased movement times and velocity corrections under auditory guidance are consistent with previous reports of decreased walking velocity under conditions of visual deprivation (Hallemans et al. <xref ref-type="bibr" rid="CR20">2010</xref>; Iosa et al. <xref ref-type="bibr" rid="CR23">2012</xref>; Reynard and Terrier <xref ref-type="bibr" rid="CR36">2015</xref>). Participants took approximately 32&#x000a0;s to move 1&#x000a0;m, equivalent to a walking speed of approximately 0.12&#x000a0;km/h, which is lower than velocities feasible for locomotion in daily life or that would be used by blind echolocators. Fiehler et al. (<xref ref-type="bibr" rid="CR11">2015</xref>) suggested that sighted people new to echolocation may need to apply more conscious, high-level spatial processing to assign meaning to echoes, whereas blind people may automatically assign directional meaning to echoes for determining direction during walking. With increasing experience, the ability of sighted participants to use information from echoes may become more automatic. Consistent with this idea, over the time course of some weeks blindfolded sighted participants improve their ability to detect obstacles and avoid colliding with them (Ammons et al. <xref ref-type="bibr" rid="CR1">1953</xref>).</p><p>Under visual guidance, participants almost always chose the side of the obstacle that afforded the most space, despite not receiving specific instructions to do so, consistent with previous reports (Fajen and Warren <xref ref-type="bibr" rid="CR10">2003</xref>; Hackney et al. <xref ref-type="bibr" rid="CR19">2014</xref>). This was not the case under auditory guidance. Possibly the auditory information was too imprecise to allow this. However, participants may have adopted a strategy whereby they scanned the obstacle using mouth clicks until they located an edge and then moved around it, even though scanning back and forth would have allowed the participant to pass on the side affording most space. The strategy chosen for obstacle circumvention is of relevance to rehabilitation programs for those who have lost their sight; increased time spent exploring the obstacle may result in safer travel.</p><p>Using sound to generate buffer space during locomotion may be of functional importance to blind individuals, who generally detect obstacles using echolocation at similar or greater ranges than sighted individuals (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>). Sensitivity to sound echoes for spatial tasks (Dufour et al. <xref ref-type="bibr" rid="CR8">2005</xref>; Kolarik et al. <xref ref-type="bibr" rid="CR24">2013</xref>) and echolocation abilities are generally increased in blind individuals (for reviews, see Stoffregen and Pittenger <xref ref-type="bibr" rid="CR44">1995</xref>; Kolarik et al. <xref ref-type="bibr" rid="CR25">2014a</xref>). Worchel and Mauney (<xref ref-type="bibr" rid="CR57">1951</xref>) showed that training blind participants in an obstacle approach and detection task resulted in fewer collisions and greater consistency in obstacle detection and moving to its location. Increased echolocation abilities may benefit blind people when circumventing obstacles. However, this requires experimental confirmation.</p><p>The mean obstacle detection range (55&#x02013;65&#x000a0;cm across different obstacle locations) was comparable to that measured for sighted participants (62&#x02013;130&#x000a0;cm) detecting an obstacle using thermal noise (similar to white noise) emitted by a moving loudspeaker (Cotzin and Dallenbach <xref ref-type="bibr" rid="CR7">1950</xref>). Studies where sighted participants used sounds to detect an obstacle reported ranges of 1.8&#x000a0;m or less (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>; Ammons et al. <xref ref-type="bibr" rid="CR1">1953</xref>). Differences in obstacle size, approach distance, experimental paradigm, training, task, room size, environment, and individual differences in abilities to use self-generated sound (Teng and Whitney <xref ref-type="bibr" rid="CR47">2011</xref>; Teng et al. <xref ref-type="bibr" rid="CR48">2012</xref>; Rowan et al. <xref ref-type="bibr" rid="CR40">2013</xref>) may have contributed to the different ranges across studies. The shorter approach distances we used may have resulted in the buffer space under visual guidance (19&#x02013;23&#x000a0;cm across different obstacle locations) being smaller than for other visually guided obstacle circumvention studies: approximately 40&#x000a0;cm for children (Hackney et al. <xref ref-type="bibr" rid="CR19">2014</xref>) and approximately 60&#x000a0;cm for adults (G&#x000e9;rin-Lajoie et al. <xref ref-type="bibr" rid="CR15">2008</xref>).</p><p>Results from the current study and others (Supa et al. <xref ref-type="bibr" rid="CR46">1944</xref>; Carlson-Smith and Weiner <xref ref-type="bibr" rid="CR6">1996</xref>) show that auditory-guided locomotion is possible in an indoor environment, where room reverberation can sometimes benefit echolocation. Schenkman and Nilsson (<xref ref-type="bibr" rid="CR41">2010</xref>) showed that echolocation detection performance was better in a reverberant room than in an anechoic room, possibly due to an &#x0201c;information surplus principle&#x0201d;, as changes in reverberation pattern offer a potential cue. Wallmeier and Wiegrebe (<xref ref-type="bibr" rid="CR53">2014a</xref>) reported that for a virtual distance discrimination task using echolocation, performance was as good as or better when room reflections were present than when they were absent, similar to findings by Sch&#x000f6;rnich et al. (<xref ref-type="bibr" rid="CR43">2012</xref>). However, long reverberation times are likely to degrade echolocation performance, as room reflections may interfere with reflections from the target object (Sch&#x000f6;rnich et al. <xref ref-type="bibr" rid="CR43">2012</xref>).</p><p>G&#x000e9;rin-Lajoie et al. (<xref ref-type="bibr" rid="CR15">2008</xref>) suggested that a multi-sensory zone constituting personal space is used to plan safe navigation around obstacles under visual guidance. Multimodal brain areas implicated in personal space representation include the ventral intraparietal area and the polysensory zone in the precentral gyrus, whose roles may include maintenance of a safety margin around the individual and coordination of motoric actions that protect the surface of the body (Graziano and Cooke <xref ref-type="bibr" rid="CR18">2006</xref>). Fiehler et al. (<xref ref-type="bibr" rid="CR11">2015</xref>) reported that echoic path direction processing was associated with brain activation in the superior parietal lobule and inferior frontal cortex in blind and sighted participants. Additional activation occurred in the inferior parietal lobule and middle and superior frontal areas in sighted echolocation novices. Kupers et al. (<xref ref-type="bibr" rid="CR28">2010</xref>) also reported superior parietal lobule activation among blind and blindfolded sighted participants using a tactile SSD called the Tongue Display Unit in a virtual navigation task, suggesting a role of the superior parietal lobule in a navigation or route-recognition network. Occipital brain areas are involved in echoic spatial processing among blind individuals (Thaler et al. <xref ref-type="bibr" rid="CR49">2011</xref>; Arnott et al. <xref ref-type="bibr" rid="CR2">2013</xref>). Fiehler et al. (<xref ref-type="bibr" rid="CR11">2015</xref>) highlighted that brain activation relating to sound echoes was mainly observed in parietal rather than occipital areas in their study, possibly due to task differences. Previous studies focused on spatial location only; their study focused on spatial location for locomotion.</p><p>Teng et al. (<xref ref-type="bibr" rid="CR48">2012</xref>) reported that the spatial resolution of expert blind participants using echolocation was comparable to that observed for the visual periphery of sighted participants. In the current study, better performance under visual guidance than with audition was probably due to the better spatial acuity in the former. Visual impairment generally reduces functional spatial resolution, making visually guided obstacle circumvention more difficult. However, the effect of central or peripheral visual loss and severity of visual loss on obstacle circumvention has not been investigated and requires further study. Further investigation of auditory-guided locomotion and a visually guided condition matched in terms of acuity, for example through the use of blurring lenses, would enable a comparison of the effectiveness of visual and auditory guidance when matched in acuity.</p><p>Several studies investigated how auditory spatial perception is affected by visual loss and interpreted the results in terms of internal representations of auditory space (Lessard et al. <xref ref-type="bibr" rid="CR30">1998</xref>; Voss et al. <xref ref-type="bibr" rid="CR52">2004</xref>; Lewald <xref ref-type="bibr" rid="CR31">2013</xref>). Their findings suggest that representations of auditory space can be generated and maintained following severe visual loss, suggesting that audiomotor feedback is sufficient to calibrate auditory spatial representations (Lewald <xref ref-type="bibr" rid="CR31">2013</xref>; Lewald and Getzmann <xref ref-type="bibr" rid="CR32">2013</xref>). However, Wallmeier and Wiegrebe (<xref ref-type="bibr" rid="CR53">2014a</xref>) noted that audiomotor feedback cannot conclusively explain why blind individuals demonstrate supra-normal abilities in far space (Voss et al. <xref ref-type="bibr" rid="CR52">2004</xref>), where feedback from self-motion cannot easily be linked to systematic changes in auditory stimuli. Echolocation provides reasonably accurate distance information (Kolarik et al. <xref ref-type="bibr" rid="CR27">2016</xref>), and it was recently hypothesized that echolocation may aid in calibrating auditory space (Kolarik et al. <xref ref-type="bibr" rid="CR25">2014a</xref>). This has been supported by Wallmeier and Wiegrebe (<xref ref-type="bibr" rid="CR53">2014a</xref>), who showed that blind and blindfolded sighted participants discriminated distances to objects using echolocation with high acuity in far space. In terms of the internal representation approach, our findings suggest that echolocation may provide internal representations that the CNS uses to guide locomotion around an obstacle, consistent with the view that sensorymotor feedback using echolocation aids in developing an accurate spatial representation of auditory space (Vercillo et al. <xref ref-type="bibr" rid="CR51">2015</xref>).</p><p>In summary, the results of the current study showed that audition could be used to perform a single-obstacle circumvention task, guiding locomotion and generating buffer space in the majority of trials. However, collisions and false perceptions sometimes occurred, indicating that sound did not always provide sufficient spatial information to judge the location of an obstacle accurately.</p></sec></body><back><ack><p>This research was supported by the Vision and Eye Research Unit (VERU), Postgraduate Medical Institute at Anglia Ruskin University. We thank Silvia Cirstea for valuable discussion regarding the design of the earlier experiment, and John Peters and George Oluwafemi for assistance in data collection. We thank the editor Melvyn Goodale and two anonymous reviewers for their helpful comments regarding the manuscript.</p></ack><notes notes-type="conflict-interest"><title>Compliance with ethical standards</title><sec id="FPar1"><title>Conflict of interest</title><p>The authors declare that they have no conflict of interest.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ammons</surname><given-names>CH</given-names></name><name><surname>Worchel</surname><given-names>P</given-names></name><name><surname>Dallenbach</surname><given-names>KM</given-names></name></person-group><article-title>Facial vision: the perception of obstacles out of doors by blindfolded and blindfolded-deafened subjects</article-title><source>Am J Psychol</source><year>1953</year><volume>66</volume><fpage>519</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.2307/1418950</pub-id><pub-id pub-id-type="pmid">13124562</pub-id></element-citation></ref><ref id="CR2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnott</surname><given-names>SR</given-names></name><name><surname>Thaler</surname><given-names>L</given-names></name><name><surname>Milne</surname><given-names>JL</given-names></name><name><surname>Kish</surname><given-names>D</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><article-title>Shape-specific activation of occipital cortex in an early blind echolocation expert</article-title><source>Neuropsychologia</source><year>2013</year><volume>51</volume><fpage>938</fpage><lpage>949</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.01.024</pub-id><pub-id pub-id-type="pmid">23391560</pub-id></element-citation></ref><ref id="CR3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashmead</surname><given-names>DH</given-names></name><name><surname>Hill</surname><given-names>EW</given-names></name><name><surname>Talor</surname><given-names>CR</given-names></name></person-group><article-title>Obstacle perception by congenitally blind children</article-title><source>Percept Psychophys</source><year>1989</year><volume>46</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.3758/BF03210857</pub-id><pub-id pub-id-type="pmid">2813027</pub-id></element-citation></ref><ref id="CR4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashmead</surname><given-names>DH</given-names></name><name><surname>Wall</surname><given-names>RS</given-names></name><name><surname>Eaton</surname><given-names>SB</given-names></name><name><surname>Ebinger</surname><given-names>KA</given-names></name><name><surname>Snook-Hill</surname><given-names>M</given-names></name><name><surname>Guth</surname><given-names>DA</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name></person-group><article-title>Echolocation reconsidered: using spatial variations in the ambient sound field to guide locomotion</article-title><source>J Vis Impair Blind</source><year>1998</year><volume>92</volume><fpage>615</fpage><lpage>632</lpage></element-citation></ref><ref id="CR5"><element-citation publication-type="book"><person-group person-group-type="author"><collab>British Society of Audiology</collab></person-group><source>Pure-tone air-conduction and bone-conduction threshold audiometry with and without masking</source><year>2011</year><publisher-loc>Reading</publisher-loc><publisher-name>British Society of Audiology</publisher-name></element-citation></ref><ref id="CR6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson-Smith</surname><given-names>C</given-names></name><name><surname>Weiner</surname><given-names>WR</given-names></name></person-group><article-title>The auditory skills necessary for echolocation: a new explanation</article-title><source>J Vis Impair Blind</source><year>1996</year><volume>90</volume><fpage>21</fpage><lpage>35</lpage></element-citation></ref><ref id="CR7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cotzin</surname><given-names>M</given-names></name><name><surname>Dallenbach</surname><given-names>KM</given-names></name></person-group><article-title>Facial vision: the role of pitch and loudness in the perception of obstacles by the blind</article-title><source>Am J Psychol</source><year>1950</year><volume>63</volume><fpage>485</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.2307/1418868</pub-id><pub-id pub-id-type="pmid">14790019</pub-id></element-citation></ref><ref id="CR8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dufour</surname><given-names>A</given-names></name><name><surname>Despr&#x000e9;s</surname><given-names>O</given-names></name><name><surname>Candas</surname><given-names>V</given-names></name></person-group><article-title>Enhanced sensitivity to echo cues in blind subjects</article-title><source>Exp Brain Res</source><year>2005</year><volume>165</volume><fpage>515</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2329-3</pub-id><pub-id pub-id-type="pmid">15991030</pub-id></element-citation></ref><ref id="CR9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fajen</surname><given-names>BR</given-names></name></person-group><article-title>Affordance-based control of visually guided action</article-title><source>Ecol Psychol</source><year>2007</year><volume>19</volume><fpage>383</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1080/10407410701557877</pub-id></element-citation></ref><ref id="CR10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fajen</surname><given-names>BR</given-names></name><name><surname>Warren</surname><given-names>WH</given-names></name></person-group><article-title>Behavioral dynamics of steering, obstacle avoidance, and route selection</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2003</year><volume>29</volume><fpage>343</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.29.2.343</pub-id><pub-id pub-id-type="pmid">12760620</pub-id></element-citation></ref><ref id="CR11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiehler</surname><given-names>K</given-names></name><name><surname>Sch&#x000fc;tz</surname><given-names>I</given-names></name><name><surname>Meller</surname><given-names>T</given-names></name><name><surname>Thaler</surname><given-names>L</given-names></name></person-group><article-title>Neural correlates of human echolocation of path direction during walking</article-title><source>Multisens Res</source><year>2015</year><volume>28</volume><fpage>195</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1163/22134808-00002491</pub-id><pub-id pub-id-type="pmid">26152058</pub-id></element-citation></ref><ref id="CR12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franchak</surname><given-names>JM</given-names></name><name><surname>Celano</surname><given-names>EC</given-names></name><name><surname>Adolph</surname><given-names>KE</given-names></name></person-group><article-title>Perception of passage through openings depends on the size of the body in motion</article-title><source>Exp Brain Res</source><year>2012</year><volume>223</volume><fpage>301</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.1007/s00221-012-3261-y</pub-id><pub-id pub-id-type="pmid">22990292</pub-id></element-citation></ref><ref id="CR13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frenz</surname><given-names>H</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name></person-group><article-title>Absolute travel distance from optic flow</article-title><source>Vision Res</source><year>2005</year><volume>45</volume><fpage>1679</fpage><lpage>1692</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.12.019</pub-id><pub-id pub-id-type="pmid">15792843</pub-id></element-citation></ref><ref id="CR14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000e9;rin-Lajoie</surname><given-names>M</given-names></name><name><surname>Richards</surname><given-names>CL</given-names></name><name><surname>McFadyen</surname><given-names>BJ</given-names></name></person-group><article-title>The negotiation of stationary and moving obstructions during walking: anticipatory locomotor adaptations and preservation of personal space</article-title><source>Mot Control</source><year>2005</year><volume>9</volume><fpage>242</fpage><lpage>269</lpage></element-citation></ref><ref id="CR15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000e9;rin-Lajoie</surname><given-names>M</given-names></name><name><surname>Richards</surname><given-names>CL</given-names></name><name><surname>Fung</surname><given-names>J</given-names></name><name><surname>McFadyen</surname><given-names>BJ</given-names></name></person-group><article-title>Characteristics of personal space during obstacle circumvention in physical and virtual environments</article-title><source>Gait Posture</source><year>2008</year><volume>27</volume><fpage>239</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2007.03.015</pub-id><pub-id pub-id-type="pmid">17512201</pub-id></element-citation></ref><ref id="CR16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>JJ</given-names></name></person-group><article-title>Visually controlled locomotion and visual orientation in animals</article-title><source>Br J Psychol</source><year>1958</year><volume>49</volume><fpage>182</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8295.1958.tb00656.x</pub-id><pub-id pub-id-type="pmid">13572790</pub-id></element-citation></ref><ref id="CR17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>MS</given-names></name><name><surname>Rosenblum</surname><given-names>LD</given-names></name></person-group><article-title>Perception of sound-obstructing surfaces using body-scaled judgments</article-title><source>Ecol Psychol</source><year>2004</year><volume>16</volume><fpage>87</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1207/s15326969eco1602_1</pub-id></element-citation></ref><ref id="CR18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MSA</given-names></name><name><surname>Cooke</surname><given-names>DF</given-names></name></person-group><article-title>Parieto-frontal interactions, personal space, and defensive behavior</article-title><source>Neuropsychologia</source><year>2006</year><volume>44</volume><fpage>845</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2005.09.009</pub-id><pub-id pub-id-type="pmid">16277998</pub-id></element-citation></ref><ref id="CR19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hackney</surname><given-names>AL</given-names></name><name><surname>Van Ruymbeke</surname><given-names>N</given-names></name><name><surname>Bryden</surname><given-names>PJ</given-names></name><name><surname>Cinelli</surname><given-names>ME</given-names></name></person-group><article-title>Direction of single obstacle circumvention in middle-aged children</article-title><source>Gait Posture</source><year>2014</year><volume>40</volume><fpage>113</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2014.03.005</pub-id><pub-id pub-id-type="pmid">24679592</pub-id></element-citation></ref><ref id="CR20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallemans</surname><given-names>A</given-names></name><name><surname>Ortibus</surname><given-names>E</given-names></name><name><surname>Meire</surname><given-names>F</given-names></name><name><surname>Aerts</surname><given-names>P</given-names></name></person-group><article-title>Low vision affects dynamic stability of gait</article-title><source>Gait Posture</source><year>2010</year><volume>32</volume><fpage>547</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2010.07.018</pub-id><pub-id pub-id-type="pmid">20801658</pub-id></element-citation></ref><ref id="CR21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higuchi</surname><given-names>T</given-names></name><name><surname>Imanaka</surname><given-names>K</given-names></name><name><surname>Patla</surname><given-names>AE</given-names></name></person-group><article-title>Action-oriented representation of peripersonal and extrapersonal space: Insights from manual and locomotor actions</article-title><source>Jpn Psychol Res</source><year>2006</year><volume>48</volume><fpage>126</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1111/j.1468-5884.2006.00314.x</pub-id></element-citation></ref><ref id="CR22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Howell</surname><given-names>DC</given-names></name></person-group><source>Statistical methods for psychology</source><year>1997</year><publisher-loc>Belmont</publisher-loc><publisher-name>Duxbury Press</publisher-name></element-citation></ref><ref id="CR23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iosa</surname><given-names>M</given-names></name><name><surname>Fusco</surname><given-names>A</given-names></name><name><surname>Morone</surname><given-names>G</given-names></name><name><surname>Paolucci</surname><given-names>S</given-names></name></person-group><article-title>Effects of visual deprivation on gait dynamic stability</article-title><source>Sci World J</source><year>2012</year><volume>2012</volume><fpage>974560</fpage><pub-id pub-id-type="doi">10.1100/2012/974560</pub-id></element-citation></ref><ref id="CR24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>AJ</given-names></name><name><surname>Cirstea</surname><given-names>S</given-names></name><name><surname>Pardhan</surname><given-names>S</given-names></name></person-group><article-title>Evidence for enhanced discrimination of virtual auditory distance among blind listeners using level and direct-to-reverberant cues</article-title><source>Exp Brain Res</source><year>2013</year><volume>224</volume><fpage>623</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1007/s00221-012-3340-0</pub-id><pub-id pub-id-type="pmid">23178908</pub-id></element-citation></ref><ref id="CR25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>AJ</given-names></name><name><surname>Cirstea</surname><given-names>S</given-names></name><name><surname>Pardhan</surname><given-names>S</given-names></name><name><surname>Moore</surname><given-names>BCJ</given-names></name></person-group><article-title>A summary of research investigating echolocation abilities of blind and sighted humans</article-title><source>Hear Res</source><year>2014</year><volume>310</volume><fpage>60</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2014.01.010</pub-id><pub-id pub-id-type="pmid">24524865</pub-id></element-citation></ref><ref id="CR26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>AJ</given-names></name><name><surname>Timmis</surname><given-names>MA</given-names></name><name><surname>Cirstea</surname><given-names>S</given-names></name><name><surname>Pardhan</surname><given-names>S</given-names></name></person-group><article-title>Sensory substitution information informs locomotor adjustments when walking through apertures</article-title><source>Exp Brain Res</source><year>2014</year><volume>232</volume><fpage>975</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1007/s00221-013-3809-5</pub-id><pub-id pub-id-type="pmid">24370580</pub-id></element-citation></ref><ref id="CR27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolarik</surname><given-names>AJ</given-names></name><name><surname>Moore</surname><given-names>BCJ</given-names></name><name><surname>Zahorik</surname><given-names>P</given-names></name><name><surname>Cirstea</surname><given-names>S</given-names></name><name><surname>Pardhan</surname><given-names>S</given-names></name></person-group><article-title>Auditory distance perception in humans: a review of cues, development, neuronal bases and effects of sensory loss</article-title><source>Atten Percept Psychophys</source><year>2016</year><volume>78</volume><fpage>373</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.3758/s13414-015-1015-1</pub-id><pub-id pub-id-type="pmid">26590050</pub-id></element-citation></ref><ref id="CR28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kupers</surname><given-names>R</given-names></name><name><surname>Chebat</surname><given-names>DR</given-names></name><name><surname>Madsen</surname><given-names>KH</given-names></name><name><surname>Paulson</surname><given-names>OB</given-names></name><name><surname>Ptito</surname><given-names>M</given-names></name></person-group><article-title>Neural correlates of virtual route recognition in congenital blindness</article-title><source>Proc Natl Acad Sci</source><year>2010</year><volume>107</volume><fpage>12716</fpage><lpage>12721</lpage><pub-id pub-id-type="doi">10.1073/pnas.1006199107</pub-id><pub-id pub-id-type="pmid">20616025</pub-id></element-citation></ref><ref id="CR29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>DN</given-names></name><name><surname>van der Weel</surname><given-names>FR</given-names></name><name><surname>Hitchcock</surname><given-names>T</given-names></name><name><surname>Matejowsky</surname><given-names>E</given-names></name><name><surname>Pettigrew</surname><given-names>JD</given-names></name></person-group><article-title>Common principle of guidance by echolocation and vision</article-title><source>J Comp Physiol A</source><year>1992</year><volume>171</volume><fpage>563</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1007/BF00194105</pub-id><pub-id pub-id-type="pmid">1494137</pub-id></element-citation></ref><ref id="CR30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lessard</surname><given-names>N</given-names></name><name><surname>Pare</surname><given-names>M</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name><name><surname>Lassonde</surname><given-names>M</given-names></name></person-group><article-title>Early-blind human subjects localize sound sources better than sighted subjects</article-title><source>Nature</source><year>1998</year><volume>395</volume><fpage>278</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1038/26228</pub-id><pub-id pub-id-type="pmid">9751055</pub-id></element-citation></ref><ref id="CR31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname><given-names>J</given-names></name></person-group><article-title>Exceptional ability of blind humans to hear sound motion: implications for the emergence of auditory space</article-title><source>Neuropsychologia</source><year>2013</year><volume>51</volume><fpage>181</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.11.017</pub-id><pub-id pub-id-type="pmid">23178211</pub-id></element-citation></ref><ref id="CR32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewald</surname><given-names>J</given-names></name><name><surname>Getzmann</surname><given-names>S</given-names></name></person-group><article-title>Ventral and dorsal visual pathways support auditory motion processing in the blind: evidence from electrical neuroimaging</article-title><source>Eur J Neurosci</source><year>2013</year><volume>38</volume><fpage>3201</fpage><lpage>3209</lpage><pub-id pub-id-type="doi">10.1111/ejn.12306</pub-id><pub-id pub-id-type="pmid">23859484</pub-id></element-citation></ref><ref id="CR33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milne</surname><given-names>JL</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Thaler</surname><given-names>L</given-names></name></person-group><article-title>The role of head movements in the discrimination of 2-D shape by blind echolocation experts</article-title><source>Atten Percept Psychophys</source><year>2014</year><volume>76</volume><fpage>1828</fpage><lpage>1837</lpage><pub-id pub-id-type="doi">10.3758/s13414-014-0695-2</pub-id><pub-id pub-id-type="pmid">24874262</pub-id></element-citation></ref><ref id="CR34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulus</surname><given-names>W</given-names></name><name><surname>Straube</surname><given-names>A</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name></person-group><article-title>Visual stabilization of posture: physiological stimulus characteristics and clinical aspects</article-title><source>Brain</source><year>1984</year><volume>107</volume><fpage>1143</fpage><lpage>1163</lpage><pub-id pub-id-type="doi">10.1093/brain/107.4.1143</pub-id><pub-id pub-id-type="pmid">6509312</pub-id></element-citation></ref><ref id="CR35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prime</surname><given-names>SL</given-names></name><name><surname>Vesia</surname><given-names>M</given-names></name><name><surname>Crawford</surname><given-names>JD</given-names></name></person-group><article-title>Cortical mechanisms for trans-saccadic memory and integration of multiple object features</article-title><source>Phil Trans R Soc B</source><year>2011</year><volume>366</volume><fpage>540</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1098/rstb.2010.0184</pub-id><pub-id pub-id-type="pmid">21242142</pub-id></element-citation></ref><ref id="CR36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynard</surname><given-names>F</given-names></name><name><surname>Terrier</surname><given-names>P</given-names></name></person-group><article-title>Role of visual input in the control of dynamic balance: variability and instability of gait in treadmill walking while blindfolded</article-title><source>Exp Brain Res</source><year>2015</year><volume>233</volume><fpage>1031</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1007/s00221-014-4177-5</pub-id><pub-id pub-id-type="pmid">25534228</pub-id></element-citation></ref><ref id="CR37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rojas</surname><given-names>JAM</given-names></name><name><surname>Hermosilla</surname><given-names>JA</given-names></name><name><surname>Montero</surname><given-names>RS</given-names></name><name><surname>Espi</surname><given-names>PLL</given-names></name></person-group><article-title>Physical analysis of several organic signals for human echolocation: oral vacuum pulses</article-title><source>Acta Acust United Acust</source><year>2009</year><volume>95</volume><fpage>325</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.3813/AAA.918155</pub-id></element-citation></ref><ref id="CR38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosenblum</surname><given-names>LD</given-names></name></person-group><source>See what I&#x02019;m saying: the extraordinary powers of our five senses</source><year>2011</year><publisher-loc>London</publisher-loc><publisher-name>WW Norton &#x00026; Company</publisher-name></element-citation></ref><ref id="CR39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblum</surname><given-names>LD</given-names></name><name><surname>Gordon</surname><given-names>MS</given-names></name><name><surname>Jarquin</surname><given-names>L</given-names></name></person-group><article-title>Echolocating distance by moving and stationary listeners</article-title><source>Ecol Psychol</source><year>2000</year><volume>12</volume><fpage>181</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1207/S15326969ECO1203_1</pub-id></element-citation></ref><ref id="CR40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rowan</surname><given-names>D</given-names></name><name><surname>Papadopoulos</surname><given-names>T</given-names></name><name><surname>Edwards</surname><given-names>D</given-names></name><name><surname>Holmes</surname><given-names>H</given-names></name><name><surname>Hollingdale</surname><given-names>A</given-names></name><name><surname>Evans</surname><given-names>L</given-names></name><name><surname>Allen</surname><given-names>R</given-names></name></person-group><article-title>Identification of the lateral position of a virtual object based on echoes by humans</article-title><source>Hear Res</source><year>2013</year><volume>300</volume><fpage>56</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.03.005</pub-id><pub-id pub-id-type="pmid">23538130</pub-id></element-citation></ref><ref id="CR41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schenkman</surname><given-names>BN</given-names></name><name><surname>Nilsson</surname><given-names>ME</given-names></name></person-group><article-title>Human echolocation: blind and sighted persons&#x02019; ability to detect sounds recorded in the presence of a reflecting object</article-title><source>Perception</source><year>2010</year><volume>39</volume><fpage>483</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1068/p6473</pub-id><pub-id pub-id-type="pmid">20514997</pub-id></element-citation></ref><ref id="CR42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiff</surname><given-names>W</given-names></name><name><surname>Oldak</surname><given-names>R</given-names></name></person-group><article-title>Accuracy of judging time to arrival: effects of modality, trajectory, and gender</article-title><source>J Exp Psychol Hum Percept Perform</source><year>1990</year><volume>16</volume><fpage>303</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.16.2.303</pub-id><pub-id pub-id-type="pmid">2142201</pub-id></element-citation></ref><ref id="CR43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sch&#x000f6;rnich</surname><given-names>S</given-names></name><name><surname>Nagy</surname><given-names>A</given-names></name><name><surname>Wiegrebe</surname><given-names>L</given-names></name></person-group><article-title>Discovering your inner bat: echo&#x02013;acoustic target ranging in humans</article-title><source>J Assoc Res Otolaryngol</source><year>2012</year><volume>13</volume><fpage>673</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1007/s10162-012-0338-z</pub-id><pub-id pub-id-type="pmid">22729842</pub-id></element-citation></ref><ref id="CR44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoffregen</surname><given-names>TA</given-names></name><name><surname>Pittenger</surname><given-names>JB</given-names></name></person-group><article-title>Human echolocation as a basic form of perception and action</article-title><source>Ecol Psychol</source><year>1995</year><volume>7</volume><fpage>181</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1207/s15326969eco0703_2</pub-id></element-citation></ref><ref id="CR45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strelow</surname><given-names>ER</given-names></name><name><surname>Brabyn</surname><given-names>JA</given-names></name></person-group><article-title>Locomotion of the blind controlled by natural sound cues</article-title><source>Perception</source><year>1982</year><volume>11</volume><fpage>635</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1068/p110635</pub-id><pub-id pub-id-type="pmid">7186614</pub-id></element-citation></ref><ref id="CR46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Supa</surname><given-names>M</given-names></name><name><surname>Cotzin</surname><given-names>M</given-names></name><name><surname>Dallenbach</surname><given-names>KM</given-names></name></person-group><article-title>Facial vision: the perception of obstacles by the blind</article-title><source>Am J Psychol</source><year>1944</year><volume>57</volume><fpage>133</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.2307/1416946</pub-id></element-citation></ref><ref id="CR47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>S</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><article-title>The acuity of echolocation: spatial resolution in the sighted compared to expert performance</article-title><source>J Vis Impair Blind</source><year>2011</year><volume>105</volume><fpage>20</fpage><lpage>32</lpage><pub-id pub-id-type="pmid">21611133</pub-id></element-citation></ref><ref id="CR48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>S</given-names></name><name><surname>Puri</surname><given-names>A</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><article-title>Ultrafine spatial acuity of blind expert human echolocators</article-title><source>Exp Brain Res</source><year>2012</year><volume>216</volume><fpage>483</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.1007/s00221-011-2951-1</pub-id><pub-id pub-id-type="pmid">22101568</pub-id></element-citation></ref><ref id="CR49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thaler</surname><given-names>L</given-names></name><name><surname>Arnott</surname><given-names>SR</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><article-title>Neural correlates of natural human echolocation in early and late blind echolocation experts</article-title><source>PLoS One</source><year>2011</year><volume>6</volume><fpage>e20162</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0020162</pub-id><pub-id pub-id-type="pmid">21633496</pub-id></element-citation></ref><ref id="CR50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turano</surname><given-names>KA</given-names></name><name><surname>Yu</surname><given-names>D</given-names></name><name><surname>Hao</surname><given-names>L</given-names></name><name><surname>Hicks</surname><given-names>JC</given-names></name></person-group><article-title>Optic-flow and egocentric-direction strategies in walking: central vs peripheral visual field</article-title><source>Vision Res</source><year>2005</year><volume>45</volume><fpage>3117</fpage><lpage>3132</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2005.06.017</pub-id><pub-id pub-id-type="pmid">16084556</pub-id></element-citation></ref><ref id="CR51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vercillo</surname><given-names>T</given-names></name><name><surname>Milne</surname><given-names>JL</given-names></name><name><surname>Gori</surname><given-names>M</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><article-title>Enhanced auditory spatial localization in blind echolocators</article-title><source>Neuropsychologia</source><year>2015</year><volume>67</volume><fpage>35</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.12.001</pub-id><pub-id pub-id-type="pmid">25484307</pub-id></element-citation></ref><ref id="CR52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname><given-names>P</given-names></name><name><surname>Lassonde</surname><given-names>M</given-names></name><name><surname>Gougoux</surname><given-names>F</given-names></name><name><surname>Fortin</surname><given-names>M</given-names></name><name><surname>Guillemot</surname><given-names>J</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name></person-group><article-title>Early- and late-onset blind individuals show supra-normal auditory abilities in far-space</article-title><source>Curr Biol</source><year>2004</year><volume>14</volume><fpage>1734</fpage><lpage>1738</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.09.051</pub-id><pub-id pub-id-type="pmid">15458644</pub-id></element-citation></ref><ref id="CR53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallmeier</surname><given-names>L</given-names></name><name><surname>Wiegrebe</surname><given-names>L</given-names></name></person-group><article-title>Ranging in human sonar: effects of additional early reflections and exploratory head movements</article-title><source>PLoS One</source><year>2014</year><volume>9</volume><fpage>e115363</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0115363</pub-id><pub-id pub-id-type="pmid">25551226</pub-id></element-citation></ref><ref id="CR54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallmeier</surname><given-names>L</given-names></name><name><surname>Wiegrebe</surname><given-names>L</given-names></name></person-group><article-title>Self-motion facilitates echo-acoustic orientation in humans</article-title><source>R Soc Open Sci</source><year>2014</year><volume>1</volume><fpage>140185</fpage><pub-id pub-id-type="doi">10.1098/rsos.140185</pub-id><pub-id pub-id-type="pmid">26064556</pub-id></element-citation></ref><ref id="CR55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name></person-group><article-title>Visually controlled locomotion: 40&#x000a0;years later</article-title><source>Ecol Psychol</source><year>1998</year><volume>10</volume><fpage>177</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1080/10407413.1998.9652682</pub-id></element-citation></ref><ref id="CR56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worchel</surname><given-names>P</given-names></name><name><surname>Dallenbach</surname><given-names>KM</given-names></name></person-group><article-title>Facial vision: perception of obstacles by the deaf-blind</article-title><source>Am J Psychol</source><year>1947</year><volume>60</volume><fpage>502</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.2307/1417725</pub-id><pub-id pub-id-type="pmid">20273385</pub-id></element-citation></ref><ref id="CR57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worchel</surname><given-names>P</given-names></name><name><surname>Mauney</surname><given-names>J</given-names></name></person-group><article-title>The effect of practice on the perception of obstacles by the blind</article-title><source>J Exp Psychol</source><year>1951</year><volume>41</volume><fpage>170</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1037/h0055653</pub-id><pub-id pub-id-type="pmid">14841326</pub-id></element-citation></ref><ref id="CR58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worchel</surname><given-names>P</given-names></name><name><surname>Mauney</surname><given-names>J</given-names></name><name><surname>Andrew</surname><given-names>JG</given-names></name></person-group><article-title>The perception of obstacles by the blind</article-title><source>J Exp Psychol</source><year>1950</year><volume>40</volume><fpage>746</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1037/h0060950</pub-id><pub-id pub-id-type="pmid">14803650</pub-id></element-citation></ref><ref id="CR59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zahorik</surname><given-names>P</given-names></name><name><surname>Brungart</surname><given-names>DS</given-names></name><name><surname>Bronkhorst</surname><given-names>AW</given-names></name></person-group><article-title>Auditory distance perception in humans: a summary of past and present research</article-title><source>Acta Acust United Acust</source><year>2005</year><volume>91</volume><fpage>409</fpage><lpage>420</lpage></element-citation></ref></ref-list></back></article>
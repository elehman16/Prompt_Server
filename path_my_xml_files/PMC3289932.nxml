<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neurorobot</journal-id><journal-id journal-id-type="publisher-id">Front. Neurorobot.</journal-id><journal-title-group><journal-title>Frontiers in Neurorobotics</journal-title></journal-title-group><issn pub-type="epub">1662-5218</issn><publisher><publisher-name>Frontiers Research Foundation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">22393319</article-id><article-id pub-id-type="pmc">3289932</article-id><article-id pub-id-type="doi">10.3389/fnbot.2012.00001</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Grounding the Meanings in Sensorimotor Behavior using Reinforcement Learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Farka&#x00161;</surname><given-names>Igor</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib><contrib contrib-type="author"><name><surname>Mal&#x000ed;k</surname><given-names>Tom&#x000e1;&#x00161;</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Rebrov&#x000e1;</surname><given-names>Krist&#x000ed;na</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Applied Informatics, Comenius University in Bratislava</institution><country>Bratislava, Slovakia</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Ricardo Chavarriaga, Ecole Polytechnique F&#x000e9;d&#x000e9;rale de Lausanne, Switzerland</p></fn><fn fn-type="edited-by"><p>Reviewed by: Mehdi Khamassi, Centre National de la Recherche Scientifique, France; Minoru Asada, Osaka University, Japan</p></fn><corresp id="fn001">*Correspondence: Igor Farka&#x00161;, Department of Applied Informatics, Comenius University in Bratislava, FMFI UK; Mlynsk&#x000e1; dolina, 824 48 Bratislava, Slovakia. e-mail: <email>farkas@fmph.uniba.sk</email></corresp></author-notes><pub-date pub-type="epub"><day>29</day><month>2</month><year>2012</year></pub-date><pub-date pub-type="collection"><year>2012</year></pub-date><volume>6</volume><elocation-id>1</elocation-id><history><date date-type="received"><day>20</day><month>10</month><year>2011</year></date><date date-type="accepted"><day>12</day><month>2</month><year>2012</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2012 Farka&#x00161;, Mal&#x000ed;k and Rebrov&#x000e1;.</copyright-statement><copyright-year>2012</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><license-p>This is an open-access article distributed under the terms of the <uri xlink:type="simple" xlink:href="http://creativecommons.org/licenses/by-nc/3.0/">Creative Commons Attribution Non Commercial License</uri>, which permits non-commercial use, distribution, and reproduction in other forums, provided the original authors and source are credited.</license-p></license></permissions><abstract><p>The recent outburst of interest in cognitive developmental robotics is fueled by the ambition to propose ecologically plausible mechanisms of how, among other things, a learning agent/robot could ground linguistic meanings in its sensorimotor behavior. Along this stream, we propose a model that allows the simulated iCub robot to learn the meanings of actions (point, touch, and push) oriented toward objects in robot&#x02019;s peripersonal space. In our experiments, the iCub learns to execute motor actions and comment on them. Architecturally, the model is composed of three neural-network-based modules that are trained in different ways. The first module, a two-layer perceptron, is trained by back-propagation to attend to the target position in the visual scene, given the low-level visual information and the feature-based target information. The second module, having the form of an actor-critic architecture, is the most distinguishing part of our model, and is trained by a continuous version of reinforcement learning to execute actions as sequences, based on a linguistic command. The third module, an echo-state network, is trained to provide the linguistic description of the executed actions. The trained model generalizes well in case of novel action-target combinations with randomized initial arm positions. It can also promptly adapt its behavior if the action/target suddenly changes during motor execution.</p></abstract><kwd-group><kwd>motor actions</kwd><kwd>grounding</kwd><kwd>reinforcement learning</kwd><kwd>connectionist modeling</kwd></kwd-group><counts><fig-count count="15"/><table-count count="4"/><equation-count count="8"/><ref-count count="58"/><page-count count="13"/><word-count count="11471"/></counts></article-meta></front><body><sec><label>1</label><title>Introduction</title><p>One of the topics central to cognitive science is the representation of meanings related to language. Unlike the standard theories, that postulate the existence of amodal symbols, being the core of internal representations, the grounded theories of language acquisition and comprehension (Harnad, <xref ref-type="bibr" rid="B15">1990</xref>; Barsalou, <xref ref-type="bibr" rid="B2">1999</xref>; Jeannerod, <xref ref-type="bibr" rid="B20">2001</xref>) claim that arbitrary symbols (of language) are grounded in some way in the world and body (Wilson, <xref ref-type="bibr" rid="B58">2002</xref>). Thought is expressed not as a symbol manipulation, but as an inner simulation drawing on lower-level capacities of the sensorimotor cognition. A growing amount of empirical evidence serves in favor of grounded cognition (see, e.g., a review in Barsalou, <xref ref-type="bibr" rid="B3">2008</xref>). The question of involvement of the motor modality in the comprehension of language was explored in various psycholinguistic and neuropsychological studies. For instance, Pulverm&#x000fc;ller and colleagues (Pulverm&#x000fc;ller et al., <xref ref-type="bibr" rid="B34">2001</xref>; Hauk et al., <xref ref-type="bibr" rid="B16">2004</xref>; Pulverm&#x000fc;ller, <xref ref-type="bibr" rid="B33">2005</xref>) measured the activity in motor areas of the brain during comprehension of simple action verbs connected to different effectors, namely &#x0201c;kick&#x0201d; executed with leg, &#x0201c;pick&#x0201d; with hand, and &#x0201c;lick&#x0201d; with mouth. Results from various experiments showed that during sole comprehension of language without movement, a somatotopic (map-like) activation appeared in the motor cortex, in accordance with the effector of the action verb. Glenberg and Kaschak (<xref ref-type="bibr" rid="B13">2002</xref>) studied this phenomenon on the basis of interference between motion execution and comprehension of so called &#x0201c;transfer&#x0201d; sentences (i.e., including motion from an agent to a patient). Results of experiments showed that the reaction time was significantly shorter when participants had to make a move congruent with the direction implied by the perceived sentence, in comparison with the incongruent direction. Interestingly, the significant difference was observed also in case of abstract sentences (Glenberg et al., <xref ref-type="bibr" rid="B14">2008</xref>), which suggests that even high-level concepts may be somehow related to the sensorimotor behavior.</p><p>Despite the rich amount of empirical evidence on the nature of language comprehension, the actual mechanisms of grounding of concepts have not yet been clarified. Computational modeling has proved fruitful in the field of grounded cognition research. Some authors (e.g., Steels, <xref ref-type="bibr" rid="B39">2008</xref>) claim that the problem of symbol grounding has been solved. This research direction of Steels (<xref ref-type="bibr" rid="B38">2003</xref>) and Steels and Belpaeme (<xref ref-type="bibr" rid="B40">2005</xref>) focuses on a basic method of grounding the language into sensorimotor cognition of the agent, and on the development of simple shared lexicon in a population of agents using language games. For instance, in the color naming domain, Steels and Belpaeme (<xref ref-type="bibr" rid="B40">2005</xref>) created a simple model of color categorization and naming, where agents play two types of games. First an agent plays a discrimination game to learn to distinguish color categories. Having acquired some basic categories, the agent is able to communicate randomly assigned names of those categories with other agents. Through such a negotiation process the agents are able to develop a shared lexicon of color names representing basic color categories qualitatively similar to human basic color categories. As the authors argue, the acquired meanings in the agents are grounded because they are autonomously generated in the world through a sensorimotor embodiment and perceptually grounded categorization methods, and the agents autonomously introduce and negotiate symbols invoking these meanings (Steels, <xref ref-type="bibr" rid="B39">2008</xref>, p. 238). It appears though that the issue of grounding (referential) meanings is still a matter of vivid debate in the community (De Vega et al., <xref ref-type="bibr" rid="B11">2008</xref>). Namely, some authors propose that the representation of meaning (e.g., in amodal or in perceptual symbols) is entirely independent from the question of whether meaning is grounded or not. Hence, the solution for symbol grounding could arise in a system using amodal symbols. The opposing argument holds that we truly need to avoid amodal symbols to guarantee the formation of meanings inherent to the agent. In their review, Taddeo and Floridi (<xref ref-type="bibr" rid="B44">2005</xref>) introduced zero semantical commitment condition as a criterion for valid solution to the symbol grounding problem, completely avoiding the designer&#x02019;s approach. This criterion, however, appears unsatisfiable in artificial systems (Vavre&#x0010d;ka, <xref ref-type="bibr" rid="B55">2006</xref>).</p><p>Cognitive developmental robotics offers a stable platform to study the grounding in separate cognitive abilities (Asada et al., <xref ref-type="bibr" rid="B1">2009</xref>). It uses physical and simulated robots operating in a simplified environment focusing on a concrete problem with a possible scalability to other skills and domains. Artificial neural networks are typically used as control architectures for these robots. Cangelosi and Riga (<xref ref-type="bibr" rid="B7">2006</xref>) examined how grounded meanings of words can be combined to form the meaning of a new word. This process is called the symbol grounding transfer, since the meaning is not grounded directly, but in its simpler components. For instance, words like &#x0201c;horse&#x0201d; and &#x0201c;horn&#x0201d; can be combined to a new concept &#x0201c;unicorn.&#x0201d; It is very likely that high-level abstract concepts, which cannot be directly associated with sensorimotor experience, are grounded using low-level concrete concepts (Barsalou and Wiemer-Hastings, <xref ref-type="bibr" rid="B4">2005</xref>). The model of Cangelosi and Riga (<xref ref-type="bibr" rid="B7">2006</xref>) is based on learning through imitation, which has been considered fundamental for acquisition of language (Tomasello, <xref ref-type="bibr" rid="B50">2003</xref>). In their experiment they used two robots simulated in open dynamics engine (ODE, <uri xlink:type="simple" xlink:href="http://www.ode.org">www.ode.org</uri>), one pre-programmed demonstrator and one imitator endowed with a multi-layer perceptron neural controller trained using a standard back-propagation algorithm (Rumelhart et al., <xref ref-type="bibr" rid="B35">1986</xref>). First the robot learns to comprehend and produce basic actions, in latter stages of development it learns to comprehend and produce composite actions based on a verbal command. In a subsequent work, Cangelosi et al. (<xref ref-type="bibr" rid="B8">2007</xref>) extended this simple architecture to not only comprehend, but also to produce verbal descriptions of actions. These important works, aimed to discover and describe the actual mechanism of learning abstract concepts, clearly demonstrate the importance of the cognitive robotics as a research method. However, a small drawback of this model is the learning procedure and the associated assumption about the availability of the required motor commands. It is not natural for the action to be literally forced to the agent, as if the instructor was dragging the child&#x02019;s hand to learn to reach for an object.</p><p>One of the most intriguing recent connectionist models of embodied language acquisition is the Recurrent Neural Network with Parametric Biases (RNNPB, Tani, <xref ref-type="bibr" rid="B45">2003</xref>; Tani and Ito, <xref ref-type="bibr" rid="B46">2003</xref>). It is a Jordan-type recurrent neural network endowed with special parametric bias nodes, which enable it to recognize and categorize various spatio-temporal patterns and thus modulate its own dynamic function. The values of the PB nodes during the execution of concrete behaviors can be stored and subsequently (manually) fed as an input to the network, which will then produce the learned behavior, or to a twin RNNPB network (trained to acquire similar PB vectors for matching concepts) to produce recollection. An important feature of this model is, that the PB vectors are formed purely by self-organization, so they are self-determined by the network in an unsupervised manner. On the other hand, the general learning algorithm used in this architecture is the (supervised) back-propagation through time (BPTT) algorithm (Rumelhart et al., <xref ref-type="bibr" rid="B35">1986</xref>). RNNPB was used in various setups where physical or simulated robots learned repertoires of actions, and/or action names (Tani et al., <xref ref-type="bibr" rid="B47">2004</xref>; Sugita and Tani, <xref ref-type="bibr" rid="B41">2005</xref>, <xref ref-type="bibr" rid="B42">2008</xref>). A typical cognitive robotics methodology, used in experiments of Tani and Sugita comprises a simple perceptual categorization followed by a verbal command leading to the execution of a simple action. One particular experiment of Sugita and Tani (<xref ref-type="bibr" rid="B41">2005</xref>) served as a model setting for our experiments. It used a small mobile robot a task of which was to comprehend a two-word command indicating either a location or a color of one of the three objects in front of it (left, center, right, red, green, blue), and an action to be produced upon the object (push, point, hit).</p><p>Currently the most &#x0201c;accurate&#x0201d; prototype of a child robot is the iCub, a small-size humanoid robot created under the European project RobotCub (Metta et al., <xref ref-type="bibr" rid="B28">2008</xref>). Designed to resemble a 2.5-year-old child, even in height and weight, the iCub is endowed with 53 degrees of freedom (DoF) in joints distributed all over its body in proportion similar to human effectors (e.g., 9 DoF for hands), movable eyes with color cameras, and various other sensors, providing a very accurate model of an actual child&#x02019;s body. The platform includes a carefully designed virtual form of the iCub robot in the ODE, the iCub simulator (Tikhanoff et al., <xref ref-type="bibr" rid="B48">2008</xref>), providing a safe, yet realistic, environment for testing the control architectures before implementing them to the real robot. Various computational models of cognitive capacities were created on the basis of the iCub simulator platform, many of which focus on sensorimotor cognition and grounded acquisition of language, the topic considered in this paper. For instance, Macura et al. (<xref ref-type="bibr" rid="B26">2009</xref>) propose a cognitive robotics model of grasping based on a hybrid neural network architecture, with Jordan recurrent neural network used as an executive component. In their experiments the iCub learns to execute various grasping sequences and to differentiate between them as different goals. Results of experiments showed that not only did the iCub learn to grasp successfully, but it also displayed similar stimulus&#x02013;response compatibility effect (Simon, <xref ref-type="bibr" rid="B36">1969</xref>) as human subjects, hence highlighting the model&#x02019;s biological relevance.</p><p>Recently, Tikhanoff et al. (<xref ref-type="bibr" rid="B49">2011</xref>) created a neural network model for reaching and grasping, incorporating the linguistic component as well. The architecture encompasses a feed-forward network for reaching and a recurrent neural network for grasping. The reaching task, trained by error back-propagation (BP), is approached as one-step process, for which the training data is first acquired during the motor babbling stage when the required joint position are stored for various target positions in robot&#x02019;s peripersonal space. In contrast, the grasping behavior is viewed as a sequential task and employs an associative reinforcement learning (RL) algorithm (Barto and Jordan, <xref ref-type="bibr" rid="B5">1987</xref>), making the approach more ecologically plausible. This task is hence based on an appropriate design of the reward function that can drive successful grasp types for different objects. Marocco et al. (<xref ref-type="bibr" rid="B27">2010</xref>) proposed a model of direct grounding of language in action (motor control), inspired by Sugita and Tani (<xref ref-type="bibr" rid="B41">2005</xref>) and Cangelosi and Riga (<xref ref-type="bibr" rid="B7">2006</xref>), in which language and action are fully interconnected. Meanings in this model are specific exclusively to the robot (simulated iCub), not to the designer. Unlike less plausible input-target mapping pairs used in some of the previous models, the model of Marocco et al. (<xref ref-type="bibr" rid="B27">2010</xref>) learns dynamical sequences of input-output patterns as they develop in time. To achieve this, the model is trained using the BPTT algorithm, hence the procedure of the training data acquisition still requires the experimenter&#x02019;s participation.</p><p>As an alternative to BP-based approach, some authors prefer the well-known self-organizing maps (SOMs; Kohonen, <xref ref-type="bibr" rid="B22">1997</xref>) as core components of the cognitive architecture. For instance, Wermter and Elshaw (<xref ref-type="bibr" rid="B57">2003</xref>) propose a model of the self-organizing memory for a robot called MirrorBot, inspired by cortical assemblies and mirror-neurons. In this model, strongly influenced by neuropsychological findings of Pulverm&#x000fc;ller and colleagues (e.g., Pulverm&#x000fc;ller et al., <xref ref-type="bibr" rid="B34">2001</xref>), verbally labeled actions are clustered according to the body parts they are associated with. It consists of multiple interconnected SOMs, each representing either a body part or an association area. First a map of the whole body projects to the maps for body parts, which, together with a linguistic module project to a higher-level association area on top. The model was trained on sensor readings from Mirror-neuron Robot Agent (MIRA; Wermter and Elshaw, <xref ref-type="bibr" rid="B57">2003</xref>) using the standard Hebbian learning and was evaluated for some primary parts of the network. Unfortunately, Wermter and Elshaw (<xref ref-type="bibr" rid="B57">2003</xref>) do not provide results for the whole model, so its functionality has not been fully proved. Recently, Morse et al. (<xref ref-type="bibr" rid="B29">2010</xref>) proposed a cognitive robotics framework (ERA), based on hierarchically interconnected SOMs as well. In this model, one SOM represents a lower-level structure (for instance a sensory modality, some of its aspect, etc.) and is connected to a higher-level &#x0201c;hub&#x0201d; SOM. In ERA architecture, multiple SOM structures are hierarchically connected through the associative hubs. Morse et al. (<xref ref-type="bibr" rid="B29">2010</xref>) evaluated the architecture on a simple naming experiment with infants called the &#x0201c;modi&#x0201d; experiment (Smith and Samuelson, <xref ref-type="bibr" rid="B37">2010</xref>). Here the child watches the experimenter showing two toys in particular locations (left and right). Later on, the child&#x02019;s attention is brought to a specific location, and a name (for a toy) is uttered. Results from such experiments with children, as well as with a simulated iCub endowed with a body-posture hub architecture, showed that an infant (or iCub) could learn the names of objects accordingly to the object&#x02019;s typical spatial location without the presence of the object.</p><p>An impressive piece of work related to grounding language in sensorimotor behavior has been done by Dominey and his colleagues (see, e.g., Dominey and Boucher, <xref ref-type="bibr" rid="B12">2005</xref>; Lallee et al., <xref ref-type="bibr" rid="B23">2010</xref>) and references therein). Dominey and Boucher (<xref ref-type="bibr" rid="B12">2005</xref>) developed a neural-network based system for language learning with a minimal pre-wired language-specific functionality. The meanings (of events and spatial relations) are extracted from video images in terms of low-level primitives, and are linked with descriptive sentence forms via learning grammatical constructions. The system was shown to reproduce various observations from developmental studies. Lallee et al. (<xref ref-type="bibr" rid="B23">2010</xref>) extend their framework for embodied language and action comprehension by including a teleological representation that allows goal-based reasoning for novel actions. This work demonstrates the advantages of a hybrid, embodied&#x02013;teleological approach to action&#x02013;language interaction, both from a theoretical perspective, and via results from human&#x02013;robot interaction experiments with the iCub robot. The novelty of the teleological approach consists in the representation of the subcomponents of actions, which includes relations between initial enabling states and final resulting states for actions. The paper also explains how language comes to reflect the structure of action, and how it can subsequently be used as an input and output vector for embodied and teleological aspects of action.</p><p>The motivation for our work was to ground the meaning of action-related commands in robot&#x02019;s sensorimotor behavior, using an approach in which the sensorimotor behavior is treated as inherently sequential and the learning is based on an ecologically justifiable feedback. Our model shares some features with the above mentioned models but its most distinguishing feature is the action learning module that is based on a continuous version of the RL paradigm (Sutton and Barto, <xref ref-type="bibr" rid="B43">1998</xref>) that lends itself to generalization and moreover is biologically inspired. As explained below, the RL approach is based on a reward function that can lead to desired behavior by exploring the state space. The RL approach distinguishes our model from earlier models that are mainly restricted to the supervised learning paradigm, which requires <italic>a priori</italic> generation of the training data. The exception is the grasping module in Tikhanoff et al. (<xref ref-type="bibr" rid="B49">2011</xref>) where the data is generated online based on the feedback. Our task of action learning is inspired by Sugita and Tani (<xref ref-type="bibr" rid="B41">2005</xref>) which is neither reaching nor grasping but something in between (<italic>point</italic>, <italic>touch</italic>, and <italic>push</italic> actions). Details of our model are described below.</p></sec><sec sec-type="materials|methods"><label>2</label><title>Materials and Methods</title><p>For our experiments we used the iCub simulator, activating 4 DoF in its right arm (the rest of robot&#x02019;s body was motionless). The experimental setup involved the robot facing three objects in its peripersonal space (lying on a table), as displayed in Figure <xref ref-type="fig" rid="F1">1</xref>.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Example of a scene with the simulated iCub acting on objects</bold>.</p></caption><graphic xlink:href="fnbot-06-00001-g001"/></fig><p>Figure <xref ref-type="fig" rid="F2">2</xref> displays the model. It comprises three neural network modules, which are trained for their respective subtasks. (1) Target localizer (TL) detects the position of the target object on the scene based on a low-level visual information about the scene and on the description of features of the target object. (2) Action learning (AL) module is responsible for motor control. Based on a command about action type and the information about the target object position, it produces a sequence of movements to execute the command. (3) Action naming (AN) module learns to generate the linguistic description of the executed action. TL and AN modules can be seen as &#x0201c;auxiliary,&#x0201d; whereas the AL module is the most distinguishing part of our model.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>The scheme of the proposed model in terms of neural network modules (TL, target localizer, AN, action learning, AN, action naming)</bold>.</p></caption><graphic xlink:href="fnbot-06-00001-g002"/></fig><sec><label>2.1</label><title>Localization of target object</title><p>The TL module is a standard multi-layer perceptron with one hidden layer. It maps three groups of input &#x02013; [<sc>image</sc>, <sc>colour</sc>, <sc>target</sc>] onto the target object <sc>position</sc>. <sc>image</sc> is a bitmap displaying a selection retrieved from the original visual percept of the iCub, processed with the OpenCV library as described in the following text. <sc>target</sc> encodes which object should be acted upon. Using a localist encoding with 6 neurons, <sc>target</sc> can be represented either by its color (red, green, blue) or shape (box, cylinder, sphere), which can be seen as concepts, that the agent has in its mind. The whole input vector of the TL module is depicted in Figure <xref ref-type="fig" rid="F3">3</xref>.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>The schema of the input vector for target localizer</bold>. In the simulations, the dimensionality of the object color and the target information was increased to counterbalance the effect of the high-dimensional image part.</p></caption><graphic xlink:href="fnbot-06-00001-g003"/></fig><p>Altogether, the input layer comprises 1200 neurons for image pixels (concatenated in a line), at least 9 neurons for coding colors, and 6 for the target (but see Section <xref ref-type="sec" rid="s2">3.1</xref> regarding the multiplication of dimensions). The three output <sc>position</sc> neurons have a sigmoid activation function with output values in the range &#x03008;0, 1&#x03009;, and after winner-takes-all evaluation they encompass one-hot encoding of the target position (left, middle, right).</p><p>In our experiments, the standard visual output of 320&#x02009;&#x000d7;&#x02009;240 pixels from one of the iCub&#x02019;s cameras encompasses a scene with three objects lying on the table, within the reach of the robot&#x02019;s right arm. This image is then processed using the OpenCV library using a pre-programmed procedure (image processing is not the focus of our model), consisting of the following steps (see Figure <xref ref-type="fig" rid="F4">4</xref>): (1) conversion to black-and-white image, (2) cropping the objects as a group, (3) color inversion, and (4) edge extraction. The output of step 4 serves as (part of) input to the TL module.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Processing of the input image using the OpenCV library</bold>.</p></caption><graphic xlink:href="fnbot-06-00001-g004"/></fig></sec><sec><label>2.2</label><title>Action learning</title><p>The AL module is based on the reinforcement learning paradigm (Sutton and Barto, <xref ref-type="bibr" rid="B43">1998</xref>), which does not require prior exact knowledge about the sequences of joint positions leading to the target movement. It operates in a continuous space of states and actions, using the CACLA algorithm (Van Hasselt and Wiering, <xref ref-type="bibr" rid="B53">2007</xref>). The AL module has an actor-critic architecture, consisting of two modules (function approximators). The actor&#x02019;s task is to select optimal actions leading to the desired outcome (hence maximizing the reward). The critic&#x02019;s task is to estimate the evaluation (reward) of the agent&#x02019;s state. The actor is, unlike standard supervised learning with pre-given targets, adjusted on the basis of the adaptive critic&#x02019;s evaluation.</p><p>Both actor and the critic consist of a two-layer perceptron. The actor maps the input [<sc>state</sc>, <sc>action</sc>, <sc>position</sc>] (together 11 neurons) onto <sc>state-change</sc> output (4 neurons). The critic maps the same input [<sc>state</sc>, <sc>action</sc>, <sc>position</sc>] into evaluation of the current state <sc>value</sc>. Both networks contain the sigmoid activation function in all neurons.</p><p>The state vector of both actor and critic network is depicted in Figure <xref ref-type="fig" rid="F5">5</xref>. The <sc>state</sc> variables represent four joints of the robot&#x02019;s arm (the hand remains still with fingers extended) and the hand&#x02019;s touch sensor (which turns to 1 when the hand touches an object, otherwise it remains 0). The joint values are scaled from the degrees to the interval [&#x02212;1, 1] using the formula</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>The input vector for the AL module</bold>. The agent is in the state of making the push action on the middle object.</p></caption><graphic xlink:href="fnbot-06-00001-g005"/></fig><disp-formula id="E1"><label>(1)</label><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">=</mml:mo><mml:mn>2</mml:mn><mml:mo class="MathClass-bin">*</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-bin">+</mml:mo><mml:mo class="MathClass-rel">|</mml:mo><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext>Min</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">|</mml:mo></mml:mrow><mml:mrow><mml:mo class="MathClass-rel">|</mml:mo><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext>Min</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">|</mml:mo><mml:mo class="MathClass-bin">+</mml:mo><mml:mo class="MathClass-rel">|</mml:mo><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext>Max</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">|</mml:mo></mml:mrow></mml:mfrac><mml:mo class="MathClass-bin">-</mml:mo><mml:mn>1</mml:mn><mml:mo class="MathClass-punc">,</mml:mo></mml:math></disp-formula><p>where Min<italic><sub>i</sub></italic> and Max<italic><sub>i</sub></italic> denote the minimal and maximal joint angles, respectively. These limit values, related to joints 1 through 4, determine the intervals [&#x02212;95; 90], [0; 161], [&#x02212;37; 100], [&#x02212;6; 106], respectively.</p><p>The <sc>action</sc> input represents the pre-processed external linguistic input denoting which action should be taken, using three neurons (with localist encoding). Finally, <sc>position</sc> denotes the output from the TL module, representing the position of the attended (target) object (left, center, right), together with <sc>action</sc> input comprising the whole action command (e.g., &#x0201c;push red&#x0201d;).</p><sec><label>2.2.1</label><title>Training the module</title><p>The value function <italic>V</italic>(<italic>s</italic>), computed by the critic, is updated based on temporal differences between agent&#x02019;s subsequent states, using the equation</p><disp-formula id="E2"><label>(2)</label><mml:math id="M9"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo class="MathClass-bin">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo class="MathClass-rel">=</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo class="MathClass-bin">+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-punc">,</mml:mo></mml:math></disp-formula><p>where &#x003b4;<italic><sub>t</sub></italic>&#x02009;=&#x02009;<italic>r</italic><sub><italic>t</italic>+1</sub>&#x02009;+&#x02009;&#x003b3;<italic>V<sub>t</sub></italic>(<italic>s</italic><sub><italic>t</italic>+1</sub>)&#x02009;&#x02212;&#x02009;<italic>V<sub>t</sub></italic>(<italic>s<sub>t</sub></italic>) is the temporal-difference (TD) error, 0&#x02009;&#x02264;&#x02009;&#x003b1;<italic><sub>t</sub></italic>&#x02009;&#x02264;&#x02009;1 denotes the learning rate and 0&#x02009;&#x02264;&#x02009;&#x003b3;&#x02009;&#x02264;&#x02009;1 is the discount factor. The reward <italic>r</italic><sub><italic>t</italic>+1</sub> is received by the agent immediately <italic>after</italic> executing the chosen action, which results in a state transition from <italic>s<sub>t</sub></italic> to <italic>s</italic><sub><italic>t</italic>+1</sub>. It is known that using the update given by equation <xref ref-type="disp-formula" rid="E2">(2)</xref> for the discrete RL will result in convergence of the values to the actual expected rewards for a fixed policy (Sutton and Barto, <xref ref-type="bibr" rid="B43">1998</xref>). CACLA extends the usability of this update in continuous RL by yielding accurate function approximators (of both the critic and the actor).</p><p>Critic&#x02019;s parameters (weights), expressed by a vector <inline-formula><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are updated using a gradient-based learning rule</p><disp-formula id="E3"><label>(3)</label><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mi>t</mml:mi><mml:mo class="MathClass-bin">+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo class="MathClass-rel">=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mo class="MathClass-bin">+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula><p>The actor chooses an action <italic>a<sub>t</sub></italic>(<italic>s<sub>t</sub></italic>) given by its parameters <inline-formula><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow><mml:mo class="MathClass-punc">,</mml:mo></mml:mrow></mml:math></inline-formula> but is also allowed to explore the environment by choosing a novel action <inline-formula><mml:math id="M3"><mml:mrow><mml:mi>&#x000e3;</mml:mi><mml:mo class="MathClass-punc">,</mml:mo></mml:mrow></mml:math></inline-formula> taken from Gaussian distribution</p><disp-formula id="E4"><label>(4)</label><mml:math id="M11"><mml:mi>&#x003c0;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-punc">,</mml:mo><mml:msub><mml:mrow><mml:mo class="MathClass-op">&#x000e3;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo class="MathClass-rel">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:msqrt><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:mfrac><mml:mo class="qopname">exp</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mo class="MathClass-bin">-</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mo class="qopname">&#x000e3;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-bin">-</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo class="MathClass-bin">&#x02215;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo class="MathClass-punc">.</mml:mo></mml:math></disp-formula><p>Then the update rule for the actor&#x02019;s weights is applied if &#x003b4;<italic><sub>t</sub></italic>&#x02009;&#x0003e;&#x02009;0:</p><disp-formula id="E5"><label>(5)</label><mml:math id="M12"><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mi>t</mml:mi><mml:mo class="MathClass-bin">+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo class="MathClass-rel">=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo class="MathClass-bin">+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mo class="MathClass-op">&#x000e3;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-bin">-</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfrac><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></disp-formula><p>The training procedure for the AL module is summarized in Algorithm 1.</p><table-wrap id="UT1" position="float"><label>Algorithm 1:</label><caption><title>CACLA learning algorithm</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" rowspan="1" colspan="1"><preformat position="float" xml:space="preserve">
<monospace>
 S<sub>0</sub>&#x02009;&#x02190;&#x02009;initial state
 Initialize parameters <inline-formula><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M5"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo class="MathClass-punc">,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> randomly
 <bold>for</bold> t&#x02009;=&#x02009;0, 1, 2&#x02026; <bold>do</bold>
         <inline-formula><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mo class="MathClass-op">&#x000e3;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">&#x02190;</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using exploration
         perform action <inline-formula><mml:math id="M7"><mml:mrow><mml:msub><mml:mrow><mml:mo class="MathClass-op">&#x000e3;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and move to <italic>s</italic><sub><italic>t</italic>+1</sub>, get new <italic>r</italic><sub><italic>t</italic>+1</sub> and
         <italic>V<sub>t</sub></italic>(<italic>s</italic><sub><italic>t</italic>+1</sub>)
        update critic&#x02019;s parameters (eq. <xref ref-type="disp-formula" rid="E3">3</xref>)
       <bold>if</bold> <italic>V</italic><sub><italic>t</italic>+1</sub>(<italic>s<sub>t</sub></italic>)&#x02009;&#x0003e;&#x02009;<italic>V<sub>t</sub></italic>(<italic>s<sub>t</sub></italic>) <bold>then</bold>
              update actor&#x02019;s parameters (eq. <xref ref-type="disp-formula" rid="E5">5</xref>)
       <bold>end if</bold>
     <bold>end for</bold>
</monospace>
</preformat></td></tr></tbody></table></table-wrap><p>CACLA differs from other actor-critic architectures in some respect. Most other actor-critic methods use the size of the TD error and also the update in the opposite direction when its sign is negative. However, this is usually not a good idea for CACLA, since this is equivalent to updating toward some action that was not performed and for which it is not known whether it is better than the current output of the actor. An extreme case would be considering an actor that already outputs the optimal action in each state for some deterministic Markov decision processes. For most exploring actions, the temporal-difference error is then negative. If the actor were updated away from such an action, its output would almost certainly no longer be optimal (Van Hasselt, <xref ref-type="bibr" rid="B52">2012</xref>, p. 238). Using only positive delta can hence be seen as a drive for improvement, and the improper behavior is implicitly unlearned by learning a new behavior.</p><p>So, CACLA only updates the actor when actual improvements have been observed. This avoids slow learning when there are plateaus in the value space and the TD errors are small. It was shown empirically that this can indeed result in better policies than when the step size depends on the size of the TD-error (Van Hasselt and Wiering, <xref ref-type="bibr" rid="B53">2007</xref>). Intuitively, it makes sense that the distance to a promising action is more important than the size of the improvement in value. Overall, CACLA has been developed for continuous spaces but it has also been shown to compare favorably to commonly used discrete TD methods such as SARSA or Q-learning (Van Hasselt and Wiering, <xref ref-type="bibr" rid="B54">2009</xref>).</p></sec><sec><label>2.2.2</label><title>Reward function</title><p>It appears that the design of the agent&#x02019;s reward function plays an important role in learning the desired behavior. Learning should not be too complicated, otherwise it might be problematic for the critic to learn it. The first complication we encountered in this task was to make the agent distinguish between quite similar <italic>point</italic> and <italic>push</italic> actions. Therefore we designed a separate reward function for each action.</p><list list-type="simple"><list-item><label>(1)</label><p>During the <italic>push</italic> action the agent receives reward in the form of negative Euclidean distance of the palm from the original position of the target object, so in order to maximize the reward, the agent has to displace the object.</p></list-item><list-item><label>(2)</label><p>For the <italic>touch</italic> action, the reward is again based on the negative Euclidean distance of the palm from the original position of the target object. Additionally, the agent receives a (constant) penalty if it touches another object during the movement (we used a value of &#x02212;0.5). Somewhat counter intuitively, the agent was designed to receive a small penalty (&#x02212;0.3) even when touching the target object, because this method yielded best results. The explanation is that in case of reward after touching the object, the agent would attempt to repeatedly touch the object which would tend to be displaced after each contact, because it has no knowledge about the absolute position of the objects. So when we introduce a small penalty factor for touching the object, the robot&#x02019;s reward increases when it is getting closer and finally it stabilizes cyclically touching the object without displacing it. Another solution could be to use a recurrent network that would provide a memory for agent&#x02019;s past positions, or include the object position into the agent&#x02019;s state representation.</p></list-item><list-item><label>(3)</label><p>The <italic>point</italic> action should be best rewarded when the robot&#x02019;s hand is pointing at the target object, for which the above mentioned negative Euclidean distance turned out to be inappropriate. The reason is that the proximity of the palm to the target object does not guarantee the correct orientation of the fingers. Therefore, we first stored the information about the joint angles corresponding to fingers pointing to the desired position. Then the agent received a reward on the basis of negative Manhattan distance (i.e., in L1 norm) between the current joint configuration and the target joint configuration. This method enforced the correct orientation of the hand with respect to the target.</p></list-item></list></sec></sec><sec id="s1"><label>2.3</label><title>Action naming</title><p>For implementation of the ANN module we chose an echo-state network (ESN; Jaeger, <xref ref-type="bibr" rid="B18">2001</xref>) as an efficient recurrent network that can process sequences. The task of the AN module is to name the executed actions by learning the mapping of [<sc>m1-hid, m2-hid</sc>] to the output <sc>sentence</sc>. <sc>m1-hid</sc> is the hidden-layer activation of the TL module, that allows to name the target property (color or shape). <sc>m2-hid</sc> is the hidden-layer activation of the actor of the AL module, that allows to predict and hence name the type of executed action. <sc>sentence</sc> is the representation at the AN&#x02019;s output layer comprising 3 units for action and 6 units for shape and color. The sentences consist of two words, so for each sentence two units (out of 9) are supposed to be active, one for action and the other for object identification.</p><p>The generic ESN architecture includes also weights from the input layer to the output layer, as well as feedback weights from outputs back to the reservoir, but we did not include these. Hence, the activation dynamics of the units in the reservoir is given by the equation</p><disp-formula id="E6"><label>(6)</label><mml:math id="M13"><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">x</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo class="MathClass-bin">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo class="MathClass-rel">=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">W</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">u</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo class="MathClass-bin">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo class="MathClass-bin">+</mml:mo><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">Wx</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow><mml:mo class="MathClass-punc">,</mml:mo></mml:math></disp-formula><p>and the ESN output is computed as</p><disp-formula id="E7"><label>(7)</label><mml:math id="M14"><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">y</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo class="MathClass-rel">=</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">W</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">x</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow><mml:mo class="MathClass-punc">,</mml:mo></mml:math></disp-formula><p>At both layers, we experimented both with sigmoid (tanh) and linear activation functions <italic>f</italic> and <italic>f&#x02009;<sup>out</sup></italic>; <italic>F</italic> and <italic>F<sup>out</sup></italic> are vectors of these functions, respectively. ESN requires proper initialization of recurrent weight in order to induce echo states as responses to input stimulation (Jaeger, <xref ref-type="bibr" rid="B18">2001</xref>; Lukosevicius and Jaeger, <xref ref-type="bibr" rid="B25">2009</xref>). This is achieved by proper scaling of recurrent weights such that the spectral radius of <bold>W</bold> is smaller than one. (The closer the radius to one from below, the slower is the signal attenuation.)</p><p>For training the ESN output weights, we applied the analytical approach, based on computing the matrix pseudoinverse. ESN is trained to map sequential data to static targets (representing two-word sentences). For each step in sequence, we computed the reservoir activations <bold>x</bold> which were concatenated (as column vectors), resulting in the matrix <bold>X</bold>. These were paired with appropriate targets <bold>d</bold> collected in the matrix <bold>D</bold>. The analytical method yields the output weight matrix in the form</p><disp-formula id="E8"><label>(8)</label><mml:math id="M15"><mml:msup><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">W</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo class="MathClass-rel">=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo class="MathClass-open">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">X</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mo class="MathClass-bin">+</mml:mo></mml:mrow></mml:msup><mml:mstyle class="text"><mml:mtext class="textbf" mathvariant="bold">D</mml:mtext></mml:mstyle></mml:mrow><mml:mo class="MathClass-close">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle class="text"><mml:mtext>T</mml:mtext></mml:mstyle></mml:mrow></mml:msup></mml:math></disp-formula><p>where <bold>X</bold><sup>+</sup> is the pseudoinverse of <bold>X</bold>. The details of data preparation are described in the Results section.</p></sec></sec><sec><label>3</label><title>Results</title><p>In all modules we systematically searched for optimal parameters. We used two thirds of data patterns for training and the remaining one third for testing, and the reported results of averages over 5 runs with a particular set of model parameters and random initial weights. The specificities of each module follow.</p><sec id="s2"><label>3.1</label><title>Target localizer</title><p>For training the TL module we used data from all 3 possible object locations, 3 possible object colors, and 6 target commands, making together 216 different configurations. The MLP was trained using standard back-propagation algorithm. The order of the inputs was shuffled for each run. The training of the displayed network took 600 epochs (i.e., sweeps through the training set) with the learning speed 0.1. Figure <xref ref-type="fig" rid="F6">6</xref> depicts the network accuracy on the test data as a function of the number of hidden neurons. It can be seen that going beyond 50 neurons does not on average lead to further increase of accuracy above 95%. Further improvement depends more on the selected stopping criterion and the learning speed.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Network accuracy on the testing data as a function of the hidden-layer size</bold>.</p></caption><graphic xlink:href="fnbot-06-00001-g006"/></fig><p>Interestingly, these results were achieved with certain modifications of input encoding (without which the accuracy would not get over 75%). We observed that the MLP output was very sensitive to the variability of shape information (high-dimensional inputs), so we multiplied the number of neurons encoding the color and target by various factors (<italic>n</italic>) as shown in Figure <xref ref-type="fig" rid="F7">7</xref>. The best results, i.e., around 95% on testing data, were obtained for <italic>n</italic>&#x02009;=&#x02009;20 (yielding the input dimensionality for TL module 1200&#x02009;+&#x02009;15&#x02009;&#x000d7;&#x02009;20&#x02009;=&#x02009;1500). However, higher values of <italic>n</italic> already lead to performance deterioration. The parameters revealed by our analysis used for final training of this modules are displayed in Table <xref ref-type="table" rid="T1">1</xref>.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Final parameters for training of the TL module</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Architecture</th><th align="left" rowspan="1" colspan="1">Learning rate</th><th align="left" rowspan="1" colspan="1">Activ. function</th><th align="left" rowspan="1" colspan="1">Multipl. factor</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1500-50-4</td><td align="left" rowspan="1" colspan="1">0.1</td><td align="left" rowspan="1" colspan="1">sigmoid</td><td align="left" rowspan="1" colspan="1">20</td></tr></tbody></table></table-wrap><fig id="F7" position="float"><label>Figure 7</label><caption><p><bold>Network accuracy on testing data as a function of multiplication of non-visual units (for explanation see the text)</bold>.</p></caption><graphic xlink:href="fnbot-06-00001-g007"/></fig><p>We trained the TL module for 500 epochs. The root-means-square-error (RMSE) tended to decrease significantly as early as during the first 50 epochs and then continued to decrease slowly. Since the testing error does not grow with time, the network with 50 hidden units did not show evident signs of over-training. The error on single output neurons in comparison with the desired value was 0.037 in average on the training data and 0.136 on the testing data.</p><p>We also briefly tested the robustness of the TL module with respect to noise. We ran some additional simulations with noisy inputs in which there was a 10% chance for each image pixel to be flipped. We observed that the training accuracy remained at 100% and the testing accuracy dropped to approximately 75% (probably due to overfitting).</p></sec><sec><label>3.2</label><title>Action learning</title><p>In training the AL module, we first focused on finding optimal parameters (the learning rate and the hidden-layer size) for the critic and actor networks. Since the learning of AL module is a sequential task (unlike TL), we count its length in terms of episodes (one episode, related to an action type and the target, is a sequence of a number of time steps). The results for the critic trained for 500 episodes, each lasting 75 time steps (or iterations), for various learning rates, are displayed in Figure <xref ref-type="fig" rid="F8">8</xref>A. The best results were achieved with the discount factor &#x003b3;&#x02009;=&#x02009;0 and using the learning rate around 0.01. For smaller rates, the learning turned out to be too slow. Figure <xref ref-type="fig" rid="F8">8</xref>B shows the critic learning as a function of the hidden-layer size. For each size, the training was set to last 250 episodes, using the previously chosen learning rate 0.01. It can be seen that any size above 10 neurons leads to similar results. For the final configuration we used the critic with 20 hidden neurons.</p><fig id="F8" position="float"><label>Figure 8</label><caption><p><bold>Error of the critic for various network parameters</bold>. RMSE as a function of <bold>(A)</bold> learning rate, <bold>(B)</bold> hidden-layer size.</p></caption><graphic xlink:href="fnbot-06-00001-g008"/></fig><p>We proceeded analogically in the case of the actor. Figure <xref ref-type="fig" rid="F9">9</xref>A reveals that the actor, trained for 200 episodes, learned best for a fairly wide range of rates between 0.01 and 0.0001. For subsequent tests, we chose the value of 0.001. Similarly, Figure <xref ref-type="fig" rid="F9">9</xref>B shows that setting the suitable number of hidden neurons is not critical. We chose 40 hidden neurons. The parameters revealed by our analysis used for final training of this modules are displayed in Table <xref ref-type="table" rid="T2">2</xref>.</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p><bold>Final parameters for training of the AL module</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">Architecture</th><th align="left" rowspan="1" colspan="1">Learning rate</th><th align="left" rowspan="1" colspan="1">Activation function</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Actor</td><td align="left" rowspan="1" colspan="1">11-40-4</td><td align="left" rowspan="1" colspan="1">0.001</td><td align="left" rowspan="1" colspan="1">tanh</td></tr><tr><td align="left" rowspan="1" colspan="1">Critic</td><td align="left" rowspan="1" colspan="1">11-20-1</td><td align="left" rowspan="1" colspan="1">0.01</td><td align="left" rowspan="1" colspan="1">tanh</td></tr></tbody></table></table-wrap><fig id="F9" position="float"><label>Figure 9</label><caption><p><bold>Error of the actor for various network parameters</bold>. RMSE as a function of <bold>(A)</bold> learning rate, <bold>(B)</bold> hidden-layer size.</p></caption><graphic xlink:href="fnbot-06-00001-g009"/></fig><p>We trained the AL module using the CACLA algorithm (Algorithm 1) in two phases. In phase 1, we trained the actor only in states resulting in higher rewards from the environment (<italic>r</italic><sub><italic>t</italic>+1</sub>&#x02009;&#x0003e;&#x02009;<italic>r<sub>t</sub></italic>), rather than reward estimates from the critic. The goal was, again, to speed up the training, because at the beginning of training, the critic is a very bad predictor. Interestingly, this modification led to the accelerated learning of the critic as well. After approximately 150 episodes we switched to phase 2, in which the training was based on the original CACLA algorithm during another 300 episodes.</p><p>Each episode represents the execution of one randomly chosen action-target pair. The length of one episode during the training consisted of 75 time steps (iterations, or movement instructions). If the agent touched and moved a wrong object, the episode was reduced to a few extra steps and terminated. In the remaining steps the agent should &#x0201c;figure out&#x0201d; which actions will lead to the increase in the reward, which was penalized by the previous movement. Since the agent has no specific knowledge about the changes in object positions, it was necessary to terminate the episode before its end to avoid learning an improper behavior.</p><p>For a randomly initialized agent at the beginning of training, the proposed actions are large (in terms of angle changes) and random in terms of direction. Since the exploration finds the right directions only randomly, we decided to reduce the &#x0201c;magnitude&#x0201d; of the proposed actions into one third of it. An even better solution could be to initialize the actor&#x02019;s weights to smaller values (as commonly used in connectionist simulations) which would lead to smaller actions in terms of actor outputs.</p><p>In sum, we were able to successfully train the model to produce all three desired actions, having used various speed-up tricks. Figure <xref ref-type="fig" rid="F10">10</xref> depicts the rewards during the execution of the trained actions for all positions of the objects. Typical characteristics for all positions is that actions are at first large in size and gradually they become smaller. The <italic>point</italic> action was executed perfectly. The reward grows during each episode, and the final joint angles perfectly match optimal values. The reward for <italic>touch</italic> action grows at first, until the agent touches the object. Afterward, the reward starts to oscillate which is due to repeated agent&#x02019;s effort to touch it, combined with slight object displacement (caused by reward design). The magnitude of oscillations corresponds to the distance between palm and the target object. The magnitude could have been made smaller by decreasing the penalty upon touch. The execution of the <italic>push</italic> action was very good. As soon as the object was touched, the agent started to displace it. We can observe a minor decrease of the reward after touching which occurs because pushing the object makes it move away from the target position (the agent does not see the target object). If the object could have been moved without becoming an obstacle, the arm could have moved to the object target position where the reward is maximal.</p><fig id="F10" position="float"><label>Figure 10</label><caption><p><bold>Reward obtained during the execution of learned actions, as a function of discrete time steps</bold>.</p></caption><graphic xlink:href="fnbot-06-00001-g010"/></fig><p>Figure <xref ref-type="fig" rid="F11">11</xref> displays the generalization of the &#x0201c;point-left&#x0201d; action from five different starting positions. The starting point of the robot&#x02019;s arm was the same for all training episodes, but after training the agent was able to perform the desired action from any other starting position. The graph on the left displays the changes in the joint angles. The graph on the right displays the values of the reward during the action. Interestingly, the agent chooses a correct trajectory even if the arm moves to another position during the action execution. The agent was observed to manifest similar behavior for other actions as well.</p><fig id="F11" position="float"><label>Figure 11</label><caption><p><bold>The joint angles and reward values during the execution of the &#x0201c;point-left&#x0201d; action from different starting positions, as a function of discrete time steps (iterations)</bold>.</p></caption><graphic xlink:href="fnbot-06-00001-g011"/></fig><p>This generalization property also ensures the robustness of the robot&#x02019;s behavior with respect to random noise. Indeed, if we initially induced a small amount of white noise into motor commands, this only had a negligible effect on correct performance, because noise only caused the arm shift in the (continuous) state space. Of course, large noise (reaching the magnitude of AL outputs) affected the arm dynamics and the compensation for noise was insufficient.</p><p>Next, in order to get insight into actor&#x02019;s internal representations, we analyzed the activations on the actor&#x02019;s hidden layer as action characteristics, and projected these high-dimensional data onto a 2D grid using a standard SOM with toroid topology (i.e., making the pairs of parallel borders of the map connected). The resulting SOM is displayed in Figure <xref ref-type="fig" rid="F12">12</xref>. It is the same map shown twice, with different labels shown (to increase transparency). The clearly identifiable spots of brighter color represent the concrete actions (left figure) and positions (right figure). The size of the spot reflects the number of respective activations. In each action spot there are some smaller and bigger marks, of which the largest ones represent the target position in which the arm of the robot remained already from the middle of an episode assigned for the testing action execution. The smaller marks indicate the trajectory of the arm during the execution of the action before reaching the target position. It can be seen, that in both displays of the SOM, each category (action type or target position) forms a compact cluster, reflecting the internal organization of actor&#x02019;s hidden layer.</p><fig id="F12" position="float"><label>Figure 12</label><caption><p><bold>The visualization of the activations on the actor&#x02019;s hidden layer using the self-organized map with toroid topology</bold>. The two plots correspond to the same SOM, with different labels shown.</p></caption><graphic xlink:href="fnbot-06-00001-g012"/></fig><p>We tested the behavior of the trained model in case of changing the action type and/or target position, to see whether the agent can react appropriately. Recall that the AL module has learned a kind of parametrized mapping toward the goal where the goal is kept active (and constant) at the AL input. Hence, we went systematically through all 3 actions and target positions (for 100 steps) and at step 20 we randomly changed action type or target position to another value. We measured the instantaneous reward (i.e., at the current step) of the model as an indicator of its performance. The results are shown in Figure <xref ref-type="fig" rid="F13">13</xref>. It can be seen that the reward always dropped at the moment of change and then started to increase toward the end of new action/target execution. Since the agent does not use any online visual feedback, it could bump into an obstacle, for instance when the change was made from the left to the right object (knocking down the middle object).</p><fig id="F13" position="float"><label>Figure 13</label><caption><p><bold>The behavior of the trained AL module (in terms of reward) in response to the sudden change of the action type or the target position</bold>. The growing value of the instantaneous reward, immediately after the change, serves as an indicator that the agent successfully responses to the new goal.</p></caption><graphic xlink:href="fnbot-06-00001-g013"/></fig><p>Last but not least, since the above learning scenario exploited various tricks to speed up and facilitate learning, we checked whether CACLA algorithm will succeed also without them, that is, in case of simultaneous learning of both AL components. Therefore, we ran a few simulations during which the actor only relied on critic&#x02019;s value function from the very beginning. We observed that the model had learned the task and could generalize equally well, as with the sped-up scenario. The only disadvantage was the computing time: it took approximately four times longer to train (&#x0223c;2000 episodes). Actions and the target position were chosen randomly for each episode. Figure <xref ref-type="fig" rid="F14">14</xref> shows an example of the training procedure where each action occurred between 160 and 180 times (i.e., each point stands for one episode). The graph reflects the stochastic nature of learning, but the gradual increase of the cumulative rewards is discernible in all cases toward the end of training.</p><fig id="F14" position="float"><label>Figure 14</label><caption><p><bold>The development of rewards decomposed into action/target combinations</bold>. <italic>x</italic>-axis denotes episodes, <italic>y</italic>-axis denotes the cumulative rewards (during an episode).</p></caption><graphic xlink:href="fnbot-06-00001-g014"/></fig></sec><sec><label>3.3</label><title>Action naming</title><p>As the AN module, the ESN was used with 50 units in the reservoir, with the spectral radius 0.9 and connectivity 20%. The matrix of input weights was initialized randomly from the interval [&#x02212;1, 1]. ESN was designed to contain linear units at both the reservoir and the output layer. ESN behavior was observed to be quite robust with respect to model parameters. For training ESN we used a batch algorithm described in Section <xref ref-type="sec" rid="s1">2.3</xref>. Before training we prepared data (216 activations in total for all combinations of inputs) from the other two modules, i.e., 40-dimensional hidden-layer activations from the TL module and corresponding sequences of 50-dimensional hidden-layer activations of the actor network from the AL module of length 100 steps each. The parameters of the AN module are summarized in Table <xref ref-type="table" rid="T3">3</xref>.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p><bold>Final parameters for training of the AN module</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Architecture</th><th align="left" rowspan="1" colspan="1">Activ.&#x02009;function</th><th align="left" rowspan="1" colspan="1">Radius</th><th align="left" rowspan="1" colspan="1">Reservoir</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">90-50-9</td><td align="left" rowspan="1" colspan="1">linear</td><td align="left" rowspan="1" colspan="1">0.9</td><td align="left" rowspan="1" colspan="1">20%</td></tr></tbody></table></table-wrap><p>Each of the 216 hidden-layer activations of the TL module corresponds to a concrete target position that can be subject to three actions (yielding 3&#x02009;&#x000d7;&#x02009;100 hidden-layer activations). The input data for AN module is hence composed of 216 blocks of length 300. The blocks are serially concatenated and each hidden-layer activation vector of AN module is associated with one block (of length 300). As a result, each block contains 300 two-component activation vectors of length 90, whose first 40 components are dynamic (coming from the AL module, as a function of the just-executed action) and the remaining 50 components are static (coming from the TL module). For each input vector of the training set we stored the target output consisting of 2 active units.</p><p>We experimented with various ways of computing the matrix of output weights (see equation 8). The mapping task here involves the linking of dynamically changing reservoir activations with static targets. The choice could be made whether to concatenate all reservoir activation vectors during each episode (in columns of matrix <bold>X</bold>) or only a subset of them. The best results were obtained if we collected all activation vectors. In this way we achieved the accuracy on both the training and testing sets over 99%. During testing, AN module was always initialized at a random position. The outputs of the trained AN module tested on sphere-related actions are shown in Figure <xref ref-type="fig" rid="F15">15</xref>. The first row shows the graph of the output on the training set. The second row shows the action naming initiated at random positions. The graphs show that the linguistic output converges very rapidly (during a few steps) to desired activations. Actually, this should not be surprising, given that the AN module receives not only the information about the current action in terms of motor command, but also the high-level information about action type and the target position. As a result of AL learning, different actions operate in different subspaces of the actor state space, hence allowing the differentiation. In other words, the performing agent has access to its internal representations and hence knows from the very beginning what it is doing and is able to unambiguously name the action from the very beginning.</p><fig id="F15" position="float"><label>Figure 15</label><caption><p><bold>The linguistic output of AN module, during actions targeted at a sphere, as a function of discrete time steps (iterations)</bold>. The actions start from (<bold>A</bold>) positions used during training, or (<bold>B</bold>) from random positions (<bold>B</bold>).</p></caption><graphic xlink:href="fnbot-06-00001-g015"/></fig><p>We can observe that for randomly initialized actions the settling process takes a bit longer, compared to the training sequences. For <italic>touch</italic> and <italic>push</italic> actions we observe at around the 40th step the contact with the target object. At this point the arm motion changes, which is reflected by slight oscillations not only at units coding the actions but also units coding the target object.</p><p>The situation would be different, though, for the observed actions. If we wanted to model the agent attempting to name the observed action, it could only have access to actor&#x02019;s output of AL module, and only in the form of visual rather than proprioceptive information. In this case, the ability to name the action could be limited to the time step when the trajectory could already be differentiated visually. We could predict that the naming of the correct target position would precede the naming of the action type.</p></sec></sec><sec><label>4</label><title>Discussion</title><p>We presented a connectionist model whose goal was to make the simulated iCub robot learn object-directed actions and to link this ability with language, hence grounding the linguistic meanings. The given task was learned with high accuracy. The model can be decomposed into three modules whose properties we will now discuss. The target localizer (TL) converts visual information to spatial information. It rests on a biologically relevant assumption that the shape and color of objects are processed separately in the visual system, so they are provided as parallel inputs. In our model the binding problem is avoided because during training, the MLP can learn to link the object shapes with corresponding colors (by associating the input components). The simplifying feature of our training data is the assumption that the target is always unambiguous (no two objects in the scene can have the same color or shape) so the target can be referred to by its color or its shape. On the other hand, any object of any color can be at any position in the visual field, which makes the task complexity somewhat higher than that used in Sugita and Tani (<xref ref-type="bibr" rid="B41">2005</xref>).</p><p>The feature of the target is assumed to be a high-level top-down information that serves as an additional (symbolic) input to the robot. Regarding the data dimensionality, from the computational perspective, it is an interesting observation, that we had to significantly extend the dimensionality of symbolic inputs to counterbalance the influence of the high-dimensional shape (contour) data. It is a question how biological systems, if understood in computational terms, solve the problem of merging data from multiple sources while maintaining the balance between them. The output of the TL module can be seen as the symbolic information (given by one-hot codes). The natural extension would be to use real-valued outputs that would allow the target to be placed anywhere in the robot&#x02019;s visual field.</p><p>The action learning (AL) module is the main distinguishing part of our model. It takes two high-level (symbolic) inputs &#x02013; action type and target location &#x02013; that serve as fixed parameters of the motor behavior to be learned. The learning is based on RL which differs from supervised scenarios (e.g., RNNPB), which assume that target sensorimotor trajectories are available for the training. On the contrary, RL assumes a more realistic feedback available to the system. In our experiments, the robot was observed to learn required behaviors and to generalize well in the given task. Although during the training of this module the robot does not actually see (it has no direct visual input), it still is assumed to receive some visual information (at each step), reflected in the reward which is based on the distance between robot&#x02019;s right hand and the target. What we reliably tested with our AL module, is that sudden change of the AL input parameters immediately led to a required change of the trajectory, either to a new target position, or resulting in a new target action.</p><p>It is clear that biological systems also evaluate the distance to the target, but they probably also use more detailed visual information (see, e.g., Oztop et al., <xref ref-type="bibr" rid="B31">2004</xref>, for a model of grasping). The typical feature of these approaches is that they involve internal models for motor control (e.g., Kawato, <xref ref-type="bibr" rid="B21">1999</xref>; Castellini et al., <xref ref-type="bibr" rid="B9">2007</xref>). Such a step would be a natural extension of our model, giving it an ability to update the trajectory during execution, e.g., if the target moves (in our model the target is assumed to be static). This feedback could also be used for overcoming our difficulties in distinguishing between <italic>touch</italic> and <italic>push</italic> actions that would require even more fine-tuned motor actions to prevent the arm from oscillating when touching the target. Another improvement could include faster, &#x0201c;goal-oriented&#x0201d; learning rather than more-or-less blind search (at the initial phase) over the state space.</p><p>The action naming (AN) module learns to generate linguistic description of the performed action, taking information from the other two modules, about the action type (from AL) and the target (from TL). Both pieces of (input) information are pre-processed and become distributed (at the hidden layers), the former being sequential and the latter being static. The action naming task can be considered simple, since the ESN only needs to extract the (conceptual) information from AL and TL modules, and to map it onto the symbolic output (words). It is true that in our model the linguistic information (at AN output) is not sequential, as for instance in the case of twin RNNPBs (Tani et al., <xref ref-type="bibr" rid="B47">2004</xref>). However, it could be made sequential with an appropriate change of the AN module architecture and the training scenario. In the current model, we did not consider this feature important.</p><p>The nature of conversion between subsymbolic sensorimotor information in both directions is hence qualitatively different, unlike the case of RNNPB, where the two modules are more-or-less symmetric both in terms of architecture and training (Sugita and Tani, <xref ref-type="bibr" rid="B41">2005</xref>). In the behavior-to-language direction, we deal with mapping sequences to spatial patterns. In the language-to-behavior direction, the static symbolic information drives the appropriate sequence generation. Actually, our model mingles the linguistic information (words describing actions) with conceptual information (intended goals, given by action type and the target identity, presented as top-down inputs for AL and TL modules, respectively), both of which are coded localistically. Of course, in realistic systems these two types of information are different, without a one-to-one mapping between the two. In our model, this correspondence is hence simplified. We trained the three modules in our model separately in order to have a better control over the performance. It might be interesting to consider a more interactive scenario in which all modules would develop simultaneously, in which case we would be facing a more difficult task (can the robot learn actions that are not yet well parametrized/coded?).</p><p>One important feature of connectionist modeling is related to the training scenario. In our model, the learning of AN module following the AL module (which renders the former to be <italic>a posteriori</italic> categorizer of learned motor actions) differs from the synchronous action&#x02013;language learning scenario used in RNNPB. The underlying motivation in RNNPB was to let the model converge to common &#x0201c;triggering&#x0201d; codes (via PB node learning) used for both action execution and action naming. This makes sense but on the other hand, we think that in this particular case this synchronous training is not necessary, nor justified. It is known from developmental psychology and neurophysiology (see, e.g., Jeannerod, <xref ref-type="bibr" rid="B19">1997</xref>) that object recognition ability and object-related actions such as reaching and grasping are prelinguistic. For instance, reaching develops at around month three, early grasping and manipulation soon after, the hand is adjusted to the object&#x02019;s size at around month nine and they are finally integrated in a single smooth action at around month 13 of age (Cangelosi et al., <xref ref-type="bibr" rid="B6">2010</xref>). So, it is not unreasonable to assume that a child first acquires certain motor behaviors (at least to a certain level of proficiency) before it learns to name them, hence grounding the meaning of words. At the same time, we acknowledge that language can and does affect cognition as such also early in life, for instance in object categorization (see evidence in Waxman, <xref ref-type="bibr" rid="B56">2003</xref>), and that the influence of language on cognition remains active throughout life (e.g., in spatial cognition; Landau et al., <xref ref-type="bibr" rid="B24">2011</xref>). On the other hand, the TL and AL modules could be trained simultaneously, which would better correspond with infants&#x02019; scenario. The development of reaching and touching objects in infants overlaps significantly with learning the concepts and recognizing physical objects.</p><p>One interesting option that emerged in developmental robotics, concerns the type of feedback used for learning motor behavior. We argued that RL uses a biologically more plausible feedback (in terms of rewards), rather than required motor trajectories such as those used in RNNPB and some other models. Technically, one can use these trajectories in human&#x02013;robot interaction, for example by manually controlling robot&#x02019;s arms and storing the proprioceptive data to be used for training the robot later (as demonstrated with iCub in some works). It seems that this scenario could be sometimes adopted in AI to facilitate learning in robots. It depends on the goal of the effort (working technical solution or a model of child development). Actually, this practice is also usable to a limited extent in humans (for instance in teaching dance postures), but humans learn primarily by observation and self practicing. On the other hand, RL can be rather slow in convergence and becomes difficult in high-dimensional state spaces, so it is a remaining challenge to improve the existing algorithms or design new ones.</p><p>One drawback of our model, often present in similar works, is that in order to name the action, the agent has to execute it as well. On the other hand, in a real life one may need to name the action that is observed. To enhance the model with the capability to produce an action name when just observing it, a mirroring mechanism could be introduced. This could be based on the motor resonance (Van der Wel et al., <xref ref-type="bibr" rid="B51">in press</xref>), a partial activation of the neural circuitry for motor control during non-motoric tasks, which can be related to the functionality of the mirror-neuron system. From the computational point of view, the mirror-neurons were considered using various approaches, for instance by Tani et al. (<xref ref-type="bibr" rid="B47">2004</xref>), Chersi et al. (<xref ref-type="bibr" rid="B10">2010</xref>), or Wermter and Elshaw (<xref ref-type="bibr" rid="B57">2003</xref>). Our perspective, influenced by the common coding theory (Prinz, <xref ref-type="bibr" rid="B32">1997</xref>; Hommel et al., <xref ref-type="bibr" rid="B17">2001</xref>), is mostly similar to the model of Wermter and Elshaw (<xref ref-type="bibr" rid="B57">2003</xref>) described in the Introduction. To implement common coding, the activations on the hidden layers of the constituting modules could be, together with other perceptual information, projected onto a higher-level association area (using some kind of Hebbian learning), which could then influence the behavior of the modules by projecting back to the hidden layers. As we have already demonstrated, activations in the hidden layer of the AL module projected on a 2-dimensional SOM tend to form clusters characterizing various actions the agent is able to produce.</p><p>Another interesting issue worth discussing is the nature of reward. In the context of the RL approach, the reward is assumed to be the property of the environment, and the agent attempts to maximize it. Humans are also sensitive to rewards, but probably they can differ in what counts for them as a reward. The reward seems to be task-dependent and also agent-dependent. Related to our work, we assumed that the reward contains two generic components: the vision-related reward (the distance) and the haptic reward. This specification of reward can be seen as the designer&#x02019;s intervention without which the robot could not learn. It could be interesting to look at the inverse RL whose goal is, based on agent&#x02019;s behavior, to recover the underlying reward function that is maximized (Ng and Russell, <xref ref-type="bibr" rid="B30">2000</xref>).</p><p>To conclude, we believe that cognitive robotics approach using neural networks is a promising path toward scalable cognitive architectures, embedded in simulated or real robots. The expected challenge could be to use ecologically plausible training algorithms and representations, with a goal to make the learning robots as autonomous as possible, hence minimizing the designer&#x02019;s intervention.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>The authors thank Martin Tak&#x000e1;&#x0010d; and two anonymous reviewers for helpful comments. The research was supported by Slovak Grant Agency for Science, no. 1/0439/11.</p></ack><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asada</surname><given-names>M.</given-names></name><name><surname>Hosoda</surname><given-names>K.</given-names></name><name><surname>Kuniyoshi</surname><given-names>Y.</given-names></name><name><surname>Ishiguro</surname><given-names>H.</given-names></name><name><surname>Inui</surname><given-names>T.</given-names></name><name><surname>Yoshikawa</surname><given-names>Y.</given-names></name><name><surname>Ogino</surname><given-names>M.</given-names></name><name><surname>Yoshida</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>). <article-title>Cognitive developmental robotics: a survey</article-title>. <source>IEEE Trans. Auton. Ment. Dev.</source>
<volume>1</volume>, <fpage>12</fpage>&#x02013;<lpage>34</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2009.2021702</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>L.</given-names></name></person-group> (<year>1999</year>). <article-title>Perceptual symbol systems</article-title>. <source>Behav. Brain Sci.</source>
<volume>22</volume>, <fpage>577</fpage>&#x02013;<lpage>660</lpage><pub-id pub-id-type="doi">10.1017/S0140525X99532147</pub-id><pub-id pub-id-type="pmid">11301525</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>L.</given-names></name></person-group> (<year>2008</year>). <article-title>Grounded cognition</article-title>. <source>Annu. Rev. Psychol.</source>
<volume>59</volume>, <fpage>617</fpage>&#x02013;<lpage>645</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.59.103006.093639</pub-id><pub-id pub-id-type="pmid">17705682</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>L.</given-names></name><name><surname>Wiemer-Hastings</surname><given-names>K.</given-names></name></person-group> (<year>2005</year>). <article-title>&#x0201c;Situating abstract concepts,&#x0201d;</article-title> in <source>Grounding Cognition: The Role of Perception and Action in Memory, Language, and Thinking</source>, eds <person-group person-group-type="editor"><name><surname>Pecher</surname><given-names>D.</given-names></name><name><surname>Zwaan</surname><given-names>R.</given-names></name></person-group> (<publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>), <fpage>129</fpage>&#x02013;<lpage>163</lpage></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Barto</surname><given-names>A.</given-names></name><name><surname>Jordan</surname><given-names>M.</given-names></name></person-group> (<year>1987</year>). <article-title>&#x0201c;Gradient following without back-propagation in layered networks,&#x0201d;</article-title> in <conf-name>Proceedings of the IEEE 1st International Conference on Neural Networks</conf-name>, <conf-loc>San Diego</conf-loc>, <fpage>629</fpage>&#x02013;<lpage>636</lpage></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cangelosi</surname><given-names>A.</given-names></name><name><surname>Metta</surname><given-names>G.</given-names></name><name><surname>Sagerer</surname><given-names>G.</given-names></name><name><surname>Nolfi</surname><given-names>S.</given-names></name><name><surname>Nehaniv</surname><given-names>C.</given-names></name><name><surname>Tani</surname><given-names>J.</given-names></name><name><surname>Belpaeme</surname><given-names>T.</given-names></name><name><surname>Giulio</surname><given-names>S.</given-names></name><name><surname>Fadiga</surname><given-names>L.</given-names></name><name><surname>Wrede</surname><given-names>B.</given-names></name><name><surname>Tuci</surname><given-names>E.</given-names></name><name><surname>Dautenhahn</surname><given-names>K.</given-names></name><name><surname>Saunders</surname><given-names>J.</given-names></name><name><surname>Zeschel</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Integration of action and language knowledge: a roadmap for developmental robotics</article-title>. <source>IEEE Trans. Auton. Ment. Dev.</source>
<volume>2</volume>, <fpage>167</fpage>&#x02013;<lpage>195</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2010.2053034</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cangelosi</surname><given-names>A.</given-names></name><name><surname>Riga</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>). <article-title>An embodied model for sensorimotor grounding and grounding transfer: experiments with epigenetic robots</article-title>. <source>Cogn. Sci.</source>
<volume>30</volume>, <fpage>673</fpage>&#x02013;<lpage>689</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog0000_72</pub-id><pub-id pub-id-type="pmid">21702830</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cangelosi</surname><given-names>A.</given-names></name><name><surname>Tikhanoff</surname><given-names>V.</given-names></name><name><surname>Fontanari</surname><given-names>J.</given-names></name><name><surname>Hourdakis</surname><given-names>E.</given-names></name></person-group> (<year>2007</year>). <article-title>Integrating language and cognition: a cognitive robotics approach</article-title>. <source>IEEE Comput. Intell. Mag.</source>
<volume>2</volume>, <fpage>65</fpage>&#x02013;<lpage>70</lpage><pub-id pub-id-type="doi">10.1109/MCI.2007.385366</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellini</surname><given-names>C.</given-names></name><name><surname>Orabona</surname><given-names>F.</given-names></name><name><surname>Metta</surname><given-names>G.</given-names></name><name><surname>Sandini</surname><given-names>G.</given-names></name></person-group> (<year>2007</year>). <article-title>Internal models of reaching and grasping</article-title>. <source>Adv. Robot.</source>
<volume>21</volume>, <fpage>1545</fpage>&#x02013;<lpage>1564</lpage></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chersi</surname><given-names>F.</given-names></name><name><surname>Thill</surname><given-names>S.</given-names></name><name><surname>Ziemke</surname><given-names>T.</given-names></name><name><surname>Borghi</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Sentence processing: linking language to motor chains</article-title>. <source>Front. Neurorobot.</source>
<volume>4</volume>:<fpage>4</fpage><pub-id pub-id-type="doi">10.3389/fnbot.2010.00004</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>De Vega</surname><given-names>M.</given-names></name><name><surname>Glenberg</surname><given-names>A.</given-names></name><name><surname>Graesser</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>&#x0201c;Reflecting on the debate,&#x0201d;</article-title> in <source>Symbols and Embodiment: Debates on Meaning and Cognition</source>, eds <person-group person-group-type="editor"><name><surname>De Vega</surname><given-names>M.</given-names></name><name><surname>Glenberg</surname><given-names>A.</given-names></name><name><surname>Graesser</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>USA</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>), <fpage>397</fpage>&#x02013;<lpage>440</lpage></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dominey</surname><given-names>P. F.</given-names></name><name><surname>Boucher</surname><given-names>J. D.</given-names></name></person-group> (<year>2005</year>). <article-title>Developmental stages of perception and language acquisition in a perceptually grounded robot</article-title>. <source>Cogn. Syst. Res.</source>
<volume>6</volume>, <fpage>243</fpage>&#x02013;<lpage>259</lpage><pub-id pub-id-type="doi">10.1016/j.cogsys.2004.11.005</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glenberg</surname><given-names>A.</given-names></name><name><surname>Kaschak</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Grounding language in action</article-title>. <source>Psychon. Bull. Rev.</source>
<volume>9</volume>, <fpage>558</fpage><pub-id pub-id-type="doi">10.3758/BF03196313</pub-id><pub-id pub-id-type="pmid">12412897</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glenberg</surname><given-names>A.</given-names></name><name><surname>Sato</surname><given-names>M.</given-names></name><name><surname>Cattaneo</surname><given-names>L.</given-names></name><name><surname>Riggio</surname><given-names>L.</given-names></name><name><surname>Palumbo</surname><given-names>D.</given-names></name><name><surname>Buccino</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Processing abstract language modulates motor system activity</article-title>. <source>Q. J. Exp. Psychol.</source>
<volume>61</volume>, <fpage>905</fpage>&#x02013;<lpage>919</lpage><pub-id pub-id-type="doi">10.1080/17470210701625550</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harnad</surname><given-names>S.</given-names></name></person-group> (<year>1990</year>). <article-title>The symbol grounding problem</article-title>. <source>Physica D</source>
<volume>42</volume>, <fpage>335</fpage>&#x02013;<lpage>346</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(90)90087-6</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauk</surname><given-names>O.</given-names></name><name><surname>Johnsrude</surname><given-names>I.</given-names></name><name><surname>Pulverm&#x000fc;ller</surname><given-names>F.</given-names></name></person-group> (<year>2004</year>). <article-title>Somatotopic representation of action words in human motor and premotor cortex</article-title>. <source>Neuron</source>
<volume>41</volume>, <fpage>301</fpage>&#x02013;<lpage>307</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00838-9</pub-id><pub-id pub-id-type="pmid">14741110</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hommel</surname><given-names>B.</given-names></name><name><surname>M&#x000fc;sseler</surname><given-names>J.</given-names></name><name><surname>Aschersleben</surname><given-names>G.</given-names></name><name><surname>Prinz</surname><given-names>W.</given-names></name></person-group> (<year>2001</year>). <article-title>The theory of event coding (TEC): a framework for perception and action planning</article-title>. <source>Behav. Brain Sci.</source>
<volume>24</volume>, <fpage>849</fpage>&#x02013;<lpage>878</lpage><pub-id pub-id-type="doi">10.1017/S0140525X01000103</pub-id><pub-id pub-id-type="pmid">12239891</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>H.</given-names></name></person-group> (<year>2001</year>). <source>Short-term memory in echo state networks</source>. Technical Report GMD Report 152. <publisher-name>German National Research Center for Information Technology</publisher-name></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeannerod</surname><given-names>M.</given-names></name></person-group> (<year>1997</year>). <source>The Cognitive Neuroscience of Action</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Blackwell</publisher-name></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeannerod</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Neural simulation of action: a unifying mechanism for motor cognition</article-title>. <source>Neuroimage</source>
<volume>14</volume>, <fpage>S103</fpage>&#x02013;<lpage>S109</lpage><pub-id pub-id-type="doi">10.1006/nimg.2001.0832</pub-id><pub-id pub-id-type="pmid">11373140</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawato</surname><given-names>M.</given-names></name></person-group> (<year>1999</year>). <article-title>Internal models for motor control and trajectory planning</article-title>. <source>Curr. Opin. Neurobiol.</source>
<volume>9</volume>, <fpage>718</fpage>&#x02013;<lpage>727</lpage><pub-id pub-id-type="doi">10.1016/S0959-4388(99)00028-8</pub-id><pub-id pub-id-type="pmid">10607637</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kohonen</surname><given-names>T.</given-names></name></person-group> (<year>1997</year>). <source>Self-Organizing Maps</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lallee</surname><given-names>S.</given-names></name><name><surname>Madden</surname><given-names>C.</given-names></name><name><surname>Hoen</surname><given-names>M.</given-names></name><name><surname>Dominey</surname><given-names>P. F.</given-names></name></person-group> (<year>2010</year>). <article-title>Linking language with embodied and teleological representations of action for humanoid cognition</article-title>. <source>Front. Neurorobot.</source>
<volume>4</volume>:<fpage>8</fpage><pub-id pub-id-type="doi">10.3389/fnbot.2010.00008</pub-id><pub-id pub-id-type="pmid">20577629</pub-id></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>B.</given-names></name><name><surname>Dessalegn</surname><given-names>B.</given-names></name><name><surname>Goldberg</surname><given-names>A. M.</given-names></name></person-group> (<year>2011</year>). <article-title>&#x0201c;Language and space: momentary interactions,&#x0201d;</article-title> in <source>Language, Cognition and Space: The State of the Art and New Directions. Advances in Cognitive Linguistics Series</source>, eds <person-group person-group-type="editor"><name><surname>Chilton</surname><given-names>P.</given-names></name><name><surname>Evans</surname><given-names>V.</given-names></name></person-group> (<publisher-loc>London</publisher-loc>: <publisher-name>Equinox Publishing</publisher-name>), <fpage>51</fpage>&#x02013;<lpage>78</lpage></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukosevicius</surname><given-names>M.</given-names></name><name><surname>Jaeger</surname><given-names>H.</given-names></name></person-group> (<year>2009</year>). <article-title>Reservoir computing approaches to recurrent neural network training</article-title>. <source>Comput. Sci. Rev.</source>
<volume>3</volume>, <fpage>127</fpage>&#x02013;<lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.cosrev.2009.03.005</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Macura</surname><given-names>Z.</given-names></name><name><surname>Cangelosi</surname><given-names>A.</given-names></name><name><surname>Ellis</surname><given-names>R.</given-names></name><name><surname>Bugmann</surname><given-names>D.</given-names></name><name><surname>Fischer</surname><given-names>M. H.</given-names></name><name><surname>Myachykov</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>). <article-title>&#x0201c;A cognitive robotic model of grasping,&#x0201d;</article-title> in <conf-name>Proceedings of the 9th International Conference on Epigenetic Robotics</conf-name>, <conf-loc>Venice</conf-loc></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marocco</surname><given-names>D.</given-names></name><name><surname>Cangelosi</surname><given-names>A.</given-names></name><name><surname>Fischer</surname><given-names>K.</given-names></name><name><surname>Belpaeme</surname><given-names>T.</given-names></name></person-group> (<year>2010</year>). <article-title>Grounding action words in the sensorimotor interaction with the world: experiments with a simulated iCub humanoid robot</article-title>. <source>Front. Neurorobot.</source>
<volume>4</volume>:<fpage>7</fpage><pub-id pub-id-type="doi">10.3389/fnbot.2010.00007</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Metta</surname><given-names>G.</given-names></name><name><surname>Sandini</surname><given-names>G.</given-names></name><name><surname>Vernon</surname><given-names>D.</given-names></name><name><surname>Natale</surname><given-names>L.</given-names></name><name><surname>Nori</surname><given-names>F.</given-names></name></person-group> (<year>2008</year>). <article-title>&#x0201c;The iCub humanoid robot: an open platform for research in embodied cognition,&#x0201d;</article-title> in <conf-name>Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems</conf-name>, <conf-loc>Washington DC</conf-loc>, <fpage>50</fpage>&#x02013;<lpage>56</lpage></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morse</surname><given-names>A.</given-names></name><name><surname>de Greeff</surname><given-names>J.</given-names></name><name><surname>Belpeame</surname><given-names>T.</given-names></name><name><surname>Cangelosi</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Epigenetic robotics architecture (ERA)</article-title>. <source>IEEE Trans. Auton. Ment. Dev.</source>
<volume>2</volume>, <fpage>325</fpage>&#x02013;<lpage>339</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2010.2087020</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>A. Y.</given-names></name><name><surname>Russell</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>). <article-title>&#x0201c;Algorithms for inverse reinforcement learning,&#x0201d;</article-title> in <conf-name>Proceedings of the 17th International Conference on Machine Learning (ICML)</conf-name> (<conf-loc>Standord, CA</conf-loc>: <conf-sponsor>Stanford University</conf-sponsor>), <fpage>663</fpage>&#x02013;<lpage>670</lpage></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oztop</surname><given-names>E.</given-names></name><name><surname>Bradley</surname><given-names>N.</given-names></name><name><surname>Arbib</surname><given-names>M.</given-names></name></person-group> (<year>2004</year>). <article-title>Infant grasp learning: a computational model</article-title>. <source>Exp. Brain Res.</source>
<volume>158</volume>, <fpage>480</fpage>&#x02013;<lpage>503</lpage><pub-id pub-id-type="doi">10.1007/s00221-004-1914-1</pub-id><pub-id pub-id-type="pmid">15221160</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname><given-names>W.</given-names></name></person-group> (<year>1997</year>). <article-title>Perception and action planning</article-title>. <source>Eur. J. Cogn. Psychol.</source>
<volume>9</volume>, <fpage>129</fpage>&#x02013;<lpage>154</lpage><pub-id pub-id-type="doi">10.1080/713752551</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulverm&#x000fc;ller</surname><given-names>F.</given-names></name></person-group> (<year>2005</year>). <article-title>Brain mechanisms linking language and action</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>6</volume>, <fpage>576</fpage>&#x02013;<lpage>582</lpage><pub-id pub-id-type="doi">10.1038/nrn1706</pub-id><pub-id pub-id-type="pmid">15959465</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulverm&#x000fc;ller</surname><given-names>F.</given-names></name><name><surname>H&#x000e4;rle</surname><given-names>M.</given-names></name><name><surname>Hummel</surname><given-names>F.</given-names></name></person-group> (<year>2001</year>). <article-title>Walking or talking? Behavioral and neurophysiological correlates of action verb processing</article-title>. <source>Brain Lang.</source>
<volume>78</volume>, <fpage>143</fpage>&#x02013;<lpage>168</lpage><pub-id pub-id-type="doi">10.1006/brln.2000.2390</pub-id><pub-id pub-id-type="pmid">11500067</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>D.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name><name><surname>Williams</surname><given-names>R.</given-names></name></person-group> (<year>1986</year>). <source>Learning Internal Representations by Error Propagation</source>, Vol. <volume>1</volume>
<publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <fpage>318</fpage>&#x02013;<lpage>362</lpage></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>J.</given-names></name></person-group> (<year>1969</year>). <article-title>Reactions toward the source of stimulation</article-title>. <source>J. Exp. Psychol.</source>
<volume>81</volume>, <fpage>174</fpage>&#x02013;<lpage>176</lpage><pub-id pub-id-type="doi">10.1037/h0027448</pub-id><pub-id pub-id-type="pmid">5812172</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>L.</given-names></name><name><surname>Samuelson</surname><given-names>L.</given-names></name></person-group> (<year>2010</year>). <article-title>&#x0201c;Objects in space and mind: from reaching to words,&#x0201d;</article-title> in <source>The Spatial Foundations of Language and Cognition</source>, eds <person-group person-group-type="editor"><name><surname>Mix</surname><given-names>K.</given-names></name><name><surname>Smith</surname><given-names>L.</given-names></name><name><surname>Gasser</surname><given-names>M.</given-names></name></person-group> (<publisher-loc>USA</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>).</mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steels</surname><given-names>L.</given-names></name></person-group> (<year>2003</year>). <article-title>Evolving grounded communication for robots</article-title>. <source>Trends Cogn. Sci. (Regul. Ed.)</source>
<volume>7</volume>, <fpage>308</fpage>&#x02013;<lpage>312</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00129-3</pub-id><pub-id pub-id-type="pmid">12860189</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Steels</surname><given-names>L.</given-names></name></person-group> (<year>2008</year>). <article-title>&#x0201c;The symbol grounding problem has been solved, so what&#x02019;s next,&#x0201d;</article-title> in <source>Symbols and Embodiment: Debates on Meaning and Cognition</source>, eds <person-group person-group-type="editor"><name><surname>de Vega</surname><given-names>M.</given-names></name><name><surname>Glenberg</surname><given-names>A.</given-names></name><name><surname>Graesser</surname><given-names>A.</given-names></name></person-group> (<publisher-loc>USA</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>), <fpage>223</fpage>&#x02013;<lpage>244</lpage></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steels</surname><given-names>L.</given-names></name><name><surname>Belpaeme</surname><given-names>T.</given-names></name></person-group> (<year>2005</year>). <article-title>Coordinating perceptually grounded categories through language: a case study for colour</article-title>. <source>Behav. Brain Sci.</source>
<volume>28</volume>, <fpage>469</fpage>&#x02013;<lpage>489</lpage><pub-id pub-id-type="doi">10.1017/S0140525X05510082</pub-id><pub-id pub-id-type="pmid">16209771</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugita</surname><given-names>Y.</given-names></name><name><surname>Tani</surname><given-names>J.</given-names></name></person-group> (<year>2005</year>). <article-title>Learning semantic combinatoriality from the interaction between linguistic and behavioral processes</article-title>. <source>Adapt. Behav.</source>
<volume>13</volume>, <fpage>33</fpage>&#x02013;<lpage>52</lpage><pub-id pub-id-type="doi">10.1177/105971230501300102</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sugita</surname><given-names>Y.</given-names></name><name><surname>Tani</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>&#x0201c;A sub-symbolic process underlying the usage-based acquisition of a compositional representation: results of robotic learning experiments of goal-directed actions,&#x0201d;</article-title> in <conf-name>7th IEEE International Conference on Development and Learning</conf-name>, <conf-loc>Monterrey</conf-loc>, <fpage>127</fpage>&#x02013;<lpage>132</lpage></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>R.</given-names></name><name><surname>Barto</surname><given-names>A.</given-names></name></person-group> (<year>1998</year>). <source>Reinforcement Learning: An Introduction</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taddeo</surname><given-names>M.</given-names></name><name><surname>Floridi</surname><given-names>L.</given-names></name></person-group> (<year>2005</year>). <article-title>The symbol grounding problem: a critical review of fifteen years of research</article-title>. <source>J. Exp. Theor. Artif. Intell.</source>
<volume>17</volume>, <fpage>419</fpage>&#x02013;<lpage>445</lpage><pub-id pub-id-type="doi">10.1080/09528130500284053</pub-id></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tani</surname><given-names>J.</given-names></name></person-group> (<year>2003</year>). <article-title>Learning to generate articulated behavior through the bottom-up and the top-down interaction processes</article-title>. <source>Neural Netw.</source>
<volume>16</volume>, <fpage>11</fpage>&#x02013;<lpage>23</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(02)00214-9</pub-id><pub-id pub-id-type="pmid">12576102</pub-id></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tani</surname><given-names>J.</given-names></name><name><surname>Ito</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>Self-organization of behavioral primitives as multiple attractor dynamics: a robot experiment</article-title>. <source>IEEE Trans. Syst. Man Cybern. A Syst. Hum.</source>
<volume>33</volume>, <fpage>481</fpage>&#x02013;<lpage>488</lpage><pub-id pub-id-type="doi">10.1109/TSMCA.2003.809171</pub-id></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tani</surname><given-names>J.</given-names></name><name><surname>Ito</surname><given-names>M.</given-names></name><name><surname>Sugita</surname><given-names>Y.</given-names></name></person-group> (<year>2004</year>). <article-title>Self-organization of distributedly represented multiple behavior schemata in a mirror system: reviews of robot experiments using RNNPB</article-title>. <source>Neural Netw.</source>
<volume>17</volume>, <fpage>1273</fpage>&#x02013;<lpage>1289</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2004.05.007</pub-id><pub-id pub-id-type="pmid">15555866</pub-id></mixed-citation></ref><ref id="B48"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tikhanoff</surname><given-names>V.</given-names></name><name><surname>Cangelosi</surname><given-names>A.</given-names></name><name><surname>Fitzpatrick</surname><given-names>P.</given-names></name><name><surname>Metta</surname><given-names>G.</given-names></name><name><surname>Natale</surname><given-names>L.</given-names></name><name><surname>Nori</surname><given-names>F.</given-names></name></person-group> (<year>2008</year>). <article-title>&#x0201c;An open-source simulator for cognitive robotics research: the prototype of the icub humanoid robot simulator,&#x0201d;</article-title> in <conf-name>Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems</conf-name>, <conf-loc>Gaithersburg</conf-loc>, <fpage>57</fpage>&#x02013;<lpage>61</lpage></mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tikhanoff</surname><given-names>V.</given-names></name><name><surname>Cangelosi</surname><given-names>A.</given-names></name><name><surname>Metta</surname><given-names>G.</given-names></name></person-group> (<year>2011</year>). <article-title>Integration of speech and action in humanoid robots: iCub simulation experiments</article-title>. <source>IEEE Trans. Auton. Ment. Dev.</source>
<volume>3</volume>, <fpage>17</fpage>&#x02013;<lpage>29</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2010.2100390</pub-id></mixed-citation></ref><ref id="B50"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tomasello</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <source>Constructing a Language: A Usage-Based Theory of Language Acquisition</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Harvard University Press</publisher-name></mixed-citation></ref><ref id="B51"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van der Wel</surname><given-names>R.</given-names></name><name><surname>Sebanz</surname><given-names>N.</given-names></name><name><surname>Knoblich</surname><given-names>G.</given-names></name></person-group> (in press). &#x0201c;Action perception from a common coding perspective,&#x0201d; in <source>Visual Perception of the Human Body in Motion</source>, eds <person-group person-group-type="editor"><name><surname>Johnson</surname><given-names>K.</given-names></name><name><surname>Shiffrar</surname><given-names>M.</given-names></name></person-group> (New York: Oxford University Press).</mixed-citation></ref><ref id="B52"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Hasselt</surname><given-names>H.</given-names></name></person-group> (<year>2012</year>). <article-title>&#x0201c;Reinforcement learning in continuous state and action spaces,&#x0201d;</article-title> in <source>Reinforcement Learning: State of the Art</source>, eds <person-group person-group-type="editor"><name><surname>Wiering</surname><given-names>M.</given-names></name><name><surname>van Otterlo</surname><given-names>M.</given-names></name></person-group> (<publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>207</fpage>&#x02013;<lpage>251</lpage></mixed-citation></ref><ref id="B53"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Van Hasselt</surname><given-names>H.</given-names></name><name><surname>Wiering</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>&#x0201c;Reinforcement learning in continuous action spaces,&#x0201d;</article-title> in <conf-name>IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning (ADPRL)</conf-name>, <conf-loc>Honolulu</conf-loc>, <fpage>272</fpage>&#x02013;<lpage>279</lpage></mixed-citation></ref><ref id="B54"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Van Hasselt</surname><given-names>H.</given-names></name><name><surname>Wiering</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>&#x0201c;Using continuous action spaces to solve discrete problems,&#x0201d;</article-title> in <conf-name>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</conf-name>, <conf-loc>Atlanta</conf-loc>, <fpage>1149</fpage>&#x02013;<lpage>1156</lpage></mixed-citation></ref><ref id="B55"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vavre&#x0010d;ka</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>&#x0201c;Symbol grounding in context of zero semantic commitment (in Czech),&#x0201d;</article-title> in <source>Kognice a um&#x00115; &#x000fd;&#x0017e;ivot VI</source>, eds <person-group person-group-type="editor"><name><surname>Kelemen</surname><given-names>J.</given-names></name><name><surname>Kvasnicka</surname><given-names>V.</given-names></name></person-group> (<publisher-loc>Opava</publisher-loc>: <publisher-name>Slezsk&#x000e1; Univerzita</publisher-name>), <fpage>401</fpage>&#x02013;<lpage>411</lpage></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Waxman</surname><given-names>S.</given-names></name></person-group> (<year>2003</year>). <article-title>&#x0201c;Links between object categorization and naming: origins and emergence in human infants,&#x0201d;</article-title> in <source>Early Category and Concept Development: Making Sense of the Blooming, Buzzing Confusion</source>, eds <person-group person-group-type="editor"><name><surname>Rakison</surname><given-names>D.</given-names></name><name><surname>Oakes</surname><given-names>L.</given-names></name></person-group> (<publisher-loc>London</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>), <fpage>213</fpage>&#x02013;<lpage>241</lpage></mixed-citation></ref><ref id="B57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wermter</surname><given-names>S.</given-names></name><name><surname>Elshaw</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>Learning robot actions based on self-organising language memory</article-title>. <source>Neural Netw.</source>
<volume>16</volume>, <fpage>691</fpage>&#x02013;<lpage>699</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(03)00100-X</pub-id><pub-id pub-id-type="pmid">12850024</pub-id></mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Six views of embodied cognition</article-title>. <source>Psychon. Bull. Rev.</source>
<volume>9</volume>, <fpage>625</fpage>&#x02013;<lpage>636</lpage><pub-id pub-id-type="doi">10.3758/BF03196314</pub-id><pub-id pub-id-type="pmid">12613670</pub-id></mixed-citation></ref></ref-list></back></article>